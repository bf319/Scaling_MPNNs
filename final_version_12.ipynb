{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final-version-2-sharded-networks.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bf319/Scaling_MPNNs/blob/main/final_version_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rLhMwiHHWbtK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37bf046e-5a47-488a-d14b-748af209b0bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-07 12:43:45--  https://raw.githubusercontent.com/deepmind/jraph/master/jraph/experimental/sharded_graphnet.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22444 (22K) [text/plain]\n",
            "Saving to: ‘sharded_graphnet.py.6’\n",
            "\n",
            "\rsharded_graphnet.py   0%[                    ]       0  --.-KB/s               \rsharded_graphnet.py 100%[===================>]  21.92K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2022-04-07 12:43:45 (14.0 MB/s) - ‘sharded_graphnet.py.6’ saved [22444/22444]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%pip install -q git+https://github.com/deepmind/dm-haiku\n",
        "%pip install -q jraph\n",
        "%pip install -q git+https://github.com/deepmind/jaxline\n",
        "%pip install -q ogb\n",
        "%pip install -q dgl\n",
        "%pip install -q optax\n",
        "%pip install -q metis\n",
        "%pip install -q torch-scatter\n",
        "\n",
        "!wget https://raw.githubusercontent.com/deepmind/jraph/master/jraph/experimental/sharded_graphnet.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "\n",
        "num_devices = jax.local_device_count()\n",
        "jax.devices()"
      ],
      "metadata": {
        "id": "iT2wqf76kIRp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c16d3b86-7d3e-464f-9e2c-985952bbce15"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ogb.nodeproppred import Evaluator\n",
        "from ogb.nodeproppred import DglNodePropPredDataset\n",
        "\n",
        "dataset = DglNodePropPredDataset(name = \"ogbn-proteins\")\n",
        "split_idx = dataset.get_idx_split()\n",
        "evaluator = Evaluator(name = 'ogbn-proteins')\n",
        "print(evaluator.expected_input_format)"
      ],
      "metadata": {
        "id": "xHClucOxWpAZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a682b5c7-bca9-4219-9d79-e1da66934ed8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using backend: pytorch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== Expected input format of Evaluator for ogbn-proteins\n",
            "{'y_true': y_true, 'y_pred': y_pred}\n",
            "- y_true: numpy ndarray or torch tensor of shape (num_node, num_task)\n",
            "- y_pred: numpy ndarray or torch tensor of shape (num_node, num_task)\n",
            "where y_pred stores score values (for computing ROC-AUC),\n",
            "num_task is 112, and each row corresponds to one node.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "import torch\n",
        "from torch_scatter import scatter\n",
        "\n",
        "# There is only one graph in Node Property Prediction datasets\n",
        "ogbn_proteins_main_graph, ogbn_proteins_main_labels = dataset[0]\n",
        "ogbn_proteins_main_graph.ndata['species'] = scatter(\n",
        "    ogbn_proteins_main_graph.edata['feat'],\n",
        "    ogbn_proteins_main_graph.edges()[0],\n",
        "    dim = 0,\n",
        "    dim_size = ogbn_proteins_main_graph.num_nodes(),\n",
        "    reduce = 'mean'\n",
        ")\n",
        "'''\n",
        "  OGBN-Proteins\n",
        "    #Nodes = 132,534\n",
        "    #Edges = 39,561,252\n",
        "    #Diameter ~ 9 (https://cs.stanford.edu/people/jure/pubs/ogb-neurips20.pdf)\n",
        "    #Tasks = 112\n",
        "    #Split Type = Species\n",
        "    #Task Type = Binary classification\n",
        "    #Metric = ROC-AUC\n",
        "\n",
        "    Task:\n",
        "      The task is to predict the presence of protein functions in a multi-label binary classification setup,\n",
        "      where there are 112 kinds of labels to predict in total. \n",
        "      The performance is measured by the average of ROC-AUC scores across the 112 tasks.\n",
        "\n",
        "    #Others:\n",
        "      **undirected**\n",
        "      **weighted**\n",
        "      **typed (according to species)**\n",
        "\n",
        "  (1) Nodes represent proteins\n",
        "    (1.1) The proteins come from 8 species\n",
        "      len(set(graph.ndata['species'].reshape(-1).tolist())) == 8\n",
        "    (1.2) Each node has one feature associated with it (its species)\n",
        "      graph.ndata['species'].shape == (#nodes, 1)\n",
        "  \n",
        "  (2) Edges indicate different types of biologically meaningful associations between proteins\n",
        "    (2.1) All edges come with 8-dimensional features\n",
        "      graph.edata['feat'].shape == (2 * #edges, 8)\n",
        "\n",
        "'''\n",
        "# Get split labels\n",
        "train_label = dataset.labels[split_idx['train']]  # (86619, 112) -- binary values (presence of protein functions)\n",
        "valid_label = dataset.labels[split_idx['valid']]  # (21236, 112) -- binary values (presence of protein functions)\n",
        "test_label = dataset.labels[split_idx['test']]    # (24679, 112) -- binary values (presence of protein functions)\n",
        "\n",
        "# Create masks\n",
        "train_mask = jnp.zeros((ogbn_proteins_main_graph.num_nodes(), 1)).at[jnp.array(split_idx['train'])].set(1)\n",
        "valid_mask = jnp.zeros((ogbn_proteins_main_graph.num_nodes(), 1)).at[jnp.array(split_idx['valid'])].set(1)\n",
        "test_mask = jnp.zeros((ogbn_proteins_main_graph.num_nodes(), 1)).at[jnp.array(split_idx['test'])].set(1)"
      ],
      "metadata": {
        "id": "jCkzIEb4WsXU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jraph\n",
        "\n",
        "# From https://colab.research.google.com/github/deepmind/educational/blob/master/colabs/summer_schools/intro_to_graph_nets_tutorial_with_jraph.ipynb#scrollTo=7vEmAsr5bKN8\n",
        "def _nearest_multiple_of_8(x: int) -> int:\n",
        "  \"\"\"Computes the nearest power of two greater than x for padding.\"\"\"\n",
        "  if x % 8 == 0:\n",
        "    return x\n",
        "  else:\n",
        "    return (x // 8 + 1) * 8 \n",
        "\n",
        "def pad_graph_to_nearest_multiple_of_8(\n",
        "    graphs_tuple: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
        "  \"\"\"Pads a batched `GraphsTuple` to the nearest power of two.\n",
        "  For example, if a `GraphsTuple` has 7 nodes, 5 edges and 3 graphs, this method\n",
        "  would pad the `GraphsTuple` nodes and edges:\n",
        "    7 nodes --> 8 nodes (2^3)\n",
        "    5 edges --> 8 edges (2^3)\n",
        "  And since padding is accomplished using `jraph.pad_with_graphs`, an extra\n",
        "  graph and node is added:\n",
        "    8 nodes --> 9 nodes\n",
        "    3 graphs --> 4 graphs\n",
        "  Args:\n",
        "    graphs_tuple: a batched `GraphsTuple` (can be batch size 1).\n",
        "  Returns:\n",
        "    A graphs_tuple batched to the nearest power of two.\n",
        "  \"\"\"\n",
        "  # Add 1 since we need at least one padding node for pad_with_graphs.\n",
        "  pad_nodes_to = _nearest_multiple_of_8(jnp.sum(graphs_tuple.n_node)) + 1\n",
        "  pad_edges_to = _nearest_multiple_of_8(jnp.sum(graphs_tuple.n_edge))\n",
        "  # Add 1 since we need at least one padding graph for pad_with_graphs.\n",
        "  # We do not pad to nearest power of two because the batch size is fixed.\n",
        "  pad_graphs_to = graphs_tuple.n_node.shape[0] + 1\n",
        "  return jraph.pad_with_graphs(graphs_tuple, pad_nodes_to, pad_edges_to,\n",
        "                               pad_graphs_to)"
      ],
      "metadata": {
        "id": "FSbePOUh2NBB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import jraph\n",
        "import sharded_graphnet\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "enc = OneHotEncoder()\n",
        "enc.fit(ogbn_proteins_main_graph.ndata['species'])\n",
        "\n",
        "def dgl_graph_to_jraph(node_ids, labels, train_mask, valid_mask, test_mask):\n",
        "  # First add back the node and edge features\n",
        "  dgl_graph_with_features = dgl.node_subgraph(ogbn_proteins_main_graph, node_ids)\n",
        "  \n",
        "  node_features = jnp.array(dgl_graph_with_features.ndata['species'])\n",
        "\n",
        "  senders = jnp.array(dgl_graph_with_features.edges()[0])\n",
        "  receivers = jnp.array(dgl_graph_with_features.edges()[1])\n",
        "\n",
        "  # Edges -- here we should include the 8-dimensional edge features\n",
        "  edges = jnp.array(dgl_graph_with_features.edata['feat'])\n",
        "\n",
        "  n_node = jnp.array([dgl_graph_with_features.num_nodes()])\n",
        "  n_edge = jnp.array([dgl_graph_with_features.num_edges()])\n",
        "\n",
        "  in_tuple = jraph.GraphsTuple(\n",
        "            nodes = node_features.astype(np.float32),\n",
        "            senders = senders.astype(np.int32), \n",
        "            receivers = receivers.astype(np.int32),\n",
        "            edges = edges.astype(np.float32),  \n",
        "            n_node = n_node, \n",
        "            n_edge = n_edge,\n",
        "            globals = None  # No global features\n",
        "          )\n",
        "  \n",
        "  in_tuple = in_tuple._replace(\n",
        "      nodes = {\n",
        "          'inputs': in_tuple.nodes, \n",
        "          'targets': labels, \n",
        "          'train_mask': train_mask, \n",
        "          'valid_mask': valid_mask, \n",
        "          'test_mask': test_mask,\n",
        "          'padding_mask': jnp.ones((in_tuple.nodes.shape[0], 1)) \n",
        "                                                        # TODO: Check this above\n",
        "                                                        # Adding this mask so that we can remove the nodes added after padding \n",
        "                                                        # for the final ROC computations on the full train / valid / test splits\n",
        "                                                        # This is because I want to pass the predictions on the true nodes to the \n",
        "                                                        # ogbn-evaluator, so I would first need to remove the predictions that come from padding.\n",
        "          }\n",
        "  )\n",
        "\n",
        "  in_tuple = pad_graph_to_nearest_multiple_of_8(in_tuple)\n",
        "  \n",
        "  return sharded_graphnet.graphs_tuple_to_broadcasted_sharded_graphs_tuple(\n",
        "      in_tuple,\n",
        "      num_shards = num_devices\n",
        "      )\n",
        "  \n",
        "def get_labels_for_subgraph(node_ids):\n",
        "  return jnp.array(ogbn_proteins_main_labels.index_select(0, node_ids))"
      ],
      "metadata": {
        "id": "fvH_XRJVWuLw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_graph_to_jraph(dgl_graph_metis_partition, num_partitions):\n",
        "  # Convert graphs to Jraph GraphsTuple\n",
        "  processed_graphs = {}\n",
        "\n",
        "  for idx in range(num_partitions):\n",
        "    node_ids = dgl_graph_metis_partition[idx].ndata['_ID']\n",
        "\n",
        "    labels = get_labels_for_subgraph(node_ids)\n",
        "    graph = dgl_graph_to_jraph(node_ids, \n",
        "                              labels, \n",
        "                              train_mask = train_mask.at[jnp.array(node_ids)].get(),\n",
        "                              valid_mask = valid_mask.at[jnp.array(node_ids)].get(),\n",
        "                              test_mask = test_mask.at[jnp.array(node_ids)].get()\n",
        "                              )\n",
        "\n",
        "    processed_graphs[f'partition_{idx}'] = {\n",
        "        'graph': graph._replace(nodes = graph.nodes['inputs']), \n",
        "        'labels': graph.nodes['targets'],\n",
        "        'train_mask': graph.nodes['train_mask'],\n",
        "        'valid_mask': graph.nodes['valid_mask'],\n",
        "        'test_mask': graph.nodes['test_mask'],\n",
        "        'padding_mask': graph.nodes['padding_mask']\n",
        "        }\n",
        "\n",
        "  return processed_graphs\n",
        "\n",
        "def bcast_local_devices(value):\n",
        "    \"\"\"Broadcasts an object to all local devices.\"\"\"\n",
        "    devices = jax.local_devices()\n",
        "\n",
        "    def _replicate(x):\n",
        "      \"\"\"Replicate an object on each device.\"\"\"\n",
        "      x = jnp.array(x)\n",
        "      return jax.device_put_sharded(len(devices) * [x], devices)\n",
        "\n",
        "    return jax.tree_util.tree_map(_replicate, value)\n",
        "\n",
        "def reshape_broadcasted_data(data):\n",
        "  '''\n",
        "    Node predictions / Labels / Masks are identical on all the devices so we only take\n",
        "    one of them in order to remove the leading axis.\n",
        "  '''\n",
        "  return np.array(data)[0]\n",
        "  \n",
        "def remove_mask_from_data(data, mask):\n",
        "  '''\n",
        "    data.shape = [num_nodes, 112]\n",
        "    mask.shape = [num_nodes, 1]\n",
        "\n",
        "    We want to only return the data where mask == True\n",
        "  '''\n",
        "  sliced_data = np.compress(np.array(mask).reshape(-1).astype(bool), data, axis = 0)\n",
        "  return np.array(sliced_data)"
      ],
      "metadata": {
        "id": "F6LhP3z4ypeI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_loss(loss_list):\n",
        "  plt.plot(range(1, len(loss_list) + 1), loss_list)\n",
        "  plt.xlabel('Iteration')\n",
        "  plt.ylabel('Training loss')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def plot_rocs(roc_train, roc_eval, roc_test, iters):\n",
        "  plt.plot(iters, roc_train, label = 'Train ROC')\n",
        "  plt.plot(iters, roc_eval, label = 'Valid ROC')\n",
        "  plt.plot(iters, roc_test, label = 'Test ROC')\n",
        "  \n",
        "  plt.xlabel('Iteration')\n",
        "  plt.ylabel('ROC')\n",
        "\n",
        "  plt.legend(loc = 'upper right')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "RpSMwPNy3J6Z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def append_row_to_csv(file_path, values):\n",
        "  with open(file_path, 'a') as csvfile:\n",
        "    csv_writer = csv.writer(csvfile, delimiter = ',')\n",
        "    csv_writer.writerow(values)\n",
        "\n",
        "    csvfile.flush()"
      ],
      "metadata": {
        "id": "cBEffNcSIXj9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import dgl\n",
        "import haiku as hk\n",
        "import jax\n",
        "import optax\n",
        "import functools\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "from typing import Sequence\n",
        "from random import randint\n",
        "from datetime import datetime\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "def run_for_configuration(config, results_path):\n",
        "  num_partitions = config['num_partitions']\n",
        "  hidden_dimension = config['hidden_dimension']\n",
        "  num_message_passing_steps = config['num_message_passing_steps']\n",
        "  num_training_steps = config['num_training_steps']\n",
        "  evaluate_every = config['evaluate_every']\n",
        "\n",
        "  # Create partitions\n",
        "  dgl_graph_metis_partition = dgl.metis_partition(ogbn_proteins_main_graph, num_partitions, balance_edges = True)\n",
        "\n",
        "  # Convert graphs to Jraph GrapshTuple\n",
        "  processed_graphs = preprocess_graph_to_jraph(dgl_graph_metis_partition, num_partitions)\n",
        "\n",
        "  # Define network functions\n",
        "  @jraph.concatenated_args\n",
        "  def node_update_fn(feats: jnp.ndarray) -> jnp.ndarray:\n",
        "    \"\"\"Node update function for graph net.\"\"\"\n",
        "    net = hk.Sequential([hk.nets.MLP(output_sizes = [hidden_dimension, hidden_dimension], activation = jax.nn.relu, activate_final = False), hk.LayerNorm(axis = -1, create_scale = True, create_offset = True)])\n",
        "    return net(feats)\n",
        "\n",
        "  @jraph.concatenated_args\n",
        "  def edge_update_fn(feats: jnp.ndarray) -> jnp.ndarray:\n",
        "    \"\"\"Edge update function for graph net.\"\"\"\n",
        "    net = hk.Sequential([hk.nets.MLP(output_sizes = [hidden_dimension, hidden_dimension], activation = jax.nn.relu, activate_final = False), hk.LayerNorm(axis = -1, create_scale = True, create_offset = True)])\n",
        "    return net(feats)\n",
        "\n",
        "  @hk.without_apply_rng\n",
        "  @hk.transform\n",
        "  def network_definition(graph):\n",
        "    \"\"\"Defines a graph neural network.\n",
        "    Args:\n",
        "      graph: Graphstuple the network processes.\n",
        "    Returns:\n",
        "      Decoded nodes.\n",
        "    \"\"\"\n",
        "    graph = graph._replace(\n",
        "        nodes = hk.Linear(hidden_dimension)(graph.nodes),\n",
        "        device_edges = hk.Linear(hidden_dimension)(graph.device_edges)\n",
        "    )\n",
        "    \n",
        "    sharded_gn = sharded_graphnet.ShardedEdgesGraphNetwork(\n",
        "        update_node_fn = node_update_fn,\n",
        "        update_edge_fn = edge_update_fn,\n",
        "        num_shards = num_devices\n",
        "        )\n",
        "\n",
        "    for _ in range(num_message_passing_steps):\n",
        "      residual_graph = sharded_gn(graph)\n",
        "      graph = graph._replace(\n",
        "          nodes = graph.nodes + residual_graph.nodes,\n",
        "          device_edges = graph.device_edges + residual_graph.device_edges\n",
        "      )\n",
        "\n",
        "    graph = graph._replace(\n",
        "        nodes = hk.Sequential([hk.Linear(hidden_dimension), jax.nn.relu, hk.Linear(112)])(graph.nodes)\n",
        "    )\n",
        "    return graph.nodes\n",
        "\n",
        "  def compute_loss(params, graph, label, mask):\n",
        "    predictions = network_definition.apply(params, graph)\n",
        "\n",
        "    # use optax here (https://github.com/deepmind/optax/blob/master/optax/_src/loss.py#L116#L139)\n",
        "    loss = optax.sigmoid_binary_cross_entropy(predictions, label)  # shape [num_nodes, num_classes]\n",
        "    loss = loss * mask\n",
        "    loss = jnp.sum(loss) / jnp.sum(mask) # loss = mean_with_mask(loss, mask)\n",
        "\n",
        "    return loss\n",
        "\n",
        "  @functools.partial(jax.pmap, axis_name='i')\n",
        "  def predict_on_graph(params, graph, label, mask):\n",
        "    decoded_nodes = network_definition.apply(params, graph)\n",
        "\n",
        "    compute_loss_fn = functools.partial(compute_loss)\n",
        "    loss = compute_loss_fn(params, graph, label, mask)\n",
        "\n",
        "    return jax.nn.sigmoid(decoded_nodes), loss\n",
        "\n",
        "  #########################\n",
        "  # Evaluations on full set\n",
        "  #########################\n",
        "  def evaluate_on_full_sets(params, dgl_graph_metis_partition, processed_graphs, num_partitions):\n",
        "    final_predictions = {}\n",
        "\n",
        "    for i in range(num_partitions):\n",
        "      node_ids = dgl_graph_metis_partition[i].ndata['_ID']\n",
        "      partition = processed_graphs[f'partition_{i}']\n",
        "      \n",
        "      predictions, _ = predict_on_graph(params, \n",
        "                                        partition['graph'], \n",
        "                                        partition['labels'], \n",
        "                                        partition['test_mask']  # Only used in the loss computation, does not affect predictions\n",
        "                                        )\n",
        "\n",
        "      predictions_after_masked_nodes_are_removed = remove_mask_from_data(\n",
        "          reshape_broadcasted_data(predictions),\n",
        "          reshape_broadcasted_data(partition['padding_mask'])\n",
        "          )\n",
        "\n",
        "      for index, node_id in enumerate(node_ids):\n",
        "        final_predictions[node_id] = predictions_after_masked_nodes_are_removed[index]\n",
        "\n",
        "      if (i + 1) % 10 == 0:\n",
        "        print(f'Evaluated {i + 1} / {num_partitions} subgraphs...')\n",
        "\n",
        "    # Sort the final predictions based on the node ids\n",
        "    predictions_in_order = dict(sorted(final_predictions.items()))\n",
        "\n",
        "    # Convert the values to a list to be able to slice based on the ids of the \n",
        "    # nodes in the test set\n",
        "    predictions_in_order = list(predictions_in_order.values())\n",
        "\n",
        "    final_roc_train = evaluator.eval({\n",
        "        \"y_true\": np.array(train_label), \n",
        "        \"y_pred\": np.array([predictions_in_order[x] for x in split_idx['train']])\n",
        "        })['rocauc']\n",
        "\n",
        "    final_roc_valid = evaluator.eval({\n",
        "        \"y_true\": np.array(valid_label), \n",
        "        \"y_pred\": np.array([predictions_in_order[x] for x in split_idx['valid']])\n",
        "        })['rocauc']\n",
        "\n",
        "    final_roc_test = evaluator.eval({\n",
        "        \"y_true\": np.array(test_label),\n",
        "        \"y_pred\": np.array([predictions_in_order[x] for x in split_idx['test']])\n",
        "        })['rocauc']\n",
        "\n",
        "    print()\n",
        "    print(f'Final ROC on the train set {final_roc_train}')\n",
        "    print(f'Final ROC on the validation set {final_roc_valid}')\n",
        "    print(f'Final ROC on the test set {final_roc_test}')\n",
        "\n",
        "    return (final_roc_train, final_roc_valid, final_roc_test)\n",
        "\n",
        "  ####################\n",
        "  # Training procedure\n",
        "  ####################\n",
        "  def train(num_training_steps, evaluate_every):\n",
        "    losses = []\n",
        "\n",
        "    replicated_params = jax.pmap(network_definition.init, axis_name = 'i')(\n",
        "        bcast_local_devices(jax.random.PRNGKey(42)), \n",
        "        processed_graphs['partition_0']['graph']\n",
        "        )\n",
        "\n",
        "    opt_init, opt_update = optax.adam(learning_rate = 0.001)  \n",
        "    replicated_opt_state = jax.pmap(opt_init, axis_name = 'i')(replicated_params)\n",
        "\n",
        "    @functools.partial(jax.pmap, axis_name='i')\n",
        "    def update(params, opt_state, graph, targets, mask):\n",
        "      # Compute the gradients on the given minibatch (individually on each device).\n",
        "      loss, grads = jax.value_and_grad(compute_loss)(params, graph, targets, mask)\n",
        "\n",
        "      # Combine the gradient across all devices.\n",
        "      grads = jax.lax.pmean(grads, axis_name='i')\n",
        "\n",
        "      # Also combine the loss. Unnecessary for the update, but useful for logging.\n",
        "      loss = jax.lax.pmean(loss, axis_name='i')\n",
        "\n",
        "      updates, opt_state = opt_update(updates = grads, state = opt_state)\n",
        "\n",
        "      return optax.apply_updates(params, updates), opt_state, loss\n",
        "\n",
        "    ############################################################\n",
        "    ### Count the number of parameters in this configuration ###\n",
        "    ############################################################\n",
        "    params_counts = hk.experimental.tabulate(update, columns = ['params_size'], tabulate_kwargs={\"tablefmt\": \"tsv\"})(\n",
        "      replicated_params, \n",
        "      replicated_opt_state, \n",
        "      processed_graphs['partition_0']['graph'], \n",
        "      processed_graphs['partition_0']['labels'],\n",
        "      processed_graphs['partition_0']['train_mask'] \n",
        "    ).replace(' ', '').replace(',', '')\n",
        "\n",
        "    df = pd.read_csv(io.StringIO(params_counts), sep = '\\t', index_col = False)\n",
        "    vals = list(df.iloc[:, 0])\n",
        "    vals = list(map(int, vals))\n",
        "\n",
        "    total_params = 0\n",
        "    for count in vals:\n",
        "      total_params += count\n",
        "\n",
        "    total_params = int(total_params / 2)\n",
        "\n",
        "    ############################################################\n",
        "\n",
        "    # Train\n",
        "    for idx in range(num_training_steps):\n",
        "      random_partition_idx = randint(0, num_partitions - 1)\n",
        "      random_partition = processed_graphs[f'partition_{random_partition_idx}']\n",
        "\n",
        "      graph = random_partition['graph']\n",
        "      labels = random_partition['labels']   # Automatically broadcasted by the sharded graph net\n",
        "      mask = random_partition['train_mask'] # Automatically broadcasted by the sharded graph net\n",
        "\n",
        "      replicated_params, replicated_opt_state, loss = update(\n",
        "          replicated_params, \n",
        "          replicated_opt_state, \n",
        "          graph, \n",
        "          labels,\n",
        "          mask\n",
        "          )\n",
        "      \n",
        "      print('Loss training:', reshape_broadcasted_data(loss))\n",
        "\n",
        "      losses.append(reshape_broadcasted_data(loss))\n",
        "\n",
        "      if (idx + 1) % 10 == 0:\n",
        "        print()\n",
        "        print(f'***************************')\n",
        "        print(f'Trained on {idx + 1} graphs')\n",
        "        print(f'***************************')\n",
        "        print()\n",
        "\n",
        "      if (idx + 1) % evaluate_every == 0:\n",
        "        print()\n",
        "        print(f'*** Full evaluations after {idx + 1} training steps ***')\n",
        "\n",
        "        epoch = idx + 1\n",
        "        roc_train, roc_eval, roc_test = evaluate_on_full_sets(replicated_params, dgl_graph_metis_partition, processed_graphs, num_partitions)\n",
        "\n",
        "        avg_loss = sum(losses) / len(losses)\n",
        "        losses = []\n",
        "\n",
        "        append_row_to_csv(results_path, [\n",
        "          str(total_params),\n",
        "          str(num_partitions),\n",
        "          str(hidden_dimension),\n",
        "          str(num_message_passing_steps),\n",
        "          str(epoch),\n",
        "          str(roc_train),\n",
        "          str(roc_eval),\n",
        "          str(roc_test),\n",
        "          str(avg_loss),\n",
        "          'N/A', # Running time\n",
        "          'N/A', # Memory consumption\n",
        "        ])\n",
        "        \n",
        "\n",
        "    # plot_loss(losses)\n",
        "    # plot_rocs(roc_train_list, roc_eval_list, roc_test_list, roc_step)    \n",
        "\n",
        "    # RESULTS CSV\n",
        "    # Every 100 iterations\n",
        "    # (#parameters, #partitions, #hidden_dim, #message_passing_steps, roc_train_100, roc_eval_100, roc_test_100, loss_100, running_time!!, mem_usage!!)\n",
        "\n",
        "  ### Now actually run the training loop and get the evaluations for this run\n",
        "  train(\n",
        "      num_training_steps = num_training_steps,\n",
        "      evaluate_every = evaluate_every\n",
        "  )"
      ],
      "metadata": {
        "id": "C4k2dXNCyEok"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_time = datetime.today().strftime('%Y-%m-%d-%H:%M:%S')\n",
        "results_path = f'/content/results_{current_time}.csv'\n",
        "\n",
        "append_row_to_csv(results_path, [\n",
        "  'Parameters',\n",
        "  'Partitions',\n",
        "  'Hidden dimension',\n",
        "  'Message passing steps',\n",
        "  'Epoch',\n",
        "  'ROC train',\n",
        "  'ROC eval',\n",
        "  'ROC test',\n",
        "  'Loss',\n",
        "  'Running time',\n",
        "  'Memory usage'\n",
        "])\n",
        "\n",
        "run_for_configuration(config = {\n",
        "    'num_partitions': 100,\n",
        "    'hidden_dimension': 256,\n",
        "    'num_message_passing_steps': 3,\n",
        "    'num_training_steps': 5000,\n",
        "    'evaluate_every': 100\n",
        "    }, results_path = results_path)"
      ],
      "metadata": {
        "id": "L1qZ8_wkOCOA",
        "outputId": "a46a9ae0-c15e-4a53-c29c-a5669d0d06d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convert a graph into a bidirected graph: 3.535 seconds\n",
            "Construct multi-constraint weights: 0.003 seconds\n",
            "Metis partitioning: 41.338 seconds\n",
            "Split the graph: 0.820 seconds\n",
            "Construct subgraphs: 0.044 seconds\n",
            "Loss training: 89.44355\n",
            "Loss training: 70.50785\n"
          ]
        }
      ]
    }
  ]
}