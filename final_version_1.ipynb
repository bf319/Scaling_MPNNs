{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final-version-1.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bf319/Scaling_MPNNs/blob/main/final_version_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rLhMwiHHWbtK",
        "outputId": "45f933cc-0ad3-4c29-f5eb-aac1d9176c06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for dm-haiku (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 75 kB 2.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 70 kB 3.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.4 MB/s \n",
            "\u001b[?25h  Building wheel for jaxline (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 78 kB 3.4 MB/s \n",
            "\u001b[?25h  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 136 kB 4.8 MB/s \n",
            "\u001b[?25h  Building wheel for metis (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install -q git+https://github.com/deepmind/dm-haiku\n",
        "%pip install -q jraph\n",
        "%pip install -q git+https://github.com/deepmind/jaxline\n",
        "%pip install -q ogb\n",
        "%pip install -q dgl\n",
        "%pip install -q optax\n",
        "%pip install -q metis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "\n",
        "num_devices = jax.local_device_count()\n",
        "jax.devices()"
      ],
      "metadata": {
        "id": "iT2wqf76kIRp",
        "outputId": "f7742fa3-c4bd-4efd-db8f-d7373b8514aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ogb.nodeproppred import Evaluator\n",
        "from ogb.nodeproppred import DglNodePropPredDataset\n",
        "\n",
        "dataset = DglNodePropPredDataset(name = \"ogbn-proteins\")\n",
        "split_idx = dataset.get_idx_split()\n",
        "evaluator = Evaluator(name = 'ogbn-proteins')"
      ],
      "metadata": {
        "id": "xHClucOxWpAZ",
        "outputId": "2d99b090-d986-4883-b6fc-b19c7d5dd411",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Using backend: pytorch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/proteins.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 0.21 GB: 100%|██████████| 216/216 [00:06<00:00, 35.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/proteins.zip\n",
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.72s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into DGL objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  2.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "# There is only one graph in Node Property Prediction datasets\n",
        "ogbn_proteins_main_graph, ogbn_proteins_main_labels = dataset[0]\n",
        "\n",
        "'''\n",
        "  OGBN-Proteins\n",
        "    #Nodes = 132,534\n",
        "    #Edges = 39,561,252\n",
        "    #Tasks = 112\n",
        "    #Split Type = Species\n",
        "    #Task Type = Binary classification\n",
        "    #Metric = ROC-AUC\n",
        "\n",
        "    Task:\n",
        "      The task is to predict the presence of protein functions in a multi-label binary classification setup,\n",
        "      where there are 112 kinds of labels to predict in total. \n",
        "      The performance is measured by the average of ROC-AUC scores across the 112 tasks.\n",
        "\n",
        "    #Others:\n",
        "      **undirected**\n",
        "      **weighted**\n",
        "      **typed (according to species)**\n",
        "\n",
        "  (1) Nodes represent proteins\n",
        "    (1.1) The proteins come from 8 species\n",
        "      len(set(graph.ndata['species'].reshape(-1).tolist())) == 8\n",
        "    (1.2) Each node has one feature associated with it (its species)\n",
        "      graph.ndata['species'].shape == (#nodes, 1)\n",
        "  \n",
        "  (2) Edges indicate different types of biologically meaningful associations between proteins\n",
        "    (2.1) All edges come with 8-dimensional features\n",
        "      graph.edata['feat'].shape == (2 * #edges, 8)\n",
        "\n",
        "'''\n",
        "# Get split labels\n",
        "train_label = dataset.labels[split_idx['train']]  # (86619, 112) -- binary values (presence of protein functions)\n",
        "valid_label = dataset.labels[split_idx['valid']]  # (21236, 112) -- binary values (presence of protein functions)\n",
        "test_label = dataset.labels[split_idx['test']]    # (24679, 112) -- binary values (presence of protein functions)\n",
        "\n",
        "# Create masks\n",
        "train_mask = jnp.zeros((ogbn_proteins_main_graph.num_nodes(), 1)).at[jnp.array(split_idx['train'])].set(1)\n",
        "valid_mask = jnp.zeros((ogbn_proteins_main_graph.num_nodes(), 1)).at[jnp.array(split_idx['valid'])].set(1)\n",
        "test_mask = jnp.zeros((ogbn_proteins_main_graph.num_nodes(), 1)).at[jnp.array(split_idx['test'])].set(1)"
      ],
      "metadata": {
        "id": "jCkzIEb4WsXU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import jraph\n",
        "\n",
        "def dgl_graph_to_jraph(node_ids):\n",
        "  # First add back the node and edge features\n",
        "  dgl_graph_with_features = dgl.node_subgraph(ogbn_proteins_main_graph, node_ids)\n",
        "\n",
        "  node_features = jnp.array(dgl_graph_with_features.ndata['species'])\n",
        "  senders = jnp.array(dgl_graph_with_features.edges()[0])\n",
        "  receivers = jnp.array(dgl_graph_with_features.edges()[1])\n",
        "\n",
        "  # Edges -- here we should include the 8-dimensional edge features\n",
        "  edges = jnp.array(dgl_graph_with_features.edata['feat'])\n",
        "\n",
        "  n_node = jnp.array([dgl_graph_with_features.num_nodes()])\n",
        "  n_edge = jnp.array([dgl_graph_with_features.num_edges()])\n",
        "\n",
        "  return jraph.GraphsTuple(\n",
        "            nodes = node_features.astype(np.float32),\n",
        "            senders = senders.astype(np.int32), \n",
        "            receivers = receivers.astype(np.int32),\n",
        "            edges = edges.astype(np.float32),  \n",
        "            n_node = n_node, \n",
        "            n_edge = n_edge,\n",
        "            globals = None  # No global features\n",
        "          )\n",
        "  \n",
        "def get_labels_for_subgraph(node_ids):\n",
        "  return jnp.array(ogbn_proteins_main_labels.index_select(0, node_ids))"
      ],
      "metadata": {
        "id": "fvH_XRJVWuLw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "\n",
        "'''\n",
        "  Generate graph partition using metis, with balanced number of edges in each partition.\n",
        "  Note: \n",
        "    The subgraphs do not contain the node/edge data in the input graph (https://docs.dgl.ai/generated/dgl.metis_partition.html)\n",
        "'''\n",
        "num_partitions = 256\n",
        "\n",
        "# reshuffle == False, so my understandinng is that\n",
        "# partition.ndata['_ID'] uses the same ids for the initial nodes\n",
        "# therefore, the initial train / valid / test masks should still work.\n",
        "dgl_graph_metis_partition = dgl.metis_partition(ogbn_proteins_main_graph, num_partitions, balance_edges = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUI9s4-0mPz9",
        "outputId": "2f63a346-eaee-483e-d98e-02d41cdd1ac2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convert a graph into a bidirected graph: 2.694 seconds\n",
            "Construct multi-constraint weights: 0.015 seconds\n",
            "Metis partitioning: 35.802 seconds\n",
            "Split the graph: 0.494 seconds\n",
            "Construct subgraphs: 0.236 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert graphs to Jraph GraphsTuple\n",
        "processed_graphs = {}\n",
        "\n",
        "for idx in range(num_partitions):\n",
        "  node_ids = dgl_graph_metis_partition[idx].ndata['_ID']\n",
        "\n",
        "  graph = dgl_graph_to_jraph(node_ids)\n",
        "  labels = get_labels_for_subgraph(node_ids)\n",
        "\n",
        "  processed_graphs[f'partition_{idx}'] = {\n",
        "      'graph': graph, \n",
        "      'labels': labels,\n",
        "      'train_mask': train_mask.at[jnp.array(node_ids)].get(),\n",
        "      'valid_mask': valid_mask.at[jnp.array(node_ids)].get(),\n",
        "      'test_mask': test_mask.at[jnp.array(node_ids)].get()\n",
        "      }"
      ],
      "metadata": {
        "id": "s8-Ln58I_Fwp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import haiku as hk\n",
        "import jax\n",
        "import optax\n",
        "\n",
        "from typing import Sequence\n",
        "\n",
        "# See https://github.com/YuxuanXie/mcl/blob/5f7ee92e2a6bc89736263873a4ba9c14d1a676ff/glassy_dynamics/train_using_jax.py for alternative to using GraphMapFeatures\n",
        "# From https://github.com/YuxuanXie/mcl/blob/5f7ee92e2a6bc89736263873a4ba9c14d1a676ff/glassy_dynamics/train_using_jax.py\n",
        "\n",
        "hidden_dimension = 128\n",
        "num_message_passing_steps = 3\n",
        "\n",
        "@jraph.concatenated_args\n",
        "def node_update_fn(feats: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Node update function for graph net.\"\"\"\n",
        "  net = hk.Sequential([hk.nets.MLP(output_sizes = [hidden_dimension, hidden_dimension]), jax.nn.relu, hk.LayerNorm(axis = -1, create_scale = False, create_offset = False)])\n",
        "  return net(feats)\n",
        "\n",
        "@jraph.concatenated_args\n",
        "def edge_update_fn(feats: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Edge update function for graph net.\"\"\"\n",
        "  net = hk.Sequential([hk.nets.MLP(output_sizes = [hidden_dimension, hidden_dimension]), jax.nn.relu, hk.LayerNorm(axis = -1, create_scale = False, create_offset = False)])\n",
        "  return net(feats)\n",
        "\n",
        "def node_decoder_fn(feats: jnp.ndarray) -> jnp.ndarray:\n",
        "  ## TODO: Check if this is correct\n",
        "  net = hk.Sequential([hk.nets.MLP(output_sizes = [112], activate_final = False), jax.nn.sigmoid])\n",
        "  return net(feats)\n",
        "\n",
        "def network_definition(graph):\n",
        "  \"\"\"Defines a graph neural network.\n",
        "  Args:\n",
        "    graph: Graphstuple the network processes.\n",
        "  Returns:\n",
        "    Decoded nodes.\n",
        "  \"\"\"\n",
        "\n",
        "  embedder = jraph.GraphMapFeatures(\n",
        "      embed_node_fn=hk.Linear(hidden_dimension),\n",
        "      embed_edge_fn=hk.Linear(hidden_dimension),\n",
        "      )\n",
        "  graph = embedder(graph)\n",
        "\n",
        "  gn = jraph.InteractionNetwork(\n",
        "      update_node_fn=node_update_fn,\n",
        "      update_edge_fn=edge_update_fn,\n",
        "      include_sent_messages_in_node_update=True\n",
        "      )\n",
        "\n",
        "  for _ in range(num_message_passing_steps):\n",
        "    graph = gn(graph)\n",
        "\n",
        "  decoder = jraph.GraphMapFeatures(embed_node_fn = node_decoder_fn)\n",
        "  \n",
        "  processed_graph = decoder(graph)\n",
        "  return processed_graph.nodes"
      ],
      "metadata": {
        "id": "gPg7ph7sWyOn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import haiku as hk\n",
        "\n",
        "from random import randint\n",
        "\n",
        "# Try to follow this tutorial https://github.com/YuxuanXie/mcl/blob/5f7ee92e2a6bc89736263873a4ba9c14d1a676ff/glassy_dynamics/train_using_jax.py\n",
        "def compute_loss(params, graph, label, mask):\n",
        "  # Question: would the net need to be passed from the train function?\n",
        "  # I don't think so because this is just a function and we pass the parameters\n",
        "  # below, but I just wanted to confirm.\n",
        "  net = hk.without_apply_rng(hk.transform(network_definition))\n",
        "\n",
        "  predictions = net.apply(params, graph) # Shape == label.shape\n",
        "  \n",
        "  # Question: Is node masking implemented properly?\n",
        "  loss = optax.sigmoid_binary_cross_entropy(predictions, label)\n",
        "  loss = loss * mask\n",
        "  loss = loss.mean()\n",
        "  return loss\n",
        "\n",
        "def train(num_training_steps):\n",
        "  # Transform the function (MPNN) into a pure function (with no side effects) so that it can be used with jax\n",
        "  net = hk.without_apply_rng(hk.transform(network_definition))\n",
        "\n",
        "  params = net.init(jax.random.PRNGKey(42), processed_graphs['partition_0']['graph'])\n",
        "\n",
        "  opt_init, opt_update = optax.adam(learning_rate = 0.1)  # This learning rate could be a bit low, try 1e-1 for full graph descent?  \n",
        "  opt_state = opt_init(params)\n",
        "\n",
        "  # n_devices = 8\n",
        "  # replicated_params = jax.tree_map(lambda x: jnp.array([x] * n_devices), params)\n",
        "  # replicated_opt_state = jax.tree_map(lambda x: jnp.array([x] * n_devices), opt_state)\n",
        "\n",
        "  # @functools.partial(jax.pmap, axis_name='num_devices')\n",
        "  def update(params, opt_state, graph, targets, mask):\n",
        "    # Compute the gradients on the given minibatch (individually on each device).\n",
        "    loss, grads = jax.value_and_grad(compute_loss)(params, graph, targets, mask)\n",
        "\n",
        "    # # Combine the gradient across all devices (by taking their mean).\n",
        "    # grads = jax.lax.pmean(grads, axis_name='num_devices')\n",
        "\n",
        "    # # Also combine the loss. Unnecessary for the update, but useful for logging.\n",
        "    # loss = jax.lax.pmean(loss, axis_name='num_devices')\n",
        "\n",
        "    updates, opt_state = opt_update(grads, opt_state)\n",
        "    return optax.apply_updates(params, updates), opt_state, loss\n",
        "\n",
        "  ## TODO - Question: How to train only using the train_split?\n",
        "\n",
        "  # Train\n",
        "  for idx in range(num_training_steps):\n",
        "    random_partition_idx = randint(0, num_partitions - 1)\n",
        "    random_partition = processed_graphs[f'partition_{random_partition_idx}']\n",
        "\n",
        "    graph = random_partition['graph']\n",
        "    labels = random_partition['labels']\n",
        "    mask = random_partition['train_mask']\n",
        "\n",
        "    params, opt_state, loss = update(\n",
        "        params, \n",
        "        opt_state, \n",
        "        graph, \n",
        "        labels,\n",
        "        mask\n",
        "        )\n",
        "    print('Loss training:', loss)\n",
        "\n",
        "    # Write params and opt_state\n",
        "\n",
        "  return params\n",
        "\n",
        "def evaluate(params, num_graphs_eval):\n",
        "  # Evaluate\n",
        "  accumulated_loss = 0.0\n",
        "  accumulated_roc = 0\n",
        "\n",
        "  for idx in range(num_graphs_eval):\n",
        "    random_partition_idx = randint(0, num_partitions - 1)\n",
        "    random_partition = processed_graphs[f'partition_{random_partition_idx}']\n",
        "\n",
        "    graph = random_partition['graph']\n",
        "    labels = random_partition['labels']\n",
        "    mask = random_partition['test_mask']\n",
        "\n",
        "    (loss, roc) = evaluate_graph(params, graph, labels, mask)\n",
        "\n",
        "    accumulated_loss += loss\n",
        "    accumulated_roc += roc\n",
        "\n",
        "  print(f'Average loss: {accumulated_loss / num_graphs_eval} | Average ROC: {accumulated_roc / num_graphs_eval}')\n",
        "\n",
        "def evaluate_graph(params, graph, label, mask):\n",
        "  compute_loss_fn = jax.jit(functools.partial(compute_loss))\n",
        "\n",
        "  net = hk.without_apply_rng(hk.transform(network_definition))\n",
        "  decoded_nodes = net.apply(params, graph) # Shape == label.shape\n",
        "  \n",
        "  ## TODO -- Question: Should I round the predictions here? (predictions = jax.lax.round(decoded_nodes))\n",
        "  predictions = jax.lax.round(decoded_nodes)\n",
        "\n",
        "  loss = compute_loss_fn(params, graph, label, mask)\n",
        "  roc = evaluator.eval({\"y_true\": np.array(label), \"y_pred\": np.array(predictions)})['rocauc']\n",
        "\n",
        "  print(f'Test loss: {loss} | ROC: {roc}')\n",
        "  return (loss, roc)\n",
        "\n",
        "final_params = train(num_training_steps = 10)\n",
        "evaluate(final_params, 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liLn_YsrW1l-",
        "outputId": "a36215c5-9322-4d87-e56e-1aa523b47f7b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss training: 0.9700462\n",
            "Loss training: 0.30946547\n",
            "Loss training: 0.25340915\n",
            "Loss training: 0.33717442\n",
            "Loss training: 0.3528573\n",
            "Loss training: 0.20972906\n",
            "Loss training: 0.69314766\n",
            "Loss training: 0.69314766\n",
            "Loss training: 0.50661224\n",
            "Loss training: 0.51450115\n",
            "Eval loss: 0.2184399664402008 | ROC: 0.5\n",
            "Eval loss: 0.006370839662849903 | ROC: 0.5\n",
            "Eval loss: 0.23177120089530945 | ROC: 0.5\n",
            "Eval loss: 0.42447325587272644 | ROC: 0.5\n",
            "Eval loss: 0.12577266991138458 | ROC: 0.5\n",
            "Average loss: 0.20136559009552002 | Average ROC: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  ## TODO: Implement masking\n",
        "\n",
        "\n",
        "\n",
        "  # Train split, [0, 1, 10]\n",
        "  # Train mask, jnp.zeros((num_nodes, 1)).at[train_split].set(1)\n",
        "  # bce(pred, target) * train_mask\n",
        "\n",
        "  # # Preprocess data\n",
        "  # processed_data = []\n",
        "  # for idx in len(data):\n",
        "  #   graph_idx = dgl_graph_to_jraph(dgl_graph_metis_partition[idx].ndata['_ID'])\n",
        "  #   labels_idx = get_labels_for_subgraph(dgl_graph_metis_partition[idx].ndata['_ID'])\n",
        "  #   labels_idx = jnp.array(labels_idx)\n",
        "\n",
        "\n",
        "  # # num training steps = 1000\n",
        "  # for idx in range(num_training_steps):\n",
        "  #   graph = random.choice(processed_data)\n",
        "  #   labels = random.choice(labels)\n",
        "  #   params, opt_state, loss = update(params, opt_state, graph_idx, labels_idx)\n",
        "  #   print('Loss training:', loss)\n",
        "  #   # save parameters and opt state\n",
        "  # return params"
      ],
      "metadata": {
        "id": "g8SkGessG0yb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}