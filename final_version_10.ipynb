{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final-version-2-sharded-networks.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bf319/Scaling_MPNNs/blob/main/final_version_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rLhMwiHHWbtK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d93c19b4-71e3-4130-9925-72ff08d921db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-06 10:59:28--  https://raw.githubusercontent.com/deepmind/jraph/master/jraph/experimental/sharded_graphnet.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22444 (22K) [text/plain]\n",
            "Saving to: ‘sharded_graphnet.py.2’\n",
            "\n",
            "\rsharded_graphnet.py   0%[                    ]       0  --.-KB/s               \rsharded_graphnet.py 100%[===================>]  21.92K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2022-04-06 10:59:28 (5.76 MB/s) - ‘sharded_graphnet.py.2’ saved [22444/22444]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%pip install -q git+https://github.com/deepmind/dm-haiku\n",
        "%pip install -q jraph\n",
        "%pip install -q git+https://github.com/deepmind/jaxline\n",
        "%pip install -q ogb\n",
        "%pip install -q dgl\n",
        "%pip install -q optax\n",
        "%pip install -q metis\n",
        "\n",
        "!wget https://raw.githubusercontent.com/deepmind/jraph/master/jraph/experimental/sharded_graphnet.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "\n",
        "num_devices = jax.local_device_count()\n",
        "jax.devices()"
      ],
      "metadata": {
        "id": "iT2wqf76kIRp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1c9f16f-dd9a-44ef-efb6-caeb86dec457"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ogb.nodeproppred import Evaluator\n",
        "from ogb.nodeproppred import DglNodePropPredDataset\n",
        "\n",
        "dataset = DglNodePropPredDataset(name = \"ogbn-proteins\")\n",
        "split_idx = dataset.get_idx_split()\n",
        "evaluator = Evaluator(name = 'ogbn-proteins')\n",
        "print(evaluator.expected_input_format)"
      ],
      "metadata": {
        "id": "xHClucOxWpAZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4817f16d-34b8-48ce-a7e7-8c0c5a2fc7ca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using backend: pytorch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== Expected input format of Evaluator for ogbn-proteins\n",
            "{'y_true': y_true, 'y_pred': y_pred}\n",
            "- y_true: numpy ndarray or torch tensor of shape (num_node, num_task)\n",
            "- y_pred: numpy ndarray or torch tensor of shape (num_node, num_task)\n",
            "where y_pred stores score values (for computing ROC-AUC),\n",
            "num_task is 112, and each row corresponds to one node.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "import torch\n",
        "\n",
        "# There is only one graph in Node Property Prediction datasets\n",
        "ogbn_proteins_main_graph, ogbn_proteins_main_labels = dataset[0]\n",
        "ogbn_proteins_main_graph.ndata['species'] = scatter(\n",
        "    ogbn_proteins_main_graph.edata['feat'],\n",
        "    ogbn_proteins_main_graph.edges()[0],\n",
        "    dim = 0,\n",
        "    dim_size = ogbn_proteins_main_graph.num_nodes(),\n",
        "    reduce = 'mean'\n",
        ")\n",
        "'''\n",
        "  OGBN-Proteins\n",
        "    #Nodes = 132,534\n",
        "    #Edges = 39,561,252\n",
        "    #Diameter ~ 9 (https://cs.stanford.edu/people/jure/pubs/ogb-neurips20.pdf)\n",
        "    #Tasks = 112\n",
        "    #Split Type = Species\n",
        "    #Task Type = Binary classification\n",
        "    #Metric = ROC-AUC\n",
        "\n",
        "    Task:\n",
        "      The task is to predict the presence of protein functions in a multi-label binary classification setup,\n",
        "      where there are 112 kinds of labels to predict in total. \n",
        "      The performance is measured by the average of ROC-AUC scores across the 112 tasks.\n",
        "\n",
        "    #Others:\n",
        "      **undirected**\n",
        "      **weighted**\n",
        "      **typed (according to species)**\n",
        "\n",
        "  (1) Nodes represent proteins\n",
        "    (1.1) The proteins come from 8 species\n",
        "      len(set(graph.ndata['species'].reshape(-1).tolist())) == 8\n",
        "    (1.2) Each node has one feature associated with it (its species)\n",
        "      graph.ndata['species'].shape == (#nodes, 1)\n",
        "  \n",
        "  (2) Edges indicate different types of biologically meaningful associations between proteins\n",
        "    (2.1) All edges come with 8-dimensional features\n",
        "      graph.edata['feat'].shape == (2 * #edges, 8)\n",
        "\n",
        "'''\n",
        "# Get split labels\n",
        "train_label = dataset.labels[split_idx['train']]  # (86619, 112) -- binary values (presence of protein functions)\n",
        "valid_label = dataset.labels[split_idx['valid']]  # (21236, 112) -- binary values (presence of protein functions)\n",
        "test_label = dataset.labels[split_idx['test']]    # (24679, 112) -- binary values (presence of protein functions)\n",
        "\n",
        "# Create masks\n",
        "train_mask = jnp.zeros((ogbn_proteins_main_graph.num_nodes(), 1)).at[jnp.array(split_idx['train'])].set(1)\n",
        "valid_mask = jnp.zeros((ogbn_proteins_main_graph.num_nodes(), 1)).at[jnp.array(split_idx['valid'])].set(1)\n",
        "test_mask = jnp.zeros((ogbn_proteins_main_graph.num_nodes(), 1)).at[jnp.array(split_idx['test'])].set(1)"
      ],
      "metadata": {
        "id": "jCkzIEb4WsXU"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jraph\n",
        "\n",
        "# From https://colab.research.google.com/github/deepmind/educational/blob/master/colabs/summer_schools/intro_to_graph_nets_tutorial_with_jraph.ipynb#scrollTo=7vEmAsr5bKN8\n",
        "def _nearest_multiple_of_8(x: int) -> int:\n",
        "  \"\"\"Computes the nearest power of two greater than x for padding.\"\"\"\n",
        "  if x % 8 == 0:\n",
        "    return x\n",
        "  else:\n",
        "    return (x // 8 + 1) * 8 \n",
        "\n",
        "def pad_graph_to_nearest_multiple_of_8(\n",
        "    graphs_tuple: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
        "  \"\"\"Pads a batched `GraphsTuple` to the nearest power of two.\n",
        "  For example, if a `GraphsTuple` has 7 nodes, 5 edges and 3 graphs, this method\n",
        "  would pad the `GraphsTuple` nodes and edges:\n",
        "    7 nodes --> 8 nodes (2^3)\n",
        "    5 edges --> 8 edges (2^3)\n",
        "  And since padding is accomplished using `jraph.pad_with_graphs`, an extra\n",
        "  graph and node is added:\n",
        "    8 nodes --> 9 nodes\n",
        "    3 graphs --> 4 graphs\n",
        "  Args:\n",
        "    graphs_tuple: a batched `GraphsTuple` (can be batch size 1).\n",
        "  Returns:\n",
        "    A graphs_tuple batched to the nearest power of two.\n",
        "  \"\"\"\n",
        "  # Add 1 since we need at least one padding node for pad_with_graphs.\n",
        "  pad_nodes_to = _nearest_multiple_of_8(jnp.sum(graphs_tuple.n_node)) + 1\n",
        "  pad_edges_to = _nearest_multiple_of_8(jnp.sum(graphs_tuple.n_edge))\n",
        "  # Add 1 since we need at least one padding graph for pad_with_graphs.\n",
        "  # We do not pad to nearest power of two because the batch size is fixed.\n",
        "  pad_graphs_to = graphs_tuple.n_node.shape[0] + 1\n",
        "  return jraph.pad_with_graphs(graphs_tuple, pad_nodes_to, pad_edges_to,\n",
        "                               pad_graphs_to)"
      ],
      "metadata": {
        "id": "FSbePOUh2NBB"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q torch-scatter"
      ],
      "metadata": {
        "id": "E54h0hKoicIb"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import jraph\n",
        "import sharded_graphnet\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from torch_scatter import scatter\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "enc = OneHotEncoder()\n",
        "enc.fit(ogbn_proteins_main_graph.ndata['species'])\n",
        "\n",
        "def dgl_graph_to_jraph(node_ids, labels, train_mask, valid_mask, test_mask):\n",
        "  # First add back the node and edge features\n",
        "  dgl_graph_with_features = dgl.node_subgraph(ogbn_proteins_main_graph, node_ids)\n",
        "\n",
        "  # node_features = jnp.array(enc.transform(dgl_graph_with_features.ndata['species']).toarray())\n",
        "  \n",
        "  node_features = jnp.array(dgl_graph_with_features.ndata['species'])\n",
        "\n",
        "  senders = jnp.array(dgl_graph_with_features.edges()[0])\n",
        "  receivers = jnp.array(dgl_graph_with_features.edges()[1])\n",
        "\n",
        "  # Edges -- here we should include the 8-dimensional edge features\n",
        "  def fixed_pos_embedding(x, seq_dim=0):\n",
        "    dim = x.shape[-1]\n",
        "    inv_freq = 1. / (10000 ** (np.arange(0, dim, 2) / dim))\n",
        "\n",
        "    sinusoid_inp = jnp.einsum('i , j -> i j', jnp.arange(x.shape[seq_dim]), inv_freq)\n",
        "\n",
        "    return jnp.concatenate([\n",
        "      jnp.sin(sinusoid_inp), \n",
        "      jnp.cos(sinusoid_inp)\n",
        "      ], axis = -1)\n",
        "\n",
        "  edges = jnp.array(dgl_graph_with_features.edata['feat'])\n",
        "  # edges = jnp.concatenate([edges, fixed_pos_embedding(edges)], axis = -1)\n",
        "\n",
        "  n_node = jnp.array([dgl_graph_with_features.num_nodes()])\n",
        "  n_edge = jnp.array([dgl_graph_with_features.num_edges()])\n",
        "\n",
        "  in_tuple = jraph.GraphsTuple(\n",
        "            nodes = node_features.astype(np.float32),\n",
        "            senders = senders.astype(np.int32), \n",
        "            receivers = receivers.astype(np.int32),\n",
        "            edges = edges.astype(np.float32),  \n",
        "            n_node = n_node, \n",
        "            n_edge = n_edge,\n",
        "            globals = None  # No global features\n",
        "          )\n",
        "  \n",
        "  in_tuple = in_tuple._replace(\n",
        "      nodes = {\n",
        "          'inputs': in_tuple.nodes, \n",
        "          'targets': labels, \n",
        "          'train_mask': train_mask, \n",
        "          'valid_mask': valid_mask, \n",
        "          'test_mask': test_mask,\n",
        "          'padding_mask': jnp.ones((in_tuple.nodes.shape[0], 1)) \n",
        "                                                        # TODO: Check this above\n",
        "                                                        # Adding this mask so that we can remove the nodes added after padding \n",
        "                                                        # for the final ROC computations on the full train / valid / test splits\n",
        "                                                        # This is because I want to pass the predictions on the true nodes to the \n",
        "                                                        # ogbn-evaluator, so I would first need to remove the predictions that come from padding.\n",
        "          }\n",
        "  )\n",
        "\n",
        "  in_tuple = pad_graph_to_nearest_multiple_of_8(in_tuple)\n",
        "  \n",
        "  return sharded_graphnet.graphs_tuple_to_broadcasted_sharded_graphs_tuple(\n",
        "      in_tuple,\n",
        "      num_shards = num_devices\n",
        "      )\n",
        "  \n",
        "def get_labels_for_subgraph(node_ids):\n",
        "  return jnp.array(ogbn_proteins_main_labels.index_select(0, node_ids))"
      ],
      "metadata": {
        "id": "fvH_XRJVWuLw"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "\n",
        "'''\n",
        "  Generate graph partition using metis, with balanced number of edges in each partition.\n",
        "  Note: \n",
        "    The subgraphs do not contain the node/edge data in the input graph (https://docs.dgl.ai/generated/dgl.metis_partition.html)\n",
        "'''\n",
        "num_partitions = 35  ## TODO: Find some way to decrease this to something reasonable (< 50)\n",
        "\n",
        "dgl_graph_metis_partition = dgl.metis_partition(ogbn_proteins_main_graph, num_partitions, balance_edges = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUI9s4-0mPz9",
        "outputId": "2f7b93e9-cbd4-4b79-f7fb-f0d5ecb1d02b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convert a graph into a bidirected graph: 3.585 seconds\n",
            "Construct multi-constraint weights: 0.003 seconds\n",
            "Metis partitioning: 31.438 seconds\n",
            "Split the graph: 0.769 seconds\n",
            "Construct subgraphs: 0.019 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert graphs to Jraph GraphsTuple\n",
        "processed_graphs = {}\n",
        "\n",
        "for idx in range(num_partitions):\n",
        "  node_ids = dgl_graph_metis_partition[idx].ndata['_ID']\n",
        "\n",
        "  labels = get_labels_for_subgraph(node_ids)\n",
        "  graph = dgl_graph_to_jraph(node_ids, \n",
        "                             labels, \n",
        "                             train_mask = train_mask.at[jnp.array(node_ids)].get(),\n",
        "                             valid_mask = valid_mask.at[jnp.array(node_ids)].get(),\n",
        "                             test_mask = test_mask.at[jnp.array(node_ids)].get()\n",
        "                             )\n",
        "\n",
        "  processed_graphs[f'partition_{idx}'] = {\n",
        "      'graph': graph._replace(nodes = graph.nodes['inputs']), \n",
        "      'labels': graph.nodes['targets'],\n",
        "      'train_mask': graph.nodes['train_mask'],\n",
        "      'valid_mask': graph.nodes['valid_mask'],\n",
        "      'test_mask': graph.nodes['test_mask'],\n",
        "      'padding_mask': graph.nodes['padding_mask']\n",
        "      }"
      ],
      "metadata": {
        "id": "s8-Ln58I_Fwp"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import haiku as hk\n",
        "import jax\n",
        "import optax\n",
        "\n",
        "from typing import Sequence\n",
        "\n",
        "# See https://github.com/YuxuanXie/mcl/blob/5f7ee92e2a6bc89736263873a4ba9c14d1a676ff/glassy_dynamics/train_using_jax.py for alternative to using GraphMapFeatures\n",
        "# From https://github.com/YuxuanXie/mcl/blob/5f7ee92e2a6bc89736263873a4ba9c14d1a676ff/glassy_dynamics/train_using_jax.py\n",
        "\n",
        "hidden_dimension = 128\n",
        "num_message_passing_steps = 5 # Question: (256, 4) fails / (128, 6) works\n",
        "\n",
        "@jraph.concatenated_args\n",
        "def node_update_fn(feats: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Node update function for graph net.\"\"\"\n",
        "  net = hk.Sequential([hk.nets.MLP(output_sizes = [hidden_dimension, hidden_dimension], activation = jax.nn.relu, activate_final = False), hk.LayerNorm(axis = -1, create_scale = True, create_offset = True)])\n",
        "  return net(feats)\n",
        "\n",
        "@jraph.concatenated_args\n",
        "def edge_update_fn(feats: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Edge update function for graph net.\"\"\"\n",
        "  net = hk.Sequential([hk.nets.MLP(output_sizes = [hidden_dimension, hidden_dimension], activation = jax.nn.relu, activate_final = False), hk.LayerNorm(axis = -1, create_scale = True, create_offset = True)])\n",
        "  return net(feats)\n",
        "\n",
        "@hk.without_apply_rng\n",
        "@hk.transform\n",
        "def network_definition(graph):\n",
        "  \"\"\"Defines a graph neural network.\n",
        "  Args:\n",
        "    graph: Graphstuple the network processes.\n",
        "  Returns:\n",
        "    Decoded nodes.\n",
        "  \"\"\"\n",
        "  graph = graph._replace(\n",
        "      nodes = hk.Linear(hidden_dimension)(graph.nodes),\n",
        "      device_edges = hk.Linear(hidden_dimension)(graph.device_edges)\n",
        "  )\n",
        "  \n",
        "  sharded_gn = sharded_graphnet.ShardedEdgesGraphNetwork(\n",
        "      update_node_fn = node_update_fn,\n",
        "      update_edge_fn = edge_update_fn,\n",
        "      num_shards = num_devices\n",
        "      )\n",
        "\n",
        "  for _ in range(num_message_passing_steps):\n",
        "    residual_graph = sharded_gn(graph)\n",
        "    graph = graph._replace(\n",
        "        nodes = graph.nodes + residual_graph.nodes,\n",
        "        device_edges = graph.device_edges + residual_graph.device_edges\n",
        "    )\n",
        "\n",
        "  graph = graph._replace(\n",
        "      nodes = hk.Sequential([hk.Linear(hidden_dimension), jax.nn.relu, hk.Linear(112)])(graph.nodes)\n",
        "  )\n",
        "  return graph.nodes"
      ],
      "metadata": {
        "id": "gPg7ph7sWyOn"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bcast_local_devices(value):\n",
        "    \"\"\"Broadcasts an object to all local devices.\"\"\"\n",
        "    devices = jax.local_devices()\n",
        "\n",
        "    def _replicate(x):\n",
        "      \"\"\"Replicate an object on each device.\"\"\"\n",
        "      x = jnp.array(x)\n",
        "      return jax.device_put_sharded(len(devices) * [x], devices)\n",
        "\n",
        "    return jax.tree_util.tree_map(_replicate, value)"
      ],
      "metadata": {
        "id": "z6Qh75qxQfii"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reshape_broadcasted_data(data):\n",
        "  '''\n",
        "    Node predictions / Labels / Masks are identical on all the devices so we only take\n",
        "    one of them in order to remove the leading axis.\n",
        "  '''\n",
        "  return np.array(data)[0]\n",
        "  \n",
        "def remove_mask_from_data(data, mask):\n",
        "  '''\n",
        "    data.shape = [num_nodes, 112]\n",
        "    mask.shape = [num_nodes, 1]\n",
        "\n",
        "    We want to only return the data where mask == True\n",
        "  '''\n",
        "  sliced_data = np.compress(np.array(mask).reshape(-1).astype(bool), data, axis = 0)\n",
        "  return np.array(sliced_data)"
      ],
      "metadata": {
        "id": "oJ5T_oplbg_t"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import haiku as hk\n",
        "\n",
        "@functools.partial(jax.pmap, axis_name='i')\n",
        "def predict_on_graph(params, graph, label, mask):\n",
        "  decoded_nodes = network_definition.apply(params, graph)\n",
        "\n",
        "  compute_loss_fn = functools.partial(compute_loss)\n",
        "  loss = compute_loss_fn(params, graph, label, mask)\n",
        "\n",
        "  return jax.nn.sigmoid(decoded_nodes), loss\n",
        "\n",
        "def evaluate_on_full_sets(params):\n",
        "  final_predictions = {}\n",
        "\n",
        "  for i in range(num_partitions):\n",
        "    node_ids = dgl_graph_metis_partition[i].ndata['_ID']\n",
        "    partition = processed_graphs[f'partition_{i}']\n",
        "    \n",
        "    predictions, _ = predict_on_graph(params, \n",
        "                                      partition['graph'], \n",
        "                                      partition['labels'], \n",
        "                                      partition['test_mask']  # Only used in the loss computation, does not affect predictions\n",
        "                                      )\n",
        "\n",
        "    predictions_after_masked_nodes_are_removed = remove_mask_from_data(\n",
        "        reshape_broadcasted_data(predictions),\n",
        "        reshape_broadcasted_data(partition['padding_mask'])\n",
        "        )\n",
        "\n",
        "    for index, node_id in enumerate(node_ids):\n",
        "      final_predictions[node_id] = predictions_after_masked_nodes_are_removed[index]\n",
        "\n",
        "    if (i + 1) % 10 == 0:\n",
        "      print(f'Evaluated {i + 1} / {num_partitions} subgraphs...')\n",
        "\n",
        "  # Sort the final predictions based on the node ids\n",
        "  predictions_in_order = dict(sorted(final_predictions.items()))\n",
        "\n",
        "  # Convert the values to a list to be able to slice based on the ids of the \n",
        "  # nodes in the test set\n",
        "  predictions_in_order = list(predictions_in_order.values())\n",
        "\n",
        "  final_roc_train = evaluator.eval({\n",
        "      \"y_true\": np.array(train_label), \n",
        "      \"y_pred\": np.array([predictions_in_order[x] for x in split_idx['train']])\n",
        "      })['rocauc']\n",
        "\n",
        "  final_roc_valid = evaluator.eval({\n",
        "      \"y_true\": np.array(valid_label), \n",
        "      \"y_pred\": np.array([predictions_in_order[x] for x in split_idx['valid']])\n",
        "      })['rocauc']\n",
        "\n",
        "  final_roc_test = evaluator.eval({\n",
        "      \"y_true\": np.array(test_label),\n",
        "      \"y_pred\": np.array([predictions_in_order[x] for x in split_idx['test']])\n",
        "      })['rocauc']\n",
        "\n",
        "  print()\n",
        "  print(f'Final ROC on the train set {final_roc_train}')\n",
        "  print(f'Final ROC on the validation set {final_roc_valid}')\n",
        "  print(f'Final ROC on the test set {final_roc_test}')\n",
        "\n",
        "  return (final_roc_train, final_roc_valid, final_roc_test)"
      ],
      "metadata": {
        "id": "xtJWXfhsCS-e"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "from random import randint\n",
        "from google.colab import files\n",
        "\n",
        "# Try to follow this tutorial https://github.com/YuxuanXie/mcl/blob/5f7ee92e2a6bc89736263873a4ba9c14d1a676ff/glassy_dynamics/train_using_jax.py\n",
        "def compute_loss(params, graph, label, mask):\n",
        "  predictions = network_definition.apply(params, graph)\n",
        "\n",
        "  # use optax here (https://github.com/deepmind/optax/blob/master/optax/_src/loss.py#L116#L139)\n",
        "  loss = optax.sigmoid_binary_cross_entropy(predictions, label)  # shape [num_nodes, num_classes]\n",
        "  loss = loss * mask\n",
        "  loss = jnp.sum(loss) / jnp.sum(mask) # loss = mean_with_mask(loss, mask)\n",
        "\n",
        "  return loss\n",
        "\n",
        "def train(num_training_steps, learning_rate, results_path):\n",
        "  losses = []\n",
        "\n",
        "  roc_train_list = []\n",
        "  roc_eval_list = []\n",
        "  roc_test_list = []\n",
        "  roc_step = []\n",
        "\n",
        "  replicated_params = jax.pmap(network_definition.init, axis_name = 'i')(\n",
        "      bcast_local_devices(jax.random.PRNGKey(42)), \n",
        "      processed_graphs['partition_0']['graph']\n",
        "      )\n",
        "\n",
        "  opt_init, opt_update = optax.adam(learning_rate = learning_rate)  \n",
        "  replicated_opt_state = jax.pmap(opt_init, axis_name = 'i')(replicated_params)\n",
        "\n",
        "  @functools.partial(jax.pmap, axis_name='i')\n",
        "  def update(params, opt_state, graph, targets, mask):\n",
        "    # Compute the gradients on the given minibatch (individually on each device).\n",
        "    loss, grads = jax.value_and_grad(compute_loss)(params, graph, targets, mask)\n",
        "\n",
        "    # Combine the gradient across all devices.\n",
        "    grads = jax.lax.pmean(grads, axis_name='i')     ## Question / TODO: Change to psum\n",
        "\n",
        "    # Also combine the loss. Unnecessary for the update, but useful for logging.\n",
        "    loss = jax.lax.pmean(loss, axis_name='i')\n",
        "\n",
        "    updates, opt_state = opt_update(updates = grads, state = opt_state, )\n",
        "\n",
        "    return optax.apply_updates(params, updates), opt_state, loss\n",
        "\n",
        "  # Train\n",
        "  for idx in range(num_training_steps):\n",
        "    random_partition_idx = randint(0, num_partitions - 1)\n",
        "    random_partition = processed_graphs[f'partition_{random_partition_idx}']\n",
        "\n",
        "    graph = random_partition['graph']\n",
        "    labels = random_partition['labels']   # Automatically broadcasted by the sharded graph net\n",
        "    mask = random_partition['train_mask'] # Automatically broadcasted by the sharded graph net\n",
        "\n",
        "    replicated_params, replicated_opt_state, loss = update(\n",
        "        replicated_params, \n",
        "        replicated_opt_state, \n",
        "        graph, \n",
        "        labels,\n",
        "        mask\n",
        "        )\n",
        "    \n",
        "    print('Loss training:', reshape_broadcasted_data(loss))\n",
        "\n",
        "    losses.append(reshape_broadcasted_data(loss))\n",
        "\n",
        "    if (idx + 1) % 10 == 0:\n",
        "      print()\n",
        "      print(f'***************************')\n",
        "      print(f'Trained on {idx + 1} graphs')\n",
        "      print(f'***************************')\n",
        "      print()\n",
        "\n",
        "    if (idx + 1) % 200 == 0:\n",
        "      print()\n",
        "      print(f'*** Full evaluations after {idx + 1} training steps ***')\n",
        "      \n",
        "      roc_train, roc_eval, roc_test = evaluate_on_full_sets(replicated_params)\n",
        "\n",
        "      roc_train_list.append(roc_train)\n",
        "      roc_eval_list.append(roc_eval)\n",
        "      roc_test_list.append(roc_test)\n",
        "      roc_step.append(idx + 1)\n",
        "\n",
        "  plot_loss(losses)\n",
        "  plot_rocs(roc_train_list, roc_eval_list, roc_test_list, roc_step)    \n",
        "\n",
        "  return replicated_params"
      ],
      "metadata": {
        "id": "xYVzddNITMSv"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_loss(loss_list):\n",
        "  plt.plot(range(1, len(loss_list) + 1), loss_list)\n",
        "  plt.xlabel('Iteration')\n",
        "  plt.ylabel('Training loss')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def plot_rocs(roc_train, roc_eval, roc_test, iters):\n",
        "  plt.plot(iters, roc_train, label = 'Train ROC')\n",
        "  plt.plot(iters, roc_eval, label = 'Valid ROC')\n",
        "  plt.plot(iters, roc_test, label = 'Test ROC')\n",
        "  \n",
        "  plt.xlabel('Iteration')\n",
        "  plt.ylabel('ROC')\n",
        "\n",
        "  plt.legend(loc = 'upper right')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "yLZ4xp-t-NFP"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "current_time = datetime.today().strftime('%Y-%m-%d-%H:%M:%S')\n",
        "exp_path = f'/content/exp_{current_time}/'\n",
        "os.makedirs(exp_path, exist_ok = False)\n",
        "\n",
        "# Main training loop\n",
        "final_params = train(\n",
        "    num_training_steps = 50000, \n",
        "    learning_rate = 0.001,\n",
        "    results_path = exp_path\n",
        "    )\n",
        "\n",
        "# with open('/content/exp_2022-03-30-09:51:17/params_epochs_9000.pickle', 'rb') as f:\n",
        "#     loaded_params = pickle.load(f)\n",
        "# loaded_params = final_params\n",
        "# evaluate_on_full_sets(loaded_params)\n",
        "\n",
        "'''\n",
        "  Previous runs (padding to power of 2)\n",
        "  (1) Configuration\n",
        "        learning_rate = 0.001\n",
        "        num_partitions = 50\n",
        "        hidden_dimension = 128\n",
        "        num_message_passing_steps = 5\n",
        "        num_training_steps = 1000\n",
        "    ROC on the train set 0.7348797273386144\n",
        "    ROC on the validation set 0.6025038939324504\n",
        "    ROC on the test set 0.5896861508337246\n",
        "\n",
        "  (2) Configuration\n",
        "        learning_rate = 0.001\n",
        "        num_partitions = 50\n",
        "        hidden_dimension = 128\n",
        "        num_message_passing_steps = 5\n",
        "        num_training_steps = 3000\n",
        "    ROC on the train set 0.8050085464161815\n",
        "    ROC on the validation set 0.6327603823722211\n",
        "    ROC on the test set 0.5078022533003436\n",
        "\n",
        "  (3) Configuration\n",
        "        learning_rate = 0.1 (Question: I think this might be too high -- based on the results in (5) with lower number of epochs)\n",
        "        num_partitions = 100\n",
        "        hidden_dimension = 128\n",
        "        num_message_passing_steps = 5\n",
        "        num_training_steps = 1000\n",
        "    ROC on the train set 0.5\n",
        "    ROC on the validation set 0.5 \n",
        "    ROC on the test set 0.5\n",
        "\n",
        "  (4) Configuration\n",
        "        learning_rate = 0.01\n",
        "        num_partitions = 100\n",
        "        hidden_dimension = 128\n",
        "        num_message_passing_steps = 5\n",
        "        num_training_steps = 100\n",
        "    ROC on the train set 0.6501172261188106\n",
        "    ROC on the validation set 0.5281974299591566\n",
        "    ROC on the test set 0.47652056321124514\n",
        "\n",
        "  (5) Configuration\n",
        "        learning_rate = 0.01\n",
        "        num_partitions = 100\n",
        "        hidden_dimension = 128\n",
        "        num_message_passing_steps = 5\n",
        "        num_training_steps = 500\n",
        "    ROC on the train set 0.6939371049645034\n",
        "    ROC on the validation set 0.559224577731843\n",
        "    ROC on the test set 0.5488968392833208\n",
        "\n",
        "  (6) Configuration\n",
        "        opt: LAMB\n",
        "        learning_rate: 1e-4\n",
        "        num-partitions = grad100\n",
        "        hidden_dimension = 128\n",
        "        num_message_passing_steps = 5\n",
        "        num_training_steps = 500\n",
        "  ROC on the train set 0.6299712777663571\n",
        "  ROC on the validation set 0.5054189612195771\n",
        "  ROC on the test set 0.5083185060310427\n",
        "\n",
        "  ********************************************\n",
        "\n",
        "  Previous runs (padding to multiple of 8)\n",
        "  (1) Configuration\n",
        "        opt: LAMB\n",
        "        learning_rate: 1e-4\n",
        "        num-partitions = 35\n",
        "        hidden_dimension = 128\n",
        "        num_message_passing_steps = 5\n",
        "        num_training_steps = 500\n",
        "  ROC on the train set 0.6438794374674618\n",
        "  ROC on the validation set 0.5162833891590899\n",
        "  ROC on the test set 0.5175085147535061\n",
        "\n",
        "  (2) Configuration\n",
        "        opt: ADAM\n",
        "        learning_rate: 1e-4\n",
        "        num-partitions = 35\n",
        "        hidden_dimension = 128\n",
        "        num_message_passing_steps = 5\n",
        "        num_training_steps = 10000\n",
        "  ROC on the train set 0.8873957875287084\n",
        "  ROC on the validation set 0.604810879077169\n",
        "  ROC on the test set 0.57961973018596\n",
        "\n",
        "  (3) Configuration\n",
        "      opt: ADAM\n",
        "      learning_rate = 0.001\n",
        "      num_partitions = 35\n",
        "      hidden_dimension = 128\n",
        "      num_message_passing_steps = 5\n",
        "      num_training_steps = 100\n",
        "      *** Edge features = Edge features + sin(edge_features) + cost(edge_features)\n",
        "  ROC on the train set 0.6798269721961256\n",
        "  ROC on the validation set 0.5658498109351704\n",
        "  ROC on the test set 0.5796600014265777\n",
        "\n",
        "  (4) Configuration\n",
        "      opt: ADAM\n",
        "      learning_rate = 0.001\n",
        "      num_partitions = 35\n",
        "      hidden_dimension = 128\n",
        "      num_message_passing_steps = 5\n",
        "      num_training_steps = 1000\n",
        "      *** Edge features = edge_features + sort of pos encoding * edge_features\n",
        "\n",
        "  ROC on the train set 0.7458021952579157\n",
        "  ROC on the validation set 0.6625978573423217\n",
        "  ROC on the test set 0.6162747073675334\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4sdL1RSWurH",
        "outputId": "be4f4f47-370d-472d-fd6b-7abc86ff70e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Loss training: 27.029886\n",
            "Loss training: 27.011608\n",
            "Loss training: 33.072083\n",
            "Loss training: 23.45915\n",
            "\n",
            "***************************\n",
            "Trained on 4150 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 16.86276\n",
            "Loss training: 30.402756\n",
            "Loss training: 23.829552\n",
            "Loss training: 34.216022\n",
            "Loss training: 44.845284\n",
            "Loss training: 32.82973\n",
            "Loss training: 37.41495\n",
            "Loss training: 27.415667\n",
            "Loss training: 32.340237\n",
            "Loss training: 33.729473\n",
            "\n",
            "***************************\n",
            "Trained on 4160 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.98507\n",
            "Loss training: 25.893616\n",
            "Loss training: 38.647976\n",
            "Loss training: 15.69771\n",
            "Loss training: 23.60596\n",
            "Loss training: 40.172943\n",
            "Loss training: 29.749893\n",
            "Loss training: 36.035954\n",
            "Loss training: 42.912083\n",
            "Loss training: 42.61013\n",
            "\n",
            "***************************\n",
            "Trained on 4170 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.32934\n",
            "Loss training: 36.997486\n",
            "Loss training: 37.726288\n",
            "Loss training: 25.187544\n",
            "Loss training: 28.803398\n",
            "Loss training: 34.02915\n",
            "Loss training: 35.623756\n",
            "Loss training: 39.311817\n",
            "Loss training: 38.79775\n",
            "Loss training: 40.12202\n",
            "\n",
            "***************************\n",
            "Trained on 4180 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.631407\n",
            "Loss training: 30.145912\n",
            "Loss training: 24.289885\n",
            "Loss training: 33.66149\n",
            "Loss training: 43.295307\n",
            "Loss training: 23.009697\n",
            "Loss training: 32.320858\n",
            "Loss training: 38.522053\n",
            "Loss training: 38.253895\n",
            "Loss training: 39.86992\n",
            "\n",
            "***************************\n",
            "Trained on 4190 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.340162\n",
            "Loss training: 29.255781\n",
            "Loss training: 36.241627\n",
            "Loss training: 28.078894\n",
            "Loss training: 27.461155\n",
            "Loss training: 24.22927\n",
            "Loss training: 35.701363\n",
            "Loss training: 23.153019\n",
            "Loss training: 37.414856\n",
            "Loss training: 23.911766\n",
            "\n",
            "***************************\n",
            "Trained on 4200 graphs\n",
            "***************************\n",
            "\n",
            "\n",
            "*** Full evaluations after 4200 training steps ***\n",
            "Evaluated 10 / 35 subgraphs...\n",
            "Evaluated 20 / 35 subgraphs...\n",
            "Evaluated 30 / 35 subgraphs...\n",
            "\n",
            "Final ROC on the train set 0.8410205649753673\n",
            "Final ROC on the validation set 0.7956653990442956\n",
            "Final ROC on the test set 0.7572524047232368\n",
            "Loss training: 25.610239\n",
            "Loss training: 28.49682\n",
            "Loss training: 40.85374\n",
            "Loss training: 27.806046\n",
            "Loss training: 34.09652\n",
            "Loss training: 32.687614\n",
            "Loss training: 34.63809\n",
            "Loss training: 32.91989\n",
            "Loss training: 44.489597\n",
            "Loss training: 35.73772\n",
            "\n",
            "***************************\n",
            "Trained on 4210 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.552523\n",
            "Loss training: 33.113533\n",
            "Loss training: 33.95167\n",
            "Loss training: 28.000998\n",
            "Loss training: 34.695904\n",
            "Loss training: 28.53894\n",
            "Loss training: 25.772615\n",
            "Loss training: 37.890656\n",
            "Loss training: 32.42655\n",
            "Loss training: 35.529957\n",
            "\n",
            "***************************\n",
            "Trained on 4220 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.96916\n",
            "Loss training: 27.691063\n",
            "Loss training: 35.810097\n",
            "Loss training: 32.073193\n",
            "Loss training: 16.121395\n",
            "Loss training: 36.237694\n",
            "Loss training: 38.253513\n",
            "Loss training: 25.243834\n",
            "Loss training: 25.057116\n",
            "Loss training: 24.559368\n",
            "\n",
            "***************************\n",
            "Trained on 4230 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.764738\n",
            "Loss training: 42.814774\n",
            "Loss training: 30.98983\n",
            "Loss training: 35.33355\n",
            "Loss training: 24.317074\n",
            "Loss training: 33.400463\n",
            "Loss training: 30.343636\n",
            "Loss training: 25.090881\n",
            "Loss training: 28.610031\n",
            "Loss training: 28.385683\n",
            "\n",
            "***************************\n",
            "Trained on 4240 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.5585\n",
            "Loss training: 38.122795\n",
            "Loss training: 29.09516\n",
            "Loss training: 36.11976\n",
            "Loss training: 23.486805\n",
            "Loss training: 37.55963\n",
            "Loss training: 24.497084\n",
            "Loss training: 32.148083\n",
            "Loss training: 45.58085\n",
            "Loss training: 35.99507\n",
            "\n",
            "***************************\n",
            "Trained on 4250 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 15.959947\n",
            "Loss training: 27.850111\n",
            "Loss training: 30.177042\n",
            "Loss training: 25.011274\n",
            "Loss training: 23.630985\n",
            "Loss training: 30.28087\n",
            "Loss training: 25.2623\n",
            "Loss training: 32.046436\n",
            "Loss training: 17.891405\n",
            "Loss training: 34.39029\n",
            "\n",
            "***************************\n",
            "Trained on 4260 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.844484\n",
            "Loss training: 40.43959\n",
            "Loss training: 38.75175\n",
            "Loss training: 37.08764\n",
            "Loss training: 27.6415\n",
            "Loss training: 23.90283\n",
            "Loss training: 16.879488\n",
            "Loss training: 35.409863\n",
            "Loss training: 27.860699\n",
            "Loss training: 35.38268\n",
            "\n",
            "***************************\n",
            "Trained on 4270 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.60107\n",
            "Loss training: 23.90202\n",
            "Loss training: 23.120699\n",
            "Loss training: 40.420124\n",
            "Loss training: 34.49318\n",
            "Loss training: 29.07818\n",
            "Loss training: 40.505146\n",
            "Loss training: 23.625431\n",
            "Loss training: 36.919785\n",
            "Loss training: 25.326057\n",
            "\n",
            "***************************\n",
            "Trained on 4280 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.315598\n",
            "Loss training: 25.570433\n",
            "Loss training: 36.11337\n",
            "Loss training: 43.262035\n",
            "Loss training: 39.758106\n",
            "Loss training: 28.411476\n",
            "Loss training: 37.42563\n",
            "Loss training: 40.494328\n",
            "Loss training: 38.784496\n",
            "Loss training: 25.01888\n",
            "\n",
            "***************************\n",
            "Trained on 4290 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.93469\n",
            "Loss training: 16.55769\n",
            "Loss training: 28.37375\n",
            "Loss training: 24.945002\n",
            "Loss training: 37.058567\n",
            "Loss training: 35.689003\n",
            "Loss training: 23.945639\n",
            "Loss training: 35.558075\n",
            "Loss training: 34.3548\n",
            "Loss training: 45.3118\n",
            "\n",
            "***************************\n",
            "Trained on 4300 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.786602\n",
            "Loss training: 25.163166\n",
            "Loss training: 24.737251\n",
            "Loss training: 22.81263\n",
            "Loss training: 35.79924\n",
            "Loss training: 36.345383\n",
            "Loss training: 29.227125\n",
            "Loss training: 31.052092\n",
            "Loss training: 36.88196\n",
            "Loss training: 17.353123\n",
            "\n",
            "***************************\n",
            "Trained on 4310 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.640524\n",
            "Loss training: 35.01178\n",
            "Loss training: 25.086712\n",
            "Loss training: 38.59878\n",
            "Loss training: 34.52595\n",
            "Loss training: 39.384823\n",
            "Loss training: 36.222595\n",
            "Loss training: 25.645746\n",
            "Loss training: 39.92466\n",
            "Loss training: 35.635426\n",
            "\n",
            "***************************\n",
            "Trained on 4320 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.584934\n",
            "Loss training: 38.476334\n",
            "Loss training: 28.797838\n",
            "Loss training: 24.163439\n",
            "Loss training: 30.639082\n",
            "Loss training: 16.12626\n",
            "Loss training: 30.73545\n",
            "Loss training: 33.945892\n",
            "Loss training: 29.42081\n",
            "Loss training: 23.774937\n",
            "\n",
            "***************************\n",
            "Trained on 4330 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.762173\n",
            "Loss training: 37.444885\n",
            "Loss training: 24.603579\n",
            "Loss training: 28.795654\n",
            "Loss training: 35.51549\n",
            "Loss training: 23.014326\n",
            "Loss training: 23.314287\n",
            "Loss training: 35.485386\n",
            "Loss training: 34.831825\n",
            "Loss training: 32.608337\n",
            "\n",
            "***************************\n",
            "Trained on 4340 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.398438\n",
            "Loss training: 41.921345\n",
            "Loss training: 39.490726\n",
            "Loss training: 17.312618\n",
            "Loss training: 35.088272\n",
            "Loss training: 29.636385\n",
            "Loss training: 28.437057\n",
            "Loss training: 28.066938\n",
            "Loss training: 29.422508\n",
            "Loss training: 24.612316\n",
            "\n",
            "***************************\n",
            "Trained on 4350 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.18139\n",
            "Loss training: 28.4914\n",
            "Loss training: 25.726624\n",
            "Loss training: 28.848639\n",
            "Loss training: 28.619078\n",
            "Loss training: 15.9336405\n",
            "Loss training: 29.467798\n",
            "Loss training: 34.388702\n",
            "Loss training: 27.949753\n",
            "Loss training: 22.962833\n",
            "\n",
            "***************************\n",
            "Trained on 4360 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.427326\n",
            "Loss training: 35.25371\n",
            "Loss training: 37.736374\n",
            "Loss training: 23.470453\n",
            "Loss training: 30.417921\n",
            "Loss training: 16.481056\n",
            "Loss training: 27.550716\n",
            "Loss training: 28.315958\n",
            "Loss training: 15.482146\n",
            "Loss training: 36.54374\n",
            "\n",
            "***************************\n",
            "Trained on 4370 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.369337\n",
            "Loss training: 28.70492\n",
            "Loss training: 34.94898\n",
            "Loss training: 35.67983\n",
            "Loss training: 33.026043\n",
            "Loss training: 27.013481\n",
            "Loss training: 25.335999\n",
            "Loss training: 23.457748\n",
            "Loss training: 25.348879\n",
            "Loss training: 32.374146\n",
            "\n",
            "***************************\n",
            "Trained on 4380 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.0712\n",
            "Loss training: 23.742785\n",
            "Loss training: 34.217667\n",
            "Loss training: 28.895676\n",
            "Loss training: 37.221836\n",
            "Loss training: 24.409237\n",
            "Loss training: 36.44482\n",
            "Loss training: 42.638218\n",
            "Loss training: 23.573555\n",
            "Loss training: 24.726288\n",
            "\n",
            "***************************\n",
            "Trained on 4390 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.690239\n",
            "Loss training: 39.23867\n",
            "Loss training: 28.394712\n",
            "Loss training: 36.643658\n",
            "Loss training: 24.493227\n",
            "Loss training: 38.709282\n",
            "Loss training: 42.045197\n",
            "Loss training: 36.04556\n",
            "Loss training: 33.589855\n",
            "Loss training: 28.716417\n",
            "\n",
            "***************************\n",
            "Trained on 4400 graphs\n",
            "***************************\n",
            "\n",
            "\n",
            "*** Full evaluations after 4400 training steps ***\n",
            "Evaluated 10 / 35 subgraphs...\n",
            "Evaluated 20 / 35 subgraphs...\n",
            "Evaluated 30 / 35 subgraphs...\n",
            "\n",
            "Final ROC on the train set 0.8439665545385964\n",
            "Final ROC on the validation set 0.8020001268971697\n",
            "Final ROC on the test set 0.764429625954815\n",
            "Loss training: 37.822956\n",
            "Loss training: 33.407654\n",
            "Loss training: 24.262543\n",
            "Loss training: 25.183634\n",
            "Loss training: 30.982456\n",
            "Loss training: 47.11073\n",
            "Loss training: 38.83868\n",
            "Loss training: 26.048761\n",
            "Loss training: 36.889698\n",
            "Loss training: 39.450993\n",
            "\n",
            "***************************\n",
            "Trained on 4410 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.160355\n",
            "Loss training: 29.989439\n",
            "Loss training: 36.82958\n",
            "Loss training: 57.33886\n",
            "Loss training: 29.131287\n",
            "Loss training: 34.03704\n",
            "Loss training: 38.22125\n",
            "Loss training: 29.753523\n",
            "Loss training: 32.86472\n",
            "Loss training: 27.336369\n",
            "\n",
            "***************************\n",
            "Trained on 4420 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.293724\n",
            "Loss training: 31.290771\n",
            "Loss training: 26.922907\n",
            "Loss training: 28.778643\n",
            "Loss training: 26.934078\n",
            "Loss training: 39.780758\n",
            "Loss training: 20.947308\n",
            "Loss training: 24.851843\n",
            "Loss training: 32.415215\n",
            "Loss training: 24.258812\n",
            "\n",
            "***************************\n",
            "Trained on 4430 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.04044\n",
            "Loss training: 29.683414\n",
            "Loss training: 34.920357\n",
            "Loss training: 30.038065\n",
            "Loss training: 42.767365\n",
            "Loss training: 39.411068\n",
            "Loss training: 71.27503\n",
            "Loss training: 30.291197\n",
            "Loss training: 29.508701\n",
            "Loss training: 16.017403\n",
            "\n",
            "***************************\n",
            "Trained on 4440 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.844532\n",
            "Loss training: 29.45157\n",
            "Loss training: 34.501038\n",
            "Loss training: 25.476532\n",
            "Loss training: 27.875269\n",
            "Loss training: 36.144314\n",
            "Loss training: 25.868917\n",
            "Loss training: 17.643795\n",
            "Loss training: 37.982403\n",
            "Loss training: 35.77018\n",
            "\n",
            "***************************\n",
            "Trained on 4450 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 46.494747\n",
            "Loss training: 36.935356\n",
            "Loss training: 50.40405\n",
            "Loss training: 35.715298\n",
            "Loss training: 25.648556\n",
            "Loss training: 36.921917\n",
            "Loss training: 35.89074\n",
            "Loss training: 44.613297\n",
            "Loss training: 35.558548\n",
            "Loss training: 40.65908\n",
            "\n",
            "***************************\n",
            "Trained on 4460 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.861393\n",
            "Loss training: 37.79784\n",
            "Loss training: 27.055536\n",
            "Loss training: 35.28959\n",
            "Loss training: 33.27284\n",
            "Loss training: 32.895645\n",
            "Loss training: 36.86635\n",
            "Loss training: 36.906796\n",
            "Loss training: 24.998537\n",
            "Loss training: 37.7669\n",
            "\n",
            "***************************\n",
            "Trained on 4470 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 46.142178\n",
            "Loss training: 24.29208\n",
            "Loss training: 33.33681\n",
            "Loss training: 33.54209\n",
            "Loss training: 28.836039\n",
            "Loss training: 16.617693\n",
            "Loss training: 28.447762\n",
            "Loss training: 34.617256\n",
            "Loss training: 43.096546\n",
            "Loss training: 28.151636\n",
            "\n",
            "***************************\n",
            "Trained on 4480 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.190174\n",
            "Loss training: 27.048279\n",
            "Loss training: 47.708603\n",
            "Loss training: 42.412354\n",
            "Loss training: 26.73146\n",
            "Loss training: 35.672222\n",
            "Loss training: 53.558247\n",
            "Loss training: 25.131714\n",
            "Loss training: 39.80524\n",
            "Loss training: 27.48248\n",
            "\n",
            "***************************\n",
            "Trained on 4490 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.144382\n",
            "Loss training: 29.836487\n",
            "Loss training: 16.607723\n",
            "Loss training: 24.761045\n",
            "Loss training: 29.232498\n",
            "Loss training: 35.25417\n",
            "Loss training: 29.000269\n",
            "Loss training: 27.30807\n",
            "Loss training: 26.953508\n",
            "Loss training: 35.695255\n",
            "\n",
            "***************************\n",
            "Trained on 4500 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.492449\n",
            "Loss training: 32.427605\n",
            "Loss training: 29.03365\n",
            "Loss training: 26.33186\n",
            "Loss training: 28.363295\n",
            "Loss training: 33.398273\n",
            "Loss training: 35.395737\n",
            "Loss training: 25.173883\n",
            "Loss training: 38.771637\n",
            "Loss training: 34.51351\n",
            "\n",
            "***************************\n",
            "Trained on 4510 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.877514\n",
            "Loss training: 41.069923\n",
            "Loss training: 34.43995\n",
            "Loss training: 36.10535\n",
            "Loss training: 30.053545\n",
            "Loss training: 27.248106\n",
            "Loss training: 33.389458\n",
            "Loss training: 15.699702\n",
            "Loss training: 32.77647\n",
            "Loss training: 25.591227\n",
            "\n",
            "***************************\n",
            "Trained on 4520 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.35298\n",
            "Loss training: 15.1289\n",
            "Loss training: 24.75528\n",
            "Loss training: 32.51885\n",
            "Loss training: 23.931969\n",
            "Loss training: 38.481323\n",
            "Loss training: 33.551743\n",
            "Loss training: 24.423384\n",
            "Loss training: 35.242085\n",
            "Loss training: 39.903442\n",
            "\n",
            "***************************\n",
            "Trained on 4530 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.355804\n",
            "Loss training: 36.153477\n",
            "Loss training: 27.376358\n",
            "Loss training: 24.64802\n",
            "Loss training: 24.608711\n",
            "Loss training: 32.58645\n",
            "Loss training: 25.538338\n",
            "Loss training: 32.019604\n",
            "Loss training: 32.103344\n",
            "Loss training: 26.682932\n",
            "\n",
            "***************************\n",
            "Trained on 4540 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.62302\n",
            "Loss training: 27.958763\n",
            "Loss training: 28.51242\n",
            "Loss training: 24.33386\n",
            "Loss training: 34.365635\n",
            "Loss training: 27.243418\n",
            "Loss training: 28.769827\n",
            "Loss training: 36.979984\n",
            "Loss training: 25.581167\n",
            "Loss training: 35.7187\n",
            "\n",
            "***************************\n",
            "Trained on 4550 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.897749\n",
            "Loss training: 15.051344\n",
            "Loss training: 35.626965\n",
            "Loss training: 34.48418\n",
            "Loss training: 28.422712\n",
            "Loss training: 24.413116\n",
            "Loss training: 32.93395\n",
            "Loss training: 23.58415\n",
            "Loss training: 28.23104\n",
            "Loss training: 16.352575\n",
            "\n",
            "***************************\n",
            "Trained on 4560 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.803278\n",
            "Loss training: 27.64557\n",
            "Loss training: 24.353794\n",
            "Loss training: 15.227024\n",
            "Loss training: 37.387444\n",
            "Loss training: 24.327711\n",
            "Loss training: 23.501442\n",
            "Loss training: 32.297085\n",
            "Loss training: 33.516834\n",
            "Loss training: 14.877074\n",
            "\n",
            "***************************\n",
            "Trained on 4570 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.9995\n",
            "Loss training: 22.972168\n",
            "Loss training: 24.936483\n",
            "Loss training: 29.062363\n",
            "Loss training: 29.560026\n",
            "Loss training: 28.902536\n",
            "Loss training: 37.03741\n",
            "Loss training: 28.8693\n",
            "Loss training: 24.268595\n",
            "Loss training: 16.28365\n",
            "\n",
            "***************************\n",
            "Trained on 4580 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.110756\n",
            "Loss training: 23.933325\n",
            "Loss training: 30.436604\n",
            "Loss training: 38.090004\n",
            "Loss training: 28.859974\n",
            "Loss training: 31.902002\n",
            "Loss training: 32.757023\n",
            "Loss training: 35.493153\n",
            "Loss training: 40.48991\n",
            "Loss training: 25.041773\n",
            "\n",
            "***************************\n",
            "Trained on 4590 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.249348\n",
            "Loss training: 25.528294\n",
            "Loss training: 38.63799\n",
            "Loss training: 28.639063\n",
            "Loss training: 35.264683\n",
            "Loss training: 35.422787\n",
            "Loss training: 33.650826\n",
            "Loss training: 38.71576\n",
            "Loss training: 33.88583\n",
            "Loss training: 34.988846\n",
            "\n",
            "***************************\n",
            "Trained on 4600 graphs\n",
            "***************************\n",
            "\n",
            "\n",
            "*** Full evaluations after 4600 training steps ***\n",
            "Evaluated 10 / 35 subgraphs...\n",
            "Evaluated 20 / 35 subgraphs...\n",
            "Evaluated 30 / 35 subgraphs...\n",
            "\n",
            "Final ROC on the train set 0.8459259432544111\n",
            "Final ROC on the validation set 0.799753350031312\n",
            "Final ROC on the test set 0.755853011716083\n",
            "Loss training: 27.792484\n",
            "Loss training: 27.433226\n",
            "Loss training: 34.85351\n",
            "Loss training: 27.35597\n",
            "Loss training: 37.071377\n",
            "Loss training: 16.153593\n",
            "Loss training: 25.091208\n",
            "Loss training: 26.550592\n",
            "Loss training: 29.703453\n",
            "Loss training: 38.30627\n",
            "\n",
            "***************************\n",
            "Trained on 4610 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.431417\n",
            "Loss training: 44.436745\n",
            "Loss training: 25.977245\n",
            "Loss training: 27.896923\n",
            "Loss training: 36.970802\n",
            "Loss training: 33.284416\n",
            "Loss training: 36.786037\n",
            "Loss training: 38.574905\n",
            "Loss training: 25.504097\n",
            "Loss training: 42.729355\n",
            "\n",
            "***************************\n",
            "Trained on 4620 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.27569\n",
            "Loss training: 30.762182\n",
            "Loss training: 24.968107\n",
            "Loss training: 33.821304\n",
            "Loss training: 16.171307\n",
            "Loss training: 24.537138\n",
            "Loss training: 45.685665\n",
            "Loss training: 38.037895\n",
            "Loss training: 24.316137\n",
            "Loss training: 33.170967\n",
            "\n",
            "***************************\n",
            "Trained on 4630 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.004456\n",
            "Loss training: 27.835745\n",
            "Loss training: 26.934412\n",
            "Loss training: 43.459064\n",
            "Loss training: 15.405206\n",
            "Loss training: 24.10735\n",
            "Loss training: 15.278985\n",
            "Loss training: 37.45608\n",
            "Loss training: 27.802416\n",
            "Loss training: 31.338297\n",
            "\n",
            "***************************\n",
            "Trained on 4640 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.403824\n",
            "Loss training: 17.184803\n",
            "Loss training: 24.243189\n",
            "Loss training: 26.225828\n",
            "Loss training: 43.426945\n",
            "Loss training: 23.408628\n",
            "Loss training: 26.99755\n",
            "Loss training: 35.22269\n",
            "Loss training: 39.45703\n",
            "Loss training: 28.626385\n",
            "\n",
            "***************************\n",
            "Trained on 4650 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.480616\n",
            "Loss training: 25.0286\n",
            "Loss training: 33.771347\n",
            "Loss training: 33.431316\n",
            "Loss training: 40.961536\n",
            "Loss training: 35.82228\n",
            "Loss training: 38.661716\n",
            "Loss training: 36.831196\n",
            "Loss training: 23.604208\n",
            "Loss training: 31.68549\n",
            "\n",
            "***************************\n",
            "Trained on 4660 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.321136\n",
            "Loss training: 33.846855\n",
            "Loss training: 37.087757\n",
            "Loss training: 29.427557\n",
            "Loss training: 38.32938\n",
            "Loss training: 24.479853\n",
            "Loss training: 34.502617\n",
            "Loss training: 32.752186\n",
            "Loss training: 27.171366\n",
            "Loss training: 42.16733\n",
            "\n",
            "***************************\n",
            "Trained on 4670 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.819756\n",
            "Loss training: 23.165354\n",
            "Loss training: 23.101847\n",
            "Loss training: 29.165255\n",
            "Loss training: 24.324081\n",
            "Loss training: 36.372433\n",
            "Loss training: 34.09184\n",
            "Loss training: 27.903437\n",
            "Loss training: 28.525038\n",
            "Loss training: 36.869617\n",
            "\n",
            "***************************\n",
            "Trained on 4680 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.859694\n",
            "Loss training: 37.254143\n",
            "Loss training: 16.561749\n",
            "Loss training: 36.601753\n",
            "Loss training: 23.732197\n",
            "Loss training: 37.046986\n",
            "Loss training: 24.139622\n",
            "Loss training: 30.955313\n",
            "Loss training: 23.277233\n",
            "Loss training: 29.283064\n",
            "\n",
            "***************************\n",
            "Trained on 4690 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.759106\n",
            "Loss training: 33.279816\n",
            "Loss training: 28.10101\n",
            "Loss training: 34.284035\n",
            "Loss training: 35.961914\n",
            "Loss training: 32.430458\n",
            "Loss training: 37.87819\n",
            "Loss training: 36.62057\n",
            "Loss training: 32.446217\n",
            "Loss training: 28.078848\n",
            "\n",
            "***************************\n",
            "Trained on 4700 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 16.212717\n",
            "Loss training: 36.19601\n",
            "Loss training: 28.901598\n",
            "Loss training: 31.038294\n",
            "Loss training: 28.038212\n",
            "Loss training: 27.861729\n",
            "Loss training: 29.71501\n",
            "Loss training: 22.879568\n",
            "Loss training: 26.325254\n",
            "Loss training: 42.587856\n",
            "\n",
            "***************************\n",
            "Trained on 4710 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.707836\n",
            "Loss training: 36.898335\n",
            "Loss training: 27.04005\n",
            "Loss training: 33.974728\n",
            "Loss training: 32.640526\n",
            "Loss training: 31.786867\n",
            "Loss training: 26.770542\n",
            "Loss training: 23.572466\n",
            "Loss training: 49.558693\n",
            "Loss training: 32.15604\n",
            "\n",
            "***************************\n",
            "Trained on 4720 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.509706\n",
            "Loss training: 27.711939\n",
            "Loss training: 28.693716\n",
            "Loss training: 15.979734\n",
            "Loss training: 25.262758\n",
            "Loss training: 24.168097\n",
            "Loss training: 26.868254\n",
            "Loss training: 23.726536\n",
            "Loss training: 23.923225\n",
            "Loss training: 23.704367\n",
            "\n",
            "***************************\n",
            "Trained on 4730 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.235046\n",
            "Loss training: 37.37401\n",
            "Loss training: 41.291782\n",
            "Loss training: 24.559374\n",
            "Loss training: 27.377932\n",
            "Loss training: 31.761503\n",
            "Loss training: 15.723677\n",
            "Loss training: 49.004898\n",
            "Loss training: 37.839035\n",
            "Loss training: 15.508776\n",
            "\n",
            "***************************\n",
            "Trained on 4740 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 15.830279\n",
            "Loss training: 28.96407\n",
            "Loss training: 44.220604\n",
            "Loss training: 25.903774\n",
            "Loss training: 24.817474\n",
            "Loss training: 50.674397\n",
            "Loss training: 27.648432\n",
            "Loss training: 31.686659\n",
            "Loss training: 38.07928\n",
            "Loss training: 15.059978\n",
            "\n",
            "***************************\n",
            "Trained on 4750 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.243732\n",
            "Loss training: 28.634441\n",
            "Loss training: 27.453278\n",
            "Loss training: 36.366116\n",
            "Loss training: 30.440145\n",
            "Loss training: 40.344696\n",
            "Loss training: 36.808704\n",
            "Loss training: 35.195904\n",
            "Loss training: 24.40013\n",
            "Loss training: 35.477703\n",
            "\n",
            "***************************\n",
            "Trained on 4760 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 46.384594\n",
            "Loss training: 15.412244\n",
            "Loss training: 22.753654\n",
            "Loss training: 38.064804\n",
            "Loss training: 27.863382\n",
            "Loss training: 32.988495\n",
            "Loss training: 38.616615\n",
            "Loss training: 28.049664\n",
            "Loss training: 27.086422\n",
            "Loss training: 37.352943\n",
            "\n",
            "***************************\n",
            "Trained on 4770 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.116835\n",
            "Loss training: 36.81016\n",
            "Loss training: 16.14598\n",
            "Loss training: 32.156624\n",
            "Loss training: 32.979935\n",
            "Loss training: 28.42284\n",
            "Loss training: 32.453655\n",
            "Loss training: 36.830357\n",
            "Loss training: 33.9474\n",
            "Loss training: 37.774185\n",
            "\n",
            "***************************\n",
            "Trained on 4780 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.442762\n",
            "Loss training: 38.99418\n",
            "Loss training: 16.351667\n",
            "Loss training: 24.458212\n",
            "Loss training: 44.720413\n",
            "Loss training: 37.695984\n",
            "Loss training: 36.183502\n",
            "Loss training: 26.300917\n",
            "Loss training: 28.528038\n",
            "Loss training: 35.26006\n",
            "\n",
            "***************************\n",
            "Trained on 4790 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.450579\n",
            "Loss training: 27.82503\n",
            "Loss training: 26.405163\n",
            "Loss training: 26.810213\n",
            "Loss training: 24.206387\n",
            "Loss training: 44.27059\n",
            "Loss training: 26.17898\n",
            "Loss training: 34.353764\n",
            "Loss training: 28.332933\n",
            "Loss training: 31.618689\n",
            "\n",
            "***************************\n",
            "Trained on 4800 graphs\n",
            "***************************\n",
            "\n",
            "\n",
            "*** Full evaluations after 4800 training steps ***\n",
            "Evaluated 10 / 35 subgraphs...\n",
            "Evaluated 20 / 35 subgraphs...\n",
            "Evaluated 30 / 35 subgraphs...\n",
            "\n",
            "Final ROC on the train set 0.8493256520294235\n",
            "Final ROC on the validation set 0.80209313980284\n",
            "Final ROC on the test set 0.7653031111985799\n",
            "Loss training: 34.44935\n",
            "Loss training: 29.935387\n",
            "Loss training: 28.539255\n",
            "Loss training: 36.908142\n",
            "Loss training: 24.091555\n",
            "Loss training: 32.873528\n",
            "Loss training: 27.008038\n",
            "Loss training: 24.654686\n",
            "Loss training: 33.853092\n",
            "Loss training: 30.173952\n",
            "\n",
            "***************************\n",
            "Trained on 4810 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.102985\n",
            "Loss training: 35.154217\n",
            "Loss training: 32.855618\n",
            "Loss training: 32.315704\n",
            "Loss training: 28.437908\n",
            "Loss training: 28.660473\n",
            "Loss training: 38.370228\n",
            "Loss training: 28.081945\n",
            "Loss training: 24.761505\n",
            "Loss training: 26.369144\n",
            "\n",
            "***************************\n",
            "Trained on 4820 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.747883\n",
            "Loss training: 27.736225\n",
            "Loss training: 28.425638\n",
            "Loss training: 23.781137\n",
            "Loss training: 14.716107\n",
            "Loss training: 24.552996\n",
            "Loss training: 25.037796\n",
            "Loss training: 26.093817\n",
            "Loss training: 27.152554\n",
            "Loss training: 33.31639\n",
            "\n",
            "***************************\n",
            "Trained on 4830 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.33082\n",
            "Loss training: 28.30115\n",
            "Loss training: 27.06575\n",
            "Loss training: 38.080124\n",
            "Loss training: 26.058836\n",
            "Loss training: 23.616207\n",
            "Loss training: 33.606144\n",
            "Loss training: 37.250042\n",
            "Loss training: 33.259647\n",
            "Loss training: 37.87602\n",
            "\n",
            "***************************\n",
            "Trained on 4840 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.61543\n",
            "Loss training: 27.501352\n",
            "Loss training: 27.908367\n",
            "Loss training: 36.530457\n",
            "Loss training: 33.342728\n",
            "Loss training: 22.95733\n",
            "Loss training: 33.014465\n",
            "Loss training: 32.563923\n",
            "Loss training: 28.422712\n",
            "Loss training: 26.768627\n",
            "\n",
            "***************************\n",
            "Trained on 4850 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.982937\n",
            "Loss training: 28.754276\n",
            "Loss training: 36.310555\n",
            "Loss training: 24.327559\n",
            "Loss training: 26.021675\n",
            "Loss training: 22.919676\n",
            "Loss training: 34.972965\n",
            "Loss training: 33.539753\n",
            "Loss training: 33.35769\n",
            "Loss training: 35.974304\n",
            "\n",
            "***************************\n",
            "Trained on 4860 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.145298\n",
            "Loss training: 36.28046\n",
            "Loss training: 31.587296\n",
            "Loss training: 28.514977\n",
            "Loss training: 24.869068\n",
            "Loss training: 36.21811\n",
            "Loss training: 26.552858\n",
            "Loss training: 26.867887\n",
            "Loss training: 31.294891\n",
            "Loss training: 15.013202\n",
            "\n",
            "***************************\n",
            "Trained on 4870 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.86298\n",
            "Loss training: 46.722393\n",
            "Loss training: 30.093836\n",
            "Loss training: 27.898151\n",
            "Loss training: 37.749226\n",
            "Loss training: 41.280582\n",
            "Loss training: 35.078396\n",
            "Loss training: 36.4371\n",
            "Loss training: 27.864113\n",
            "Loss training: 30.289928\n",
            "\n",
            "***************************\n",
            "Trained on 4880 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.163164\n",
            "Loss training: 37.48824\n",
            "Loss training: 23.726938\n",
            "Loss training: 24.442953\n",
            "Loss training: 35.262672\n",
            "Loss training: 26.724607\n",
            "Loss training: 27.35463\n",
            "Loss training: 26.245735\n",
            "Loss training: 44.07307\n",
            "Loss training: 35.624237\n",
            "\n",
            "***************************\n",
            "Trained on 4890 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.368235\n",
            "Loss training: 28.584187\n",
            "Loss training: 31.847054\n",
            "Loss training: 33.64307\n",
            "Loss training: 27.602253\n",
            "Loss training: 40.954678\n",
            "Loss training: 26.197865\n",
            "Loss training: 16.225405\n",
            "Loss training: 25.980282\n",
            "Loss training: 25.494692\n",
            "\n",
            "***************************\n",
            "Trained on 4900 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.96155\n",
            "Loss training: 28.599003\n",
            "Loss training: 41.064854\n",
            "Loss training: 33.490894\n",
            "Loss training: 23.050959\n",
            "Loss training: 24.100712\n",
            "Loss training: 33.344986\n",
            "Loss training: 28.201433\n",
            "Loss training: 34.3888\n",
            "Loss training: 34.056244\n",
            "\n",
            "***************************\n",
            "Trained on 4910 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.631268\n",
            "Loss training: 32.3219\n",
            "Loss training: 33.362705\n",
            "Loss training: 23.185116\n",
            "Loss training: 31.680223\n",
            "Loss training: 28.170084\n",
            "Loss training: 36.85956\n",
            "Loss training: 39.180588\n",
            "Loss training: 26.630295\n",
            "Loss training: 32.42089\n",
            "\n",
            "***************************\n",
            "Trained on 4920 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.06994\n",
            "Loss training: 27.708933\n",
            "Loss training: 28.086432\n",
            "Loss training: 31.202118\n",
            "Loss training: 38.4838\n",
            "Loss training: 31.567966\n",
            "Loss training: 37.624855\n",
            "Loss training: 26.199766\n",
            "Loss training: 16.106897\n",
            "Loss training: 25.721743\n",
            "\n",
            "***************************\n",
            "Trained on 4930 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.995277\n",
            "Loss training: 14.984483\n",
            "Loss training: 28.833374\n",
            "Loss training: 33.1081\n",
            "Loss training: 40.754612\n",
            "Loss training: 31.93697\n",
            "Loss training: 40.163914\n",
            "Loss training: 28.241604\n",
            "Loss training: 28.557766\n",
            "Loss training: 31.86229\n",
            "\n",
            "***************************\n",
            "Trained on 4940 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.433949\n",
            "Loss training: 32.61108\n",
            "Loss training: 35.350525\n",
            "Loss training: 24.093994\n",
            "Loss training: 28.524963\n",
            "Loss training: 28.0862\n",
            "Loss training: 28.274889\n",
            "Loss training: 27.101818\n",
            "Loss training: 24.68012\n",
            "Loss training: 16.231722\n",
            "\n",
            "***************************\n",
            "Trained on 4950 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.78583\n",
            "Loss training: 27.142859\n",
            "Loss training: 30.640162\n",
            "Loss training: 23.89976\n",
            "Loss training: 30.702112\n",
            "Loss training: 24.211927\n",
            "Loss training: 24.29775\n",
            "Loss training: 26.138853\n",
            "Loss training: 37.501957\n",
            "Loss training: 34.885727\n",
            "\n",
            "***************************\n",
            "Trained on 4960 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.616043\n",
            "Loss training: 35.43274\n",
            "Loss training: 35.85715\n",
            "Loss training: 25.995947\n",
            "Loss training: 26.221666\n",
            "Loss training: 25.881475\n",
            "Loss training: 26.046608\n",
            "Loss training: 15.791489\n",
            "Loss training: 22.34711\n",
            "Loss training: 26.531185\n",
            "\n",
            "***************************\n",
            "Trained on 4970 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.671719\n",
            "Loss training: 34.94261\n",
            "Loss training: 27.887234\n",
            "Loss training: 25.043606\n",
            "Loss training: 35.40893\n",
            "Loss training: 25.400946\n",
            "Loss training: 35.444527\n",
            "Loss training: 35.19327\n",
            "Loss training: 24.038437\n",
            "Loss training: 28.491959\n",
            "\n",
            "***************************\n",
            "Trained on 4980 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.330097\n",
            "Loss training: 41.40704\n",
            "Loss training: 35.969593\n",
            "Loss training: 28.387348\n",
            "Loss training: 23.746523\n",
            "Loss training: 33.053375\n",
            "Loss training: 28.37194\n",
            "Loss training: 32.04383\n",
            "Loss training: 32.440914\n",
            "Loss training: 27.8839\n",
            "\n",
            "***************************\n",
            "Trained on 4990 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.748928\n",
            "Loss training: 34.712574\n",
            "Loss training: 14.621894\n",
            "Loss training: 36.75504\n",
            "Loss training: 27.834589\n",
            "Loss training: 31.314205\n",
            "Loss training: 33.450645\n",
            "Loss training: 30.733692\n",
            "Loss training: 26.56816\n",
            "Loss training: 26.937248\n",
            "\n",
            "***************************\n",
            "Trained on 5000 graphs\n",
            "***************************\n",
            "\n",
            "\n",
            "*** Full evaluations after 5000 training steps ***\n",
            "Evaluated 10 / 35 subgraphs...\n",
            "Evaluated 20 / 35 subgraphs...\n",
            "Evaluated 30 / 35 subgraphs...\n",
            "\n",
            "Final ROC on the train set 0.854578012082577\n",
            "Final ROC on the validation set 0.8085319334596486\n",
            "Final ROC on the test set 0.769176572606605\n",
            "Loss training: 23.40573\n",
            "Loss training: 35.29435\n",
            "Loss training: 26.18803\n",
            "Loss training: 26.819937\n",
            "Loss training: 31.91699\n",
            "Loss training: 34.80588\n",
            "Loss training: 30.961102\n",
            "Loss training: 39.852226\n",
            "Loss training: 23.886555\n",
            "Loss training: 27.683577\n",
            "\n",
            "***************************\n",
            "Trained on 5010 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.77187\n",
            "Loss training: 26.92595\n",
            "Loss training: 37.339035\n",
            "Loss training: 24.4194\n",
            "Loss training: 32.509037\n",
            "Loss training: 31.136156\n",
            "Loss training: 31.799845\n",
            "Loss training: 34.321766\n",
            "Loss training: 23.796223\n",
            "Loss training: 37.378853\n",
            "\n",
            "***************************\n",
            "Trained on 5020 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.241817\n",
            "Loss training: 35.48112\n",
            "Loss training: 28.401018\n",
            "Loss training: 23.47929\n",
            "Loss training: 16.300665\n",
            "Loss training: 24.660614\n",
            "Loss training: 32.56071\n",
            "Loss training: 28.156586\n",
            "Loss training: 23.223515\n",
            "Loss training: 37.130413\n",
            "\n",
            "***************************\n",
            "Trained on 5030 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.615553\n",
            "Loss training: 30.54726\n",
            "Loss training: 32.946144\n",
            "Loss training: 27.60902\n",
            "Loss training: 36.925808\n",
            "Loss training: 28.02213\n",
            "Loss training: 26.15188\n",
            "Loss training: 36.284237\n",
            "Loss training: 32.207943\n",
            "Loss training: 26.68936\n",
            "\n",
            "***************************\n",
            "Trained on 5040 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.972336\n",
            "Loss training: 23.83308\n",
            "Loss training: 35.597317\n",
            "Loss training: 36.06066\n",
            "Loss training: 37.3703\n",
            "Loss training: 31.810492\n",
            "Loss training: 22.986378\n",
            "Loss training: 41.14079\n",
            "Loss training: 23.374725\n",
            "Loss training: 26.350157\n",
            "\n",
            "***************************\n",
            "Trained on 5050 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.162052\n",
            "Loss training: 33.089222\n",
            "Loss training: 36.950573\n",
            "Loss training: 35.515923\n",
            "Loss training: 36.762207\n",
            "Loss training: 23.156454\n",
            "Loss training: 34.845787\n",
            "Loss training: 36.74005\n",
            "Loss training: 23.006021\n",
            "Loss training: 36.4755\n",
            "\n",
            "***************************\n",
            "Trained on 5060 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.426197\n",
            "Loss training: 23.021324\n",
            "Loss training: 22.729792\n",
            "Loss training: 31.838842\n",
            "Loss training: 29.236385\n",
            "Loss training: 32.415325\n",
            "Loss training: 23.00067\n",
            "Loss training: 28.050484\n",
            "Loss training: 29.201883\n",
            "Loss training: 26.314524\n",
            "\n",
            "***************************\n",
            "Trained on 5070 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.171286\n",
            "Loss training: 43.27743\n",
            "Loss training: 15.358001\n",
            "Loss training: 22.69274\n",
            "Loss training: 27.923616\n",
            "Loss training: 37.127403\n",
            "Loss training: 24.595997\n",
            "Loss training: 16.811483\n",
            "Loss training: 22.374474\n",
            "Loss training: 27.251812\n",
            "\n",
            "***************************\n",
            "Trained on 5080 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.885008\n",
            "Loss training: 30.149256\n",
            "Loss training: 26.963863\n",
            "Loss training: 27.428438\n",
            "Loss training: 35.831753\n",
            "Loss training: 31.055086\n",
            "Loss training: 24.259705\n",
            "Loss training: 25.658266\n",
            "Loss training: 40.98489\n",
            "Loss training: 27.539843\n",
            "\n",
            "***************************\n",
            "Trained on 5090 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.39048\n",
            "Loss training: 34.15773\n",
            "Loss training: 32.216724\n",
            "Loss training: 28.122515\n",
            "Loss training: 28.22132\n",
            "Loss training: 26.690313\n",
            "Loss training: 41.660686\n",
            "Loss training: 25.075098\n",
            "Loss training: 40.603035\n",
            "Loss training: 26.844217\n",
            "\n",
            "***************************\n",
            "Trained on 5100 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.209059\n",
            "Loss training: 43.640285\n",
            "Loss training: 27.959557\n",
            "Loss training: 33.788574\n",
            "Loss training: 25.253729\n",
            "Loss training: 33.1523\n",
            "Loss training: 35.56478\n",
            "Loss training: 36.60784\n",
            "Loss training: 35.288895\n",
            "Loss training: 33.979702\n",
            "\n",
            "***************************\n",
            "Trained on 5110 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.089851\n",
            "Loss training: 36.314087\n",
            "Loss training: 27.989454\n",
            "Loss training: 27.933401\n",
            "Loss training: 36.2588\n",
            "Loss training: 31.79189\n",
            "Loss training: 35.6271\n",
            "Loss training: 32.679367\n",
            "Loss training: 28.80698\n",
            "Loss training: 33.431408\n",
            "\n",
            "***************************\n",
            "Trained on 5120 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.123756\n",
            "Loss training: 28.761251\n",
            "Loss training: 34.16711\n",
            "Loss training: 35.518147\n",
            "Loss training: 25.397938\n",
            "Loss training: 38.15517\n",
            "Loss training: 48.551548\n",
            "Loss training: 28.13256\n",
            "Loss training: 17.55435\n",
            "Loss training: 38.481976\n",
            "\n",
            "***************************\n",
            "Trained on 5130 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.23933\n",
            "Loss training: 28.513132\n",
            "Loss training: 42.517628\n",
            "Loss training: 26.158922\n",
            "Loss training: 15.467232\n",
            "Loss training: 27.958897\n",
            "Loss training: 32.91514\n",
            "Loss training: 36.49679\n",
            "Loss training: 43.119545\n",
            "Loss training: 23.392582\n",
            "\n",
            "***************************\n",
            "Trained on 5140 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.38033\n",
            "Loss training: 23.337118\n",
            "Loss training: 37.564545\n",
            "Loss training: 32.44987\n",
            "Loss training: 26.92752\n",
            "Loss training: 36.159046\n",
            "Loss training: 35.584194\n",
            "Loss training: 37.548885\n",
            "Loss training: 26.575209\n",
            "Loss training: 28.59363\n",
            "\n",
            "***************************\n",
            "Trained on 5150 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.08427\n",
            "Loss training: 33.338577\n",
            "Loss training: 35.386604\n",
            "Loss training: 33.908936\n",
            "Loss training: 26.941729\n",
            "Loss training: 29.48843\n",
            "Loss training: 30.935823\n",
            "Loss training: 37.116215\n",
            "Loss training: 32.84649\n",
            "Loss training: 24.94278\n",
            "\n",
            "***************************\n",
            "Trained on 5160 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.213715\n",
            "Loss training: 34.076313\n",
            "Loss training: 27.635826\n",
            "Loss training: 27.090012\n",
            "Loss training: 26.557554\n",
            "Loss training: 30.643818\n",
            "Loss training: 26.708832\n",
            "Loss training: 35.983543\n",
            "Loss training: 16.243006\n",
            "Loss training: 38.768715\n",
            "\n",
            "***************************\n",
            "Trained on 5170 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.29287\n",
            "Loss training: 28.084589\n",
            "Loss training: 33.23472\n",
            "Loss training: 14.853223\n",
            "Loss training: 32.421795\n",
            "Loss training: 30.13656\n",
            "Loss training: 33.026417\n",
            "Loss training: 24.991957\n",
            "Loss training: 23.799496\n",
            "Loss training: 39.91621\n",
            "\n",
            "***************************\n",
            "Trained on 5180 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.374758\n",
            "Loss training: 29.09599\n",
            "Loss training: 31.973295\n",
            "Loss training: 17.026823\n",
            "Loss training: 26.52246\n",
            "Loss training: 26.19954\n",
            "Loss training: 23.720926\n",
            "Loss training: 30.345636\n",
            "Loss training: 33.02868\n",
            "Loss training: 35.89881\n",
            "\n",
            "***************************\n",
            "Trained on 5190 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.437927\n",
            "Loss training: 23.06044\n",
            "Loss training: 28.154114\n",
            "Loss training: 22.748096\n",
            "Loss training: 35.19095\n",
            "Loss training: 27.729383\n",
            "Loss training: 22.389174\n",
            "Loss training: 30.221754\n",
            "Loss training: 29.077675\n",
            "Loss training: 22.17357\n",
            "\n",
            "***************************\n",
            "Trained on 5200 graphs\n",
            "***************************\n",
            "\n",
            "\n",
            "*** Full evaluations after 5200 training steps ***\n",
            "Evaluated 10 / 35 subgraphs...\n",
            "Evaluated 20 / 35 subgraphs...\n",
            "Evaluated 30 / 35 subgraphs...\n",
            "\n",
            "Final ROC on the train set 0.8554163562760798\n",
            "Final ROC on the validation set 0.8057324872205499\n",
            "Final ROC on the test set 0.7655483993098857\n",
            "Loss training: 22.936106\n",
            "Loss training: 27.021849\n",
            "Loss training: 23.825897\n",
            "Loss training: 26.374014\n",
            "Loss training: 42.089375\n",
            "Loss training: 34.386875\n",
            "Loss training: 25.089249\n",
            "Loss training: 24.76616\n",
            "Loss training: 25.958738\n",
            "Loss training: 25.67565\n",
            "\n",
            "***************************\n",
            "Trained on 5210 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.706165\n",
            "Loss training: 25.59698\n",
            "Loss training: 16.149942\n",
            "Loss training: 15.867618\n",
            "Loss training: 30.61692\n",
            "Loss training: 36.945812\n",
            "Loss training: 15.110966\n",
            "Loss training: 25.771181\n",
            "Loss training: 27.97011\n",
            "Loss training: 33.032185\n",
            "\n",
            "***************************\n",
            "Trained on 5220 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.676018\n",
            "Loss training: 23.999132\n",
            "Loss training: 32.588223\n",
            "Loss training: 14.81471\n",
            "Loss training: 31.75539\n",
            "Loss training: 31.601109\n",
            "Loss training: 31.4091\n",
            "Loss training: 32.145393\n",
            "Loss training: 26.324718\n",
            "Loss training: 38.34296\n",
            "\n",
            "***************************\n",
            "Trained on 5230 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.422573\n",
            "Loss training: 22.151796\n",
            "Loss training: 35.467075\n",
            "Loss training: 31.434185\n",
            "Loss training: 27.259308\n",
            "Loss training: 23.526237\n",
            "Loss training: 24.381536\n",
            "Loss training: 32.80978\n",
            "Loss training: 30.76875\n",
            "Loss training: 22.478079\n",
            "\n",
            "***************************\n",
            "Trained on 5240 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.770693\n",
            "Loss training: 29.377005\n",
            "Loss training: 31.616713\n",
            "Loss training: 33.143578\n",
            "Loss training: 15.93607\n",
            "Loss training: 29.000143\n",
            "Loss training: 14.273504\n",
            "Loss training: 15.396156\n",
            "Loss training: 31.122316\n",
            "Loss training: 30.29604\n",
            "\n",
            "***************************\n",
            "Trained on 5250 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.747602\n",
            "Loss training: 41.99458\n",
            "Loss training: 24.664734\n",
            "Loss training: 36.610123\n",
            "Loss training: 32.173996\n",
            "Loss training: 28.7695\n",
            "Loss training: 30.650805\n",
            "Loss training: 26.89593\n",
            "Loss training: 22.787874\n",
            "Loss training: 26.97658\n",
            "\n",
            "***************************\n",
            "Trained on 5260 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.712975\n",
            "Loss training: 24.188421\n",
            "Loss training: 28.521702\n",
            "Loss training: 23.988232\n",
            "Loss training: 38.436867\n",
            "Loss training: 29.334753\n",
            "Loss training: 38.555004\n",
            "Loss training: 28.657385\n",
            "Loss training: 34.495796\n",
            "Loss training: 35.138973\n",
            "\n",
            "***************************\n",
            "Trained on 5270 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.67728\n",
            "Loss training: 35.478874\n",
            "Loss training: 26.455997\n",
            "Loss training: 29.892492\n",
            "Loss training: 35.349663\n",
            "Loss training: 26.797287\n",
            "Loss training: 33.560078\n",
            "Loss training: 25.905512\n",
            "Loss training: 27.932726\n",
            "Loss training: 38.252872\n",
            "\n",
            "***************************\n",
            "Trained on 5280 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.102322\n",
            "Loss training: 41.49656\n",
            "Loss training: 35.506874\n",
            "Loss training: 28.807323\n",
            "Loss training: 32.41889\n",
            "Loss training: 40.950123\n",
            "Loss training: 30.43802\n",
            "Loss training: 24.585508\n",
            "Loss training: 23.077364\n",
            "Loss training: 41.09845\n",
            "\n",
            "***************************\n",
            "Trained on 5290 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.078777\n",
            "Loss training: 27.711878\n",
            "Loss training: 27.73451\n",
            "Loss training: 29.185434\n",
            "Loss training: 35.017143\n",
            "Loss training: 38.413395\n",
            "Loss training: 36.757267\n",
            "Loss training: 25.11221\n",
            "Loss training: 26.260342\n",
            "Loss training: 35.238644\n",
            "\n",
            "***************************\n",
            "Trained on 5300 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.936817\n",
            "Loss training: 28.435854\n",
            "Loss training: 15.477203\n",
            "Loss training: 26.623596\n",
            "Loss training: 35.263294\n",
            "Loss training: 35.198288\n",
            "Loss training: 23.724173\n",
            "Loss training: 29.018135\n",
            "Loss training: 28.381985\n",
            "Loss training: 28.835516\n",
            "\n",
            "***************************\n",
            "Trained on 5310 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.413536\n",
            "Loss training: 35.28898\n",
            "Loss training: 26.73201\n",
            "Loss training: 23.68353\n",
            "Loss training: 24.513416\n",
            "Loss training: 36.789753\n",
            "Loss training: 24.487852\n",
            "Loss training: 35.375523\n",
            "Loss training: 27.955006\n",
            "Loss training: 36.332424\n",
            "\n",
            "***************************\n",
            "Trained on 5320 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.515501\n",
            "Loss training: 25.341066\n",
            "Loss training: 35.329002\n",
            "Loss training: 22.994905\n",
            "Loss training: 24.398994\n",
            "Loss training: 24.17938\n",
            "Loss training: 32.78403\n",
            "Loss training: 22.896801\n",
            "Loss training: 34.15805\n",
            "Loss training: 26.700905\n",
            "\n",
            "***************************\n",
            "Trained on 5330 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.76669\n",
            "Loss training: 26.188406\n",
            "Loss training: 23.756512\n",
            "Loss training: 37.249863\n",
            "Loss training: 15.266228\n",
            "Loss training: 33.114304\n",
            "Loss training: 31.365019\n",
            "Loss training: 34.368504\n",
            "Loss training: 40.548103\n",
            "Loss training: 23.803217\n",
            "\n",
            "***************************\n",
            "Trained on 5340 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.573868\n",
            "Loss training: 23.548517\n",
            "Loss training: 28.187542\n",
            "Loss training: 28.544579\n",
            "Loss training: 34.904007\n",
            "Loss training: 27.486422\n",
            "Loss training: 29.694613\n",
            "Loss training: 35.48069\n",
            "Loss training: 32.98895\n",
            "Loss training: 35.249897\n",
            "\n",
            "***************************\n",
            "Trained on 5350 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.574028\n",
            "Loss training: 32.20903\n",
            "Loss training: 26.67181\n",
            "Loss training: 27.909002\n",
            "Loss training: 33.153305\n",
            "Loss training: 30.39069\n",
            "Loss training: 15.688031\n",
            "Loss training: 35.727367\n",
            "Loss training: 37.232216\n",
            "Loss training: 29.643116\n",
            "\n",
            "***************************\n",
            "Trained on 5360 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.881853\n",
            "Loss training: 16.074915\n",
            "Loss training: 28.070732\n",
            "Loss training: 37.012783\n",
            "Loss training: 24.1751\n",
            "Loss training: 28.145758\n",
            "Loss training: 37.034233\n",
            "Loss training: 26.902338\n",
            "Loss training: 34.43562\n",
            "Loss training: 26.700596\n",
            "\n",
            "***************************\n",
            "Trained on 5370 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.249542\n",
            "Loss training: 30.944227\n",
            "Loss training: 34.724163\n",
            "Loss training: 36.816788\n",
            "Loss training: 36.16381\n",
            "Loss training: 32.448513\n",
            "Loss training: 17.754528\n",
            "Loss training: 23.969234\n",
            "Loss training: 23.645496\n",
            "Loss training: 40.7416\n",
            "\n",
            "***************************\n",
            "Trained on 5380 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.55993\n",
            "Loss training: 30.12787\n",
            "Loss training: 27.378704\n",
            "Loss training: 32.666645\n",
            "Loss training: 36.25221\n",
            "Loss training: 30.53053\n",
            "Loss training: 25.970057\n",
            "Loss training: 26.544594\n",
            "Loss training: 24.107698\n",
            "Loss training: 44.204903\n",
            "\n",
            "***************************\n",
            "Trained on 5390 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.60343\n",
            "Loss training: 41.541943\n",
            "Loss training: 41.527927\n",
            "Loss training: 38.554058\n",
            "Loss training: 22.999132\n",
            "Loss training: 25.998508\n",
            "Loss training: 15.902643\n",
            "Loss training: 34.110256\n",
            "Loss training: 33.2278\n",
            "Loss training: 15.639925\n",
            "\n",
            "***************************\n",
            "Trained on 5400 graphs\n",
            "***************************\n",
            "\n",
            "\n",
            "*** Full evaluations after 5400 training steps ***\n",
            "Evaluated 10 / 35 subgraphs...\n",
            "Evaluated 20 / 35 subgraphs...\n",
            "Evaluated 30 / 35 subgraphs...\n",
            "\n",
            "Final ROC on the train set 0.8510219242446257\n",
            "Final ROC on the validation set 0.8054011332038736\n",
            "Final ROC on the test set 0.7632438427247864\n",
            "Loss training: 28.834455\n",
            "Loss training: 28.814568\n",
            "Loss training: 28.982672\n",
            "Loss training: 46.8054\n",
            "Loss training: 39.665165\n",
            "Loss training: 29.143148\n",
            "Loss training: 39.083996\n",
            "Loss training: 32.528698\n",
            "Loss training: 42.60243\n",
            "Loss training: 23.350899\n",
            "\n",
            "***************************\n",
            "Trained on 5410 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.885561\n",
            "Loss training: 36.323673\n",
            "Loss training: 31.109173\n",
            "Loss training: 27.409393\n",
            "Loss training: 24.78728\n",
            "Loss training: 32.587147\n",
            "Loss training: 29.528791\n",
            "Loss training: 31.732754\n",
            "Loss training: 31.36681\n",
            "Loss training: 24.874134\n",
            "\n",
            "***************************\n",
            "Trained on 5420 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.700264\n",
            "Loss training: 42.55451\n",
            "Loss training: 27.968708\n",
            "Loss training: 28.476475\n",
            "Loss training: 24.861814\n",
            "Loss training: 35.371\n",
            "Loss training: 30.24499\n",
            "Loss training: 24.705442\n",
            "Loss training: 28.01903\n",
            "Loss training: 15.798312\n",
            "\n",
            "***************************\n",
            "Trained on 5430 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 15.406029\n",
            "Loss training: 33.448307\n",
            "Loss training: 35.141098\n",
            "Loss training: 26.603838\n",
            "Loss training: 27.682743\n",
            "Loss training: 27.882471\n",
            "Loss training: 27.998499\n",
            "Loss training: 45.470573\n",
            "Loss training: 35.067703\n",
            "Loss training: 43.30345\n",
            "\n",
            "***************************\n",
            "Trained on 5440 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.238483\n",
            "Loss training: 31.436827\n",
            "Loss training: 36.925762\n",
            "Loss training: 26.345284\n",
            "Loss training: 27.626068\n",
            "Loss training: 24.7741\n",
            "Loss training: 34.498554\n",
            "Loss training: 34.169968\n",
            "Loss training: 25.057375\n",
            "Loss training: 35.177162\n",
            "\n",
            "***************************\n",
            "Trained on 5450 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.13176\n",
            "Loss training: 27.938604\n",
            "Loss training: 32.341663\n",
            "Loss training: 31.996218\n",
            "Loss training: 34.005398\n",
            "Loss training: 32.565002\n",
            "Loss training: 32.864094\n",
            "Loss training: 23.167639\n",
            "Loss training: 31.05293\n",
            "Loss training: 28.11783\n",
            "\n",
            "***************************\n",
            "Trained on 5460 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.639376\n",
            "Loss training: 39.230873\n",
            "Loss training: 31.816069\n",
            "Loss training: 28.727304\n",
            "Loss training: 17.77369\n",
            "Loss training: 38.61809\n",
            "Loss training: 29.093573\n",
            "Loss training: 31.126234\n",
            "Loss training: 26.72881\n",
            "Loss training: 36.54437\n",
            "\n",
            "***************************\n",
            "Trained on 5470 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.678158\n",
            "Loss training: 28.748068\n",
            "Loss training: 30.756508\n",
            "Loss training: 27.605612\n",
            "Loss training: 42.09717\n",
            "Loss training: 26.599825\n",
            "Loss training: 16.175444\n",
            "Loss training: 34.060204\n",
            "Loss training: 30.183321\n",
            "Loss training: 25.591965\n",
            "\n",
            "***************************\n",
            "Trained on 5480 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.741512\n",
            "Loss training: 23.881899\n",
            "Loss training: 23.88215\n",
            "Loss training: 24.361044\n",
            "Loss training: 24.271751\n",
            "Loss training: 30.618824\n",
            "Loss training: 29.29689\n",
            "Loss training: 32.99012\n",
            "Loss training: 15.128223\n",
            "Loss training: 16.084753\n",
            "\n",
            "***************************\n",
            "Trained on 5490 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.021427\n",
            "Loss training: 34.131065\n",
            "Loss training: 26.786768\n",
            "Loss training: 29.190605\n",
            "Loss training: 36.382847\n",
            "Loss training: 30.250378\n",
            "Loss training: 27.983065\n",
            "Loss training: 27.889803\n",
            "Loss training: 29.969694\n",
            "Loss training: 23.247051\n",
            "\n",
            "***************************\n",
            "Trained on 5500 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.422457\n",
            "Loss training: 27.499008\n",
            "Loss training: 32.58892\n",
            "Loss training: 27.526583\n",
            "Loss training: 36.772526\n",
            "Loss training: 34.80079\n",
            "Loss training: 36.263454\n",
            "Loss training: 36.656914\n",
            "Loss training: 33.58961\n",
            "Loss training: 35.273636\n",
            "\n",
            "***************************\n",
            "Trained on 5510 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.3554\n",
            "Loss training: 27.460354\n",
            "Loss training: 30.913582\n",
            "Loss training: 30.51124\n",
            "Loss training: 28.89932\n",
            "Loss training: 30.921412\n",
            "Loss training: 28.317638\n",
            "Loss training: 27.247856\n",
            "Loss training: 33.976204\n",
            "Loss training: 27.593933\n",
            "\n",
            "***************************\n",
            "Trained on 5520 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.64686\n",
            "Loss training: 28.087107\n",
            "Loss training: 33.369236\n",
            "Loss training: 28.997696\n",
            "Loss training: 23.878336\n",
            "Loss training: 27.447609\n",
            "Loss training: 24.49132\n",
            "Loss training: 27.74248\n",
            "Loss training: 30.000412\n",
            "Loss training: 32.67377\n",
            "\n",
            "***************************\n",
            "Trained on 5530 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.420757\n",
            "Loss training: 31.087313\n",
            "Loss training: 28.70871\n",
            "Loss training: 32.42884\n",
            "Loss training: 33.771755\n",
            "Loss training: 27.942694\n",
            "Loss training: 29.697296\n",
            "Loss training: 33.751366\n",
            "Loss training: 29.502117\n",
            "Loss training: 27.838985\n",
            "\n",
            "***************************\n",
            "Trained on 5540 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.110092\n",
            "Loss training: 27.388802\n",
            "Loss training: 27.129105\n",
            "Loss training: 42.275764\n",
            "Loss training: 15.910214\n",
            "Loss training: 35.648216\n",
            "Loss training: 25.94029\n",
            "Loss training: 35.487537\n",
            "Loss training: 24.434042\n",
            "Loss training: 15.5589485\n",
            "\n",
            "***************************\n",
            "Trained on 5550 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.051577\n",
            "Loss training: 23.259075\n",
            "Loss training: 14.433961\n",
            "Loss training: 24.113104\n",
            "Loss training: 27.821716\n",
            "Loss training: 36.0277\n",
            "Loss training: 33.251183\n",
            "Loss training: 27.794386\n",
            "Loss training: 37.112507\n",
            "Loss training: 26.504011\n",
            "\n",
            "***************************\n",
            "Trained on 5560 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.248415\n",
            "Loss training: 13.635639\n",
            "Loss training: 27.262089\n",
            "Loss training: 40.83419\n",
            "Loss training: 27.331602\n",
            "Loss training: 23.873293\n",
            "Loss training: 40.96563\n",
            "Loss training: 30.653946\n",
            "Loss training: 30.665125\n",
            "Loss training: 36.22349\n",
            "\n",
            "***************************\n",
            "Trained on 5570 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.49138\n",
            "Loss training: 37.908367\n",
            "Loss training: 24.280323\n",
            "Loss training: 33.771297\n",
            "Loss training: 34.329002\n",
            "Loss training: 37.669758\n",
            "Loss training: 31.057583\n",
            "Loss training: 23.33808\n",
            "Loss training: 36.98668\n",
            "Loss training: 37.65615\n",
            "\n",
            "***************************\n",
            "Trained on 5580 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.81618\n",
            "Loss training: 24.145914\n",
            "Loss training: 24.377644\n",
            "Loss training: 38.345795\n",
            "Loss training: 26.335197\n",
            "Loss training: 29.877867\n",
            "Loss training: 30.519808\n",
            "Loss training: 28.226713\n",
            "Loss training: 34.985172\n",
            "Loss training: 36.38592\n",
            "\n",
            "***************************\n",
            "Trained on 5590 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.510004\n",
            "Loss training: 24.029022\n",
            "Loss training: 35.868557\n",
            "Loss training: 29.086672\n",
            "Loss training: 31.155594\n",
            "Loss training: 28.50713\n",
            "Loss training: 29.623302\n",
            "Loss training: 27.778055\n",
            "Loss training: 32.733234\n",
            "Loss training: 33.352154\n",
            "\n",
            "***************************\n",
            "Trained on 5600 graphs\n",
            "***************************\n",
            "\n",
            "\n",
            "*** Full evaluations after 5600 training steps ***\n",
            "Evaluated 10 / 35 subgraphs...\n",
            "Evaluated 20 / 35 subgraphs...\n",
            "Evaluated 30 / 35 subgraphs...\n",
            "\n",
            "Final ROC on the train set 0.861182057807318\n",
            "Final ROC on the validation set 0.8157597866262026\n",
            "Final ROC on the test set 0.7688841926900017\n",
            "Loss training: 33.00075\n",
            "Loss training: 26.167953\n",
            "Loss training: 36.927177\n",
            "Loss training: 27.281933\n",
            "Loss training: 23.589481\n",
            "Loss training: 23.490284\n",
            "Loss training: 30.037952\n",
            "Loss training: 25.946482\n",
            "Loss training: 28.112589\n",
            "Loss training: 35.004158\n",
            "\n",
            "***************************\n",
            "Trained on 5610 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.02643\n",
            "Loss training: 41.308964\n",
            "Loss training: 29.636003\n",
            "Loss training: 25.04412\n",
            "Loss training: 27.643362\n",
            "Loss training: 30.119925\n",
            "Loss training: 26.931122\n",
            "Loss training: 23.093258\n",
            "Loss training: 40.128273\n",
            "Loss training: 25.99507\n",
            "\n",
            "***************************\n",
            "Trained on 5620 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.734297\n",
            "Loss training: 35.97669\n",
            "Loss training: 30.815819\n",
            "Loss training: 25.345375\n",
            "Loss training: 30.02766\n",
            "Loss training: 26.20819\n",
            "Loss training: 40.45326\n",
            "Loss training: 31.051147\n",
            "Loss training: 37.1147\n",
            "Loss training: 28.491304\n",
            "\n",
            "***************************\n",
            "Trained on 5630 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.673359\n",
            "Loss training: 17.126995\n",
            "Loss training: 26.545528\n",
            "Loss training: 26.320654\n",
            "Loss training: 23.031677\n",
            "Loss training: 31.270897\n",
            "Loss training: 24.260012\n",
            "Loss training: 39.780518\n",
            "Loss training: 31.015762\n",
            "Loss training: 29.03405\n",
            "\n",
            "***************************\n",
            "Trained on 5640 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.289053\n",
            "Loss training: 23.031197\n",
            "Loss training: 30.710665\n",
            "Loss training: 35.6772\n",
            "Loss training: 26.906979\n",
            "Loss training: 23.880838\n",
            "Loss training: 41.231625\n",
            "Loss training: 31.950449\n",
            "Loss training: 24.429144\n",
            "Loss training: 28.683893\n",
            "\n",
            "***************************\n",
            "Trained on 5650 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.853128\n",
            "Loss training: 31.665232\n",
            "Loss training: 36.22352\n",
            "Loss training: 23.690264\n",
            "Loss training: 30.407927\n",
            "Loss training: 24.21337\n",
            "Loss training: 38.743607\n",
            "Loss training: 29.358791\n",
            "Loss training: 23.704596\n",
            "Loss training: 25.263159\n",
            "\n",
            "***************************\n",
            "Trained on 5660 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.929714\n",
            "Loss training: 37.915333\n",
            "Loss training: 37.062\n",
            "Loss training: 27.62675\n",
            "Loss training: 21.933922\n",
            "Loss training: 28.054108\n",
            "Loss training: 26.33935\n",
            "Loss training: 26.12458\n",
            "Loss training: 34.723633\n",
            "Loss training: 34.68836\n",
            "\n",
            "***************************\n",
            "Trained on 5670 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.993423\n",
            "Loss training: 23.186838\n",
            "Loss training: 34.03353\n",
            "Loss training: 29.04776\n",
            "Loss training: 27.283348\n",
            "Loss training: 25.574518\n",
            "Loss training: 29.804562\n",
            "Loss training: 29.68469\n",
            "Loss training: 22.27943\n",
            "Loss training: 26.002594\n",
            "\n",
            "***************************\n",
            "Trained on 5680 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.622334\n",
            "Loss training: 27.565214\n",
            "Loss training: 36.97953\n",
            "Loss training: 23.995567\n",
            "Loss training: 22.474203\n",
            "Loss training: 39.64844\n",
            "Loss training: 36.85451\n",
            "Loss training: 26.502045\n",
            "Loss training: 36.622818\n",
            "Loss training: 26.91966\n",
            "\n",
            "***************************\n",
            "Trained on 5690 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.306665\n",
            "Loss training: 24.30101\n",
            "Loss training: 39.53569\n",
            "Loss training: 33.814156\n",
            "Loss training: 26.077457\n",
            "Loss training: 22.2694\n",
            "Loss training: 23.560932\n",
            "Loss training: 26.524912\n",
            "Loss training: 23.446058\n",
            "Loss training: 23.144608\n",
            "\n",
            "***************************\n",
            "Trained on 5700 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.869873\n",
            "Loss training: 22.07351\n",
            "Loss training: 35.19474\n",
            "Loss training: 27.256031\n",
            "Loss training: 24.039968\n",
            "Loss training: 31.4658\n",
            "Loss training: 22.966991\n",
            "Loss training: 28.110777\n",
            "Loss training: 23.655851\n",
            "Loss training: 27.067822\n",
            "\n",
            "***************************\n",
            "Trained on 5710 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 16.065886\n",
            "Loss training: 26.69537\n",
            "Loss training: 40.762424\n",
            "Loss training: 21.897808\n",
            "Loss training: 21.594418\n",
            "Loss training: 27.168472\n",
            "Loss training: 41.305275\n",
            "Loss training: 32.53858\n",
            "Loss training: 26.86912\n",
            "Loss training: 33.102016\n",
            "\n",
            "***************************\n",
            "Trained on 5720 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.200552\n",
            "Loss training: 22.21775\n",
            "Loss training: 38.850487\n",
            "Loss training: 22.43442\n",
            "Loss training: 16.326662\n",
            "Loss training: 28.774675\n",
            "Loss training: 22.472637\n",
            "Loss training: 23.724714\n",
            "Loss training: 28.02122\n",
            "Loss training: 36.014275\n",
            "\n",
            "***************************\n",
            "Trained on 5730 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 21.587795\n",
            "Loss training: 34.625618\n",
            "Loss training: 24.537552\n",
            "Loss training: 28.456598\n",
            "Loss training: 32.16291\n",
            "Loss training: 23.342\n",
            "Loss training: 21.369026\n",
            "Loss training: 27.613966\n",
            "Loss training: 32.999012\n",
            "Loss training: 25.918243\n",
            "\n",
            "***************************\n",
            "Trained on 5740 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.2786\n",
            "Loss training: 15.488242\n",
            "Loss training: 15.381147\n",
            "Loss training: 35.95369\n",
            "Loss training: 27.872112\n",
            "Loss training: 33.953224\n",
            "Loss training: 33.45424\n",
            "Loss training: 30.95681\n",
            "Loss training: 25.981112\n",
            "Loss training: 24.452946\n",
            "\n",
            "***************************\n",
            "Trained on 5750 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.666515\n",
            "Loss training: 35.833813\n",
            "Loss training: 28.998842\n",
            "Loss training: 28.678768\n",
            "Loss training: 14.730711\n",
            "Loss training: 28.36622\n",
            "Loss training: 27.859318\n",
            "Loss training: 36.06447\n",
            "Loss training: 27.562094\n",
            "Loss training: 35.764084\n",
            "\n",
            "***************************\n",
            "Trained on 5760 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.87489\n",
            "Loss training: 34.1229\n",
            "Loss training: 40.403725\n",
            "Loss training: 34.17931\n",
            "Loss training: 30.538479\n",
            "Loss training: 38.55344\n",
            "Loss training: 32.597595\n",
            "Loss training: 33.441246\n",
            "Loss training: 30.330835\n",
            "Loss training: 31.80841\n",
            "\n",
            "***************************\n",
            "Trained on 5770 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.257032\n",
            "Loss training: 35.44757\n",
            "Loss training: 29.134674\n",
            "Loss training: 31.031141\n",
            "Loss training: 15.188838\n",
            "Loss training: 26.380144\n",
            "Loss training: 40.283646\n",
            "Loss training: 39.559025\n",
            "Loss training: 23.675486\n",
            "Loss training: 33.18784\n",
            "\n",
            "***************************\n",
            "Trained on 5780 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.326672\n",
            "Loss training: 35.352257\n",
            "Loss training: 36.370476\n",
            "Loss training: 29.772652\n",
            "Loss training: 23.425722\n",
            "Loss training: 35.86298\n",
            "Loss training: 40.238743\n",
            "Loss training: 34.286938\n",
            "Loss training: 38.919174\n",
            "Loss training: 31.81489\n",
            "\n",
            "***************************\n",
            "Trained on 5790 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.339333\n",
            "Loss training: 23.312849\n",
            "Loss training: 34.65457\n",
            "Loss training: 39.984215\n",
            "Loss training: 14.940802\n",
            "Loss training: 34.771328\n",
            "Loss training: 22.178211\n",
            "Loss training: 14.578144\n",
            "Loss training: 14.313377\n",
            "Loss training: 34.685726\n",
            "\n",
            "***************************\n",
            "Trained on 5800 graphs\n",
            "***************************\n",
            "\n",
            "\n",
            "*** Full evaluations after 5800 training steps ***\n",
            "Evaluated 10 / 35 subgraphs...\n",
            "Evaluated 20 / 35 subgraphs...\n",
            "Evaluated 30 / 35 subgraphs...\n",
            "\n",
            "Final ROC on the train set 0.8632527060905152\n",
            "Final ROC on the validation set 0.8148366532929999\n",
            "Final ROC on the test set 0.7626689614619434\n",
            "Loss training: 28.848055\n",
            "Loss training: 24.231966\n",
            "Loss training: 24.380928\n",
            "Loss training: 26.428478\n",
            "Loss training: 36.46755\n",
            "Loss training: 26.034794\n",
            "Loss training: 23.066765\n",
            "Loss training: 34.86315\n",
            "Loss training: 26.374609\n",
            "Loss training: 31.105776\n",
            "\n",
            "***************************\n",
            "Trained on 5810 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.010273\n",
            "Loss training: 27.861269\n",
            "Loss training: 32.768417\n",
            "Loss training: 30.757944\n",
            "Loss training: 35.774437\n",
            "Loss training: 25.161802\n",
            "Loss training: 24.976337\n",
            "Loss training: 31.220585\n",
            "Loss training: 15.930602\n",
            "Loss training: 23.435469\n",
            "\n",
            "***************************\n",
            "Trained on 5820 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.465797\n",
            "Loss training: 34.21783\n",
            "Loss training: 23.551308\n",
            "Loss training: 27.422485\n",
            "Loss training: 26.016573\n",
            "Loss training: 36.460896\n",
            "Loss training: 35.042065\n",
            "Loss training: 24.00333\n",
            "Loss training: 35.867996\n",
            "Loss training: 23.156763\n",
            "\n",
            "***************************\n",
            "Trained on 5830 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.652855\n",
            "Loss training: 24.555006\n",
            "Loss training: 30.109203\n",
            "Loss training: 29.978386\n",
            "Loss training: 28.622757\n",
            "Loss training: 34.03303\n",
            "Loss training: 32.98305\n",
            "Loss training: 22.65283\n",
            "Loss training: 22.6127\n",
            "Loss training: 28.110558\n",
            "\n",
            "***************************\n",
            "Trained on 5840 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.745533\n",
            "Loss training: 25.294004\n",
            "Loss training: 27.866716\n",
            "Loss training: 22.500006\n",
            "Loss training: 27.87434\n",
            "Loss training: 39.748795\n",
            "Loss training: 35.36937\n",
            "Loss training: 22.490072\n",
            "Loss training: 25.379663\n",
            "Loss training: 27.267149\n",
            "\n",
            "***************************\n",
            "Trained on 5850 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.268566\n",
            "Loss training: 32.504192\n",
            "Loss training: 34.825867\n",
            "Loss training: 26.201681\n",
            "Loss training: 21.469158\n",
            "Loss training: 14.468551\n",
            "Loss training: 31.17592\n",
            "Loss training: 14.110395\n",
            "Loss training: 13.895767\n",
            "Loss training: 30.275606\n",
            "\n",
            "***************************\n",
            "Trained on 5860 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.37835\n",
            "Loss training: 24.652096\n",
            "Loss training: 28.594225\n",
            "Loss training: 26.00945\n",
            "Loss training: 28.332445\n",
            "Loss training: 28.128942\n",
            "Loss training: 29.84643\n",
            "Loss training: 24.698784\n",
            "Loss training: 25.315594\n",
            "Loss training: 26.771114\n",
            "\n",
            "***************************\n",
            "Trained on 5870 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.470793\n",
            "Loss training: 39.747528\n",
            "Loss training: 25.818502\n",
            "Loss training: 16.013432\n",
            "Loss training: 28.995962\n",
            "Loss training: 27.267422\n",
            "Loss training: 28.586386\n",
            "Loss training: 34.080585\n",
            "Loss training: 25.805813\n",
            "Loss training: 32.233807\n",
            "\n",
            "***************************\n",
            "Trained on 5880 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.765785\n",
            "Loss training: 34.973053\n",
            "Loss training: 29.83398\n",
            "Loss training: 35.571877\n",
            "Loss training: 35.23812\n",
            "Loss training: 36.897118\n",
            "Loss training: 36.627193\n",
            "Loss training: 27.747826\n",
            "Loss training: 24.89789\n",
            "Loss training: 25.009008\n",
            "\n",
            "***************************\n",
            "Trained on 5890 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.42346\n",
            "Loss training: 22.606403\n",
            "Loss training: 24.84401\n",
            "Loss training: 23.422926\n",
            "Loss training: 24.102484\n",
            "Loss training: 23.68887\n",
            "Loss training: 36.141468\n",
            "Loss training: 39.622204\n",
            "Loss training: 28.126528\n",
            "Loss training: 41.85596\n",
            "\n",
            "***************************\n",
            "Trained on 5900 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.8975\n",
            "Loss training: 14.423726\n",
            "Loss training: 22.069822\n",
            "Loss training: 25.53154\n",
            "Loss training: 36.233036\n",
            "Loss training: 23.619057\n",
            "Loss training: 40.512684\n",
            "Loss training: 13.709191\n",
            "Loss training: 36.208282\n",
            "Loss training: 24.108297\n",
            "\n",
            "***************************\n",
            "Trained on 5910 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.697958\n",
            "Loss training: 33.334198\n",
            "Loss training: 37.269665\n",
            "Loss training: 37.002842\n",
            "Loss training: 35.457577\n",
            "Loss training: 23.50599\n",
            "Loss training: 34.656857\n",
            "Loss training: 39.128048\n",
            "Loss training: 31.727785\n",
            "Loss training: 29.46009\n",
            "\n",
            "***************************\n",
            "Trained on 5920 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.877968\n",
            "Loss training: 25.007734\n",
            "Loss training: 30.6018\n",
            "Loss training: 30.010273\n",
            "Loss training: 25.488241\n",
            "Loss training: 33.66287\n",
            "Loss training: 21.731167\n",
            "Loss training: 28.346247\n",
            "Loss training: 28.840574\n",
            "Loss training: 33.787865\n",
            "\n",
            "***************************\n",
            "Trained on 5930 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.179153\n",
            "Loss training: 27.987825\n",
            "Loss training: 32.438328\n",
            "Loss training: 39.490707\n",
            "Loss training: 38.59589\n",
            "Loss training: 35.028316\n",
            "Loss training: 33.815636\n",
            "Loss training: 34.80745\n",
            "Loss training: 31.371838\n",
            "Loss training: 27.481056\n",
            "\n",
            "***************************\n",
            "Trained on 5940 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.360031\n",
            "Loss training: 32.71846\n",
            "Loss training: 27.226618\n",
            "Loss training: 14.128509\n",
            "Loss training: 25.513905\n",
            "Loss training: 30.478699\n",
            "Loss training: 24.90423\n",
            "Loss training: 26.60181\n",
            "Loss training: 36.574852\n",
            "Loss training: 31.275444\n",
            "\n",
            "***************************\n",
            "Trained on 5950 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.26273\n",
            "Loss training: 22.147518\n",
            "Loss training: 25.39423\n",
            "Loss training: 41.308067\n",
            "Loss training: 34.302593\n",
            "Loss training: 32.168587\n",
            "Loss training: 26.306643\n",
            "Loss training: 23.764074\n",
            "Loss training: 28.51684\n",
            "Loss training: 24.794048\n",
            "\n",
            "***************************\n",
            "Trained on 5960 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.73428\n",
            "Loss training: 27.678417\n",
            "Loss training: 30.075027\n",
            "Loss training: 30.74097\n",
            "Loss training: 24.059418\n",
            "Loss training: 37.680214\n",
            "Loss training: 36.23498\n",
            "Loss training: 26.741398\n",
            "Loss training: 33.192375\n",
            "Loss training: 29.080196\n",
            "\n",
            "***************************\n",
            "Trained on 5970 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.45638\n",
            "Loss training: 27.954903\n",
            "Loss training: 38.251965\n",
            "Loss training: 28.665346\n",
            "Loss training: 23.20275\n",
            "Loss training: 32.39984\n",
            "Loss training: 23.696411\n",
            "Loss training: 27.728851\n",
            "Loss training: 30.133293\n",
            "Loss training: 23.058609\n",
            "\n",
            "***************************\n",
            "Trained on 5980 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.089493\n",
            "Loss training: 26.391346\n",
            "Loss training: 31.627216\n",
            "Loss training: 35.486603\n",
            "Loss training: 33.710197\n",
            "Loss training: 27.635069\n",
            "Loss training: 23.249023\n",
            "Loss training: 32.23717\n",
            "Loss training: 27.193562\n",
            "Loss training: 33.01397\n",
            "\n",
            "***************************\n",
            "Trained on 5990 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.667513\n",
            "Loss training: 30.114021\n",
            "Loss training: 23.008066\n",
            "Loss training: 40.374527\n",
            "Loss training: 33.30926\n",
            "Loss training: 30.191309\n",
            "Loss training: 29.604092\n",
            "Loss training: 32.43009\n",
            "Loss training: 34.26972\n",
            "Loss training: 32.272873\n",
            "\n",
            "***************************\n",
            "Trained on 6000 graphs\n",
            "***************************\n",
            "\n",
            "\n",
            "*** Full evaluations after 6000 training steps ***\n",
            "Evaluated 10 / 35 subgraphs...\n",
            "Evaluated 20 / 35 subgraphs...\n",
            "Evaluated 30 / 35 subgraphs...\n",
            "\n",
            "Final ROC on the train set 0.86647996924703\n",
            "Final ROC on the validation set 0.8197250665954973\n",
            "Final ROC on the test set 0.7667211620115812\n",
            "Loss training: 31.767504\n",
            "Loss training: 29.352201\n",
            "Loss training: 36.54355\n",
            "Loss training: 28.855051\n",
            "Loss training: 31.74586\n",
            "Loss training: 24.009624\n",
            "Loss training: 24.134676\n",
            "Loss training: 28.094784\n",
            "Loss training: 28.838291\n",
            "Loss training: 28.173336\n",
            "\n",
            "***************************\n",
            "Trained on 6010 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.59332\n",
            "Loss training: 25.579477\n",
            "Loss training: 34.010044\n",
            "Loss training: 39.243965\n",
            "Loss training: 31.81426\n",
            "Loss training: 25.080961\n",
            "Loss training: 31.34457\n",
            "Loss training: 32.723564\n",
            "Loss training: 28.850834\n",
            "Loss training: 32.384014\n",
            "\n",
            "***************************\n",
            "Trained on 6020 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.223438\n",
            "Loss training: 22.512768\n",
            "Loss training: 26.718079\n",
            "Loss training: 23.016909\n",
            "Loss training: 36.79907\n",
            "Loss training: 30.438513\n",
            "Loss training: 31.521133\n",
            "Loss training: 28.800137\n",
            "Loss training: 23.346212\n",
            "Loss training: 38.19896\n",
            "\n",
            "***************************\n",
            "Trained on 6030 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.412306\n",
            "Loss training: 24.150051\n",
            "Loss training: 30.276867\n",
            "Loss training: 32.458355\n",
            "Loss training: 37.96961\n",
            "Loss training: 27.116407\n",
            "Loss training: 22.659784\n",
            "Loss training: 29.05997\n",
            "Loss training: 24.072077\n",
            "Loss training: 25.922752\n",
            "\n",
            "***************************\n",
            "Trained on 6040 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.835781\n",
            "Loss training: 33.43455\n",
            "Loss training: 22.432175\n",
            "Loss training: 25.507322\n",
            "Loss training: 27.598852\n",
            "Loss training: 32.911743\n",
            "Loss training: 23.096231\n",
            "Loss training: 28.490591\n",
            "Loss training: 37.521645\n",
            "Loss training: 23.725183\n",
            "\n",
            "***************************\n",
            "Trained on 6050 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 14.655439\n",
            "Loss training: 25.291578\n",
            "Loss training: 15.539079\n",
            "Loss training: 23.137257\n",
            "Loss training: 26.671234\n",
            "Loss training: 22.814705\n",
            "Loss training: 33.41638\n",
            "Loss training: 25.952627\n",
            "Loss training: 31.991528\n",
            "Loss training: 22.069376\n",
            "\n",
            "***************************\n",
            "Trained on 6060 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.895552\n",
            "Loss training: 23.371016\n",
            "Loss training: 22.61212\n",
            "Loss training: 32.365246\n",
            "Loss training: 29.996553\n",
            "Loss training: 36.398438\n",
            "Loss training: 26.524078\n",
            "Loss training: 26.248318\n",
            "Loss training: 22.824408\n",
            "Loss training: 35.852806\n",
            "\n",
            "***************************\n",
            "Trained on 6070 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.838688\n",
            "Loss training: 15.751517\n",
            "Loss training: 29.212118\n",
            "Loss training: 21.74376\n",
            "Loss training: 27.178017\n",
            "Loss training: 24.571877\n",
            "Loss training: 14.176693\n",
            "Loss training: 28.035017\n",
            "Loss training: 34.771984\n",
            "Loss training: 15.675087\n",
            "\n",
            "***************************\n",
            "Trained on 6080 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.820583\n",
            "Loss training: 22.604023\n",
            "Loss training: 33.398136\n",
            "Loss training: 22.245747\n",
            "Loss training: 29.673859\n",
            "Loss training: 35.829506\n",
            "Loss training: 30.889685\n",
            "Loss training: 25.95469\n",
            "Loss training: 14.623193\n",
            "Loss training: 26.728365\n",
            "\n",
            "***************************\n",
            "Trained on 6090 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.940624\n",
            "Loss training: 28.147467\n",
            "Loss training: 24.32994\n",
            "Loss training: 29.009415\n",
            "Loss training: 30.422636\n",
            "Loss training: 25.763824\n",
            "Loss training: 32.67606\n",
            "Loss training: 36.72439\n",
            "Loss training: 32.147438\n",
            "Loss training: 27.746347\n",
            "\n",
            "***************************\n",
            "Trained on 6100 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.032202\n",
            "Loss training: 13.907677\n",
            "Loss training: 36.788273\n",
            "Loss training: 28.876917\n",
            "Loss training: 23.765684\n",
            "Loss training: 26.560495\n",
            "Loss training: 29.306963\n",
            "Loss training: 30.347239\n",
            "Loss training: 22.377348\n",
            "Loss training: 22.855713\n",
            "\n",
            "***************************\n",
            "Trained on 6110 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.970362\n",
            "Loss training: 36.33972\n",
            "Loss training: 26.380112\n",
            "Loss training: 39.838078\n",
            "Loss training: 27.27929\n",
            "Loss training: 21.92386\n",
            "Loss training: 35.11277\n",
            "Loss training: 29.792366\n",
            "Loss training: 27.198463\n",
            "Loss training: 31.865032\n",
            "\n",
            "***************************\n",
            "Trained on 6120 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.600042\n",
            "Loss training: 35.71993\n",
            "Loss training: 33.13729\n",
            "Loss training: 22.395872\n",
            "Loss training: 23.210045\n",
            "Loss training: 32.967846\n",
            "Loss training: 31.3387\n",
            "Loss training: 35.522984\n",
            "Loss training: 24.217777\n",
            "Loss training: 38.625217\n",
            "\n",
            "***************************\n",
            "Trained on 6130 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.504913\n",
            "Loss training: 34.778427\n",
            "Loss training: 25.22651\n",
            "Loss training: 14.634309\n",
            "Loss training: 26.994837\n",
            "Loss training: 25.046436\n",
            "Loss training: 32.61682\n",
            "Loss training: 24.527885\n",
            "Loss training: 24.798294\n",
            "Loss training: 31.876816\n",
            "\n",
            "***************************\n",
            "Trained on 6140 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.695192\n",
            "Loss training: 25.897713\n",
            "Loss training: 14.42733\n",
            "Loss training: 26.889082\n",
            "Loss training: 23.490513\n",
            "Loss training: 31.074501\n",
            "Loss training: 38.400696\n",
            "Loss training: 29.96642\n",
            "Loss training: 30.977257\n",
            "Loss training: 36.062023\n",
            "\n",
            "***************************\n",
            "Trained on 6150 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 21.778383\n",
            "Loss training: 26.008318\n",
            "Loss training: 27.131332\n",
            "Loss training: 27.344536\n",
            "Loss training: 22.52306\n",
            "Loss training: 23.0535\n",
            "Loss training: 28.592827\n",
            "Loss training: 22.460548\n",
            "Loss training: 26.90964\n",
            "Loss training: 26.641502\n",
            "\n",
            "***************************\n",
            "Trained on 6160 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.018026\n",
            "Loss training: 14.790483\n",
            "Loss training: 26.391\n",
            "Loss training: 31.249266\n",
            "Loss training: 21.869371\n",
            "Loss training: 32.308582\n",
            "Loss training: 35.557343\n",
            "Loss training: 14.327805\n",
            "Loss training: 21.565296\n",
            "Loss training: 23.722986\n",
            "\n",
            "***************************\n",
            "Trained on 6170 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.20567\n",
            "Loss training: 26.05543\n",
            "Loss training: 14.965659\n",
            "Loss training: 22.73526\n",
            "Loss training: 29.431095\n",
            "Loss training: 31.620186\n",
            "Loss training: 39.955143\n",
            "Loss training: 22.350876\n",
            "Loss training: 22.337069\n",
            "Loss training: 31.619207\n",
            "\n",
            "***************************\n",
            "Trained on 6180 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.040207\n",
            "Loss training: 14.259926\n",
            "Loss training: 27.282274\n",
            "Loss training: 34.02355\n",
            "Loss training: 27.596027\n",
            "Loss training: 28.291843\n",
            "Loss training: 27.331476\n",
            "Loss training: 14.114415\n",
            "Loss training: 28.160398\n",
            "Loss training: 22.165253\n",
            "\n",
            "***************************\n",
            "Trained on 6190 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.974958\n",
            "Loss training: 25.94647\n",
            "Loss training: 25.630402\n",
            "Loss training: 21.65388\n",
            "Loss training: 30.592947\n",
            "Loss training: 28.188946\n",
            "Loss training: 26.3039\n",
            "Loss training: 15.296327\n",
            "Loss training: 32.304363\n",
            "Loss training: 31.919857\n",
            "\n",
            "***************************\n",
            "Trained on 6200 graphs\n",
            "***************************\n",
            "\n",
            "\n",
            "*** Full evaluations after 6200 training steps ***\n",
            "Evaluated 10 / 35 subgraphs...\n",
            "Evaluated 20 / 35 subgraphs...\n",
            "Evaluated 30 / 35 subgraphs...\n",
            "\n",
            "Final ROC on the train set 0.8667483493225514\n",
            "Final ROC on the validation set 0.8208345063653024\n",
            "Final ROC on the test set 0.7775959753066697\n",
            "Loss training: 29.872559\n",
            "Loss training: 36.28532\n",
            "Loss training: 32.12819\n",
            "Loss training: 21.877779\n",
            "Loss training: 27.758724\n",
            "Loss training: 24.781336\n",
            "Loss training: 27.657473\n",
            "Loss training: 28.926634\n",
            "Loss training: 34.337086\n",
            "Loss training: 30.460402\n",
            "\n",
            "***************************\n",
            "Trained on 6210 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.056793\n",
            "Loss training: 35.88756\n",
            "Loss training: 27.965\n",
            "Loss training: 14.934007\n",
            "Loss training: 27.313564\n",
            "Loss training: 22.806303\n",
            "Loss training: 26.139154\n",
            "Loss training: 39.516994\n",
            "Loss training: 26.228083\n",
            "Loss training: 36.051994\n",
            "\n",
            "***************************\n",
            "Trained on 6220 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.86466\n",
            "Loss training: 34.935265\n",
            "Loss training: 31.195345\n",
            "Loss training: 24.561094\n",
            "Loss training: 27.872656\n",
            "Loss training: 31.894857\n",
            "Loss training: 27.718857\n",
            "Loss training: 27.430813\n",
            "Loss training: 22.0826\n",
            "Loss training: 24.401384\n",
            "\n",
            "***************************\n",
            "Trained on 6230 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.51328\n",
            "Loss training: 27.543955\n",
            "Loss training: 26.142746\n",
            "Loss training: 27.076227\n",
            "Loss training: 42.099266\n",
            "Loss training: 23.539925\n",
            "Loss training: 31.348263\n",
            "Loss training: 28.202938\n",
            "Loss training: 32.39171\n",
            "Loss training: 41.33737\n",
            "\n",
            "***************************\n",
            "Trained on 6240 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.544643\n",
            "Loss training: 15.307684\n",
            "Loss training: 37.048862\n",
            "Loss training: 28.737425\n",
            "Loss training: 35.938457\n",
            "Loss training: 32.061024\n",
            "Loss training: 36.585995\n",
            "Loss training: 35.63368\n",
            "Loss training: 27.896362\n",
            "Loss training: 36.316742\n",
            "\n",
            "***************************\n",
            "Trained on 6250 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.36999\n",
            "Loss training: 22.653069\n",
            "Loss training: 26.558212\n",
            "Loss training: 28.080645\n",
            "Loss training: 32.65096\n",
            "Loss training: 25.469822\n",
            "Loss training: 25.147305\n",
            "Loss training: 23.40472\n",
            "Loss training: 38.529484\n",
            "Loss training: 27.168972\n",
            "\n",
            "***************************\n",
            "Trained on 6260 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 21.650724\n",
            "Loss training: 29.93194\n",
            "Loss training: 25.78433\n",
            "Loss training: 33.801193\n",
            "Loss training: 24.013216\n",
            "Loss training: 29.901634\n",
            "Loss training: 26.865225\n",
            "Loss training: 31.823614\n",
            "Loss training: 26.223743\n",
            "Loss training: 26.30523\n",
            "\n",
            "***************************\n",
            "Trained on 6270 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 14.253795\n",
            "Loss training: 35.722984\n",
            "Loss training: 21.494242\n",
            "Loss training: 14.065998\n",
            "Loss training: 35.48046\n",
            "Loss training: 37.712177\n",
            "Loss training: 25.140547\n",
            "Loss training: 25.540693\n",
            "Loss training: 33.97243\n",
            "Loss training: 30.209581\n",
            "\n",
            "***************************\n",
            "Trained on 6280 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.65869\n",
            "Loss training: 27.452356\n",
            "Loss training: 23.804356\n",
            "Loss training: 27.198393\n",
            "Loss training: 33.442307\n",
            "Loss training: 32.799725\n",
            "Loss training: 45.68422\n",
            "Loss training: 33.70655\n",
            "Loss training: 24.437654\n",
            "Loss training: 37.86932\n",
            "\n",
            "***************************\n",
            "Trained on 6290 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.614218\n",
            "Loss training: 28.13892\n",
            "Loss training: 35.536686\n",
            "Loss training: 35.23697\n",
            "Loss training: 24.239992\n",
            "Loss training: 24.47614\n",
            "Loss training: 73.19729\n",
            "Loss training: 37.612007\n",
            "Loss training: 30.446608\n",
            "Loss training: 24.537123\n",
            "\n",
            "***************************\n",
            "Trained on 6300 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 15.422362\n",
            "Loss training: 25.117157\n",
            "Loss training: 26.636824\n",
            "Loss training: 31.594547\n",
            "Loss training: 26.37713\n",
            "Loss training: 29.255304\n",
            "Loss training: 37.508354\n",
            "Loss training: 36.63371\n",
            "Loss training: 48.45851\n",
            "Loss training: 15.493553\n",
            "\n",
            "***************************\n",
            "Trained on 6310 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.149033\n",
            "Loss training: 23.448948\n",
            "Loss training: 28.912743\n",
            "Loss training: 26.77908\n",
            "Loss training: 15.2434\n",
            "Loss training: 35.15046\n",
            "Loss training: 33.61759\n",
            "Loss training: 41.488228\n",
            "Loss training: 28.619638\n",
            "Loss training: 42.84308\n",
            "\n",
            "***************************\n",
            "Trained on 6320 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.241734\n",
            "Loss training: 32.700764\n",
            "Loss training: 30.945957\n",
            "Loss training: 31.253029\n",
            "Loss training: 15.729267\n",
            "Loss training: 29.681335\n",
            "Loss training: 29.155695\n",
            "Loss training: 32.493217\n",
            "Loss training: 27.491278\n",
            "Loss training: 34.439648\n",
            "\n",
            "***************************\n",
            "Trained on 6330 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.948847\n",
            "Loss training: 32.016186\n",
            "Loss training: 22.434313\n",
            "Loss training: 30.768343\n",
            "Loss training: 24.1008\n",
            "Loss training: 35.81515\n",
            "Loss training: 32.47887\n",
            "Loss training: 29.549597\n",
            "Loss training: 26.490639\n",
            "Loss training: 40.009323\n",
            "\n",
            "***************************\n",
            "Trained on 6340 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.279354\n",
            "Loss training: 25.960924\n",
            "Loss training: 26.924942\n",
            "Loss training: 39.210133\n",
            "Loss training: 27.923859\n",
            "Loss training: 23.351234\n",
            "Loss training: 23.10167\n",
            "Loss training: 35.307056\n",
            "Loss training: 39.08584\n",
            "Loss training: 23.018381\n",
            "\n",
            "***************************\n",
            "Trained on 6350 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.773014\n",
            "Loss training: 24.781593\n",
            "Loss training: 22.380716\n",
            "Loss training: 25.996891\n",
            "Loss training: 24.02042\n",
            "Loss training: 32.642086\n",
            "Loss training: 28.970486\n",
            "Loss training: 22.33811\n",
            "Loss training: 34.997986\n",
            "Loss training: 34.73844\n",
            "\n",
            "***************************\n",
            "Trained on 6360 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.932291\n",
            "Loss training: 22.988052\n",
            "Loss training: 27.714434\n",
            "Loss training: 32.292824\n",
            "Loss training: 27.05627\n",
            "Loss training: 26.422989\n",
            "Loss training: 24.90911\n",
            "Loss training: 23.807528\n",
            "Loss training: 32.839973\n",
            "Loss training: 33.052635\n",
            "\n",
            "***************************\n",
            "Trained on 6370 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.438509\n",
            "Loss training: 32.594193\n",
            "Loss training: 32.524616\n",
            "Loss training: 23.897036\n",
            "Loss training: 33.895893\n",
            "Loss training: 23.77266\n",
            "Loss training: 31.011189\n",
            "Loss training: 24.027893\n",
            "Loss training: 28.949087\n",
            "Loss training: 26.060713\n",
            "\n",
            "***************************\n",
            "Trained on 6380 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.760147\n",
            "Loss training: 27.510473\n",
            "Loss training: 32.656384\n",
            "Loss training: 35.81011\n",
            "Loss training: 29.037971\n",
            "Loss training: 38.20835\n",
            "Loss training: 27.561737\n",
            "Loss training: 37.678932\n",
            "Loss training: 24.961586\n",
            "Loss training: 32.735546\n",
            "\n",
            "***************************\n",
            "Trained on 6390 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.025085\n",
            "Loss training: 23.784122\n",
            "Loss training: 27.00727\n",
            "Loss training: 31.961668\n",
            "Loss training: 21.804401\n",
            "Loss training: 23.758307\n",
            "Loss training: 30.177364\n",
            "Loss training: 31.419388\n",
            "Loss training: 23.600176\n",
            "Loss training: 31.594982\n",
            "\n",
            "***************************\n",
            "Trained on 6400 graphs\n",
            "***************************\n",
            "\n",
            "\n",
            "*** Full evaluations after 6400 training steps ***\n",
            "Evaluated 10 / 35 subgraphs...\n",
            "Evaluated 20 / 35 subgraphs...\n",
            "Evaluated 30 / 35 subgraphs...\n",
            "\n",
            "Final ROC on the train set 0.8722105296031694\n",
            "Final ROC on the validation set 0.819960588048625\n",
            "Final ROC on the test set 0.7687096304551858\n",
            "Loss training: 32.608086\n",
            "Loss training: 25.182114\n",
            "Loss training: 35.599316\n",
            "Loss training: 24.909199\n",
            "Loss training: 36.47033\n",
            "Loss training: 27.262781\n",
            "Loss training: 27.731737\n",
            "Loss training: 31.257654\n",
            "Loss training: 14.2637005\n",
            "Loss training: 26.71852\n",
            "\n",
            "***************************\n",
            "Trained on 6410 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.444088\n",
            "Loss training: 35.961636\n",
            "Loss training: 32.708714\n",
            "Loss training: 32.026115\n",
            "Loss training: 34.836754\n",
            "Loss training: 34.078613\n",
            "Loss training: 25.039803\n",
            "Loss training: 26.619228\n",
            "Loss training: 30.022331\n",
            "Loss training: 24.61339\n",
            "\n",
            "***************************\n",
            "Trained on 6420 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.45851\n",
            "Loss training: 36.425983\n",
            "Loss training: 24.116653\n",
            "Loss training: 23.718042\n",
            "Loss training: 24.286768\n",
            "Loss training: 21.659243\n",
            "Loss training: 23.556385\n",
            "Loss training: 23.493795\n",
            "Loss training: 23.071306\n",
            "Loss training: 26.690462\n",
            "\n",
            "***************************\n",
            "Trained on 6430 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.220436\n",
            "Loss training: 25.316126\n",
            "Loss training: 25.132534\n",
            "Loss training: 29.952398\n",
            "Loss training: 32.406673\n",
            "Loss training: 34.680588\n",
            "Loss training: 27.13972\n",
            "Loss training: 31.713675\n",
            "Loss training: 15.057607\n",
            "Loss training: 31.371088\n",
            "\n",
            "***************************\n",
            "Trained on 6440 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.62971\n",
            "Loss training: 22.861673\n",
            "Loss training: 21.306927\n",
            "Loss training: 14.318999\n",
            "Loss training: 22.44577\n",
            "Loss training: 33.98814\n",
            "Loss training: 23.603718\n",
            "Loss training: 26.94737\n",
            "Loss training: 29.920658\n",
            "Loss training: 27.742413\n",
            "\n",
            "***************************\n",
            "Trained on 6450 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.504885\n",
            "Loss training: 29.206161\n",
            "Loss training: 38.3252\n",
            "Loss training: 23.527796\n",
            "Loss training: 34.373108\n",
            "Loss training: 27.580038\n",
            "Loss training: 13.987416\n",
            "Loss training: 14.172367\n",
            "Loss training: 13.657558\n",
            "Loss training: 22.982258\n",
            "\n",
            "***************************\n",
            "Trained on 6460 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.203615\n",
            "Loss training: 21.437346\n",
            "Loss training: 27.237282\n",
            "Loss training: 22.446402\n",
            "Loss training: 23.081274\n",
            "Loss training: 38.691425\n",
            "Loss training: 29.469053\n",
            "Loss training: 33.09741\n",
            "Loss training: 29.101416\n",
            "Loss training: 29.110888\n",
            "\n",
            "***************************\n",
            "Trained on 6470 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.86836\n",
            "Loss training: 27.173044\n",
            "Loss training: 31.680904\n",
            "Loss training: 32.053947\n",
            "Loss training: 27.196442\n",
            "Loss training: 24.582518\n",
            "Loss training: 34.92178\n",
            "Loss training: 29.191504\n",
            "Loss training: 28.170374\n",
            "Loss training: 24.161623\n",
            "\n",
            "***************************\n",
            "Trained on 6480 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.32364\n",
            "Loss training: 14.190057\n",
            "Loss training: 23.145947\n",
            "Loss training: 29.063433\n",
            "Loss training: 26.624279\n",
            "Loss training: 30.852892\n",
            "Loss training: 23.72501\n",
            "Loss training: 13.875564\n",
            "Loss training: 28.77146\n",
            "Loss training: 45.42543\n",
            "\n",
            "***************************\n",
            "Trained on 6490 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.006222\n",
            "Loss training: 27.492176\n",
            "Loss training: 21.920963\n",
            "Loss training: 34.866634\n",
            "Loss training: 30.446552\n",
            "Loss training: 25.579409\n",
            "Loss training: 24.291872\n",
            "Loss training: 34.978855\n",
            "Loss training: 31.49831\n",
            "Loss training: 35.873135\n",
            "\n",
            "***************************\n",
            "Trained on 6500 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.894527\n",
            "Loss training: 36.899204\n",
            "Loss training: 25.419596\n",
            "Loss training: 25.85867\n",
            "Loss training: 26.967558\n",
            "Loss training: 22.413176\n",
            "Loss training: 25.428623\n",
            "Loss training: 22.570492\n",
            "Loss training: 35.718243\n",
            "Loss training: 14.490788\n",
            "\n",
            "***************************\n",
            "Trained on 6510 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.098372\n",
            "Loss training: 15.38527\n",
            "Loss training: 25.14446\n",
            "Loss training: 35.194904\n",
            "Loss training: 37.485035\n",
            "Loss training: 37.29189\n",
            "Loss training: 29.16067\n",
            "Loss training: 38.860546\n",
            "Loss training: 34.435654\n",
            "Loss training: 26.843218\n",
            "\n",
            "***************************\n",
            "Trained on 6520 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.149055\n",
            "Loss training: 29.834501\n",
            "Loss training: 22.793182\n",
            "Loss training: 32.950985\n",
            "Loss training: 29.700468\n",
            "Loss training: 27.003204\n",
            "Loss training: 29.838299\n",
            "Loss training: 14.006355\n",
            "Loss training: 24.359352\n",
            "Loss training: 30.006247\n",
            "\n",
            "***************************\n",
            "Trained on 6530 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.32938\n",
            "Loss training: 26.825445\n",
            "Loss training: 24.07622\n",
            "Loss training: 26.594637\n",
            "Loss training: 21.613949\n",
            "Loss training: 27.66169\n",
            "Loss training: 38.262238\n",
            "Loss training: 23.592361\n",
            "Loss training: 30.243982\n",
            "Loss training: 14.533981\n",
            "\n",
            "***************************\n",
            "Trained on 6540 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.441885\n",
            "Loss training: 24.041506\n",
            "Loss training: 29.412846\n",
            "Loss training: 27.292633\n",
            "Loss training: 24.036268\n",
            "Loss training: 34.4563\n",
            "Loss training: 31.143034\n",
            "Loss training: 26.989193\n",
            "Loss training: 27.244122\n",
            "Loss training: 32.955254\n",
            "\n",
            "***************************\n",
            "Trained on 6550 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.041765\n",
            "Loss training: 14.659402\n",
            "Loss training: 28.26765\n",
            "Loss training: 38.058327\n",
            "Loss training: 23.685505\n",
            "Loss training: 22.582178\n",
            "Loss training: 32.305405\n",
            "Loss training: 28.128536\n",
            "Loss training: 24.981127\n",
            "Loss training: 30.437292\n",
            "\n",
            "***************************\n",
            "Trained on 6560 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.334026\n",
            "Loss training: 21.706503\n",
            "Loss training: 34.740677\n",
            "Loss training: 27.261301\n",
            "Loss training: 29.203472\n",
            "Loss training: 22.679611\n",
            "Loss training: 30.224052\n",
            "Loss training: 31.120556\n",
            "Loss training: 23.392538\n",
            "Loss training: 31.102823\n",
            "\n",
            "***************************\n",
            "Trained on 6570 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 21.277025\n",
            "Loss training: 23.643543\n",
            "Loss training: 30.891094\n",
            "Loss training: 29.233597\n",
            "Loss training: 21.02778\n",
            "Loss training: 27.265537\n",
            "Loss training: 20.70683\n",
            "Loss training: 36.51696\n",
            "Loss training: 21.698193\n",
            "Loss training: 23.444403\n",
            "\n",
            "***************************\n",
            "Trained on 6580 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.18671\n",
            "Loss training: 30.483046\n",
            "Loss training: 26.729088\n",
            "Loss training: 21.32194\n",
            "Loss training: 32.189552\n",
            "Loss training: 27.737043\n",
            "Loss training: 28.966917\n",
            "Loss training: 31.66259\n",
            "Loss training: 33.20432\n",
            "Loss training: 36.16067\n",
            "\n",
            "***************************\n",
            "Trained on 6590 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.610453\n",
            "Loss training: 23.370405\n",
            "Loss training: 34.954727\n",
            "Loss training: 31.357925\n",
            "Loss training: 36.491467\n",
            "Loss training: 29.10789\n",
            "Loss training: 27.098341\n",
            "Loss training: 27.116\n",
            "Loss training: 23.116154\n",
            "Loss training: 26.958172\n",
            "\n",
            "***************************\n",
            "Trained on 6600 graphs\n",
            "***************************\n",
            "\n",
            "\n",
            "*** Full evaluations after 6600 training steps ***\n",
            "Evaluated 10 / 35 subgraphs...\n",
            "Evaluated 20 / 35 subgraphs...\n",
            "Evaluated 30 / 35 subgraphs...\n",
            "\n",
            "Final ROC on the train set 0.8766217705330918\n",
            "Final ROC on the validation set 0.8237042501846173\n",
            "Final ROC on the test set 0.7714408485592694\n",
            "Loss training: 27.2824\n",
            "Loss training: 22.39717\n",
            "Loss training: 25.147541\n",
            "Loss training: 30.717794\n",
            "Loss training: 35.278717\n",
            "Loss training: 23.620981\n",
            "Loss training: 13.844128\n",
            "Loss training: 24.259361\n",
            "Loss training: 24.113052\n",
            "Loss training: 22.802702\n",
            "\n",
            "***************************\n",
            "Trained on 6610 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.433098\n",
            "Loss training: 26.275599\n",
            "Loss training: 32.603127\n",
            "Loss training: 37.993126\n",
            "Loss training: 26.447382\n",
            "Loss training: 25.902363\n",
            "Loss training: 27.510712\n",
            "Loss training: 27.043716\n",
            "Loss training: 27.894459\n",
            "Loss training: 37.98197\n",
            "\n",
            "***************************\n",
            "Trained on 6620 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.511341\n",
            "Loss training: 34.74383\n",
            "Loss training: 25.728664\n",
            "Loss training: 29.018528\n",
            "Loss training: 26.899899\n",
            "Loss training: 33.98706\n",
            "Loss training: 26.691574\n",
            "Loss training: 24.159887\n",
            "Loss training: 22.537228\n",
            "Loss training: 34.553577\n",
            "\n",
            "***************************\n",
            "Trained on 6630 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.609938\n",
            "Loss training: 22.150454\n",
            "Loss training: 24.170977\n",
            "Loss training: 23.48749\n",
            "Loss training: 22.435343\n",
            "Loss training: 26.466837\n",
            "Loss training: 35.440727\n",
            "Loss training: 31.031961\n",
            "Loss training: 14.551399\n",
            "Loss training: 23.686268\n",
            "\n",
            "***************************\n",
            "Trained on 6640 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.617626\n",
            "Loss training: 23.60637\n",
            "Loss training: 27.945671\n",
            "Loss training: 23.421032\n",
            "Loss training: 27.07415\n",
            "Loss training: 22.767273\n",
            "Loss training: 31.004406\n",
            "Loss training: 24.406054\n",
            "Loss training: 45.536194\n",
            "Loss training: 31.995832\n",
            "\n",
            "***************************\n",
            "Trained on 6650 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.13892\n",
            "Loss training: 25.746796\n",
            "Loss training: 30.238012\n",
            "Loss training: 24.60852\n",
            "Loss training: 22.669823\n",
            "Loss training: 52.377224\n",
            "Loss training: 39.19557\n",
            "Loss training: 30.57562\n",
            "Loss training: 36.488564\n",
            "Loss training: 32.38322\n",
            "\n",
            "***************************\n",
            "Trained on 6660 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 15.5091\n",
            "Loss training: 55.06723\n",
            "Loss training: 22.359383\n",
            "Loss training: 27.775543\n",
            "Loss training: 40.45502\n",
            "Loss training: 29.573917\n",
            "Loss training: 27.573498\n",
            "Loss training: 23.97321\n",
            "Loss training: 26.65275\n",
            "Loss training: 30.49099\n",
            "\n",
            "***************************\n",
            "Trained on 6670 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.601782\n",
            "Loss training: 25.46186\n",
            "Loss training: 41.166058\n",
            "Loss training: 29.612932\n",
            "Loss training: 35.40629\n",
            "Loss training: 24.592117\n",
            "Loss training: 28.715916\n",
            "Loss training: 24.232105\n",
            "Loss training: 23.889776\n",
            "Loss training: 39.444176\n",
            "\n",
            "***************************\n",
            "Trained on 6680 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.81099\n",
            "Loss training: 61.597443\n",
            "Loss training: 28.836088\n",
            "Loss training: 30.450668\n",
            "Loss training: 36.408504\n",
            "Loss training: 29.18753\n",
            "Loss training: 24.572866\n",
            "Loss training: 28.198248\n",
            "Loss training: 36.778576\n",
            "Loss training: 26.966196\n",
            "\n",
            "***************************\n",
            "Trained on 6690 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 47.490013\n",
            "Loss training: 16.428677\n",
            "Loss training: 16.613737\n",
            "Loss training: 32.203026\n",
            "Loss training: 30.07391\n",
            "Loss training: 41.92887\n",
            "Loss training: 26.456247\n",
            "Loss training: 35.681973\n",
            "Loss training: 41.999702\n",
            "Loss training: 17.053083\n",
            "\n",
            "***************************\n",
            "Trained on 6700 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.78987\n",
            "Loss training: 33.328346\n",
            "Loss training: 41.383156\n",
            "Loss training: 25.57288\n",
            "Loss training: 42.440975\n",
            "Loss training: 38.30947\n",
            "Loss training: 35.870605\n",
            "Loss training: 34.450653\n",
            "Loss training: 24.531857\n",
            "Loss training: 36.752205\n",
            "\n",
            "***************************\n",
            "Trained on 6710 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.291107\n",
            "Loss training: 40.36921\n",
            "Loss training: 46.637478\n",
            "Loss training: 35.075645\n",
            "Loss training: 35.557373\n",
            "Loss training: 28.265839\n",
            "Loss training: 37.274803\n",
            "Loss training: 36.991695\n",
            "Loss training: 31.410492\n",
            "Loss training: 36.835117\n",
            "\n",
            "***************************\n",
            "Trained on 6720 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.824\n",
            "Loss training: 43.042236\n",
            "Loss training: 39.766724\n",
            "Loss training: 34.07487\n",
            "Loss training: 32.07941\n",
            "Loss training: 34.89372\n",
            "Loss training: 50.60348\n",
            "Loss training: 32.38648\n",
            "Loss training: 31.561567\n",
            "Loss training: 37.59422\n",
            "\n",
            "***************************\n",
            "Trained on 6730 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.63356\n",
            "Loss training: 41.13635\n",
            "Loss training: 43.154526\n",
            "Loss training: 39.667107\n",
            "Loss training: 38.35029\n",
            "Loss training: 37.99415\n",
            "Loss training: 36.629612\n",
            "Loss training: 23.507357\n",
            "Loss training: 24.416073\n",
            "Loss training: 39.549877\n",
            "\n",
            "***************************\n",
            "Trained on 6740 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.405315\n",
            "Loss training: 29.796463\n",
            "Loss training: 33.186005\n",
            "Loss training: 36.06635\n",
            "Loss training: 30.792925\n",
            "Loss training: 34.155827\n",
            "Loss training: 33.850185\n",
            "Loss training: 25.102295\n",
            "Loss training: 33.15592\n",
            "Loss training: 24.038288\n",
            "\n",
            "***************************\n",
            "Trained on 6750 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.362617\n",
            "Loss training: 22.36278\n",
            "Loss training: 23.062634\n",
            "Loss training: 31.651232\n",
            "Loss training: 39.110416\n",
            "Loss training: 31.59745\n",
            "Loss training: 24.384378\n",
            "Loss training: 31.130142\n",
            "Loss training: 35.290783\n",
            "Loss training: 37.564365\n",
            "\n",
            "***************************\n",
            "Trained on 6760 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.881897\n",
            "Loss training: 23.46433\n",
            "Loss training: 37.952835\n",
            "Loss training: 37.790646\n",
            "Loss training: 35.693764\n",
            "Loss training: 36.227047\n",
            "Loss training: 35.289165\n",
            "Loss training: 30.924171\n",
            "Loss training: 26.814676\n",
            "Loss training: 45.030766\n",
            "\n",
            "***************************\n",
            "Trained on 6770 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.112972\n",
            "Loss training: 36.574913\n",
            "Loss training: 34.472305\n",
            "Loss training: 32.29393\n",
            "Loss training: 22.905828\n",
            "Loss training: 31.215942\n",
            "Loss training: 27.134426\n",
            "Loss training: 32.935123\n",
            "Loss training: 33.873768\n",
            "Loss training: 29.720572\n",
            "\n",
            "***************************\n",
            "Trained on 6780 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.22304\n",
            "Loss training: 23.41755\n",
            "Loss training: 25.681541\n",
            "Loss training: 25.559702\n",
            "Loss training: 17.03329\n",
            "Loss training: 25.184738\n",
            "Loss training: 37.803493\n",
            "Loss training: 27.468801\n",
            "Loss training: 27.021206\n",
            "Loss training: 26.544731\n",
            "\n",
            "***************************\n",
            "Trained on 6790 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.31032\n",
            "Loss training: 37.70657\n",
            "Loss training: 40.728424\n",
            "Loss training: 30.419968\n",
            "Loss training: 23.93627\n",
            "Loss training: 32.936943\n",
            "Loss training: 23.173721\n",
            "Loss training: 33.170692\n",
            "Loss training: 38.615074\n",
            "Loss training: 26.850368\n",
            "\n",
            "***************************\n",
            "Trained on 6800 graphs\n",
            "***************************\n",
            "\n",
            "\n",
            "*** Full evaluations after 6800 training steps ***\n",
            "Evaluated 10 / 35 subgraphs...\n",
            "Evaluated 20 / 35 subgraphs...\n",
            "Evaluated 30 / 35 subgraphs...\n",
            "\n",
            "Final ROC on the train set 0.8565110512507574\n",
            "Final ROC on the validation set 0.8047692423640089\n",
            "Final ROC on the test set 0.7592609052245212\n",
            "Loss training: 32.805775\n",
            "Loss training: 29.683966\n",
            "Loss training: 22.13755\n",
            "Loss training: 35.701584\n",
            "Loss training: 14.717343\n",
            "Loss training: 16.450405\n",
            "Loss training: 32.05997\n",
            "Loss training: 31.918486\n",
            "Loss training: 31.565557\n",
            "Loss training: 27.157034\n",
            "\n",
            "***************************\n",
            "Trained on 6810 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.988207\n",
            "Loss training: 26.67229\n",
            "Loss training: 25.807356\n",
            "Loss training: 31.562664\n",
            "Loss training: 87.96993\n",
            "Loss training: 31.299421\n",
            "Loss training: 27.745516\n",
            "Loss training: 53.87967\n",
            "Loss training: 33.812492\n",
            "Loss training: 37.9164\n",
            "\n",
            "***************************\n",
            "Trained on 6820 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.152824\n",
            "Loss training: 26.318405\n",
            "Loss training: 33.049465\n",
            "Loss training: 30.90255\n",
            "Loss training: 23.274523\n",
            "Loss training: 30.824423\n",
            "Loss training: 45.2299\n",
            "Loss training: 43.332237\n",
            "Loss training: 48.95399\n",
            "Loss training: 28.104048\n",
            "\n",
            "***************************\n",
            "Trained on 6830 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 61.154415\n",
            "Loss training: 36.112675\n",
            "Loss training: 39.95679\n",
            "Loss training: 38.036137\n",
            "Loss training: 28.939825\n",
            "Loss training: 35.596115\n",
            "Loss training: 38.58846\n",
            "Loss training: 38.65748\n",
            "Loss training: 37.09556\n",
            "Loss training: 21.310581\n",
            "\n",
            "***************************\n",
            "Trained on 6840 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 47.822983\n",
            "Loss training: 32.860504\n",
            "Loss training: 38.618496\n",
            "Loss training: 24.709015\n",
            "Loss training: 28.680511\n",
            "Loss training: 30.364431\n",
            "Loss training: 43.659885\n",
            "Loss training: 34.918293\n",
            "Loss training: 38.54183\n",
            "Loss training: 32.548786\n",
            "\n",
            "***************************\n",
            "Trained on 6850 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.266853\n",
            "Loss training: 41.911507\n",
            "Loss training: 28.253021\n",
            "Loss training: 31.933384\n",
            "Loss training: 17.323881\n",
            "Loss training: 28.700294\n",
            "Loss training: 37.68003\n",
            "Loss training: 37.54793\n",
            "Loss training: 32.17145\n",
            "Loss training: 27.114603\n",
            "\n",
            "***************************\n",
            "Trained on 6860 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.7836\n",
            "Loss training: 33.099976\n",
            "Loss training: 26.37448\n",
            "Loss training: 35.954\n",
            "Loss training: 28.369598\n",
            "Loss training: 30.903332\n",
            "Loss training: 22.341183\n",
            "Loss training: 36.813084\n",
            "Loss training: 23.14521\n",
            "Loss training: 40.2833\n",
            "\n",
            "***************************\n",
            "Trained on 6870 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.168995\n",
            "Loss training: 29.272783\n",
            "Loss training: 29.00242\n",
            "Loss training: 29.939194\n",
            "Loss training: 34.346508\n",
            "Loss training: 26.136868\n",
            "Loss training: 28.388264\n",
            "Loss training: 31.704695\n",
            "Loss training: 22.95548\n",
            "Loss training: 16.881498\n",
            "\n",
            "***************************\n",
            "Trained on 6880 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.867699\n",
            "Loss training: 41.77121\n",
            "Loss training: 28.652\n",
            "Loss training: 15.658629\n",
            "Loss training: 21.490555\n",
            "Loss training: 51.402977\n",
            "Loss training: 24.12164\n",
            "Loss training: 35.64308\n",
            "Loss training: 27.194157\n",
            "Loss training: 25.28293\n",
            "\n",
            "***************************\n",
            "Trained on 6890 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.63789\n",
            "Loss training: 25.043392\n",
            "Loss training: 27.78564\n",
            "Loss training: 29.400312\n",
            "Loss training: 34.461964\n",
            "Loss training: 26.066196\n",
            "Loss training: 32.65477\n",
            "Loss training: 31.758076\n",
            "Loss training: 40.9594\n",
            "Loss training: 22.218435\n",
            "\n",
            "***************************\n",
            "Trained on 6900 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.092964\n",
            "Loss training: 35.71863\n",
            "Loss training: 29.02596\n",
            "Loss training: 26.257717\n",
            "Loss training: 20.835497\n",
            "Loss training: 24.078224\n",
            "Loss training: 31.986616\n",
            "Loss training: 37.37036\n",
            "Loss training: 21.360891\n",
            "Loss training: 24.383833\n",
            "\n",
            "***************************\n",
            "Trained on 6910 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.46791\n",
            "Loss training: 33.03775\n",
            "Loss training: 31.851255\n",
            "Loss training: 37.58089\n",
            "Loss training: 38.192474\n",
            "Loss training: 27.011631\n",
            "Loss training: 30.941683\n",
            "Loss training: 39.343\n",
            "Loss training: 17.309462\n",
            "Loss training: 28.440588\n",
            "\n",
            "***************************\n",
            "Trained on 6920 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.84356\n",
            "Loss training: 25.43935\n",
            "Loss training: 23.094133\n",
            "Loss training: 26.299116\n",
            "Loss training: 24.33013\n",
            "Loss training: 36.065273\n",
            "Loss training: 24.32231\n",
            "Loss training: 31.965654\n",
            "Loss training: 30.637333\n",
            "Loss training: 35.747253\n",
            "\n",
            "***************************\n",
            "Trained on 6930 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.317804\n",
            "Loss training: 36.834263\n",
            "Loss training: 26.111938\n",
            "Loss training: 24.017336\n",
            "Loss training: 26.941261\n",
            "Loss training: 32.417583\n",
            "Loss training: 28.452282\n",
            "Loss training: 26.077982\n",
            "Loss training: 22.083767\n",
            "Loss training: 27.979616\n",
            "\n",
            "***************************\n",
            "Trained on 6940 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.461767\n",
            "Loss training: 32.113083\n",
            "Loss training: 24.376043\n",
            "Loss training: 27.490719\n",
            "Loss training: 20.44207\n",
            "Loss training: 36.169666\n",
            "Loss training: 15.060997\n",
            "Loss training: 35.115257\n",
            "Loss training: 36.934772\n",
            "Loss training: 31.606415\n",
            "\n",
            "***************************\n",
            "Trained on 6950 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.253403\n",
            "Loss training: 27.271172\n",
            "Loss training: 24.506578\n",
            "Loss training: 26.86054\n",
            "Loss training: 33.974983\n",
            "Loss training: 21.795336\n",
            "Loss training: 33.250443\n",
            "Loss training: 26.46505\n",
            "Loss training: 19.670399\n",
            "Loss training: 27.182093\n",
            "\n",
            "***************************\n",
            "Trained on 6960 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.17009\n",
            "Loss training: 31.076288\n",
            "Loss training: 25.554699\n",
            "Loss training: 30.811222\n",
            "Loss training: 31.937311\n",
            "Loss training: 31.978048\n",
            "Loss training: 35.967697\n",
            "Loss training: 25.861986\n",
            "Loss training: 34.643757\n",
            "Loss training: 24.351847\n",
            "\n",
            "***************************\n",
            "Trained on 6970 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.886536\n",
            "Loss training: 24.572456\n",
            "Loss training: 23.918537\n",
            "Loss training: 34.516125\n",
            "Loss training: 25.72557\n",
            "Loss training: 35.28188\n",
            "Loss training: 23.476093\n",
            "Loss training: 28.218082\n",
            "Loss training: 24.467813\n",
            "Loss training: 35.075455\n",
            "\n",
            "***************************\n",
            "Trained on 6980 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.82879\n",
            "Loss training: 36.185368\n",
            "Loss training: 25.79262\n",
            "Loss training: 29.291983\n",
            "Loss training: 27.34595\n",
            "Loss training: 23.062012\n",
            "Loss training: 33.88415\n",
            "Loss training: 25.184267\n",
            "Loss training: 15.33736\n",
            "Loss training: 32.38366\n",
            "\n",
            "***************************\n",
            "Trained on 6990 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 18.047161\n",
            "Loss training: 35.416237\n",
            "Loss training: 31.631445\n",
            "Loss training: 31.536005\n",
            "Loss training: 14.567984\n",
            "Loss training: 30.791103\n",
            "Loss training: 21.263428\n",
            "Loss training: 30.056273\n",
            "Loss training: 22.889446\n",
            "Loss training: 34.62055\n",
            "\n",
            "***************************\n",
            "Trained on 7000 graphs\n",
            "***************************\n",
            "\n",
            "\n",
            "*** Full evaluations after 7000 training steps ***\n",
            "Evaluated 10 / 35 subgraphs...\n",
            "Evaluated 20 / 35 subgraphs...\n",
            "Evaluated 30 / 35 subgraphs...\n",
            "\n",
            "Final ROC on the train set 0.8609162780956293\n",
            "Final ROC on the validation set 0.8070049613205365\n",
            "Final ROC on the test set 0.7600845186280131\n",
            "Loss training: 36.588333\n",
            "Loss training: 39.555332\n",
            "Loss training: 28.359676\n",
            "Loss training: 14.963589\n",
            "Loss training: 25.187098\n",
            "Loss training: 29.55323\n",
            "Loss training: 31.643023\n",
            "Loss training: 35.99372\n",
            "Loss training: 22.978224\n",
            "Loss training: 35.6007\n",
            "\n",
            "***************************\n",
            "Trained on 7010 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.047125\n",
            "Loss training: 27.147644\n",
            "Loss training: 23.740673\n",
            "Loss training: 27.626726\n",
            "Loss training: 29.1789\n",
            "Loss training: 25.63098\n",
            "Loss training: 36.21854\n",
            "Loss training: 23.788523\n",
            "Loss training: 22.512056\n",
            "Loss training: 26.043934\n",
            "\n",
            "***************************\n",
            "Trained on 7020 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.974585\n",
            "Loss training: 21.821194\n",
            "Loss training: 15.270925\n",
            "Loss training: 24.586922\n",
            "Loss training: 33.90552\n",
            "Loss training: 30.595734\n",
            "Loss training: 21.935848\n",
            "Loss training: 34.824047\n",
            "Loss training: 23.054052\n",
            "Loss training: 24.753395\n",
            "\n",
            "***************************\n",
            "Trained on 7030 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 43.254158\n",
            "Loss training: 27.654642\n",
            "Loss training: 36.20575\n",
            "Loss training: 14.537418\n",
            "Loss training: 24.067102\n",
            "Loss training: 22.799845\n",
            "Loss training: 23.770956\n",
            "Loss training: 23.460234\n",
            "Loss training: 31.695744\n",
            "Loss training: 31.832087\n",
            "\n",
            "***************************\n",
            "Trained on 7040 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.484129\n",
            "Loss training: 31.36336\n",
            "Loss training: 35.393387\n",
            "Loss training: 28.6732\n",
            "Loss training: 27.526926\n",
            "Loss training: 35.218117\n",
            "Loss training: 22.844437\n",
            "Loss training: 36.893734\n",
            "Loss training: 23.973799\n",
            "Loss training: 21.591452\n",
            "\n",
            "***************************\n",
            "Trained on 7050 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.896027\n",
            "Loss training: 26.256042\n",
            "Loss training: 35.985508\n",
            "Loss training: 23.445942\n",
            "Loss training: 21.910252\n",
            "Loss training: 23.710825\n",
            "Loss training: 27.479195\n",
            "Loss training: 27.639107\n",
            "Loss training: 20.738867\n",
            "Loss training: 27.43784\n",
            "\n",
            "***************************\n",
            "Trained on 7060 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.331532\n",
            "Loss training: 23.532814\n",
            "Loss training: 31.523754\n",
            "Loss training: 27.104658\n",
            "Loss training: 27.285439\n",
            "Loss training: 18.287365\n",
            "Loss training: 40.804985\n",
            "Loss training: 29.569466\n",
            "Loss training: 23.922575\n",
            "Loss training: 21.830717\n",
            "\n",
            "***************************\n",
            "Trained on 7070 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 20.569756\n",
            "Loss training: 26.919964\n",
            "Loss training: 26.858477\n",
            "Loss training: 39.510334\n",
            "Loss training: 27.281183\n",
            "Loss training: 15.135148\n",
            "Loss training: 22.537416\n",
            "Loss training: 14.164236\n",
            "Loss training: 24.851843\n",
            "Loss training: 31.415606\n",
            "\n",
            "***************************\n",
            "Trained on 7080 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.327423\n",
            "Loss training: 33.948765\n",
            "Loss training: 25.821032\n",
            "Loss training: 33.70786\n",
            "Loss training: 25.822392\n",
            "Loss training: 23.516674\n",
            "Loss training: 27.655205\n",
            "Loss training: 35.553185\n",
            "Loss training: 27.46878\n",
            "Loss training: 28.170368\n",
            "\n",
            "***************************\n",
            "Trained on 7090 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.14583\n",
            "Loss training: 24.93712\n",
            "Loss training: 24.288393\n",
            "Loss training: 27.234228\n",
            "Loss training: 23.30457\n",
            "Loss training: 20.95406\n",
            "Loss training: 34.350906\n",
            "Loss training: 38.73325\n",
            "Loss training: 29.244354\n",
            "Loss training: 35.742294\n",
            "\n",
            "***************************\n",
            "Trained on 7100 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.60102\n",
            "Loss training: 30.578587\n",
            "Loss training: 25.468044\n",
            "Loss training: 27.859554\n",
            "Loss training: 27.475456\n",
            "Loss training: 32.578377\n",
            "Loss training: 28.869913\n",
            "Loss training: 34.374313\n",
            "Loss training: 34.84699\n",
            "Loss training: 25.499762\n",
            "\n",
            "***************************\n",
            "Trained on 7110 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.42329\n",
            "Loss training: 23.674667\n",
            "Loss training: 23.55849\n",
            "Loss training: 22.462137\n",
            "Loss training: 33.163124\n",
            "Loss training: 22.835032\n",
            "Loss training: 34.91331\n",
            "Loss training: 23.130535\n",
            "Loss training: 33.979282\n",
            "Loss training: 30.130138\n",
            "\n",
            "***************************\n",
            "Trained on 7120 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.630327\n",
            "Loss training: 28.204481\n",
            "Loss training: 22.10696\n",
            "Loss training: 35.11714\n",
            "Loss training: 24.663294\n",
            "Loss training: 27.66643\n",
            "Loss training: 20.960272\n",
            "Loss training: 25.249315\n",
            "Loss training: 33.955296\n",
            "Loss training: 23.769838\n",
            "\n",
            "***************************\n",
            "Trained on 7130 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.251577\n",
            "Loss training: 32.079365\n",
            "Loss training: 21.431389\n",
            "Loss training: 29.438875\n",
            "Loss training: 18.276464\n",
            "Loss training: 23.70245\n",
            "Loss training: 20.675097\n",
            "Loss training: 39.635628\n",
            "Loss training: 33.86912\n",
            "Loss training: 33.672718\n",
            "\n",
            "***************************\n",
            "Trained on 7140 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.080618\n",
            "Loss training: 24.20766\n",
            "Loss training: 20.308739\n",
            "Loss training: 22.362558\n",
            "Loss training: 23.993372\n",
            "Loss training: 29.219341\n",
            "Loss training: 24.753178\n",
            "Loss training: 33.202946\n",
            "Loss training: 34.655155\n",
            "Loss training: 28.140045\n",
            "\n",
            "***************************\n",
            "Trained on 7150 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.378565\n",
            "Loss training: 34.284836\n",
            "Loss training: 32.136642\n",
            "Loss training: 22.517836\n",
            "Loss training: 31.198492\n",
            "Loss training: 27.477816\n",
            "Loss training: 34.09463\n",
            "Loss training: 22.132633\n",
            "Loss training: 27.339931\n",
            "Loss training: 20.19667\n",
            "\n",
            "***************************\n",
            "Trained on 7160 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.569176\n",
            "Loss training: 28.967472\n",
            "Loss training: 31.22637\n",
            "Loss training: 32.832928\n",
            "Loss training: 27.14461\n",
            "Loss training: 28.590754\n",
            "Loss training: 26.673775\n",
            "Loss training: 28.506657\n",
            "Loss training: 23.726654\n",
            "Loss training: 24.105898\n",
            "\n",
            "***************************\n",
            "Trained on 7170 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.514353\n",
            "Loss training: 25.41997\n",
            "Loss training: 34.774395\n",
            "Loss training: 27.032686\n",
            "Loss training: 39.436687\n",
            "Loss training: 22.50226\n",
            "Loss training: 21.914394\n",
            "Loss training: 35.023773\n",
            "Loss training: 17.899878\n",
            "Loss training: 26.715702\n",
            "\n",
            "***************************\n",
            "Trained on 7180 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.249208\n",
            "Loss training: 27.544882\n",
            "Loss training: 21.93142\n",
            "Loss training: 14.58545\n",
            "Loss training: 38.090057\n",
            "Loss training: 26.053574\n",
            "Loss training: 28.778679\n",
            "Loss training: 28.294004\n",
            "Loss training: 28.970242\n",
            "Loss training: 31.130842\n",
            "\n",
            "***************************\n",
            "Trained on 7190 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.730186\n",
            "Loss training: 23.363144\n",
            "Loss training: 24.326275\n",
            "Loss training: 22.62072\n",
            "Loss training: 27.715305\n",
            "Loss training: 24.260977\n",
            "Loss training: 24.097647\n",
            "Loss training: 30.17057\n",
            "Loss training: 34.63034\n",
            "Loss training: 27.43813\n",
            "\n",
            "***************************\n",
            "Trained on 7200 graphs\n",
            "***************************\n",
            "\n",
            "\n",
            "*** Full evaluations after 7200 training steps ***\n",
            "Evaluated 10 / 35 subgraphs...\n",
            "Evaluated 20 / 35 subgraphs...\n",
            "Evaluated 30 / 35 subgraphs...\n",
            "\n",
            "Final ROC on the train set 0.875983722015223\n",
            "Final ROC on the validation set 0.8275789474896191\n",
            "Final ROC on the test set 0.7756042349547092\n",
            "Loss training: 26.804863\n",
            "Loss training: 26.559948\n",
            "Loss training: 17.512192\n",
            "Loss training: 26.283964\n",
            "Loss training: 28.67694\n",
            "Loss training: 32.059967\n",
            "Loss training: 32.94551\n",
            "Loss training: 24.518654\n",
            "Loss training: 22.354975\n",
            "Loss training: 26.796545\n",
            "\n",
            "***************************\n",
            "Trained on 7210 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.052914\n",
            "Loss training: 39.550594\n",
            "Loss training: 24.118053\n",
            "Loss training: 34.228653\n",
            "Loss training: 37.832413\n",
            "Loss training: 23.760973\n",
            "Loss training: 34.57786\n",
            "Loss training: 23.290468\n",
            "Loss training: 34.08353\n",
            "Loss training: 22.091125\n",
            "\n",
            "***************************\n",
            "Trained on 7220 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.58221\n",
            "Loss training: 23.153852\n",
            "Loss training: 34.369175\n",
            "Loss training: 31.63373\n",
            "Loss training: 26.56469\n",
            "Loss training: 22.92499\n",
            "Loss training: 23.134626\n",
            "Loss training: 22.942327\n",
            "Loss training: 41.486443\n",
            "Loss training: 23.608637\n",
            "\n",
            "***************************\n",
            "Trained on 7230 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.747623\n",
            "Loss training: 30.303396\n",
            "Loss training: 28.70482\n",
            "Loss training: 21.147703\n",
            "Loss training: 29.24405\n",
            "Loss training: 22.325926\n",
            "Loss training: 25.887459\n",
            "Loss training: 28.597643\n",
            "Loss training: 23.28902\n",
            "Loss training: 34.40917\n",
            "\n",
            "***************************\n",
            "Trained on 7240 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.194235\n",
            "Loss training: 28.746414\n",
            "Loss training: 32.96662\n",
            "Loss training: 23.450653\n",
            "Loss training: 27.005903\n",
            "Loss training: 16.83025\n",
            "Loss training: 20.26478\n",
            "Loss training: 34.492847\n",
            "Loss training: 20.119442\n",
            "Loss training: 23.204876\n",
            "\n",
            "***************************\n",
            "Trained on 7250 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.239033\n",
            "Loss training: 19.847681\n",
            "Loss training: 19.75302\n",
            "Loss training: 33.75718\n",
            "Loss training: 31.460032\n",
            "Loss training: 33.86089\n",
            "Loss training: 28.795801\n",
            "Loss training: 27.328953\n",
            "Loss training: 31.708204\n",
            "Loss training: 24.619066\n",
            "\n",
            "***************************\n",
            "Trained on 7260 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.030138\n",
            "Loss training: 27.201487\n",
            "Loss training: 27.062191\n",
            "Loss training: 33.796505\n",
            "Loss training: 38.44757\n",
            "Loss training: 26.142294\n",
            "Loss training: 19.70924\n",
            "Loss training: 33.42219\n",
            "Loss training: 27.169775\n",
            "Loss training: 29.164907\n",
            "\n",
            "***************************\n",
            "Trained on 7270 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.097889\n",
            "Loss training: 21.663157\n",
            "Loss training: 28.961582\n",
            "Loss training: 26.426485\n",
            "Loss training: 26.739584\n",
            "Loss training: 19.634262\n",
            "Loss training: 37.727337\n",
            "Loss training: 35.855404\n",
            "Loss training: 37.66318\n",
            "Loss training: 31.98712\n",
            "\n",
            "***************************\n",
            "Trained on 7280 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.845047\n",
            "Loss training: 27.674065\n",
            "Loss training: 32.43231\n",
            "Loss training: 19.725037\n",
            "Loss training: 14.322281\n",
            "Loss training: 27.098408\n",
            "Loss training: 26.728775\n",
            "Loss training: 34.46813\n",
            "Loss training: 25.433596\n",
            "Loss training: 33.63998\n",
            "\n",
            "***************************\n",
            "Trained on 7290 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.883904\n",
            "Loss training: 32.745945\n",
            "Loss training: 30.97415\n",
            "Loss training: 23.505142\n",
            "Loss training: 26.325827\n",
            "Loss training: 22.804346\n",
            "Loss training: 22.57175\n",
            "Loss training: 23.891108\n",
            "Loss training: 31.738937\n",
            "Loss training: 35.33333\n",
            "\n",
            "***************************\n",
            "Trained on 7300 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 13.893657\n",
            "Loss training: 28.852654\n",
            "Loss training: 24.501749\n",
            "Loss training: 25.829193\n",
            "Loss training: 33.384872\n",
            "Loss training: 35.227566\n",
            "Loss training: 28.478266\n",
            "Loss training: 23.074762\n",
            "Loss training: 27.216345\n",
            "Loss training: 21.932238\n",
            "\n",
            "***************************\n",
            "Trained on 7310 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.51964\n",
            "Loss training: 34.537495\n",
            "Loss training: 28.723816\n",
            "Loss training: 20.067192\n",
            "Loss training: 34.45504\n",
            "Loss training: 30.171946\n",
            "Loss training: 26.902906\n",
            "Loss training: 23.390911\n",
            "Loss training: 23.65245\n",
            "Loss training: 25.916414\n",
            "\n",
            "***************************\n",
            "Trained on 7320 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.343128\n",
            "Loss training: 26.961712\n",
            "Loss training: 27.102407\n",
            "Loss training: 22.758821\n",
            "Loss training: 29.238607\n",
            "Loss training: 29.59246\n",
            "Loss training: 23.487759\n",
            "Loss training: 22.661045\n",
            "Loss training: 28.868391\n",
            "Loss training: 33.85564\n",
            "\n",
            "***************************\n",
            "Trained on 7330 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 21.992674\n",
            "Loss training: 21.570396\n",
            "Loss training: 27.432547\n",
            "Loss training: 26.987377\n",
            "Loss training: 23.294268\n",
            "Loss training: 29.903027\n",
            "Loss training: 26.63898\n",
            "Loss training: 29.577003\n",
            "Loss training: 27.049746\n",
            "Loss training: 26.859858\n",
            "\n",
            "***************************\n",
            "Trained on 7340 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.39967\n",
            "Loss training: 25.817476\n",
            "Loss training: 20.719803\n",
            "Loss training: 33.44313\n",
            "Loss training: 26.316704\n",
            "Loss training: 33.44881\n",
            "Loss training: 28.965904\n",
            "Loss training: 26.245874\n",
            "Loss training: 24.115988\n",
            "Loss training: 26.273087\n",
            "\n",
            "***************************\n",
            "Trained on 7350 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 19.936224\n",
            "Loss training: 29.6288\n",
            "Loss training: 23.455276\n",
            "Loss training: 23.274841\n",
            "Loss training: 23.8959\n",
            "Loss training: 33.164635\n",
            "Loss training: 28.67343\n",
            "Loss training: 25.323458\n",
            "Loss training: 29.178476\n",
            "Loss training: 32.23263\n",
            "\n",
            "***************************\n",
            "Trained on 7360 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.705067\n",
            "Loss training: 26.060484\n",
            "Loss training: 37.68338\n",
            "Loss training: 24.516886\n",
            "Loss training: 35.61809\n",
            "Loss training: 27.993628\n",
            "Loss training: 25.969973\n",
            "Loss training: 26.939215\n",
            "Loss training: 19.80061\n",
            "Loss training: 14.484359\n",
            "\n",
            "***************************\n",
            "Trained on 7370 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.35018\n",
            "Loss training: 37.804443\n",
            "Loss training: 31.759117\n",
            "Loss training: 28.382313\n",
            "Loss training: 24.067873\n",
            "Loss training: 28.30056\n",
            "Loss training: 32.585453\n",
            "Loss training: 28.298615\n",
            "Loss training: 33.506985\n",
            "Loss training: 35.449486\n",
            "\n",
            "***************************\n",
            "Trained on 7380 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.777872\n",
            "Loss training: 29.22794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###################################### FUNCTIONS FOR TESTING ######################################\n",
        "run_overfit_on_single_partition = False\n",
        "run_overfit_on_demo_graph = False"
      ],
      "metadata": {
        "id": "yK0Qy8CdU6tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def overfit_on_single_graph(\n",
        "    num_training_steps, \n",
        "    learning_rate,\n",
        "    graph,\n",
        "    labels,\n",
        "    mask\n",
        "    ):\n",
        "  replicated_params = jax.pmap(network_definition.init, axis_name = 'i')(bcast_local_devices(jax.random.PRNGKey(42)), graph)\n",
        "\n",
        "  opt_init, opt_update = optax.adam(learning_rate = learning_rate)  \n",
        "  replicated_opt_state = jax.pmap(opt_init, axis_name = 'i')(replicated_params)\n",
        "\n",
        "  @functools.partial(jax.pmap, axis_name='i')\n",
        "  def update(params, opt_state, graph, targets, mask):\n",
        "    # Compute the gradients on the given minibatch (individually on each device).\n",
        "    loss, grads = jax.value_and_grad(compute_loss)(params, graph, targets, mask)\n",
        "\n",
        "    # Combine the gradient across all devices (by taking their mean).\n",
        "    grads = jax.lax.pmean(grads, axis_name='i')\n",
        "\n",
        "    # Also combine the loss. Unnecessary for the update, but useful for logging.\n",
        "    loss = jax.lax.pmean(loss, axis_name='i')\n",
        "\n",
        "    updates, opt_state = opt_update(grads, opt_state)\n",
        "\n",
        "    return optax.apply_updates(params, updates), opt_state, loss\n",
        "\n",
        "  # Train on a single partition\n",
        "  for idx in range(num_training_steps):\n",
        "    replicated_params, replicated_opt_state, loss = update(\n",
        "      replicated_params, \n",
        "      replicated_opt_state, \n",
        "      graph, \n",
        "      labels,\n",
        "      mask\n",
        "      ) \n",
        "\n",
        "    print('Loss training:', reshape_broadcasted_data(loss))\n",
        "\n",
        "    if (idx + 1) % 10 == 0:\n",
        "      print()\n",
        "      print(f'***************************')\n",
        "      print(f'Trained on {idx + 1} graphs')\n",
        "      print(f'***************************')\n",
        "      print()"
      ],
      "metadata": {
        "id": "DNchIrnxPk1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_demo_training_graph(num_nodes, num_edges):\n",
        "  rand_dgl_graph = dgl.rand_graph(num_nodes = num_nodes, num_edges = num_edges)\n",
        "\n",
        "  node_features = jnp.array([[randint(0, 7)] for i in range(num_nodes)])\n",
        "  edge_features = jnp.array([[0.1 * randint(0, 10) for _ in range(8)] for i in range(num_edges)])\n",
        "\n",
        "  senders = jnp.array(rand_dgl_graph.edges()[0])\n",
        "  receivers = jnp.array(rand_dgl_graph.edges()[1])\n",
        "\n",
        "  in_tuple = jraph.GraphsTuple(\n",
        "            nodes = node_features.astype(np.float32),\n",
        "            edges = edge_features.astype(np.float32),  \n",
        "            senders = senders.astype(np.int32), \n",
        "            receivers = receivers.astype(np.int32),\n",
        "            n_node = jnp.array([num_nodes]), \n",
        "            n_edge = jnp.array([num_edges]),\n",
        "            globals = None  # No global features\n",
        "          )\n",
        "  \n",
        "  labels = jnp.array([[randint(0, 1) for j in range(112)] for i in range(num_nodes)])\n",
        "  \n",
        "  in_tuple = in_tuple._replace(\n",
        "      nodes = {\n",
        "          'inputs': in_tuple.nodes, \n",
        "          'targets': labels, \n",
        "          'train_mask': jnp.ones((num_nodes, 1)), # No nodes are masked \n",
        "          }\n",
        "  )\n",
        "\n",
        "  # in_tuple = pad_graph_to_nearest_multiple_of_8(in_tuple)\n",
        "  \n",
        "  return sharded_graphnet.graphs_tuple_to_broadcasted_sharded_graphs_tuple(\n",
        "      in_tuple,\n",
        "      num_shards = num_devices\n",
        "      )\n",
        "\n",
        "def overfit_on_demo_graph(num_training_steps, learning_rate):\n",
        "  demo_graph = get_demo_training_graph(num_nodes = 16, num_edges = 8)\n",
        "  demo_labels = demo_graph.nodes['targets']\n",
        "  demo_mask = demo_graph.nodes['train_mask']\n",
        "  demo_graph = demo_graph._replace(nodes = demo_graph.nodes['inputs'])\n",
        "\n",
        "  overfit_on_single_graph(\n",
        "      num_training_steps = num_training_steps,\n",
        "      learning_rate = learning_rate,\n",
        "      graph = demo_graph,\n",
        "      labels = demo_labels,\n",
        "      mask = demo_mask\n",
        "  )"
      ],
      "metadata": {
        "id": "zQdvmYQETQNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Overfit on an existing partition\n",
        "if run_overfit_on_single_partition:\n",
        "  print('*** Trying to overfit on a single partition ***')\n",
        "  overfit_on_single_graph(\n",
        "      num_training_steps = 5000,\n",
        "      learning_rate = 0.001,\n",
        "      graph = processed_graphs['partition_0']['graph'],\n",
        "      labels = processed_graphs['partition_0']['labels'],\n",
        "      mask = processed_graphs['partition_0']['train_mask']\n",
        "      )"
      ],
      "metadata": {
        "id": "xfmlIPlpUfNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if run_overfit_on_demo_graph:\n",
        "  print('*** Trying to overfit on a random demo graph ***')\n",
        "  overfit_on_demo_graph(\n",
        "      num_training_steps = 1000,\n",
        "      learning_rate = 0.001\n",
        "  )"
      ],
      "metadata": {
        "id": "ILwic3w-d5mI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}