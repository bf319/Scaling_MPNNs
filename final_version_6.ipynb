{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final-version-2-sharded-networks.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bf319/Scaling_MPNNs/blob/main/final_version_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rLhMwiHHWbtK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53786504-0aaf-4865-a3af-8a1ebd8c0e3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for dm-haiku (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 75 kB 2.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 70 kB 3.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 77 kB 6.6 MB/s \n",
            "\u001b[?25h  Building wheel for jaxline (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 78 kB 3.6 MB/s \n",
            "\u001b[?25h  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 136 kB 5.3 MB/s \n",
            "\u001b[?25h  Building wheel for metis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "--2022-03-30 19:14:30--  https://raw.githubusercontent.com/deepmind/jraph/master/jraph/experimental/sharded_graphnet.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22444 (22K) [text/plain]\n",
            "Saving to: ‘sharded_graphnet.py’\n",
            "\n",
            "sharded_graphnet.py 100%[===================>]  21.92K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2022-03-30 19:14:30 (17.1 MB/s) - ‘sharded_graphnet.py’ saved [22444/22444]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%pip install -q git+https://github.com/deepmind/dm-haiku\n",
        "%pip install -q jraph\n",
        "%pip install -q git+https://github.com/deepmind/jaxline\n",
        "%pip install -q ogb\n",
        "%pip install -q dgl\n",
        "%pip install -q optax\n",
        "%pip install -q metis\n",
        "\n",
        "!wget https://raw.githubusercontent.com/deepmind/jraph/master/jraph/experimental/sharded_graphnet.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "\n",
        "num_devices = jax.local_device_count()\n",
        "jax.devices()"
      ],
      "metadata": {
        "id": "iT2wqf76kIRp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be44e51b-99fc-4bbe-87d9-59ec3c10454b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ogb.nodeproppred import Evaluator\n",
        "from ogb.nodeproppred import DglNodePropPredDataset\n",
        "\n",
        "dataset = DglNodePropPredDataset(name = \"ogbn-proteins\")\n",
        "split_idx = dataset.get_idx_split()\n",
        "evaluator = Evaluator(name = 'ogbn-proteins')\n",
        "print(evaluator.expected_input_format)"
      ],
      "metadata": {
        "id": "xHClucOxWpAZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f7b00cd-e04e-4fa7-f3d4-f0db929f5584"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Using backend: pytorch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/proteins.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 0.21 GB: 100%|██████████| 216/216 [00:12<00:00, 17.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/proteins.zip\n",
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:02<00:00,  2.31s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into DGL objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  2.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n",
            "==== Expected input format of Evaluator for ogbn-proteins\n",
            "{'y_true': y_true, 'y_pred': y_pred}\n",
            "- y_true: numpy ndarray or torch tensor of shape (num_node, num_task)\n",
            "- y_pred: numpy ndarray or torch tensor of shape (num_node, num_task)\n",
            "where y_pred stores score values (for computing ROC-AUC),\n",
            "num_task is 112, and each row corresponds to one node.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "# There is only one graph in Node Property Prediction datasets\n",
        "ogbn_proteins_main_graph, ogbn_proteins_main_labels = dataset[0]\n",
        "\n",
        "'''\n",
        "  OGBN-Proteins\n",
        "    #Nodes = 132,534\n",
        "    #Edges = 39,561,252\n",
        "    #Diameter ~ 9 (https://cs.stanford.edu/people/jure/pubs/ogb-neurips20.pdf)\n",
        "    #Tasks = 112\n",
        "    #Split Type = Species\n",
        "    #Task Type = Binary classification\n",
        "    #Metric = ROC-AUC\n",
        "\n",
        "    Task:\n",
        "      The task is to predict the presence of protein functions in a multi-label binary classification setup,\n",
        "      where there are 112 kinds of labels to predict in total. \n",
        "      The performance is measured by the average of ROC-AUC scores across the 112 tasks.\n",
        "\n",
        "    #Others:\n",
        "      **undirected**\n",
        "      **weighted**\n",
        "      **typed (according to species)**\n",
        "\n",
        "  (1) Nodes represent proteins\n",
        "    (1.1) The proteins come from 8 species\n",
        "      len(set(graph.ndata['species'].reshape(-1).tolist())) == 8\n",
        "    (1.2) Each node has one feature associated with it (its species)\n",
        "      graph.ndata['species'].shape == (#nodes, 1)\n",
        "  \n",
        "  (2) Edges indicate different types of biologically meaningful associations between proteins\n",
        "    (2.1) All edges come with 8-dimensional features\n",
        "      graph.edata['feat'].shape == (2 * #edges, 8)\n",
        "\n",
        "'''\n",
        "# Get split labels\n",
        "train_label = dataset.labels[split_idx['train']]  # (86619, 112) -- binary values (presence of protein functions)\n",
        "valid_label = dataset.labels[split_idx['valid']]  # (21236, 112) -- binary values (presence of protein functions)\n",
        "test_label = dataset.labels[split_idx['test']]    # (24679, 112) -- binary values (presence of protein functions)\n",
        "\n",
        "# Create masks\n",
        "train_mask = jnp.zeros((ogbn_proteins_main_graph.num_nodes(), 1)).at[jnp.array(split_idx['train'])].set(1)\n",
        "valid_mask = jnp.zeros((ogbn_proteins_main_graph.num_nodes(), 1)).at[jnp.array(split_idx['valid'])].set(1)\n",
        "test_mask = jnp.zeros((ogbn_proteins_main_graph.num_nodes(), 1)).at[jnp.array(split_idx['test'])].set(1)"
      ],
      "metadata": {
        "id": "jCkzIEb4WsXU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jraph\n",
        "\n",
        "# From https://colab.research.google.com/github/deepmind/educational/blob/master/colabs/summer_schools/intro_to_graph_nets_tutorial_with_jraph.ipynb#scrollTo=7vEmAsr5bKN8\n",
        "def _nearest_bigger_power_of_two(x: int) -> int:\n",
        "  \"\"\"Computes the nearest power of two greater than x for padding.\"\"\"\n",
        "  y = 2\n",
        "  while y < x:\n",
        "    y *= 2\n",
        "  return y\n",
        "\n",
        "def pad_graph_to_nearest_power_of_two(\n",
        "    graphs_tuple: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
        "  \"\"\"Pads a batched `GraphsTuple` to the nearest power of two.\n",
        "  For example, if a `GraphsTuple` has 7 nodes, 5 edges and 3 graphs, this method\n",
        "  would pad the `GraphsTuple` nodes and edges:\n",
        "    7 nodes --> 8 nodes (2^3)\n",
        "    5 edges --> 8 edges (2^3)\n",
        "  And since padding is accomplished using `jraph.pad_with_graphs`, an extra\n",
        "  graph and node is added:\n",
        "    8 nodes --> 9 nodes\n",
        "    3 graphs --> 4 graphs\n",
        "  Args:\n",
        "    graphs_tuple: a batched `GraphsTuple` (can be batch size 1).\n",
        "  Returns:\n",
        "    A graphs_tuple batched to the nearest power of two.\n",
        "  \"\"\"\n",
        "  # Add 1 since we need at least one padding node for pad_with_graphs.\n",
        "  pad_nodes_to = _nearest_bigger_power_of_two(jnp.sum(graphs_tuple.n_node)) + 1\n",
        "  pad_edges_to = _nearest_bigger_power_of_two(jnp.sum(graphs_tuple.n_edge))\n",
        "  # Add 1 since we need at least one padding graph for pad_with_graphs.\n",
        "  # We do not pad to nearest power of two because the batch size is fixed.\n",
        "  pad_graphs_to = graphs_tuple.n_node.shape[0] + 1\n",
        "  return jraph.pad_with_graphs(graphs_tuple, pad_nodes_to, pad_edges_to,\n",
        "                               pad_graphs_to)"
      ],
      "metadata": {
        "id": "FSbePOUh2NBB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "from random import randint\n",
        "import numpy as np\n",
        "import sharded_graphnet\n",
        "\n",
        "def get_demo_training_graph():\n",
        "  num_nodes = 16\n",
        "  num_edges = 8\n",
        "\n",
        "  rand_dgl_graph = dgl.rand_graph(num_nodes = num_nodes, num_edges = num_edges)\n",
        "\n",
        "  node_features = jnp.array([[randint(0, 7)] for i in range(num_nodes)])\n",
        "  edge_features = jnp.array([[0.1 * randint(0, 10) for _ in range(8)] for i in range(num_edges)])\n",
        "\n",
        "  senders = jnp.array(rand_dgl_graph.edges()[0])\n",
        "  receivers = jnp.array(rand_dgl_graph.edges()[1])\n",
        "\n",
        "  in_tuple = jraph.GraphsTuple(\n",
        "            nodes = node_features.astype(np.float32),\n",
        "            edges = edge_features.astype(np.float32),  \n",
        "            senders = senders.astype(np.int32), \n",
        "            receivers = receivers.astype(np.int32),\n",
        "            n_node = jnp.array([num_nodes]), \n",
        "            n_edge = jnp.array([num_edges]),\n",
        "            globals = None  # No global features\n",
        "          )\n",
        "  \n",
        "  labels = jnp.array([[randint(0, 1) for j in range(112)] for i in range(num_nodes)])\n",
        "  train_mask = jnp.ones((num_nodes, 1))\n",
        "  \n",
        "  in_tuple = in_tuple._replace(\n",
        "      nodes = {\n",
        "          'inputs': in_tuple.nodes, \n",
        "          'targets': labels, \n",
        "          'train_mask': train_mask, \n",
        "          }\n",
        "  )\n",
        "\n",
        "  # in_tuple = pad_graph_to_nearest_power_of_two(in_tuple)\n",
        "  \n",
        "  return sharded_graphnet.graphs_tuple_to_broadcasted_sharded_graphs_tuple(\n",
        "      in_tuple,\n",
        "      num_shards = num_devices\n",
        "      )\n",
        "  \n",
        "demo_graph = get_demo_training_graph()\n",
        "demo_labels = demo_graph.nodes['targets']\n",
        "demo_mask = demo_graph.nodes['train_mask']\n",
        "demo_graph = demo_graph._replace(nodes = demo_graph.nodes['inputs']) "
      ],
      "metadata": {
        "id": "SKoX4h1z9ItW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import jraph\n",
        "import sharded_graphnet\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "enc = OneHotEncoder()\n",
        "enc.fit(ogbn_proteins_main_graph.ndata['species'])\n",
        "\n",
        "def dgl_graph_to_jraph(node_ids, labels, train_mask, valid_mask, test_mask):\n",
        "  # First add back the node and edge features\n",
        "  dgl_graph_with_features = dgl.node_subgraph(ogbn_proteins_main_graph, node_ids)\n",
        "\n",
        "  node_features = jnp.array(enc.transform(dgl_graph_with_features.ndata['species']).toarray())\n",
        "  senders = jnp.array(dgl_graph_with_features.edges()[0])\n",
        "  receivers = jnp.array(dgl_graph_with_features.edges()[1])\n",
        "\n",
        "  # Edges -- here we should include the 8-dimensional edge features\n",
        "  edges = jnp.array(dgl_graph_with_features.edata['feat'])\n",
        "\n",
        "  n_node = jnp.array([dgl_graph_with_features.num_nodes()])\n",
        "  n_edge = jnp.array([dgl_graph_with_features.num_edges()])\n",
        "\n",
        "  in_tuple = jraph.GraphsTuple(\n",
        "            nodes = node_features.astype(np.float32),\n",
        "            senders = senders.astype(np.int32), \n",
        "            receivers = receivers.astype(np.int32),\n",
        "            edges = edges.astype(np.float32),  \n",
        "            n_node = n_node, \n",
        "            n_edge = n_edge,\n",
        "            globals = None  # No global features\n",
        "          )\n",
        "  \n",
        "  in_tuple = in_tuple._replace(\n",
        "      nodes = {\n",
        "          'inputs': in_tuple.nodes, \n",
        "          'targets': labels, \n",
        "          'train_mask': train_mask, \n",
        "          'valid_mask': valid_mask, \n",
        "          'test_mask': test_mask,\n",
        "          'padding_mask': jnp.ones((in_tuple.nodes.shape[0], 1)) \n",
        "                                                        # TODO: Check this above\n",
        "                                                        # Adding this mask so that we can remove the nodes added after padding \n",
        "                                                        # for the final ROC computations on the full train / valid / test splits\n",
        "                                                        # This is because I want to pass the predictions on the true nodes to the \n",
        "                                                        # ogbn-evaluator, so I would first need to remove the predictions that come from padding.\n",
        "          }\n",
        "  )\n",
        "\n",
        "  in_tuple = pad_graph_to_nearest_power_of_two(in_tuple)\n",
        "  \n",
        "  return sharded_graphnet.graphs_tuple_to_broadcasted_sharded_graphs_tuple(\n",
        "      in_tuple,\n",
        "      num_shards = num_devices\n",
        "      )\n",
        "  \n",
        "def get_labels_for_subgraph(node_ids):\n",
        "  return jnp.array(ogbn_proteins_main_labels.index_select(0, node_ids))"
      ],
      "metadata": {
        "id": "fvH_XRJVWuLw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "\n",
        "'''\n",
        "  Generate graph partition using metis, with balanced number of edges in each partition.\n",
        "  Note: \n",
        "    The subgraphs do not contain the node/edge data in the input graph (https://docs.dgl.ai/generated/dgl.metis_partition.html)\n",
        "'''\n",
        "num_partitions = 50  ## TODO: Find some way to decrease this to something reasonable (< 50)\n",
        "\n",
        "dgl_graph_metis_partition = dgl.metis_partition(ogbn_proteins_main_graph, num_partitions, balance_edges = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUI9s4-0mPz9",
        "outputId": "bbadcb1a-2b04-41b2-8dca-64a8ab120362"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convert a graph into a bidirected graph: 1.891 seconds\n",
            "Construct multi-constraint weights: 0.014 seconds\n",
            "Metis partitioning: 24.423 seconds\n",
            "Split the graph: 0.601 seconds\n",
            "Construct subgraphs: 0.031 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert graphs to Jraph GraphsTuple\n",
        "processed_graphs = {}\n",
        "\n",
        "for idx in range(num_partitions):\n",
        "  node_ids = dgl_graph_metis_partition[idx].ndata['_ID']\n",
        "\n",
        "  labels = get_labels_for_subgraph(node_ids)\n",
        "  graph = dgl_graph_to_jraph(node_ids, \n",
        "                             labels, \n",
        "                             train_mask = train_mask.at[jnp.array(node_ids)].get(),\n",
        "                             valid_mask = valid_mask.at[jnp.array(node_ids)].get(),\n",
        "                             test_mask = test_mask.at[jnp.array(node_ids)].get()\n",
        "                             )\n",
        "\n",
        "  processed_graphs[f'partition_{idx}'] = {\n",
        "      'graph': graph._replace(nodes = graph.nodes['inputs']), \n",
        "      'labels': graph.nodes['targets'],\n",
        "      'train_mask': graph.nodes['train_mask'],\n",
        "      'valid_mask': graph.nodes['valid_mask'],\n",
        "      'test_mask': graph.nodes['test_mask'],\n",
        "      'padding_mask': graph.nodes['padding_mask']\n",
        "      }"
      ],
      "metadata": {
        "id": "s8-Ln58I_Fwp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import haiku as hk\n",
        "import jax\n",
        "import optax\n",
        "\n",
        "from typing import Sequence\n",
        "\n",
        "# See https://github.com/YuxuanXie/mcl/blob/5f7ee92e2a6bc89736263873a4ba9c14d1a676ff/glassy_dynamics/train_using_jax.py for alternative to using GraphMapFeatures\n",
        "# From https://github.com/YuxuanXie/mcl/blob/5f7ee92e2a6bc89736263873a4ba9c14d1a676ff/glassy_dynamics/train_using_jax.py\n",
        "\n",
        "hidden_dimension = 128\n",
        "num_message_passing_steps = 5 # Question: (256, 4) fails / (128, 6) works\n",
        "\n",
        "@jraph.concatenated_args\n",
        "def node_update_fn(feats: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Node update function for graph net.\"\"\"\n",
        "  net = hk.Sequential([hk.nets.MLP(output_sizes = [hidden_dimension, hidden_dimension], activation = jax.nn.relu, activate_final = True), hk.LayerNorm(axis = -1, create_scale = True, create_offset = True)])\n",
        "  return net(feats)\n",
        "\n",
        "@jraph.concatenated_args\n",
        "def edge_update_fn(feats: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Edge update function for graph net.\"\"\"\n",
        "  net = hk.Sequential([hk.nets.MLP(output_sizes = [hidden_dimension, hidden_dimension], activation = jax.nn.relu, activate_final = True), hk.LayerNorm(axis = -1, create_scale = True, create_offset = True)])\n",
        "  return net(feats)\n",
        "\n",
        "@hk.without_apply_rng\n",
        "@hk.transform\n",
        "def network_definition(graph):\n",
        "  \"\"\"Defines a graph neural network.\n",
        "  Args:\n",
        "    graph: Graphstuple the network processes.\n",
        "  Returns:\n",
        "    Decoded nodes.\n",
        "  \"\"\"\n",
        "  graph = graph._replace(\n",
        "      nodes = hk.Linear(hidden_dimension)(graph.nodes),\n",
        "      device_edges = hk.Linear(hidden_dimension)(graph.device_edges)\n",
        "  )\n",
        "  \n",
        "  sharded_gn = sharded_graphnet.ShardedEdgesGraphNetwork(\n",
        "      update_node_fn = node_update_fn,\n",
        "      update_edge_fn = edge_update_fn,\n",
        "      num_shards = num_devices\n",
        "      )\n",
        "\n",
        "  for _ in range(num_message_passing_steps):\n",
        "    residual_graph = sharded_gn(graph)\n",
        "    graph = graph._replace(\n",
        "        nodes = graph.nodes + residual_graph.nodes, # Question: Should I be using relu afterwards?\n",
        "        device_edges = graph.device_edges + residual_graph.device_edges\n",
        "    )\n",
        "\n",
        "    # graph = sharded_gn(graph)\n",
        "\n",
        "  graph = graph._replace(\n",
        "      nodes = hk.Sequential([hk.Linear(hidden_dimension), jax.nn.relu, hk.Linear(112)])(graph.nodes)\n",
        "  )\n",
        "  return graph.nodes"
      ],
      "metadata": {
        "id": "gPg7ph7sWyOn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bcast_local_devices(value):\n",
        "    \"\"\"Broadcasts an object to all local devices.\"\"\"\n",
        "    devices = jax.local_devices()\n",
        "\n",
        "    def _replicate(x):\n",
        "      \"\"\"Replicate an object on each device.\"\"\"\n",
        "      x = jnp.array(x)\n",
        "      return jax.device_put_sharded(len(devices) * [x], devices)\n",
        "\n",
        "    return jax.tree_util.tree_map(_replicate, value)"
      ],
      "metadata": {
        "id": "z6Qh75qxQfii"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reshape_broadcasted_data(data):\n",
        "  '''\n",
        "    Node predictions / Labels / Masks are identical on all the devices so we only take\n",
        "    one of them in order to remove the leading axis.\n",
        "  '''\n",
        "  return np.array(data)[0]\n",
        "  \n",
        "def remove_mask_from_data(data, mask):\n",
        "  '''\n",
        "    data.shape = [num_nodes, 112]\n",
        "    mask.shape = [num_nodes, 1]\n",
        "\n",
        "    We want to only return the data where mask == True\n",
        "  '''\n",
        "  sliced_data = np.compress(np.array(mask).reshape(-1).astype(bool), data, axis = 0)\n",
        "  return np.array(sliced_data)"
      ],
      "metadata": {
        "id": "oJ5T_oplbg_t"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import functools\n",
        "import haiku as hk\n",
        "\n",
        "from random import randint\n",
        "from google.colab import files\n",
        "\n",
        "# Try to follow this tutorial https://github.com/YuxuanXie/mcl/blob/5f7ee92e2a6bc89736263873a4ba9c14d1a676ff/glassy_dynamics/train_using_jax.py\n",
        "def compute_loss(params, graph, label, mask):\n",
        "  predictions = network_definition.apply(params, graph)\n",
        "\n",
        "  # use optax here (https://github.com/deepmind/optax/blob/master/optax/_src/loss.py#L116#L139)\n",
        "  loss = optax.sigmoid_binary_cross_entropy(predictions, label)  # shape [num_nodes, num_classes]\n",
        "  loss = loss * mask\n",
        "  loss = jnp.sum(loss) / jnp.sum(mask) # loss = mean_with_mask(loss, mask)\n",
        "\n",
        "  return loss\n",
        "\n",
        "def train(num_training_steps, results_path):\n",
        "  replicated_params = jax.pmap(network_definition.init, axis_name = 'i')(bcast_local_devices(jax.random.PRNGKey(42)), processed_graphs['partition_0']['graph'])\n",
        "\n",
        "  opt_init, opt_update = optax.adam(learning_rate = 0.001)  ## TODO: Maybe modify the learning rate (0.01 used in some paper for ogbn-proteins) https://arxiv.org/pdf/1609.02907.pdf  \n",
        "  replicated_opt_state = jax.pmap(opt_init, axis_name = 'i')(replicated_params)\n",
        "\n",
        "  @functools.partial(jax.pmap, axis_name='i')\n",
        "  def update(params, opt_state, graph, targets, mask):\n",
        "    # Compute the gradients on the given minibatch (individually on each device).\n",
        "    loss, grads = jax.value_and_grad(compute_loss)(params, graph, targets, mask)\n",
        "\n",
        "    # Combine the gradient across all devices (by taking their mean).\n",
        "    grads = jax.lax.pmean(grads, axis_name='i')\n",
        "\n",
        "    # Also combine the loss. Unnecessary for the update, but useful for logging.\n",
        "    loss = jax.lax.pmean(loss, axis_name='i')\n",
        "\n",
        "    updates, opt_state = opt_update(grads, opt_state)\n",
        "\n",
        "    return optax.apply_updates(params, updates), opt_state, loss\n",
        "\n",
        "  # Train\n",
        "  for idx in range(num_training_steps):\n",
        "    random_partition_idx = randint(0, num_partitions - 1)\n",
        "    random_partition = processed_graphs[f'partition_{random_partition_idx}']\n",
        "\n",
        "    graph = random_partition['graph']\n",
        "    labels = random_partition['labels']   # Automatically broadcasted by the sharded graph net\n",
        "    mask = random_partition['train_mask'] # Automatically broadcasted by the sharded graph net\n",
        "\n",
        "    replicated_params, replicated_opt_state, loss = update(\n",
        "        replicated_params, \n",
        "        replicated_opt_state, \n",
        "        graph, \n",
        "        labels,\n",
        "        mask\n",
        "        )\n",
        "    \n",
        "    print('Loss training:', reshape_broadcasted_data(loss))\n",
        "\n",
        "    if (idx + 1) % 10 == 0:\n",
        "      print()\n",
        "      print(f'***************************')\n",
        "      print(f'Trained on {idx + 1} graphs')\n",
        "      print(f'***************************')\n",
        "      print()\n",
        "\n",
        "    if (idx + 1) % 2000 == 0:\n",
        "      # Save parameters every 1000 epochs\n",
        "      params_file = f'{results_path}/params_epochs_{idx + 1}.pickle'\n",
        "      opt_state_file = f'{results_path}/opt_state_epochs_{idx + 1}.pickle'\n",
        "\n",
        "      with open(params_file, 'wb') as f:\n",
        "        # Save parameters to file\n",
        "        pickle.dump(replicated_params, f)\n",
        "\n",
        "        # Download in case workspace gets restarted\n",
        "        files.download(params_file)\n",
        "\n",
        "      with open(opt_state_file, 'wb') as f:\n",
        "        # Save optimiser state to file\n",
        "        pickle.dump(replicated_opt_state, f)\n",
        "\n",
        "        # Download in case workspace gets restarted\n",
        "        files.download(opt_state_file)\n",
        "\n",
        "  return replicated_params\n",
        "\n",
        "def evaluate(params, num_graphs_eval):\n",
        "  # Evaluate\n",
        "  accumulated_loss = 0.0\n",
        "  accumulated_roc = 0\n",
        "  graphs_evaluated = 0\n",
        "\n",
        "  print('**** Note that this uses the train_mask ****')\n",
        "  print('**** Note that this uses the train_mask ****')\n",
        "  print('**** Note that this uses the train_mask ****')\n",
        "\n",
        "  for idx in range(num_graphs_eval):\n",
        "    random_partition_idx = idx\n",
        "    random_partition = processed_graphs[f'partition_{random_partition_idx}']\n",
        "\n",
        "    graph = random_partition['graph']\n",
        "    labels = random_partition['labels']     # Automatically broadcasted by the sharded graph net\n",
        "    mask = random_partition['train_mask']    # Automatically broadcasted by the sharded graph net ## TODO: Change this back\n",
        "\n",
        "    predictions, loss = predict_on_graph(params, graph, labels, mask)\n",
        "    loss = reshape_broadcasted_data(loss)\n",
        "    \n",
        "    collected_labels = reshape_broadcasted_data(labels)\n",
        "    collected_predictions = reshape_broadcasted_data(predictions)\n",
        "    collected_mask = reshape_broadcasted_data(mask)\n",
        "\n",
        "    try:\n",
        "      roc = evaluator.eval({\n",
        "          \"y_true\": remove_mask_from_data(collected_labels, collected_mask), \n",
        "          \"y_pred\": remove_mask_from_data(collected_predictions, collected_mask)\n",
        "          })['rocauc']\n",
        "\n",
        "      accumulated_loss += loss\n",
        "      accumulated_roc += roc\n",
        "      graphs_evaluated += 1\n",
        "\n",
        "      print(f'Test loss: {loss} | ROC: {roc}')\n",
        "    except Exception as err:\n",
        "      print(f'Could not compute ROC for partition {idx}')\n",
        "      print('Most likely all of the nodes are hidden at test / validation')\n",
        "      print('Check counts in mask')\n",
        "      print(np.unique(collected_mask, return_counts = True))\n",
        "      print('Check counts in labels after removing the test / validation mask')\n",
        "      print(np.unique(remove_mask_from_data(collected_labels, collected_mask), return_counts = True))\n",
        "      print(f'Error message: {str(err)}')\n",
        "      print()\n",
        "\n",
        "    if (idx + 1) % 10 == 0:\n",
        "      print()\n",
        "      print(f'***************************')\n",
        "      print(f'Evaluated on {idx + 1} graphs')\n",
        "      print(f'***************************')\n",
        "      print()\n",
        "\n",
        "  print(f'Average test loss: {accumulated_loss / graphs_evaluated} | Average ROC: {accumulated_roc / graphs_evaluated}')\n",
        "\n",
        "@functools.partial(jax.pmap, axis_name='i')\n",
        "def predict_on_graph(params, graph, label, mask):\n",
        "  decoded_nodes = network_definition.apply(params, graph)\n",
        "\n",
        "  compute_loss_fn = functools.partial(compute_loss)\n",
        "  loss = compute_loss_fn(params, graph, label, mask)\n",
        "\n",
        "  return jax.nn.sigmoid(decoded_nodes), loss"
      ],
      "metadata": {
        "id": "xYVzddNITMSv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "current_time = datetime.today().strftime('%Y-%m-%d-%H:%M:%S')\n",
        "exp_path = f'/content/exp_{current_time}/'\n",
        "os.makedirs(exp_path, exist_ok = False)\n",
        "\n",
        "final_params = train(num_training_steps = 3000, results_path = exp_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "omSyev-9yNPQ",
        "outputId": "64956e5e-72c6-4829-d7ab-47ecfb69f585"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss training: 99.76628\n",
            "Loss training: 86.45688\n",
            "Loss training: 76.63085\n",
            "Loss training: 70.91376\n",
            "Loss training: 66.32066\n",
            "Loss training: 69.68262\n",
            "Loss training: 56.54441\n",
            "Loss training: 54.27189\n",
            "Loss training: 52.84412\n",
            "Loss training: 63.652992\n",
            "\n",
            "***************************\n",
            "Trained on 10 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.06473\n",
            "Loss training: 68.47095\n",
            "Loss training: 49.33829\n",
            "Loss training: 64.94133\n",
            "Loss training: 41.242874\n",
            "Loss training: 61.253242\n",
            "Loss training: 46.7417\n",
            "Loss training: 28.932396\n",
            "Loss training: 36.569168\n",
            "Loss training: 51.10079\n",
            "\n",
            "***************************\n",
            "Trained on 20 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 52.355667\n",
            "Loss training: 45.655174\n",
            "Loss training: 51.6027\n",
            "Loss training: 36.248417\n",
            "Loss training: 22.49827\n",
            "Loss training: 33.87976\n",
            "Loss training: 53.994198\n",
            "Loss training: 35.311474\n",
            "Loss training: 20.027168\n",
            "Loss training: 42.409565\n",
            "\n",
            "***************************\n",
            "Trained on 30 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 18.715042\n",
            "Loss training: 68.38434\n",
            "Loss training: 40.747997\n",
            "Loss training: 49.175682\n",
            "Loss training: 37.00435\n",
            "Loss training: 62.94807\n",
            "Loss training: 34.847084\n",
            "Loss training: 53.896492\n",
            "Loss training: 40.684986\n",
            "Loss training: 57.55868\n",
            "\n",
            "***************************\n",
            "Trained on 40 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 21.863806\n",
            "Loss training: 49.48129\n",
            "Loss training: 52.77484\n",
            "Loss training: 36.41816\n",
            "Loss training: 48.219975\n",
            "Loss training: 57.288803\n",
            "Loss training: 30.483488\n",
            "Loss training: 37.89301\n",
            "Loss training: 67.23395\n",
            "Loss training: 64.35144\n",
            "\n",
            "***************************\n",
            "Trained on 50 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 47.44109\n",
            "Loss training: 48.936043\n",
            "Loss training: 43.735203\n",
            "Loss training: 47.428574\n",
            "Loss training: 33.48418\n",
            "Loss training: 53.162014\n",
            "Loss training: 37.11664\n",
            "Loss training: 22.004023\n",
            "Loss training: 34.237007\n",
            "Loss training: 52.600517\n",
            "\n",
            "***************************\n",
            "Trained on 60 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 48.22561\n",
            "Loss training: 25.971872\n",
            "Loss training: 42.14562\n",
            "Loss training: 51.67865\n",
            "Loss training: 46.12702\n",
            "Loss training: 45.863983\n",
            "Loss training: 31.707893\n",
            "Loss training: 53.71326\n",
            "Loss training: 48.78309\n",
            "Loss training: 49.434223\n",
            "\n",
            "***************************\n",
            "Trained on 70 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.04692\n",
            "Loss training: 35.88339\n",
            "Loss training: 45.609436\n",
            "Loss training: 29.373707\n",
            "Loss training: 42.47314\n",
            "Loss training: 46.378647\n",
            "Loss training: 37.88037\n",
            "Loss training: 37.89003\n",
            "Loss training: 37.32653\n",
            "Loss training: 38.019596\n",
            "\n",
            "***************************\n",
            "Trained on 80 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.383377\n",
            "Loss training: 34.974728\n",
            "Loss training: 37.966007\n",
            "Loss training: 65.64281\n",
            "Loss training: 39.352165\n",
            "Loss training: 30.628683\n",
            "Loss training: 34.851818\n",
            "Loss training: 49.509766\n",
            "Loss training: 50.581364\n",
            "Loss training: 49.806835\n",
            "\n",
            "***************************\n",
            "Trained on 90 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.591988\n",
            "Loss training: 63.90165\n",
            "Loss training: 45.65435\n",
            "Loss training: 34.957848\n",
            "Loss training: 39.34299\n",
            "Loss training: 39.060528\n",
            "Loss training: 41.749584\n",
            "Loss training: 38.56069\n",
            "Loss training: 47.274185\n",
            "Loss training: 46.770878\n",
            "\n",
            "***************************\n",
            "Trained on 100 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 62.63477\n",
            "Loss training: 46.099606\n",
            "Loss training: 38.845837\n",
            "Loss training: 44.609917\n",
            "Loss training: 28.586462\n",
            "Loss training: 48.36397\n",
            "Loss training: 30.850033\n",
            "Loss training: 31.58573\n",
            "Loss training: 34.13251\n",
            "Loss training: 19.249308\n",
            "\n",
            "***************************\n",
            "Trained on 110 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.446049\n",
            "Loss training: 29.438122\n",
            "Loss training: 33.986927\n",
            "Loss training: 42.935272\n",
            "Loss training: 38.039803\n",
            "Loss training: 45.86271\n",
            "Loss training: 31.394676\n",
            "Loss training: 51.580093\n",
            "Loss training: 41.44425\n",
            "Loss training: 51.897842\n",
            "\n",
            "***************************\n",
            "Trained on 120 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 61.0993\n",
            "Loss training: 38.978855\n",
            "Loss training: 37.052055\n",
            "Loss training: 55.54274\n",
            "Loss training: 59.13028\n",
            "Loss training: 51.52532\n",
            "Loss training: 54.844532\n",
            "Loss training: 32.150055\n",
            "Loss training: 39.624474\n",
            "Loss training: 59.265602\n",
            "\n",
            "***************************\n",
            "Trained on 130 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 67.35255\n",
            "Loss training: 44.60691\n",
            "Loss training: 40.875866\n",
            "Loss training: 41.94054\n",
            "Loss training: 42.22574\n",
            "Loss training: 36.02625\n",
            "Loss training: 72.4602\n",
            "Loss training: 44.76843\n",
            "Loss training: 40.60771\n",
            "Loss training: 43.126244\n",
            "\n",
            "***************************\n",
            "Trained on 140 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 46.35886\n",
            "Loss training: 47.427986\n",
            "Loss training: 52.24113\n",
            "Loss training: 41.65736\n",
            "Loss training: 37.13169\n",
            "Loss training: 33.928127\n",
            "Loss training: 48.11821\n",
            "Loss training: 39.12573\n",
            "Loss training: 36.93581\n",
            "Loss training: 38.693153\n",
            "\n",
            "***************************\n",
            "Trained on 150 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 47.036045\n",
            "Loss training: 63.99248\n",
            "Loss training: 64.4478\n",
            "Loss training: 38.24686\n",
            "Loss training: 56.7921\n",
            "Loss training: 32.43319\n",
            "Loss training: 45.690792\n",
            "Loss training: 43.374714\n",
            "Loss training: 63.440563\n",
            "Loss training: 62.56651\n",
            "\n",
            "***************************\n",
            "Trained on 160 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 55.749348\n",
            "Loss training: 38.800705\n",
            "Loss training: 59.22662\n",
            "Loss training: 37.18291\n",
            "Loss training: 49.81899\n",
            "Loss training: 48.04528\n",
            "Loss training: 44.93571\n",
            "Loss training: 56.279514\n",
            "Loss training: 32.836456\n",
            "Loss training: 39.841076\n",
            "\n",
            "***************************\n",
            "Trained on 170 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.1041\n",
            "Loss training: 39.98677\n",
            "Loss training: 22.379206\n",
            "Loss training: 35.56369\n",
            "Loss training: 41.788837\n",
            "Loss training: 37.86869\n",
            "Loss training: 35.389225\n",
            "Loss training: 50.44584\n",
            "Loss training: 32.54781\n",
            "Loss training: 22.614489\n",
            "\n",
            "***************************\n",
            "Trained on 180 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.056137\n",
            "Loss training: 35.680225\n",
            "Loss training: 61.23725\n",
            "Loss training: 33.97092\n",
            "Loss training: 36.784523\n",
            "Loss training: 42.055637\n",
            "Loss training: 27.769445\n",
            "Loss training: 28.322361\n",
            "Loss training: 50.34285\n",
            "Loss training: 39.393017\n",
            "\n",
            "***************************\n",
            "Trained on 190 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.388756\n",
            "Loss training: 37.280384\n",
            "Loss training: 53.541954\n",
            "Loss training: 39.753536\n",
            "Loss training: 25.399319\n",
            "Loss training: 63.250908\n",
            "Loss training: 47.32095\n",
            "Loss training: 70.51974\n",
            "Loss training: 44.360172\n",
            "Loss training: 34.199535\n",
            "\n",
            "***************************\n",
            "Trained on 200 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.15518\n",
            "Loss training: 45.861942\n",
            "Loss training: 45.516045\n",
            "Loss training: 38.683987\n",
            "Loss training: 34.22386\n",
            "Loss training: 36.10799\n",
            "Loss training: 41.7139\n",
            "Loss training: 39.771122\n",
            "Loss training: 42.092804\n",
            "Loss training: 28.60007\n",
            "\n",
            "***************************\n",
            "Trained on 210 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.204002\n",
            "Loss training: 22.102821\n",
            "Loss training: 41.272575\n",
            "Loss training: 37.322594\n",
            "Loss training: 33.859837\n",
            "Loss training: 53.84253\n",
            "Loss training: 53.208336\n",
            "Loss training: 36.91085\n",
            "Loss training: 64.93589\n",
            "Loss training: 38.251347\n",
            "\n",
            "***************************\n",
            "Trained on 220 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.946396\n",
            "Loss training: 49.95605\n",
            "Loss training: 31.510927\n",
            "Loss training: 42.206036\n",
            "Loss training: 56.96865\n",
            "Loss training: 38.426384\n",
            "Loss training: 56.70347\n",
            "Loss training: 47.036526\n",
            "Loss training: 32.245518\n",
            "Loss training: 35.13739\n",
            "\n",
            "***************************\n",
            "Trained on 230 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.85231\n",
            "Loss training: 33.61725\n",
            "Loss training: 35.449287\n",
            "Loss training: 37.436275\n",
            "Loss training: 57.414642\n",
            "Loss training: 18.132877\n",
            "Loss training: 28.320238\n",
            "Loss training: 21.312319\n",
            "Loss training: 50.293404\n",
            "Loss training: 55.048027\n",
            "\n",
            "***************************\n",
            "Trained on 240 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 58.007927\n",
            "Loss training: 32.75673\n",
            "Loss training: 21.370806\n",
            "Loss training: 45.64675\n",
            "Loss training: 41.50687\n",
            "Loss training: 19.37804\n",
            "Loss training: 44.219177\n",
            "Loss training: 30.633728\n",
            "Loss training: 43.901295\n",
            "Loss training: 49.779873\n",
            "\n",
            "***************************\n",
            "Trained on 250 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 74.117714\n",
            "Loss training: 37.814075\n",
            "Loss training: 42.67439\n",
            "Loss training: 20.67862\n",
            "Loss training: 40.520138\n",
            "Loss training: 39.66234\n",
            "Loss training: 28.424986\n",
            "Loss training: 30.491219\n",
            "Loss training: 62.331738\n",
            "Loss training: 52.310272\n",
            "\n",
            "***************************\n",
            "Trained on 260 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.118927\n",
            "Loss training: 42.247635\n",
            "Loss training: 40.781124\n",
            "Loss training: 22.120466\n",
            "Loss training: 72.61105\n",
            "Loss training: 37.10056\n",
            "Loss training: 39.448586\n",
            "Loss training: 38.97091\n",
            "Loss training: 42.26197\n",
            "Loss training: 46.976494\n",
            "\n",
            "***************************\n",
            "Trained on 270 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 49.97238\n",
            "Loss training: 37.498104\n",
            "Loss training: 31.018831\n",
            "Loss training: 29.670326\n",
            "Loss training: 44.24048\n",
            "Loss training: 48.23377\n",
            "Loss training: 60.058575\n",
            "Loss training: 57.329403\n",
            "Loss training: 44.654217\n",
            "Loss training: 52.53388\n",
            "\n",
            "***************************\n",
            "Trained on 280 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 51.69491\n",
            "Loss training: 43.489563\n",
            "Loss training: 44.241768\n",
            "Loss training: 56.276276\n",
            "Loss training: 54.640556\n",
            "Loss training: 31.82955\n",
            "Loss training: 58.933525\n",
            "Loss training: 58.019394\n",
            "Loss training: 44.06865\n",
            "Loss training: 44.067333\n",
            "\n",
            "***************************\n",
            "Trained on 290 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.362104\n",
            "Loss training: 37.12709\n",
            "Loss training: 48.605507\n",
            "Loss training: 52.547752\n",
            "Loss training: 34.779392\n",
            "Loss training: 35.829388\n",
            "Loss training: 31.509869\n",
            "Loss training: 29.853018\n",
            "Loss training: 52.423687\n",
            "Loss training: 33.884678\n",
            "\n",
            "***************************\n",
            "Trained on 300 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 49.596783\n",
            "Loss training: 65.796074\n",
            "Loss training: 65.10217\n",
            "Loss training: 42.591293\n",
            "Loss training: 39.46858\n",
            "Loss training: 36.280804\n",
            "Loss training: 30.163578\n",
            "Loss training: 36.339718\n",
            "Loss training: 31.071375\n",
            "Loss training: 57.01021\n",
            "\n",
            "***************************\n",
            "Trained on 310 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.80408\n",
            "Loss training: 37.19153\n",
            "Loss training: 40.68341\n",
            "Loss training: 42.4177\n",
            "Loss training: 59.666683\n",
            "Loss training: 37.336452\n",
            "Loss training: 55.34745\n",
            "Loss training: 40.258682\n",
            "Loss training: 54.333313\n",
            "Loss training: 35.27713\n",
            "\n",
            "***************************\n",
            "Trained on 320 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.843264\n",
            "Loss training: 39.080517\n",
            "Loss training: 30.669222\n",
            "Loss training: 47.66368\n",
            "Loss training: 56.211456\n",
            "Loss training: 65.741745\n",
            "Loss training: 57.207073\n",
            "Loss training: 38.8781\n",
            "Loss training: 59.350998\n",
            "Loss training: 38.425156\n",
            "\n",
            "***************************\n",
            "Trained on 330 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.10818\n",
            "Loss training: 54.283157\n",
            "Loss training: 44.288116\n",
            "Loss training: 41.142624\n",
            "Loss training: 26.824812\n",
            "Loss training: 46.82056\n",
            "Loss training: 31.79193\n",
            "Loss training: 31.808142\n",
            "Loss training: 32.49404\n",
            "Loss training: 37.96883\n",
            "\n",
            "***************************\n",
            "Trained on 340 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.698032\n",
            "Loss training: 32.703392\n",
            "Loss training: 37.6744\n",
            "Loss training: 31.973871\n",
            "Loss training: 66.58881\n",
            "Loss training: 64.49243\n",
            "Loss training: 35.524242\n",
            "Loss training: 48.57239\n",
            "Loss training: 42.5771\n",
            "Loss training: 34.587635\n",
            "\n",
            "***************************\n",
            "Trained on 350 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 58.518295\n",
            "Loss training: 52.387383\n",
            "Loss training: 42.5454\n",
            "Loss training: 33.72655\n",
            "Loss training: 33.812775\n",
            "Loss training: 39.591743\n",
            "Loss training: 53.115147\n",
            "Loss training: 35.921356\n",
            "Loss training: 49.638664\n",
            "Loss training: 36.069534\n",
            "\n",
            "***************************\n",
            "Trained on 360 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.15008\n",
            "Loss training: 35.08919\n",
            "Loss training: 34.707684\n",
            "Loss training: 31.12952\n",
            "Loss training: 34.44578\n",
            "Loss training: 46.6572\n",
            "Loss training: 49.82259\n",
            "Loss training: 22.74531\n",
            "Loss training: 26.706877\n",
            "Loss training: 73.13397\n",
            "\n",
            "***************************\n",
            "Trained on 370 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.855068\n",
            "Loss training: 33.786095\n",
            "Loss training: 51.139484\n",
            "Loss training: 63.874775\n",
            "Loss training: 34.97756\n",
            "Loss training: 22.37624\n",
            "Loss training: 64.803894\n",
            "Loss training: 38.860065\n",
            "Loss training: 57.881447\n",
            "Loss training: 46.385838\n",
            "\n",
            "***************************\n",
            "Trained on 380 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.521036\n",
            "Loss training: 42.231712\n",
            "Loss training: 37.05567\n",
            "Loss training: 51.949936\n",
            "Loss training: 32.513885\n",
            "Loss training: 33.647636\n",
            "Loss training: 35.454105\n",
            "Loss training: 22.19122\n",
            "Loss training: 40.403763\n",
            "Loss training: 30.147003\n",
            "\n",
            "***************************\n",
            "Trained on 390 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 48.44797\n",
            "Loss training: 36.31151\n",
            "Loss training: 30.13145\n",
            "Loss training: 43.84641\n",
            "Loss training: 31.417816\n",
            "Loss training: 42.209003\n",
            "Loss training: 36.449135\n",
            "Loss training: 45.255558\n",
            "Loss training: 69.26304\n",
            "Loss training: 19.872225\n",
            "\n",
            "***************************\n",
            "Trained on 400 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.81926\n",
            "Loss training: 56.381878\n",
            "Loss training: 42.976463\n",
            "Loss training: 54.26528\n",
            "Loss training: 42.82622\n",
            "Loss training: 38.521614\n",
            "Loss training: 60.035088\n",
            "Loss training: 52.72698\n",
            "Loss training: 30.699686\n",
            "Loss training: 34.70892\n",
            "\n",
            "***************************\n",
            "Trained on 410 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 48.474014\n",
            "Loss training: 24.504845\n",
            "Loss training: 50.81433\n",
            "Loss training: 23.119333\n",
            "Loss training: 37.380302\n",
            "Loss training: 28.631523\n",
            "Loss training: 37.510643\n",
            "Loss training: 46.219677\n",
            "Loss training: 40.207314\n",
            "Loss training: 50.07021\n",
            "\n",
            "***************************\n",
            "Trained on 420 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.302078\n",
            "Loss training: 37.102444\n",
            "Loss training: 39.38127\n",
            "Loss training: 32.276184\n",
            "Loss training: 36.534748\n",
            "Loss training: 57.01292\n",
            "Loss training: 30.137844\n",
            "Loss training: 49.853855\n",
            "Loss training: 61.422802\n",
            "Loss training: 30.003819\n",
            "\n",
            "***************************\n",
            "Trained on 430 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 49.43216\n",
            "Loss training: 38.97292\n",
            "Loss training: 53.564075\n",
            "Loss training: 55.909615\n",
            "Loss training: 43.687958\n",
            "Loss training: 47.623245\n",
            "Loss training: 57.739983\n",
            "Loss training: 61.948463\n",
            "Loss training: 27.18589\n",
            "Loss training: 49.222507\n",
            "\n",
            "***************************\n",
            "Trained on 440 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 47.488636\n",
            "Loss training: 31.390192\n",
            "Loss training: 37.684105\n",
            "Loss training: 55.828102\n",
            "Loss training: 37.124702\n",
            "Loss training: 51.449085\n",
            "Loss training: 31.579056\n",
            "Loss training: 45.96549\n",
            "Loss training: 51.79198\n",
            "Loss training: 46.59567\n",
            "\n",
            "***************************\n",
            "Trained on 450 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 63.0834\n",
            "Loss training: 29.449112\n",
            "Loss training: 38.220177\n",
            "Loss training: 61.772583\n",
            "Loss training: 31.271023\n",
            "Loss training: 52.360546\n",
            "Loss training: 55.75773\n",
            "Loss training: 37.466885\n",
            "Loss training: 42.559036\n",
            "Loss training: 39.314472\n",
            "\n",
            "***************************\n",
            "Trained on 460 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.479042\n",
            "Loss training: 40.82544\n",
            "Loss training: 39.040146\n",
            "Loss training: 32.45792\n",
            "Loss training: 19.776302\n",
            "Loss training: 19.26031\n",
            "Loss training: 18.948442\n",
            "Loss training: 61.783916\n",
            "Loss training: 65.4417\n",
            "Loss training: 18.003004\n",
            "\n",
            "***************************\n",
            "Trained on 470 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 57.203808\n",
            "Loss training: 35.741974\n",
            "Loss training: 35.577168\n",
            "Loss training: 45.58881\n",
            "Loss training: 46.522514\n",
            "Loss training: 47.65607\n",
            "Loss training: 38.190254\n",
            "Loss training: 44.831295\n",
            "Loss training: 29.321096\n",
            "Loss training: 46.648666\n",
            "\n",
            "***************************\n",
            "Trained on 480 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 43.332146\n",
            "Loss training: 47.080833\n",
            "Loss training: 43.945507\n",
            "Loss training: 36.731186\n",
            "Loss training: 38.766888\n",
            "Loss training: 19.208471\n",
            "Loss training: 37.16168\n",
            "Loss training: 41.831123\n",
            "Loss training: 37.379234\n",
            "Loss training: 47.15781\n",
            "\n",
            "***************************\n",
            "Trained on 490 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 71.3115\n",
            "Loss training: 32.826725\n",
            "Loss training: 46.099083\n",
            "Loss training: 39.490974\n",
            "Loss training: 18.40129\n",
            "Loss training: 42.241577\n",
            "Loss training: 33.92932\n",
            "Loss training: 31.144783\n",
            "Loss training: 30.872414\n",
            "Loss training: 21.48996\n",
            "\n",
            "***************************\n",
            "Trained on 500 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 58.252697\n",
            "Loss training: 33.59667\n",
            "Loss training: 33.509754\n",
            "Loss training: 55.525116\n",
            "Loss training: 44.8315\n",
            "Loss training: 47.282978\n",
            "Loss training: 29.834362\n",
            "Loss training: 33.47542\n",
            "Loss training: 38.879986\n",
            "Loss training: 37.373062\n",
            "\n",
            "***************************\n",
            "Trained on 510 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 54.833824\n",
            "Loss training: 25.476835\n",
            "Loss training: 23.942066\n",
            "Loss training: 22.841948\n",
            "Loss training: 73.36221\n",
            "Loss training: 47.371513\n",
            "Loss training: 63.320145\n",
            "Loss training: 34.858894\n",
            "Loss training: 34.40156\n",
            "Loss training: 54.75365\n",
            "\n",
            "***************************\n",
            "Trained on 520 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.76321\n",
            "Loss training: 42.66716\n",
            "Loss training: 47.575893\n",
            "Loss training: 46.489307\n",
            "Loss training: 31.36877\n",
            "Loss training: 39.697594\n",
            "Loss training: 39.78406\n",
            "Loss training: 54.986797\n",
            "Loss training: 36.373623\n",
            "Loss training: 38.519043\n",
            "\n",
            "***************************\n",
            "Trained on 530 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.459133\n",
            "Loss training: 51.375168\n",
            "Loss training: 49.592022\n",
            "Loss training: 37.202656\n",
            "Loss training: 31.74208\n",
            "Loss training: 36.547134\n",
            "Loss training: 17.91045\n",
            "Loss training: 59.386032\n",
            "Loss training: 28.729342\n",
            "Loss training: 33.128185\n",
            "\n",
            "***************************\n",
            "Trained on 540 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.27543\n",
            "Loss training: 37.189304\n",
            "Loss training: 21.49445\n",
            "Loss training: 33.192524\n",
            "Loss training: 31.693213\n",
            "Loss training: 47.697052\n",
            "Loss training: 32.48784\n",
            "Loss training: 27.476189\n",
            "Loss training: 43.577156\n",
            "Loss training: 49.901794\n",
            "\n",
            "***************************\n",
            "Trained on 550 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.39661\n",
            "Loss training: 31.158794\n",
            "Loss training: 44.97964\n",
            "Loss training: 53.20466\n",
            "Loss training: 33.508636\n",
            "Loss training: 34.02362\n",
            "Loss training: 45.154934\n",
            "Loss training: 30.16854\n",
            "Loss training: 31.608398\n",
            "Loss training: 50.309612\n",
            "\n",
            "***************************\n",
            "Trained on 560 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.851204\n",
            "Loss training: 32.516712\n",
            "Loss training: 38.4438\n",
            "Loss training: 31.802593\n",
            "Loss training: 30.400091\n",
            "Loss training: 30.760553\n",
            "Loss training: 28.14706\n",
            "Loss training: 53.088676\n",
            "Loss training: 19.86013\n",
            "Loss training: 19.86915\n",
            "\n",
            "***************************\n",
            "Trained on 570 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.241398\n",
            "Loss training: 32.95354\n",
            "Loss training: 41.755314\n",
            "Loss training: 32.799957\n",
            "Loss training: 27.304424\n",
            "Loss training: 39.06823\n",
            "Loss training: 45.588413\n",
            "Loss training: 34.072662\n",
            "Loss training: 37.407444\n",
            "Loss training: 26.105173\n",
            "\n",
            "***************************\n",
            "Trained on 580 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.358734\n",
            "Loss training: 33.276165\n",
            "Loss training: 36.837177\n",
            "Loss training: 30.507965\n",
            "Loss training: 24.009144\n",
            "Loss training: 45.999\n",
            "Loss training: 36.108025\n",
            "Loss training: 41.914753\n",
            "Loss training: 40.66619\n",
            "Loss training: 46.631397\n",
            "\n",
            "***************************\n",
            "Trained on 590 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.88774\n",
            "Loss training: 49.552288\n",
            "Loss training: 51.12544\n",
            "Loss training: 32.334904\n",
            "Loss training: 53.798565\n",
            "Loss training: 47.99095\n",
            "Loss training: 28.703547\n",
            "Loss training: 31.776754\n",
            "Loss training: 40.661976\n",
            "Loss training: 47.38733\n",
            "\n",
            "***************************\n",
            "Trained on 600 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.257504\n",
            "Loss training: 26.991938\n",
            "Loss training: 28.652115\n",
            "Loss training: 36.856068\n",
            "Loss training: 36.986244\n",
            "Loss training: 31.836868\n",
            "Loss training: 43.553776\n",
            "Loss training: 29.433624\n",
            "Loss training: 47.462814\n",
            "Loss training: 42.47967\n",
            "\n",
            "***************************\n",
            "Trained on 610 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 17.552809\n",
            "Loss training: 31.817966\n",
            "Loss training: 55.4068\n",
            "Loss training: 30.7143\n",
            "Loss training: 28.590487\n",
            "Loss training: 54.08951\n",
            "Loss training: 35.054333\n",
            "Loss training: 33.497765\n",
            "Loss training: 34.66327\n",
            "Loss training: 39.026745\n",
            "\n",
            "***************************\n",
            "Trained on 620 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.323376\n",
            "Loss training: 43.566994\n",
            "Loss training: 58.454243\n",
            "Loss training: 37.20262\n",
            "Loss training: 44.492126\n",
            "Loss training: 18.684544\n",
            "Loss training: 32.114815\n",
            "Loss training: 31.279692\n",
            "Loss training: 31.960922\n",
            "Loss training: 43.551933\n",
            "\n",
            "***************************\n",
            "Trained on 630 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.971706\n",
            "Loss training: 33.61728\n",
            "Loss training: 49.230675\n",
            "Loss training: 30.358217\n",
            "Loss training: 35.58271\n",
            "Loss training: 49.64959\n",
            "Loss training: 30.506426\n",
            "Loss training: 53.92121\n",
            "Loss training: 30.118887\n",
            "Loss training: 69.256134\n",
            "\n",
            "***************************\n",
            "Trained on 640 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.783585\n",
            "Loss training: 17.90661\n",
            "Loss training: 37.792095\n",
            "Loss training: 32.716076\n",
            "Loss training: 22.897133\n",
            "Loss training: 34.78648\n",
            "Loss training: 41.51807\n",
            "Loss training: 37.486244\n",
            "Loss training: 34.84229\n",
            "Loss training: 56.63952\n",
            "\n",
            "***************************\n",
            "Trained on 650 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.34229\n",
            "Loss training: 48.457825\n",
            "Loss training: 64.921074\n",
            "Loss training: 63.4606\n",
            "Loss training: 35.05531\n",
            "Loss training: 45.981945\n",
            "Loss training: 30.577063\n",
            "Loss training: 38.884335\n",
            "Loss training: 55.76568\n",
            "Loss training: 46.94447\n",
            "\n",
            "***************************\n",
            "Trained on 660 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.79424\n",
            "Loss training: 51.92451\n",
            "Loss training: 37.76974\n",
            "Loss training: 53.52648\n",
            "Loss training: 42.197746\n",
            "Loss training: 30.814632\n",
            "Loss training: 44.134594\n",
            "Loss training: 33.84766\n",
            "Loss training: 39.857567\n",
            "Loss training: 38.846806\n",
            "\n",
            "***************************\n",
            "Trained on 670 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.139023\n",
            "Loss training: 49.891678\n",
            "Loss training: 51.49992\n",
            "Loss training: 20.541742\n",
            "Loss training: 33.59621\n",
            "Loss training: 31.146406\n",
            "Loss training: 30.338009\n",
            "Loss training: 33.24686\n",
            "Loss training: 19.221565\n",
            "Loss training: 38.704704\n",
            "\n",
            "***************************\n",
            "Trained on 680 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.19998\n",
            "Loss training: 58.041245\n",
            "Loss training: 35.05769\n",
            "Loss training: 54.509155\n",
            "Loss training: 48.731735\n",
            "Loss training: 35.546436\n",
            "Loss training: 38.565037\n",
            "Loss training: 56.639652\n",
            "Loss training: 31.897846\n",
            "Loss training: 31.095373\n",
            "\n",
            "***************************\n",
            "Trained on 690 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.34409\n",
            "Loss training: 18.08901\n",
            "Loss training: 29.764202\n",
            "Loss training: 33.285362\n",
            "Loss training: 37.834522\n",
            "Loss training: 21.873701\n",
            "Loss training: 52.165943\n",
            "Loss training: 31.484201\n",
            "Loss training: 56.41328\n",
            "Loss training: 31.203217\n",
            "\n",
            "***************************\n",
            "Trained on 700 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.508827\n",
            "Loss training: 30.784777\n",
            "Loss training: 35.388935\n",
            "Loss training: 41.653206\n",
            "Loss training: 52.4753\n",
            "Loss training: 54.90458\n",
            "Loss training: 51.38299\n",
            "Loss training: 64.0559\n",
            "Loss training: 66.094894\n",
            "Loss training: 26.434425\n",
            "\n",
            "***************************\n",
            "Trained on 710 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.055103\n",
            "Loss training: 37.39473\n",
            "Loss training: 40.552406\n",
            "Loss training: 41.085674\n",
            "Loss training: 41.687428\n",
            "Loss training: 35.120697\n",
            "Loss training: 30.162704\n",
            "Loss training: 34.320824\n",
            "Loss training: 29.744572\n",
            "Loss training: 44.181152\n",
            "\n",
            "***************************\n",
            "Trained on 720 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.994915\n",
            "Loss training: 62.43437\n",
            "Loss training: 49.79642\n",
            "Loss training: 55.292187\n",
            "Loss training: 41.662663\n",
            "Loss training: 37.811687\n",
            "Loss training: 31.827827\n",
            "Loss training: 42.580563\n",
            "Loss training: 42.77171\n",
            "Loss training: 52.4281\n",
            "\n",
            "***************************\n",
            "Trained on 730 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.80147\n",
            "Loss training: 42.266285\n",
            "Loss training: 32.199173\n",
            "Loss training: 34.631405\n",
            "Loss training: 34.11387\n",
            "Loss training: 38.122913\n",
            "Loss training: 47.685585\n",
            "Loss training: 29.569376\n",
            "Loss training: 48.847874\n",
            "Loss training: 29.193092\n",
            "\n",
            "***************************\n",
            "Trained on 740 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 56.72148\n",
            "Loss training: 55.81491\n",
            "Loss training: 43.9163\n",
            "Loss training: 30.12296\n",
            "Loss training: 57.489494\n",
            "Loss training: 42.62505\n",
            "Loss training: 47.083626\n",
            "Loss training: 31.503044\n",
            "Loss training: 41.585995\n",
            "Loss training: 53.718582\n",
            "\n",
            "***************************\n",
            "Trained on 750 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.543325\n",
            "Loss training: 22.862873\n",
            "Loss training: 37.378124\n",
            "Loss training: 40.665195\n",
            "Loss training: 46.15105\n",
            "Loss training: 33.779587\n",
            "Loss training: 37.20887\n",
            "Loss training: 27.7708\n",
            "Loss training: 60.828625\n",
            "Loss training: 32.35909\n",
            "\n",
            "***************************\n",
            "Trained on 760 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.84249\n",
            "Loss training: 37.997723\n",
            "Loss training: 18.539925\n",
            "Loss training: 46.29541\n",
            "Loss training: 29.90935\n",
            "Loss training: 30.61486\n",
            "Loss training: 35.500004\n",
            "Loss training: 30.468449\n",
            "Loss training: 79.11772\n",
            "Loss training: 26.7556\n",
            "\n",
            "***************************\n",
            "Trained on 770 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 43.047417\n",
            "Loss training: 31.889946\n",
            "Loss training: 37.45909\n",
            "Loss training: 42.361797\n",
            "Loss training: 49.436184\n",
            "Loss training: 63.53842\n",
            "Loss training: 23.987713\n",
            "Loss training: 33.170235\n",
            "Loss training: 43.62807\n",
            "Loss training: 44.47293\n",
            "\n",
            "***************************\n",
            "Trained on 780 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.733955\n",
            "Loss training: 47.15283\n",
            "Loss training: 55.113167\n",
            "Loss training: 34.709236\n",
            "Loss training: 50.55091\n",
            "Loss training: 30.336946\n",
            "Loss training: 42.453197\n",
            "Loss training: 36.103718\n",
            "Loss training: 62.509457\n",
            "Loss training: 28.249275\n",
            "\n",
            "***************************\n",
            "Trained on 790 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.14419\n",
            "Loss training: 31.411367\n",
            "Loss training: 27.586325\n",
            "Loss training: 25.340334\n",
            "Loss training: 40.33478\n",
            "Loss training: 52.249577\n",
            "Loss training: 37.38205\n",
            "Loss training: 30.592318\n",
            "Loss training: 22.181164\n",
            "Loss training: 33.419872\n",
            "\n",
            "***************************\n",
            "Trained on 800 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.077246\n",
            "Loss training: 32.48221\n",
            "Loss training: 62.12122\n",
            "Loss training: 26.368631\n",
            "Loss training: 47.78951\n",
            "Loss training: 33.69622\n",
            "Loss training: 59.72223\n",
            "Loss training: 31.698332\n",
            "Loss training: 30.147093\n",
            "Loss training: 29.760458\n",
            "\n",
            "***************************\n",
            "Trained on 810 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 17.92402\n",
            "Loss training: 45.795624\n",
            "Loss training: 21.244442\n",
            "Loss training: 37.9545\n",
            "Loss training: 55.153423\n",
            "Loss training: 37.762478\n",
            "Loss training: 48.079967\n",
            "Loss training: 20.961178\n",
            "Loss training: 54.011345\n",
            "Loss training: 62.74664\n",
            "\n",
            "***************************\n",
            "Trained on 820 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 61.709373\n",
            "Loss training: 29.656666\n",
            "Loss training: 44.53196\n",
            "Loss training: 32.872005\n",
            "Loss training: 42.83494\n",
            "Loss training: 17.241262\n",
            "Loss training: 38.620968\n",
            "Loss training: 37.970757\n",
            "Loss training: 48.630157\n",
            "Loss training: 27.933838\n",
            "\n",
            "***************************\n",
            "Trained on 830 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 66.95452\n",
            "Loss training: 34.017696\n",
            "Loss training: 27.47946\n",
            "Loss training: 32.922672\n",
            "Loss training: 31.828623\n",
            "Loss training: 22.426918\n",
            "Loss training: 31.917799\n",
            "Loss training: 34.52623\n",
            "Loss training: 35.25664\n",
            "Loss training: 36.033318\n",
            "\n",
            "***************************\n",
            "Trained on 840 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.610935\n",
            "Loss training: 37.942593\n",
            "Loss training: 20.48702\n",
            "Loss training: 33.25306\n",
            "Loss training: 41.68795\n",
            "Loss training: 32.976658\n",
            "Loss training: 16.095734\n",
            "Loss training: 41.43859\n",
            "Loss training: 42.30189\n",
            "Loss training: 37.544483\n",
            "\n",
            "***************************\n",
            "Trained on 850 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.290413\n",
            "Loss training: 19.412615\n",
            "Loss training: 29.939024\n",
            "Loss training: 28.377708\n",
            "Loss training: 58.778053\n",
            "Loss training: 24.958141\n",
            "Loss training: 49.715565\n",
            "Loss training: 25.42\n",
            "Loss training: 36.309418\n",
            "Loss training: 44.674725\n",
            "\n",
            "***************************\n",
            "Trained on 860 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 17.20473\n",
            "Loss training: 50.719357\n",
            "Loss training: 32.19232\n",
            "Loss training: 39.911007\n",
            "Loss training: 49.66542\n",
            "Loss training: 33.54706\n",
            "Loss training: 35.80475\n",
            "Loss training: 37.08527\n",
            "Loss training: 27.824268\n",
            "Loss training: 31.551483\n",
            "\n",
            "***************************\n",
            "Trained on 870 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.585716\n",
            "Loss training: 36.77379\n",
            "Loss training: 38.6705\n",
            "Loss training: 32.54578\n",
            "Loss training: 16.218962\n",
            "Loss training: 34.048145\n",
            "Loss training: 37.54236\n",
            "Loss training: 41.61504\n",
            "Loss training: 38.91446\n",
            "Loss training: 16.042524\n",
            "\n",
            "***************************\n",
            "Trained on 880 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.437027\n",
            "Loss training: 39.9642\n",
            "Loss training: 40.833168\n",
            "Loss training: 32.288635\n",
            "Loss training: 40.29642\n",
            "Loss training: 33.49913\n",
            "Loss training: 35.020668\n",
            "Loss training: 37.516148\n",
            "Loss training: 34.876953\n",
            "Loss training: 41.283737\n",
            "\n",
            "***************************\n",
            "Trained on 890 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.24588\n",
            "Loss training: 34.19384\n",
            "Loss training: 51.124584\n",
            "Loss training: 31.249966\n",
            "Loss training: 31.510113\n",
            "Loss training: 26.48121\n",
            "Loss training: 36.874344\n",
            "Loss training: 41.7331\n",
            "Loss training: 38.35361\n",
            "Loss training: 30.085009\n",
            "\n",
            "***************************\n",
            "Trained on 900 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.352474\n",
            "Loss training: 35.585377\n",
            "Loss training: 17.495687\n",
            "Loss training: 19.88338\n",
            "Loss training: 34.286186\n",
            "Loss training: 33.84947\n",
            "Loss training: 47.032448\n",
            "Loss training: 15.667927\n",
            "Loss training: 26.522314\n",
            "Loss training: 52.395985\n",
            "\n",
            "***************************\n",
            "Trained on 910 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.952835\n",
            "Loss training: 73.365295\n",
            "Loss training: 53.148235\n",
            "Loss training: 36.20626\n",
            "Loss training: 52.884666\n",
            "Loss training: 37.49438\n",
            "Loss training: 23.839525\n",
            "Loss training: 35.610218\n",
            "Loss training: 26.948034\n",
            "Loss training: 40.95932\n",
            "\n",
            "***************************\n",
            "Trained on 920 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.707247\n",
            "Loss training: 38.188164\n",
            "Loss training: 17.396275\n",
            "Loss training: 65.16669\n",
            "Loss training: 48.778584\n",
            "Loss training: 29.060436\n",
            "Loss training: 48.25256\n",
            "Loss training: 30.080645\n",
            "Loss training: 46.308525\n",
            "Loss training: 42.009384\n",
            "\n",
            "***************************\n",
            "Trained on 930 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.057922\n",
            "Loss training: 34.77973\n",
            "Loss training: 40.26523\n",
            "Loss training: 27.273483\n",
            "Loss training: 35.926067\n",
            "Loss training: 50.817726\n",
            "Loss training: 34.19275\n",
            "Loss training: 38.782543\n",
            "Loss training: 42.190876\n",
            "Loss training: 19.991009\n",
            "\n",
            "***************************\n",
            "Trained on 940 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 17.25331\n",
            "Loss training: 47.86593\n",
            "Loss training: 32.68888\n",
            "Loss training: 49.39771\n",
            "Loss training: 32.36607\n",
            "Loss training: 26.023859\n",
            "Loss training: 29.792757\n",
            "Loss training: 55.39423\n",
            "Loss training: 27.700893\n",
            "Loss training: 30.800886\n",
            "\n",
            "***************************\n",
            "Trained on 950 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.87741\n",
            "Loss training: 51.29974\n",
            "Loss training: 17.164633\n",
            "Loss training: 29.50288\n",
            "Loss training: 49.788864\n",
            "Loss training: 23.9608\n",
            "Loss training: 45.078682\n",
            "Loss training: 54.25343\n",
            "Loss training: 36.218132\n",
            "Loss training: 48.165276\n",
            "\n",
            "***************************\n",
            "Trained on 960 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 53.86235\n",
            "Loss training: 47.495323\n",
            "Loss training: 27.215445\n",
            "Loss training: 17.443027\n",
            "Loss training: 21.125622\n",
            "Loss training: 30.76829\n",
            "Loss training: 50.418312\n",
            "Loss training: 53.51789\n",
            "Loss training: 36.228844\n",
            "Loss training: 27.915369\n",
            "\n",
            "***************************\n",
            "Trained on 970 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.417164\n",
            "Loss training: 39.93356\n",
            "Loss training: 44.35432\n",
            "Loss training: 51.041664\n",
            "Loss training: 35.501705\n",
            "Loss training: 17.188177\n",
            "Loss training: 28.697712\n",
            "Loss training: 15.462687\n",
            "Loss training: 54.434097\n",
            "Loss training: 42.70959\n",
            "\n",
            "***************************\n",
            "Trained on 980 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.61605\n",
            "Loss training: 37.087128\n",
            "Loss training: 51.490295\n",
            "Loss training: 23.918276\n",
            "Loss training: 37.677773\n",
            "Loss training: 31.240566\n",
            "Loss training: 22.434715\n",
            "Loss training: 53.270573\n",
            "Loss training: 38.780724\n",
            "Loss training: 36.489105\n",
            "\n",
            "***************************\n",
            "Trained on 990 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.29925\n",
            "Loss training: 53.384624\n",
            "Loss training: 53.239006\n",
            "Loss training: 26.49382\n",
            "Loss training: 66.99163\n",
            "Loss training: 55.34888\n",
            "Loss training: 44.566696\n",
            "Loss training: 44.35033\n",
            "Loss training: 39.85572\n",
            "Loss training: 48.11969\n",
            "\n",
            "***************************\n",
            "Trained on 1000 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 58.818497\n",
            "Loss training: 60.220566\n",
            "Loss training: 38.0892\n",
            "Loss training: 56.955574\n",
            "Loss training: 20.009363\n",
            "Loss training: 37.165695\n",
            "Loss training: 37.14362\n",
            "Loss training: 23.709986\n",
            "Loss training: 29.127254\n",
            "Loss training: 28.29471\n",
            "\n",
            "***************************\n",
            "Trained on 1010 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 50.418472\n",
            "Loss training: 16.895546\n",
            "Loss training: 29.373175\n",
            "Loss training: 19.213734\n",
            "Loss training: 47.37908\n",
            "Loss training: 38.423985\n",
            "Loss training: 37.862293\n",
            "Loss training: 24.610365\n",
            "Loss training: 44.27128\n",
            "Loss training: 37.544483\n",
            "\n",
            "***************************\n",
            "Trained on 1020 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 52.908157\n",
            "Loss training: 48.61167\n",
            "Loss training: 50.860218\n",
            "Loss training: 29.646692\n",
            "Loss training: 27.87162\n",
            "Loss training: 36.886208\n",
            "Loss training: 36.91995\n",
            "Loss training: 39.062138\n",
            "Loss training: 48.177162\n",
            "Loss training: 21.561771\n",
            "\n",
            "***************************\n",
            "Trained on 1030 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 62.576942\n",
            "Loss training: 52.878117\n",
            "Loss training: 29.249826\n",
            "Loss training: 50.15018\n",
            "Loss training: 27.45455\n",
            "Loss training: 30.108427\n",
            "Loss training: 30.699984\n",
            "Loss training: 36.491974\n",
            "Loss training: 52.24375\n",
            "Loss training: 27.142456\n",
            "\n",
            "***************************\n",
            "Trained on 1040 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.466034\n",
            "Loss training: 40.036655\n",
            "Loss training: 39.178844\n",
            "Loss training: 32.297836\n",
            "Loss training: 47.456062\n",
            "Loss training: 36.033012\n",
            "Loss training: 16.31912\n",
            "Loss training: 31.738615\n",
            "Loss training: 37.623196\n",
            "Loss training: 47.48914\n",
            "\n",
            "***************************\n",
            "Trained on 1050 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.787727\n",
            "Loss training: 26.867304\n",
            "Loss training: 32.77979\n",
            "Loss training: 58.542564\n",
            "Loss training: 15.922768\n",
            "Loss training: 49.930847\n",
            "Loss training: 64.718185\n",
            "Loss training: 43.733627\n",
            "Loss training: 40.18393\n",
            "Loss training: 40.1091\n",
            "\n",
            "***************************\n",
            "Trained on 1060 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.345448\n",
            "Loss training: 46.39917\n",
            "Loss training: 41.36815\n",
            "Loss training: 44.33567\n",
            "Loss training: 27.237404\n",
            "Loss training: 16.359707\n",
            "Loss training: 53.20463\n",
            "Loss training: 25.99847\n",
            "Loss training: 34.422985\n",
            "Loss training: 36.058033\n",
            "\n",
            "***************************\n",
            "Trained on 1070 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.10332\n",
            "Loss training: 18.327051\n",
            "Loss training: 29.765678\n",
            "Loss training: 24.734463\n",
            "Loss training: 32.3616\n",
            "Loss training: 54.878414\n",
            "Loss training: 47.367535\n",
            "Loss training: 36.736042\n",
            "Loss training: 39.29689\n",
            "Loss training: 53.74844\n",
            "\n",
            "***************************\n",
            "Trained on 1080 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 20.615917\n",
            "Loss training: 32.890358\n",
            "Loss training: 63.382957\n",
            "Loss training: 16.024517\n",
            "Loss training: 56.70591\n",
            "Loss training: 36.801014\n",
            "Loss training: 40.338337\n",
            "Loss training: 54.88655\n",
            "Loss training: 40.505955\n",
            "Loss training: 49.814754\n",
            "\n",
            "***************************\n",
            "Trained on 1090 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 17.615313\n",
            "Loss training: 23.576366\n",
            "Loss training: 50.388973\n",
            "Loss training: 28.443136\n",
            "Loss training: 39.861683\n",
            "Loss training: 59.740177\n",
            "Loss training: 45.086857\n",
            "Loss training: 27.572332\n",
            "Loss training: 31.756996\n",
            "Loss training: 40.45959\n",
            "\n",
            "***************************\n",
            "Trained on 1100 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.347088\n",
            "Loss training: 44.68005\n",
            "Loss training: 34.610226\n",
            "Loss training: 46.892784\n",
            "Loss training: 30.481398\n",
            "Loss training: 56.557262\n",
            "Loss training: 33.913147\n",
            "Loss training: 42.419846\n",
            "Loss training: 20.975657\n",
            "Loss training: 31.406052\n",
            "\n",
            "***************************\n",
            "Trained on 1110 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.924133\n",
            "Loss training: 41.58032\n",
            "Loss training: 55.077686\n",
            "Loss training: 35.04773\n",
            "Loss training: 40.546486\n",
            "Loss training: 39.86647\n",
            "Loss training: 30.643185\n",
            "Loss training: 51.11673\n",
            "Loss training: 24.392258\n",
            "Loss training: 32.097023\n",
            "\n",
            "***************************\n",
            "Trained on 1120 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.37693\n",
            "Loss training: 37.03958\n",
            "Loss training: 35.890724\n",
            "Loss training: 65.186874\n",
            "Loss training: 33.371227\n",
            "Loss training: 39.361084\n",
            "Loss training: 39.95869\n",
            "Loss training: 30.191092\n",
            "Loss training: 49.39211\n",
            "Loss training: 45.861042\n",
            "\n",
            "***************************\n",
            "Trained on 1130 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.773485\n",
            "Loss training: 24.426483\n",
            "Loss training: 47.508896\n",
            "Loss training: 19.045717\n",
            "Loss training: 44.456074\n",
            "Loss training: 26.854803\n",
            "Loss training: 50.78748\n",
            "Loss training: 21.237782\n",
            "Loss training: 28.954975\n",
            "Loss training: 45.390026\n",
            "\n",
            "***************************\n",
            "Trained on 1140 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 19.478714\n",
            "Loss training: 51.824802\n",
            "Loss training: 37.03187\n",
            "Loss training: 30.335754\n",
            "Loss training: 15.195494\n",
            "Loss training: 50.895405\n",
            "Loss training: 37.821995\n",
            "Loss training: 36.301647\n",
            "Loss training: 29.675547\n",
            "Loss training: 30.340773\n",
            "\n",
            "***************************\n",
            "Trained on 1150 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 59.93741\n",
            "Loss training: 33.06481\n",
            "Loss training: 44.37049\n",
            "Loss training: 42.780388\n",
            "Loss training: 35.113644\n",
            "Loss training: 34.971592\n",
            "Loss training: 24.667181\n",
            "Loss training: 30.74675\n",
            "Loss training: 38.068413\n",
            "Loss training: 61.57131\n",
            "\n",
            "***************************\n",
            "Trained on 1160 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 47.35546\n",
            "Loss training: 28.383738\n",
            "Loss training: 48.77888\n",
            "Loss training: 47.802486\n",
            "Loss training: 40.984436\n",
            "Loss training: 25.596125\n",
            "Loss training: 37.47459\n",
            "Loss training: 25.145277\n",
            "Loss training: 31.36945\n",
            "Loss training: 34.43118\n",
            "\n",
            "***************************\n",
            "Trained on 1170 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.793777\n",
            "Loss training: 24.330107\n",
            "Loss training: 54.36307\n",
            "Loss training: 37.46845\n",
            "Loss training: 34.56878\n",
            "Loss training: 48.36075\n",
            "Loss training: 37.34046\n",
            "Loss training: 29.460403\n",
            "Loss training: 26.035492\n",
            "Loss training: 31.826138\n",
            "\n",
            "***************************\n",
            "Trained on 1180 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.868576\n",
            "Loss training: 38.0605\n",
            "Loss training: 50.971786\n",
            "Loss training: 28.680779\n",
            "Loss training: 25.053717\n",
            "Loss training: 30.691153\n",
            "Loss training: 41.55707\n",
            "Loss training: 34.827023\n",
            "Loss training: 42.76242\n",
            "Loss training: 37.49347\n",
            "\n",
            "***************************\n",
            "Trained on 1190 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 47.9382\n",
            "Loss training: 33.092133\n",
            "Loss training: 36.734562\n",
            "Loss training: 31.29729\n",
            "Loss training: 50.209995\n",
            "Loss training: 37.492817\n",
            "Loss training: 54.412266\n",
            "Loss training: 29.546618\n",
            "Loss training: 18.4421\n",
            "Loss training: 32.016544\n",
            "\n",
            "***************************\n",
            "Trained on 1200 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.77976\n",
            "Loss training: 57.01789\n",
            "Loss training: 62.487938\n",
            "Loss training: 36.874035\n",
            "Loss training: 39.95971\n",
            "Loss training: 30.674303\n",
            "Loss training: 30.745573\n",
            "Loss training: 37.598175\n",
            "Loss training: 26.420177\n",
            "Loss training: 18.369135\n",
            "\n",
            "***************************\n",
            "Trained on 1210 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 51.234627\n",
            "Loss training: 47.72011\n",
            "Loss training: 36.642178\n",
            "Loss training: 35.746857\n",
            "Loss training: 50.286095\n",
            "Loss training: 30.055931\n",
            "Loss training: 51.797882\n",
            "Loss training: 31.746439\n",
            "Loss training: 34.09582\n",
            "Loss training: 41.60838\n",
            "\n",
            "***************************\n",
            "Trained on 1220 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 47.571987\n",
            "Loss training: 46.78261\n",
            "Loss training: 24.205376\n",
            "Loss training: 35.572174\n",
            "Loss training: 19.544674\n",
            "Loss training: 27.78112\n",
            "Loss training: 31.38494\n",
            "Loss training: 62.58568\n",
            "Loss training: 31.060143\n",
            "Loss training: 47.701138\n",
            "\n",
            "***************************\n",
            "Trained on 1230 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.972742\n",
            "Loss training: 27.301632\n",
            "Loss training: 46.489845\n",
            "Loss training: 30.423655\n",
            "Loss training: 30.905096\n",
            "Loss training: 27.166725\n",
            "Loss training: 44.447433\n",
            "Loss training: 40.4181\n",
            "Loss training: 27.01908\n",
            "Loss training: 35.324207\n",
            "\n",
            "***************************\n",
            "Trained on 1240 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.770016\n",
            "Loss training: 52.916298\n",
            "Loss training: 49.015663\n",
            "Loss training: 33.426342\n",
            "Loss training: 61.29694\n",
            "Loss training: 23.917252\n",
            "Loss training: 46.745617\n",
            "Loss training: 46.90773\n",
            "Loss training: 19.609411\n",
            "Loss training: 38.71797\n",
            "\n",
            "***************************\n",
            "Trained on 1250 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.91801\n",
            "Loss training: 31.466465\n",
            "Loss training: 25.791616\n",
            "Loss training: 31.114681\n",
            "Loss training: 46.645164\n",
            "Loss training: 30.502447\n",
            "Loss training: 42.31421\n",
            "Loss training: 42.12304\n",
            "Loss training: 45.630512\n",
            "Loss training: 55.003445\n",
            "\n",
            "***************************\n",
            "Trained on 1260 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.9469\n",
            "Loss training: 37.244907\n",
            "Loss training: 21.147532\n",
            "Loss training: 51.778828\n",
            "Loss training: 35.766785\n",
            "Loss training: 53.627506\n",
            "Loss training: 30.752954\n",
            "Loss training: 37.254093\n",
            "Loss training: 34.83315\n",
            "Loss training: 40.02285\n",
            "\n",
            "***************************\n",
            "Trained on 1270 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.194458\n",
            "Loss training: 43.62573\n",
            "Loss training: 18.30239\n",
            "Loss training: 31.168257\n",
            "Loss training: 45.615086\n",
            "Loss training: 40.10103\n",
            "Loss training: 57.76954\n",
            "Loss training: 42.69287\n",
            "Loss training: 33.491375\n",
            "Loss training: 27.065735\n",
            "\n",
            "***************************\n",
            "Trained on 1280 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 55.995113\n",
            "Loss training: 43.893604\n",
            "Loss training: 38.8488\n",
            "Loss training: 30.144783\n",
            "Loss training: 22.662884\n",
            "Loss training: 19.936678\n",
            "Loss training: 41.826233\n",
            "Loss training: 32.074486\n",
            "Loss training: 39.395306\n",
            "Loss training: 41.711388\n",
            "\n",
            "***************************\n",
            "Trained on 1290 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.81416\n",
            "Loss training: 36.71488\n",
            "Loss training: 31.030436\n",
            "Loss training: 40.298832\n",
            "Loss training: 29.627974\n",
            "Loss training: 36.16117\n",
            "Loss training: 40.445663\n",
            "Loss training: 28.632677\n",
            "Loss training: 34.464306\n",
            "Loss training: 51.089413\n",
            "\n",
            "***************************\n",
            "Trained on 1300 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.40638\n",
            "Loss training: 34.56621\n",
            "Loss training: 45.763145\n",
            "Loss training: 22.874914\n",
            "Loss training: 24.398603\n",
            "Loss training: 32.202038\n",
            "Loss training: 46.15918\n",
            "Loss training: 46.12668\n",
            "Loss training: 50.164394\n",
            "Loss training: 31.075277\n",
            "\n",
            "***************************\n",
            "Trained on 1310 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.075806\n",
            "Loss training: 37.42772\n",
            "Loss training: 37.302597\n",
            "Loss training: 45.26756\n",
            "Loss training: 35.37515\n",
            "Loss training: 50.91605\n",
            "Loss training: 41.09634\n",
            "Loss training: 32.55468\n",
            "Loss training: 19.33492\n",
            "Loss training: 47.631805\n",
            "\n",
            "***************************\n",
            "Trained on 1320 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 48.72342\n",
            "Loss training: 36.460236\n",
            "Loss training: 19.610834\n",
            "Loss training: 28.782505\n",
            "Loss training: 36.692715\n",
            "Loss training: 43.579628\n",
            "Loss training: 34.33475\n",
            "Loss training: 35.48183\n",
            "Loss training: 34.42015\n",
            "Loss training: 27.23212\n",
            "\n",
            "***************************\n",
            "Trained on 1330 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.539612\n",
            "Loss training: 42.513737\n",
            "Loss training: 36.578564\n",
            "Loss training: 41.279007\n",
            "Loss training: 36.402496\n",
            "Loss training: 27.264135\n",
            "Loss training: 30.9149\n",
            "Loss training: 37.193096\n",
            "Loss training: 29.801327\n",
            "Loss training: 47.463898\n",
            "\n",
            "***************************\n",
            "Trained on 1340 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.197914\n",
            "Loss training: 41.30249\n",
            "Loss training: 27.303755\n",
            "Loss training: 24.02127\n",
            "Loss training: 29.897926\n",
            "Loss training: 29.44071\n",
            "Loss training: 31.84291\n",
            "Loss training: 26.6901\n",
            "Loss training: 46.783306\n",
            "Loss training: 54.383274\n",
            "\n",
            "***************************\n",
            "Trained on 1350 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 69.424095\n",
            "Loss training: 30.624245\n",
            "Loss training: 20.40523\n",
            "Loss training: 34.196754\n",
            "Loss training: 30.559217\n",
            "Loss training: 29.070232\n",
            "Loss training: 48.45917\n",
            "Loss training: 48.003643\n",
            "Loss training: 47.14908\n",
            "Loss training: 16.981674\n",
            "\n",
            "***************************\n",
            "Trained on 1360 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.263783\n",
            "Loss training: 37.262802\n",
            "Loss training: 35.276947\n",
            "Loss training: 26.23559\n",
            "Loss training: 20.283333\n",
            "Loss training: 33.282845\n",
            "Loss training: 34.971924\n",
            "Loss training: 48.23971\n",
            "Loss training: 40.6586\n",
            "Loss training: 46.346283\n",
            "\n",
            "***************************\n",
            "Trained on 1370 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 49.062706\n",
            "Loss training: 41.06863\n",
            "Loss training: 31.599566\n",
            "Loss training: 33.250984\n",
            "Loss training: 49.01165\n",
            "Loss training: 36.34351\n",
            "Loss training: 38.065536\n",
            "Loss training: 16.646095\n",
            "Loss training: 31.886845\n",
            "Loss training: 38.87415\n",
            "\n",
            "***************************\n",
            "Trained on 1380 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.5836\n",
            "Loss training: 19.638058\n",
            "Loss training: 46.318398\n",
            "Loss training: 31.315754\n",
            "Loss training: 30.977942\n",
            "Loss training: 53.187695\n",
            "Loss training: 67.02553\n",
            "Loss training: 47.74489\n",
            "Loss training: 50.490273\n",
            "Loss training: 16.14061\n",
            "\n",
            "***************************\n",
            "Trained on 1390 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.746885\n",
            "Loss training: 15.842681\n",
            "Loss training: 40.236176\n",
            "Loss training: 43.816956\n",
            "Loss training: 40.355957\n",
            "Loss training: 36.778248\n",
            "Loss training: 38.308525\n",
            "Loss training: 26.042835\n",
            "Loss training: 37.841167\n",
            "Loss training: 43.79624\n",
            "\n",
            "***************************\n",
            "Trained on 1400 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 15.310846\n",
            "Loss training: 31.174692\n",
            "Loss training: 36.540955\n",
            "Loss training: 16.036047\n",
            "Loss training: 42.518295\n",
            "Loss training: 37.712337\n",
            "Loss training: 28.392818\n",
            "Loss training: 47.481277\n",
            "Loss training: 48.428978\n",
            "Loss training: 41.82207\n",
            "\n",
            "***************************\n",
            "Trained on 1410 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 47.16522\n",
            "Loss training: 33.9139\n",
            "Loss training: 63.35211\n",
            "Loss training: 52.471767\n",
            "Loss training: 45.798134\n",
            "Loss training: 39.66332\n",
            "Loss training: 30.53907\n",
            "Loss training: 18.440144\n",
            "Loss training: 35.440693\n",
            "Loss training: 30.628447\n",
            "\n",
            "***************************\n",
            "Trained on 1420 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.06153\n",
            "Loss training: 30.493673\n",
            "Loss training: 39.757584\n",
            "Loss training: 38.939793\n",
            "Loss training: 51.884644\n",
            "Loss training: 47.277523\n",
            "Loss training: 35.9046\n",
            "Loss training: 26.761866\n",
            "Loss training: 34.20859\n",
            "Loss training: 64.922264\n",
            "\n",
            "***************************\n",
            "Trained on 1430 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.905104\n",
            "Loss training: 50.29653\n",
            "Loss training: 30.643877\n",
            "Loss training: 36.454594\n",
            "Loss training: 29.909\n",
            "Loss training: 42.027733\n",
            "Loss training: 37.993202\n",
            "Loss training: 49.25834\n",
            "Loss training: 33.839848\n",
            "Loss training: 33.280415\n",
            "\n",
            "***************************\n",
            "Trained on 1440 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.35126\n",
            "Loss training: 36.18883\n",
            "Loss training: 29.68295\n",
            "Loss training: 24.754955\n",
            "Loss training: 33.01361\n",
            "Loss training: 25.523891\n",
            "Loss training: 56.140697\n",
            "Loss training: 29.159739\n",
            "Loss training: 36.80999\n",
            "Loss training: 23.544838\n",
            "\n",
            "***************************\n",
            "Trained on 1450 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.420712\n",
            "Loss training: 16.672863\n",
            "Loss training: 27.077042\n",
            "Loss training: 40.09528\n",
            "Loss training: 40.96745\n",
            "Loss training: 19.712286\n",
            "Loss training: 40.78531\n",
            "Loss training: 28.93222\n",
            "Loss training: 31.615509\n",
            "Loss training: 45.50556\n",
            "\n",
            "***************************\n",
            "Trained on 1460 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 18.957485\n",
            "Loss training: 50.116703\n",
            "Loss training: 37.93588\n",
            "Loss training: 43.237385\n",
            "Loss training: 23.910847\n",
            "Loss training: 45.396233\n",
            "Loss training: 47.274487\n",
            "Loss training: 37.130573\n",
            "Loss training: 29.387602\n",
            "Loss training: 25.723595\n",
            "\n",
            "***************************\n",
            "Trained on 1470 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 50.845707\n",
            "Loss training: 31.71307\n",
            "Loss training: 38.100822\n",
            "Loss training: 29.050228\n",
            "Loss training: 30.811764\n",
            "Loss training: 52.865643\n",
            "Loss training: 28.26847\n",
            "Loss training: 20.131525\n",
            "Loss training: 36.664536\n",
            "Loss training: 26.163862\n",
            "\n",
            "***************************\n",
            "Trained on 1480 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.860828\n",
            "Loss training: 24.072258\n",
            "Loss training: 40.913734\n",
            "Loss training: 36.921547\n",
            "Loss training: 49.85438\n",
            "Loss training: 66.78897\n",
            "Loss training: 39.73153\n",
            "Loss training: 29.122654\n",
            "Loss training: 41.503677\n",
            "Loss training: 36.348965\n",
            "\n",
            "***************************\n",
            "Trained on 1490 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 52.03237\n",
            "Loss training: 39.352383\n",
            "Loss training: 49.940628\n",
            "Loss training: 29.172144\n",
            "Loss training: 36.522602\n",
            "Loss training: 46.1124\n",
            "Loss training: 35.40893\n",
            "Loss training: 36.000282\n",
            "Loss training: 47.96068\n",
            "Loss training: 30.984184\n",
            "\n",
            "***************************\n",
            "Trained on 1500 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 46.433964\n",
            "Loss training: 45.81143\n",
            "Loss training: 31.59584\n",
            "Loss training: 30.70577\n",
            "Loss training: 41.16019\n",
            "Loss training: 25.819658\n",
            "Loss training: 28.537695\n",
            "Loss training: 25.435785\n",
            "Loss training: 31.878588\n",
            "Loss training: 56.339363\n",
            "\n",
            "***************************\n",
            "Trained on 1510 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.979477\n",
            "Loss training: 20.62314\n",
            "Loss training: 48.52312\n",
            "Loss training: 37.004837\n",
            "Loss training: 36.013668\n",
            "Loss training: 19.444965\n",
            "Loss training: 26.937925\n",
            "Loss training: 24.752338\n",
            "Loss training: 33.226936\n",
            "Loss training: 33.59603\n",
            "\n",
            "***************************\n",
            "Trained on 1520 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.539696\n",
            "Loss training: 24.417772\n",
            "Loss training: 49.10601\n",
            "Loss training: 33.617332\n",
            "Loss training: 39.579845\n",
            "Loss training: 37.851547\n",
            "Loss training: 58.915108\n",
            "Loss training: 36.649586\n",
            "Loss training: 30.15258\n",
            "Loss training: 33.230972\n",
            "\n",
            "***************************\n",
            "Trained on 1530 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.994045\n",
            "Loss training: 45.333717\n",
            "Loss training: 29.388626\n",
            "Loss training: 48.22876\n",
            "Loss training: 37.323067\n",
            "Loss training: 36.206905\n",
            "Loss training: 34.247173\n",
            "Loss training: 64.18196\n",
            "Loss training: 43.805138\n",
            "Loss training: 46.993534\n",
            "\n",
            "***************************\n",
            "Trained on 1540 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.91099\n",
            "Loss training: 30.79247\n",
            "Loss training: 36.843575\n",
            "Loss training: 25.164633\n",
            "Loss training: 32.559525\n",
            "Loss training: 28.192268\n",
            "Loss training: 60.961014\n",
            "Loss training: 35.24061\n",
            "Loss training: 27.966549\n",
            "Loss training: 38.71613\n",
            "\n",
            "***************************\n",
            "Trained on 1550 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.14785\n",
            "Loss training: 35.150204\n",
            "Loss training: 32.808403\n",
            "Loss training: 22.161402\n",
            "Loss training: 41.79096\n",
            "Loss training: 33.570324\n",
            "Loss training: 44.480545\n",
            "Loss training: 48.549026\n",
            "Loss training: 28.039068\n",
            "Loss training: 32.26019\n",
            "\n",
            "***************************\n",
            "Trained on 1560 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.969296\n",
            "Loss training: 30.65359\n",
            "Loss training: 36.051365\n",
            "Loss training: 64.74643\n",
            "Loss training: 45.28906\n",
            "Loss training: 34.786987\n",
            "Loss training: 21.188814\n",
            "Loss training: 47.307194\n",
            "Loss training: 41.956608\n",
            "Loss training: 33.194366\n",
            "\n",
            "***************************\n",
            "Trained on 1570 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 52.75229\n",
            "Loss training: 38.96936\n",
            "Loss training: 33.5583\n",
            "Loss training: 46.8058\n",
            "Loss training: 28.74358\n",
            "Loss training: 30.199076\n",
            "Loss training: 36.79859\n",
            "Loss training: 46.75025\n",
            "Loss training: 33.21969\n",
            "Loss training: 47.73769\n",
            "\n",
            "***************************\n",
            "Trained on 1580 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.48133\n",
            "Loss training: 37.01185\n",
            "Loss training: 42.954456\n",
            "Loss training: 51.74574\n",
            "Loss training: 51.51456\n",
            "Loss training: 36.746437\n",
            "Loss training: 50.3428\n",
            "Loss training: 38.301388\n",
            "Loss training: 17.479435\n",
            "Loss training: 46.402428\n",
            "\n",
            "***************************\n",
            "Trained on 1590 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.910046\n",
            "Loss training: 45.271126\n",
            "Loss training: 19.954355\n",
            "Loss training: 55.376312\n",
            "Loss training: 18.058887\n",
            "Loss training: 43.354916\n",
            "Loss training: 43.50823\n",
            "Loss training: 30.537628\n",
            "Loss training: 17.594175\n",
            "Loss training: 34.924614\n",
            "\n",
            "***************************\n",
            "Trained on 1600 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.631836\n",
            "Loss training: 53.779884\n",
            "Loss training: 34.853928\n",
            "Loss training: 38.68879\n",
            "Loss training: 36.722095\n",
            "Loss training: 33.567024\n",
            "Loss training: 36.498974\n",
            "Loss training: 36.31862\n",
            "Loss training: 28.264086\n",
            "Loss training: 41.421173\n",
            "\n",
            "***************************\n",
            "Trained on 1610 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 48.06031\n",
            "Loss training: 47.733967\n",
            "Loss training: 31.588205\n",
            "Loss training: 36.827454\n",
            "Loss training: 34.666092\n",
            "Loss training: 31.664068\n",
            "Loss training: 17.216993\n",
            "Loss training: 66.336815\n",
            "Loss training: 56.629444\n",
            "Loss training: 33.124214\n",
            "\n",
            "***************************\n",
            "Trained on 1620 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.26023\n",
            "Loss training: 36.43252\n",
            "Loss training: 48.554398\n",
            "Loss training: 36.452778\n",
            "Loss training: 85.672295\n",
            "Loss training: 36.483837\n",
            "Loss training: 38.67887\n",
            "Loss training: 61.099968\n",
            "Loss training: 28.707308\n",
            "Loss training: 48.28248\n",
            "\n",
            "***************************\n",
            "Trained on 1630 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.85093\n",
            "Loss training: 42.96187\n",
            "Loss training: 45.41171\n",
            "Loss training: 51.43724\n",
            "Loss training: 16.311491\n",
            "Loss training: 22.948402\n",
            "Loss training: 70.019875\n",
            "Loss training: 36.59589\n",
            "Loss training: 32.56606\n",
            "Loss training: 36.19933\n",
            "\n",
            "***************************\n",
            "Trained on 1640 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 46.22549\n",
            "Loss training: 40.920822\n",
            "Loss training: 40.18582\n",
            "Loss training: 52.44607\n",
            "Loss training: 31.16351\n",
            "Loss training: 63.22484\n",
            "Loss training: 35.19518\n",
            "Loss training: 39.33152\n",
            "Loss training: 20.978527\n",
            "Loss training: 36.626564\n",
            "\n",
            "***************************\n",
            "Trained on 1650 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 48.47317\n",
            "Loss training: 48.98771\n",
            "Loss training: 24.995594\n",
            "Loss training: 15.950736\n",
            "Loss training: 29.529276\n",
            "Loss training: 29.24399\n",
            "Loss training: 46.042007\n",
            "Loss training: 36.57048\n",
            "Loss training: 49.430973\n",
            "Loss training: 29.892715\n",
            "\n",
            "***************************\n",
            "Trained on 1660 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.09607\n",
            "Loss training: 36.13251\n",
            "Loss training: 30.415237\n",
            "Loss training: 21.542276\n",
            "Loss training: 31.184917\n",
            "Loss training: 30.794079\n",
            "Loss training: 36.523838\n",
            "Loss training: 46.415657\n",
            "Loss training: 29.86563\n",
            "Loss training: 34.759144\n",
            "\n",
            "***************************\n",
            "Trained on 1670 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.830439\n",
            "Loss training: 35.686905\n",
            "Loss training: 32.542618\n",
            "Loss training: 34.2611\n",
            "Loss training: 42.942303\n",
            "Loss training: 48.20724\n",
            "Loss training: 37.414967\n",
            "Loss training: 27.794373\n",
            "Loss training: 31.097822\n",
            "Loss training: 54.6921\n",
            "\n",
            "***************************\n",
            "Trained on 1680 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 47.318607\n",
            "Loss training: 44.690166\n",
            "Loss training: 45.137215\n",
            "Loss training: 32.349304\n",
            "Loss training: 41.501183\n",
            "Loss training: 19.398317\n",
            "Loss training: 37.94573\n",
            "Loss training: 25.04546\n",
            "Loss training: 25.993536\n",
            "Loss training: 16.407124\n",
            "\n",
            "***************************\n",
            "Trained on 1690 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.876057\n",
            "Loss training: 36.805134\n",
            "Loss training: 26.454956\n",
            "Loss training: 44.008152\n",
            "Loss training: 33.281788\n",
            "Loss training: 24.39745\n",
            "Loss training: 50.169926\n",
            "Loss training: 28.48287\n",
            "Loss training: 28.311567\n",
            "Loss training: 31.193567\n",
            "\n",
            "***************************\n",
            "Trained on 1700 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.522665\n",
            "Loss training: 31.274355\n",
            "Loss training: 35.65521\n",
            "Loss training: 17.204535\n",
            "Loss training: 36.56568\n",
            "Loss training: 37.417576\n",
            "Loss training: 31.3173\n",
            "Loss training: 16.11142\n",
            "Loss training: 50.310352\n",
            "Loss training: 59.80991\n",
            "\n",
            "***************************\n",
            "Trained on 1710 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.497326\n",
            "Loss training: 15.284972\n",
            "Loss training: 46.208347\n",
            "Loss training: 49.47113\n",
            "Loss training: 32.756615\n",
            "Loss training: 33.893204\n",
            "Loss training: 35.089603\n",
            "Loss training: 32.953518\n",
            "Loss training: 48.30587\n",
            "Loss training: 37.270542\n",
            "\n",
            "***************************\n",
            "Trained on 1720 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.898083\n",
            "Loss training: 30.033514\n",
            "Loss training: 26.52541\n",
            "Loss training: 47.045654\n",
            "Loss training: 30.012606\n",
            "Loss training: 54.171703\n",
            "Loss training: 62.78201\n",
            "Loss training: 38.485416\n",
            "Loss training: 37.08662\n",
            "Loss training: 30.22888\n",
            "\n",
            "***************************\n",
            "Trained on 1730 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.38439\n",
            "Loss training: 48.00839\n",
            "Loss training: 17.55891\n",
            "Loss training: 52.01477\n",
            "Loss training: 46.158203\n",
            "Loss training: 35.087513\n",
            "Loss training: 35.005398\n",
            "Loss training: 29.256365\n",
            "Loss training: 38.088207\n",
            "Loss training: 34.78414\n",
            "\n",
            "***************************\n",
            "Trained on 1740 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 15.401618\n",
            "Loss training: 15.203665\n",
            "Loss training: 37.13677\n",
            "Loss training: 36.873997\n",
            "Loss training: 36.94343\n",
            "Loss training: 40.26377\n",
            "Loss training: 49.785755\n",
            "Loss training: 24.976149\n",
            "Loss training: 31.93062\n",
            "Loss training: 35.13146\n",
            "\n",
            "***************************\n",
            "Trained on 1750 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.35082\n",
            "Loss training: 45.56525\n",
            "Loss training: 46.618114\n",
            "Loss training: 31.312716\n",
            "Loss training: 46.393738\n",
            "Loss training: 42.360172\n",
            "Loss training: 31.96159\n",
            "Loss training: 48.91777\n",
            "Loss training: 27.3975\n",
            "Loss training: 26.427464\n",
            "\n",
            "***************************\n",
            "Trained on 1760 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 64.799736\n",
            "Loss training: 31.591497\n",
            "Loss training: 25.991716\n",
            "Loss training: 24.884087\n",
            "Loss training: 14.933979\n",
            "Loss training: 39.869\n",
            "Loss training: 25.555773\n",
            "Loss training: 48.372944\n",
            "Loss training: 50.690533\n",
            "Loss training: 58.736107\n",
            "\n",
            "***************************\n",
            "Trained on 1770 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.206768\n",
            "Loss training: 35.909748\n",
            "Loss training: 44.456783\n",
            "Loss training: 47.9689\n",
            "Loss training: 47.7336\n",
            "Loss training: 46.53835\n",
            "Loss training: 46.2322\n",
            "Loss training: 36.267338\n",
            "Loss training: 35.284397\n",
            "Loss training: 25.333225\n",
            "\n",
            "***************************\n",
            "Trained on 1780 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.568403\n",
            "Loss training: 24.9638\n",
            "Loss training: 37.934093\n",
            "Loss training: 29.703228\n",
            "Loss training: 31.890919\n",
            "Loss training: 46.111797\n",
            "Loss training: 33.220707\n",
            "Loss training: 40.816017\n",
            "Loss training: 63.873333\n",
            "Loss training: 58.71597\n",
            "\n",
            "***************************\n",
            "Trained on 1790 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.2077\n",
            "Loss training: 31.624826\n",
            "Loss training: 43.88696\n",
            "Loss training: 31.905336\n",
            "Loss training: 22.907822\n",
            "Loss training: 39.149662\n",
            "Loss training: 26.997662\n",
            "Loss training: 38.910576\n",
            "Loss training: 24.9095\n",
            "Loss training: 22.506111\n",
            "\n",
            "***************************\n",
            "Trained on 1800 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 53.36922\n",
            "Loss training: 27.98896\n",
            "Loss training: 47.599487\n",
            "Loss training: 29.439095\n",
            "Loss training: 30.928207\n",
            "Loss training: 30.982477\n",
            "Loss training: 18.00422\n",
            "Loss training: 39.262287\n",
            "Loss training: 35.779316\n",
            "Loss training: 26.801365\n",
            "\n",
            "***************************\n",
            "Trained on 1810 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.768826\n",
            "Loss training: 41.18817\n",
            "Loss training: 47.113403\n",
            "Loss training: 41.453457\n",
            "Loss training: 32.742218\n",
            "Loss training: 40.53009\n",
            "Loss training: 30.588402\n",
            "Loss training: 36.070385\n",
            "Loss training: 35.862053\n",
            "Loss training: 51.28218\n",
            "\n",
            "***************************\n",
            "Trained on 1820 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.389486\n",
            "Loss training: 30.78587\n",
            "Loss training: 31.893187\n",
            "Loss training: 25.354454\n",
            "Loss training: 44.348866\n",
            "Loss training: 36.64172\n",
            "Loss training: 30.487997\n",
            "Loss training: 40.546223\n",
            "Loss training: 36.445873\n",
            "Loss training: 31.903082\n",
            "\n",
            "***************************\n",
            "Trained on 1830 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 48.748924\n",
            "Loss training: 36.842785\n",
            "Loss training: 35.110718\n",
            "Loss training: 47.153027\n",
            "Loss training: 37.033813\n",
            "Loss training: 24.857445\n",
            "Loss training: 46.877125\n",
            "Loss training: 30.571136\n",
            "Loss training: 38.117588\n",
            "Loss training: 45.99336\n",
            "\n",
            "***************************\n",
            "Trained on 1840 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.892958\n",
            "Loss training: 45.385426\n",
            "Loss training: 35.9686\n",
            "Loss training: 33.701317\n",
            "Loss training: 16.508883\n",
            "Loss training: 47.721878\n",
            "Loss training: 15.606998\n",
            "Loss training: 30.253883\n",
            "Loss training: 26.515795\n",
            "Loss training: 38.408817\n",
            "\n",
            "***************************\n",
            "Trained on 1850 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 48.84176\n",
            "Loss training: 40.548214\n",
            "Loss training: 35.43503\n",
            "Loss training: 33.29896\n",
            "Loss training: 31.845097\n",
            "Loss training: 52.93485\n",
            "Loss training: 45.80771\n",
            "Loss training: 45.48092\n",
            "Loss training: 34.423367\n",
            "Loss training: 36.75043\n",
            "\n",
            "***************************\n",
            "Trained on 1860 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.31438\n",
            "Loss training: 64.04334\n",
            "Loss training: 30.334301\n",
            "Loss training: 25.39169\n",
            "Loss training: 40.3598\n",
            "Loss training: 28.474714\n",
            "Loss training: 34.457226\n",
            "Loss training: 36.5458\n",
            "Loss training: 33.826965\n",
            "Loss training: 29.893679\n",
            "\n",
            "***************************\n",
            "Trained on 1870 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.788094\n",
            "Loss training: 28.870163\n",
            "Loss training: 37.01973\n",
            "Loss training: 34.554043\n",
            "Loss training: 40.905636\n",
            "Loss training: 48.962685\n",
            "Loss training: 36.84123\n",
            "Loss training: 25.650845\n",
            "Loss training: 40.711\n",
            "Loss training: 36.127472\n",
            "\n",
            "***************************\n",
            "Trained on 1880 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.865541\n",
            "Loss training: 47.883427\n",
            "Loss training: 31.74269\n",
            "Loss training: 36.445656\n",
            "Loss training: 15.968185\n",
            "Loss training: 57.35203\n",
            "Loss training: 35.19055\n",
            "Loss training: 29.73733\n",
            "Loss training: 30.937826\n",
            "Loss training: 41.613525\n",
            "\n",
            "***************************\n",
            "Trained on 1890 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.953749\n",
            "Loss training: 35.392376\n",
            "Loss training: 45.91357\n",
            "Loss training: 31.195126\n",
            "Loss training: 30.398626\n",
            "Loss training: 36.212894\n",
            "Loss training: 35.957336\n",
            "Loss training: 40.115635\n",
            "Loss training: 34.243866\n",
            "Loss training: 14.3229265\n",
            "\n",
            "***************************\n",
            "Trained on 1900 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.322773\n",
            "Loss training: 23.349287\n",
            "Loss training: 34.00194\n",
            "Loss training: 51.10309\n",
            "Loss training: 18.198215\n",
            "Loss training: 37.1174\n",
            "Loss training: 28.49818\n",
            "Loss training: 34.93193\n",
            "Loss training: 46.29408\n",
            "Loss training: 58.314205\n",
            "\n",
            "***************************\n",
            "Trained on 1910 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.657574\n",
            "Loss training: 32.307793\n",
            "Loss training: 16.79441\n",
            "Loss training: 37.942425\n",
            "Loss training: 24.714897\n",
            "Loss training: 24.51422\n",
            "Loss training: 40.60618\n",
            "Loss training: 33.653942\n",
            "Loss training: 45.208294\n",
            "Loss training: 32.581573\n",
            "\n",
            "***************************\n",
            "Trained on 1920 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.182976\n",
            "Loss training: 18.685253\n",
            "Loss training: 25.08967\n",
            "Loss training: 22.34249\n",
            "Loss training: 30.918081\n",
            "Loss training: 29.357897\n",
            "Loss training: 28.046396\n",
            "Loss training: 55.041622\n",
            "Loss training: 29.132208\n",
            "Loss training: 14.8737955\n",
            "\n",
            "***************************\n",
            "Trained on 1930 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.524212\n",
            "Loss training: 24.755373\n",
            "Loss training: 27.895857\n",
            "Loss training: 40.124866\n",
            "Loss training: 24.488073\n",
            "Loss training: 39.978\n",
            "Loss training: 43.316673\n",
            "Loss training: 34.769547\n",
            "Loss training: 29.02006\n",
            "Loss training: 33.431274\n",
            "\n",
            "***************************\n",
            "Trained on 1940 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.759266\n",
            "Loss training: 22.138403\n",
            "Loss training: 42.84187\n",
            "Loss training: 32.513077\n",
            "Loss training: 40.493626\n",
            "Loss training: 45.091934\n",
            "Loss training: 48.67762\n",
            "Loss training: 23.839285\n",
            "Loss training: 28.557512\n",
            "Loss training: 19.360079\n",
            "\n",
            "***************************\n",
            "Trained on 1950 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.290321\n",
            "Loss training: 32.571136\n",
            "Loss training: 45.523735\n",
            "Loss training: 18.990938\n",
            "Loss training: 46.019085\n",
            "Loss training: 67.7538\n",
            "Loss training: 40.4776\n",
            "Loss training: 33.500027\n",
            "Loss training: 63.84734\n",
            "Loss training: 46.291298\n",
            "\n",
            "***************************\n",
            "Trained on 1960 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.183853\n",
            "Loss training: 38.60968\n",
            "Loss training: 43.55191\n",
            "Loss training: 36.784115\n",
            "Loss training: 42.370777\n",
            "Loss training: 34.82276\n",
            "Loss training: 42.391823\n",
            "Loss training: 58.09005\n",
            "Loss training: 42.02048\n",
            "Loss training: 47.993263\n",
            "\n",
            "***************************\n",
            "Trained on 1970 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 18.101324\n",
            "Loss training: 44.2268\n",
            "Loss training: 35.186897\n",
            "Loss training: 31.377028\n",
            "Loss training: 29.123077\n",
            "Loss training: 35.565647\n",
            "Loss training: 40.92698\n",
            "Loss training: 25.181458\n",
            "Loss training: 40.88074\n",
            "Loss training: 40.72414\n",
            "\n",
            "***************************\n",
            "Trained on 1980 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.83718\n",
            "Loss training: 43.694508\n",
            "Loss training: 28.042398\n",
            "Loss training: 35.156174\n",
            "Loss training: 30.808651\n",
            "Loss training: 24.903643\n",
            "Loss training: 32.93957\n",
            "Loss training: 48.1009\n",
            "Loss training: 32.56793\n",
            "Loss training: 27.864033\n",
            "\n",
            "***************************\n",
            "Trained on 1990 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 56.854412\n",
            "Loss training: 17.318533\n",
            "Loss training: 30.722189\n",
            "Loss training: 28.85419\n",
            "Loss training: 44.73512\n",
            "Loss training: 34.65598\n",
            "Loss training: 40.996235\n",
            "Loss training: 39.966953\n",
            "Loss training: 26.217638\n",
            "Loss training: 37.234753\n",
            "\n",
            "***************************\n",
            "Trained on 2000 graphs\n",
            "***************************\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_68ddb485-1b1e-4436-851c-ce5888ff89c6\", \"params_epochs_2000.pickle\", 22204504)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5434df3d-ad6c-4f92-aa10-ef406f701347\", \"opt_state_epochs_2000.pickle\", 44409100)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss training: 41.089745\n",
            "Loss training: 40.129494\n",
            "Loss training: 42.240547\n",
            "Loss training: 16.793087\n",
            "Loss training: 38.719368\n",
            "Loss training: 30.116524\n",
            "Loss training: 38.61201\n",
            "Loss training: 46.25449\n",
            "Loss training: 28.289072\n",
            "Loss training: 51.75172\n",
            "\n",
            "***************************\n",
            "Trained on 2010 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.737\n",
            "Loss training: 35.287796\n",
            "Loss training: 29.98757\n",
            "Loss training: 34.778744\n",
            "Loss training: 42.22739\n",
            "Loss training: 30.305315\n",
            "Loss training: 46.804398\n",
            "Loss training: 24.31731\n",
            "Loss training: 18.623177\n",
            "Loss training: 27.578678\n",
            "\n",
            "***************************\n",
            "Trained on 2020 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 19.18424\n",
            "Loss training: 22.558495\n",
            "Loss training: 18.827671\n",
            "Loss training: 33.206013\n",
            "Loss training: 47.39353\n",
            "Loss training: 40.21608\n",
            "Loss training: 35.31816\n",
            "Loss training: 35.46291\n",
            "Loss training: 39.486164\n",
            "Loss training: 28.329844\n",
            "\n",
            "***************************\n",
            "Trained on 2030 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.320404\n",
            "Loss training: 39.884148\n",
            "Loss training: 39.617092\n",
            "Loss training: 34.508823\n",
            "Loss training: 34.68155\n",
            "Loss training: 32.3563\n",
            "Loss training: 32.214733\n",
            "Loss training: 42.718857\n",
            "Loss training: 24.26362\n",
            "Loss training: 23.760317\n",
            "\n",
            "***************************\n",
            "Trained on 2040 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.721405\n",
            "Loss training: 32.947746\n",
            "Loss training: 32.76131\n",
            "Loss training: 64.51011\n",
            "Loss training: 40.301174\n",
            "Loss training: 34.495842\n",
            "Loss training: 36.027733\n",
            "Loss training: 40.252777\n",
            "Loss training: 40.428856\n",
            "Loss training: 43.559616\n",
            "\n",
            "***************************\n",
            "Trained on 2050 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.57046\n",
            "Loss training: 54.773235\n",
            "Loss training: 29.72763\n",
            "Loss training: 20.559397\n",
            "Loss training: 36.505188\n",
            "Loss training: 34.427166\n",
            "Loss training: 22.198908\n",
            "Loss training: 39.74872\n",
            "Loss training: 20.240595\n",
            "Loss training: 27.859726\n",
            "\n",
            "***************************\n",
            "Trained on 2060 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.826557\n",
            "Loss training: 25.214739\n",
            "Loss training: 31.875044\n",
            "Loss training: 15.790583\n",
            "Loss training: 48.004883\n",
            "Loss training: 36.218815\n",
            "Loss training: 45.265457\n",
            "Loss training: 30.67605\n",
            "Loss training: 35.926067\n",
            "Loss training: 44.83922\n",
            "\n",
            "***************************\n",
            "Trained on 2070 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.691067\n",
            "Loss training: 24.440939\n",
            "Loss training: 69.68509\n",
            "Loss training: 30.016846\n",
            "Loss training: 28.987646\n",
            "Loss training: 30.788687\n",
            "Loss training: 28.572426\n",
            "Loss training: 42.977455\n",
            "Loss training: 25.454689\n",
            "Loss training: 52.190105\n",
            "\n",
            "***************************\n",
            "Trained on 2080 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.355516\n",
            "Loss training: 28.626696\n",
            "Loss training: 25.425901\n",
            "Loss training: 40.07317\n",
            "Loss training: 32.698093\n",
            "Loss training: 29.912394\n",
            "Loss training: 41.960403\n",
            "Loss training: 40.54993\n",
            "Loss training: 37.26106\n",
            "Loss training: 37.93536\n",
            "\n",
            "***************************\n",
            "Trained on 2090 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.863049\n",
            "Loss training: 34.504356\n",
            "Loss training: 47.693195\n",
            "Loss training: 43.567806\n",
            "Loss training: 22.326115\n",
            "Loss training: 36.13106\n",
            "Loss training: 19.612349\n",
            "Loss training: 43.016083\n",
            "Loss training: 16.477663\n",
            "Loss training: 70.317085\n",
            "\n",
            "***************************\n",
            "Trained on 2100 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.029457\n",
            "Loss training: 48.88009\n",
            "Loss training: 40.134914\n",
            "Loss training: 38.426735\n",
            "Loss training: 39.870193\n",
            "Loss training: 25.098524\n",
            "Loss training: 28.550703\n",
            "Loss training: 40.24862\n",
            "Loss training: 27.847097\n",
            "Loss training: 32.371307\n",
            "\n",
            "***************************\n",
            "Trained on 2110 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.680595\n",
            "Loss training: 55.134277\n",
            "Loss training: 32.55534\n",
            "Loss training: 30.214783\n",
            "Loss training: 27.971825\n",
            "Loss training: 28.62015\n",
            "Loss training: 49.021557\n",
            "Loss training: 33.392857\n",
            "Loss training: 46.679523\n",
            "Loss training: 29.15324\n",
            "\n",
            "***************************\n",
            "Trained on 2120 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 46.72547\n",
            "Loss training: 29.344936\n",
            "Loss training: 35.648216\n",
            "Loss training: 32.457256\n",
            "Loss training: 20.229912\n",
            "Loss training: 54.210552\n",
            "Loss training: 19.846348\n",
            "Loss training: 34.562588\n",
            "Loss training: 43.45733\n",
            "Loss training: 36.547897\n",
            "\n",
            "***************************\n",
            "Trained on 2130 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.102024\n",
            "Loss training: 53.328335\n",
            "Loss training: 48.60702\n",
            "Loss training: 52.83072\n",
            "Loss training: 40.072594\n",
            "Loss training: 24.542738\n",
            "Loss training: 46.164062\n",
            "Loss training: 38.17516\n",
            "Loss training: 50.12238\n",
            "Loss training: 40.202377\n",
            "\n",
            "***************************\n",
            "Trained on 2140 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 46.498524\n",
            "Loss training: 34.992104\n",
            "Loss training: 44.04959\n",
            "Loss training: 17.306616\n",
            "Loss training: 30.650858\n",
            "Loss training: 16.318686\n",
            "Loss training: 35.602436\n",
            "Loss training: 37.589684\n",
            "Loss training: 15.242922\n",
            "Loss training: 14.962591\n",
            "\n",
            "***************************\n",
            "Trained on 2150 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.806757\n",
            "Loss training: 36.718082\n",
            "Loss training: 14.290626\n",
            "Loss training: 42.83791\n",
            "Loss training: 34.373165\n",
            "Loss training: 44.70586\n",
            "Loss training: 33.371693\n",
            "Loss training: 31.82752\n",
            "Loss training: 27.467405\n",
            "Loss training: 29.268642\n",
            "\n",
            "***************************\n",
            "Trained on 2160 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.268425\n",
            "Loss training: 17.56167\n",
            "Loss training: 16.803295\n",
            "Loss training: 31.908052\n",
            "Loss training: 14.271893\n",
            "Loss training: 28.096521\n",
            "Loss training: 30.174868\n",
            "Loss training: 35.641563\n",
            "Loss training: 19.336685\n",
            "Loss training: 34.942543\n",
            "\n",
            "***************************\n",
            "Trained on 2170 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 46.086086\n",
            "Loss training: 40.282448\n",
            "Loss training: 36.940758\n",
            "Loss training: 46.00199\n",
            "Loss training: 45.024532\n",
            "Loss training: 35.514385\n",
            "Loss training: 17.191467\n",
            "Loss training: 34.035984\n",
            "Loss training: 26.062868\n",
            "Loss training: 19.0386\n",
            "\n",
            "***************************\n",
            "Trained on 2180 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.96628\n",
            "Loss training: 27.078789\n",
            "Loss training: 14.946739\n",
            "Loss training: 32.88931\n",
            "Loss training: 25.42196\n",
            "Loss training: 35.723892\n",
            "Loss training: 32.418095\n",
            "Loss training: 43.99898\n",
            "Loss training: 32.647537\n",
            "Loss training: 43.77942\n",
            "\n",
            "***************************\n",
            "Trained on 2190 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.31773\n",
            "Loss training: 29.42656\n",
            "Loss training: 30.334558\n",
            "Loss training: 44.79754\n",
            "Loss training: 40.84186\n",
            "Loss training: 34.93847\n",
            "Loss training: 17.71008\n",
            "Loss training: 48.500244\n",
            "Loss training: 35.71498\n",
            "Loss training: 35.644615\n",
            "\n",
            "***************************\n",
            "Trained on 2200 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.2489\n",
            "Loss training: 46.223152\n",
            "Loss training: 40.25113\n",
            "Loss training: 31.981554\n",
            "Loss training: 30.441944\n",
            "Loss training: 68.31944\n",
            "Loss training: 47.707123\n",
            "Loss training: 47.083607\n",
            "Loss training: 37.73199\n",
            "Loss training: 14.545852\n",
            "\n",
            "***************************\n",
            "Trained on 2210 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 51.670418\n",
            "Loss training: 36.99669\n",
            "Loss training: 34.723293\n",
            "Loss training: 28.856178\n",
            "Loss training: 48.472866\n",
            "Loss training: 49.016735\n",
            "Loss training: 34.945908\n",
            "Loss training: 22.109917\n",
            "Loss training: 27.4924\n",
            "Loss training: 18.44935\n",
            "\n",
            "***************************\n",
            "Trained on 2220 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.858646\n",
            "Loss training: 43.131706\n",
            "Loss training: 31.696606\n",
            "Loss training: 28.84672\n",
            "Loss training: 28.688423\n",
            "Loss training: 36.947197\n",
            "Loss training: 49.24901\n",
            "Loss training: 47.615387\n",
            "Loss training: 28.254997\n",
            "Loss training: 32.513035\n",
            "\n",
            "***************************\n",
            "Trained on 2230 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.611237\n",
            "Loss training: 38.128456\n",
            "Loss training: 30.043144\n",
            "Loss training: 28.42148\n",
            "Loss training: 44.36397\n",
            "Loss training: 29.678076\n",
            "Loss training: 37.757008\n",
            "Loss training: 36.14972\n",
            "Loss training: 33.0199\n",
            "Loss training: 30.187124\n",
            "\n",
            "***************************\n",
            "Trained on 2240 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.057413\n",
            "Loss training: 44.681206\n",
            "Loss training: 33.044804\n",
            "Loss training: 48.308365\n",
            "Loss training: 46.257477\n",
            "Loss training: 37.392155\n",
            "Loss training: 27.58005\n",
            "Loss training: 36.897205\n",
            "Loss training: 25.334932\n",
            "Loss training: 35.6735\n",
            "\n",
            "***************************\n",
            "Trained on 2250 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.836876\n",
            "Loss training: 33.143158\n",
            "Loss training: 35.52002\n",
            "Loss training: 34.542492\n",
            "Loss training: 40.954407\n",
            "Loss training: 46.47312\n",
            "Loss training: 27.191093\n",
            "Loss training: 54.17299\n",
            "Loss training: 26.314169\n",
            "Loss training: 48.086815\n",
            "\n",
            "***************************\n",
            "Trained on 2260 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.845192\n",
            "Loss training: 40.60733\n",
            "Loss training: 35.892372\n",
            "Loss training: 32.295925\n",
            "Loss training: 34.75949\n",
            "Loss training: 52.291134\n",
            "Loss training: 20.418026\n",
            "Loss training: 29.22495\n",
            "Loss training: 45.761684\n",
            "Loss training: 23.522936\n",
            "\n",
            "***************************\n",
            "Trained on 2270 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 16.30238\n",
            "Loss training: 19.055426\n",
            "Loss training: 45.417286\n",
            "Loss training: 42.39379\n",
            "Loss training: 51.297344\n",
            "Loss training: 15.776075\n",
            "Loss training: 32.758217\n",
            "Loss training: 29.80911\n",
            "Loss training: 32.22132\n",
            "Loss training: 33.0786\n",
            "\n",
            "***************************\n",
            "Trained on 2280 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.328476\n",
            "Loss training: 32.413006\n",
            "Loss training: 46.745598\n",
            "Loss training: 34.643757\n",
            "Loss training: 26.162172\n",
            "Loss training: 29.637814\n",
            "Loss training: 45.066513\n",
            "Loss training: 19.169405\n",
            "Loss training: 46.402676\n",
            "Loss training: 36.580627\n",
            "\n",
            "***************************\n",
            "Trained on 2290 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.7843\n",
            "Loss training: 36.528587\n",
            "Loss training: 40.95071\n",
            "Loss training: 32.294464\n",
            "Loss training: 47.765778\n",
            "Loss training: 23.135536\n",
            "Loss training: 43.72245\n",
            "Loss training: 23.107971\n",
            "Loss training: 29.05418\n",
            "Loss training: 25.596148\n",
            "\n",
            "***************************\n",
            "Trained on 2300 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.104603\n",
            "Loss training: 32.36348\n",
            "Loss training: 37.84545\n",
            "Loss training: 53.929935\n",
            "Loss training: 30.088915\n",
            "Loss training: 35.38221\n",
            "Loss training: 29.912937\n",
            "Loss training: 37.60346\n",
            "Loss training: 35.55087\n",
            "Loss training: 32.45365\n",
            "\n",
            "***************************\n",
            "Trained on 2310 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.9554\n",
            "Loss training: 38.197975\n",
            "Loss training: 34.24419\n",
            "Loss training: 46.72916\n",
            "Loss training: 46.537624\n",
            "Loss training: 28.523165\n",
            "Loss training: 45.32223\n",
            "Loss training: 33.021797\n",
            "Loss training: 27.357523\n",
            "Loss training: 19.036913\n",
            "\n",
            "***************************\n",
            "Trained on 2320 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 20.311394\n",
            "Loss training: 24.62166\n",
            "Loss training: 35.726223\n",
            "Loss training: 15.622117\n",
            "Loss training: 39.220078\n",
            "Loss training: 43.42075\n",
            "Loss training: 43.175724\n",
            "Loss training: 14.599697\n",
            "Loss training: 34.463352\n",
            "Loss training: 28.590784\n",
            "\n",
            "***************************\n",
            "Trained on 2330 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.52984\n",
            "Loss training: 23.46519\n",
            "Loss training: 32.470623\n",
            "Loss training: 35.087856\n",
            "Loss training: 34.380768\n",
            "Loss training: 36.445755\n",
            "Loss training: 34.612106\n",
            "Loss training: 28.857616\n",
            "Loss training: 27.180582\n",
            "Loss training: 19.620855\n",
            "\n",
            "***************************\n",
            "Trained on 2340 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.850956\n",
            "Loss training: 46.958504\n",
            "Loss training: 44.660812\n",
            "Loss training: 42.691578\n",
            "Loss training: 27.880531\n",
            "Loss training: 34.786964\n",
            "Loss training: 34.624435\n",
            "Loss training: 19.586292\n",
            "Loss training: 42.123398\n",
            "Loss training: 33.274\n",
            "\n",
            "***************************\n",
            "Trained on 2350 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.65156\n",
            "Loss training: 45.179314\n",
            "Loss training: 38.253468\n",
            "Loss training: 40.53962\n",
            "Loss training: 31.431845\n",
            "Loss training: 20.704391\n",
            "Loss training: 14.69758\n",
            "Loss training: 36.201553\n",
            "Loss training: 23.420837\n",
            "Loss training: 35.39823\n",
            "\n",
            "***************************\n",
            "Trained on 2360 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.643917\n",
            "Loss training: 39.8314\n",
            "Loss training: 14.342875\n",
            "Loss training: 28.41548\n",
            "Loss training: 32.591976\n",
            "Loss training: 39.20516\n",
            "Loss training: 38.554535\n",
            "Loss training: 32.07987\n",
            "Loss training: 17.724592\n",
            "Loss training: 32.190285\n",
            "\n",
            "***************************\n",
            "Trained on 2370 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.082943\n",
            "Loss training: 34.861095\n",
            "Loss training: 43.716724\n",
            "Loss training: 26.990408\n",
            "Loss training: 35.608814\n",
            "Loss training: 32.52004\n",
            "Loss training: 22.168299\n",
            "Loss training: 22.577623\n",
            "Loss training: 30.930275\n",
            "Loss training: 24.475592\n",
            "\n",
            "***************************\n",
            "Trained on 2380 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.766647\n",
            "Loss training: 69.96152\n",
            "Loss training: 42.09804\n",
            "Loss training: 64.29664\n",
            "Loss training: 45.40828\n",
            "Loss training: 39.165447\n",
            "Loss training: 52.933636\n",
            "Loss training: 35.921192\n",
            "Loss training: 37.841206\n",
            "Loss training: 22.164701\n",
            "\n",
            "***************************\n",
            "Trained on 2390 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.372555\n",
            "Loss training: 34.64785\n",
            "Loss training: 39.59147\n",
            "Loss training: 52.31832\n",
            "Loss training: 38.44177\n",
            "Loss training: 28.699184\n",
            "Loss training: 45.842827\n",
            "Loss training: 28.364481\n",
            "Loss training: 28.213516\n",
            "Loss training: 28.047085\n",
            "\n",
            "***************************\n",
            "Trained on 2400 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.021267\n",
            "Loss training: 41.83491\n",
            "Loss training: 30.988535\n",
            "Loss training: 55.995384\n",
            "Loss training: 46.794056\n",
            "Loss training: 29.736475\n",
            "Loss training: 23.733076\n",
            "Loss training: 58.61184\n",
            "Loss training: 25.406029\n",
            "Loss training: 30.052317\n",
            "\n",
            "***************************\n",
            "Trained on 2410 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.81896\n",
            "Loss training: 49.95586\n",
            "Loss training: 23.965355\n",
            "Loss training: 49.13623\n",
            "Loss training: 33.45206\n",
            "Loss training: 51.757534\n",
            "Loss training: 32.222473\n",
            "Loss training: 36.84987\n",
            "Loss training: 32.487072\n",
            "Loss training: 19.000753\n",
            "\n",
            "***************************\n",
            "Trained on 2420 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.44124\n",
            "Loss training: 35.78076\n",
            "Loss training: 47.50216\n",
            "Loss training: 30.47148\n",
            "Loss training: 46.703125\n",
            "Loss training: 30.275139\n",
            "Loss training: 29.08751\n",
            "Loss training: 28.03953\n",
            "Loss training: 36.522766\n",
            "Loss training: 25.066425\n",
            "\n",
            "***************************\n",
            "Trained on 2430 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.22217\n",
            "Loss training: 29.919058\n",
            "Loss training: 40.506798\n",
            "Loss training: 29.704779\n",
            "Loss training: 30.61354\n",
            "Loss training: 45.515476\n",
            "Loss training: 22.521658\n",
            "Loss training: 26.464376\n",
            "Loss training: 35.766518\n",
            "Loss training: 29.31042\n",
            "\n",
            "***************************\n",
            "Trained on 2440 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.286026\n",
            "Loss training: 36.960182\n",
            "Loss training: 36.266155\n",
            "Loss training: 27.689419\n",
            "Loss training: 35.078987\n",
            "Loss training: 21.989758\n",
            "Loss training: 27.48154\n",
            "Loss training: 45.03828\n",
            "Loss training: 43.404743\n",
            "Loss training: 34.95066\n",
            "\n",
            "***************************\n",
            "Trained on 2450 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.420918\n",
            "Loss training: 27.647749\n",
            "Loss training: 53.74506\n",
            "Loss training: 23.56524\n",
            "Loss training: 25.299274\n",
            "Loss training: 39.94921\n",
            "Loss training: 35.448833\n",
            "Loss training: 32.372776\n",
            "Loss training: 45.98567\n",
            "Loss training: 44.590355\n",
            "\n",
            "***************************\n",
            "Trained on 2460 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.024591\n",
            "Loss training: 28.14935\n",
            "Loss training: 29.514318\n",
            "Loss training: 40.897846\n",
            "Loss training: 59.73622\n",
            "Loss training: 19.214645\n",
            "Loss training: 47.318043\n",
            "Loss training: 27.895748\n",
            "Loss training: 34.10735\n",
            "Loss training: 36.47168\n",
            "\n",
            "***************************\n",
            "Trained on 2470 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.62265\n",
            "Loss training: 36.208435\n",
            "Loss training: 37.438828\n",
            "Loss training: 27.347902\n",
            "Loss training: 38.80585\n",
            "Loss training: 34.652977\n",
            "Loss training: 26.67651\n",
            "Loss training: 39.975266\n",
            "Loss training: 27.571405\n",
            "Loss training: 22.185688\n",
            "\n",
            "***************************\n",
            "Trained on 2480 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.857777\n",
            "Loss training: 40.146877\n",
            "Loss training: 31.972902\n",
            "Loss training: 51.34016\n",
            "Loss training: 54.463215\n",
            "Loss training: 53.5643\n",
            "Loss training: 30.11508\n",
            "Loss training: 45.3863\n",
            "Loss training: 37.459312\n",
            "Loss training: 20.093407\n",
            "\n",
            "***************************\n",
            "Trained on 2490 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.752754\n",
            "Loss training: 41.328598\n",
            "Loss training: 17.411657\n",
            "Loss training: 31.538519\n",
            "Loss training: 34.57519\n",
            "Loss training: 30.189667\n",
            "Loss training: 40.11328\n",
            "Loss training: 40.323895\n",
            "Loss training: 29.175951\n",
            "Loss training: 27.105194\n",
            "\n",
            "***************************\n",
            "Trained on 2500 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.914413\n",
            "Loss training: 37.72873\n",
            "Loss training: 23.636656\n",
            "Loss training: 25.837545\n",
            "Loss training: 45.68059\n",
            "Loss training: 39.5334\n",
            "Loss training: 32.42454\n",
            "Loss training: 33.579216\n",
            "Loss training: 28.24746\n",
            "Loss training: 46.86514\n",
            "\n",
            "***************************\n",
            "Trained on 2510 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.031025\n",
            "Loss training: 45.134186\n",
            "Loss training: 28.806274\n",
            "Loss training: 53.47373\n",
            "Loss training: 36.25377\n",
            "Loss training: 28.384817\n",
            "Loss training: 45.754627\n",
            "Loss training: 36.425762\n",
            "Loss training: 50.963463\n",
            "Loss training: 22.479366\n",
            "\n",
            "***************************\n",
            "Trained on 2520 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.074305\n",
            "Loss training: 24.160831\n",
            "Loss training: 44.689087\n",
            "Loss training: 28.501684\n",
            "Loss training: 35.89342\n",
            "Loss training: 31.788021\n",
            "Loss training: 61.25706\n",
            "Loss training: 36.376194\n",
            "Loss training: 27.979631\n",
            "Loss training: 29.312286\n",
            "\n",
            "***************************\n",
            "Trained on 2530 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.843512\n",
            "Loss training: 28.430103\n",
            "Loss training: 37.10064\n",
            "Loss training: 32.375687\n",
            "Loss training: 40.580334\n",
            "Loss training: 34.625492\n",
            "Loss training: 29.833805\n",
            "Loss training: 44.993782\n",
            "Loss training: 36.21767\n",
            "Loss training: 27.80789\n",
            "\n",
            "***************************\n",
            "Trained on 2540 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.259619\n",
            "Loss training: 25.60345\n",
            "Loss training: 45.690407\n",
            "Loss training: 44.806282\n",
            "Loss training: 29.16434\n",
            "Loss training: 43.940044\n",
            "Loss training: 27.637135\n",
            "Loss training: 52.83042\n",
            "Loss training: 44.5235\n",
            "Loss training: 35.164314\n",
            "\n",
            "***************************\n",
            "Trained on 2550 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.208595\n",
            "Loss training: 22.13277\n",
            "Loss training: 27.467365\n",
            "Loss training: 42.9085\n",
            "Loss training: 45.565166\n",
            "Loss training: 36.416027\n",
            "Loss training: 35.06277\n",
            "Loss training: 45.394226\n",
            "Loss training: 29.800625\n",
            "Loss training: 37.96085\n",
            "\n",
            "***************************\n",
            "Trained on 2560 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.16979\n",
            "Loss training: 40.345604\n",
            "Loss training: 26.738468\n",
            "Loss training: 32.471394\n",
            "Loss training: 27.754349\n",
            "Loss training: 33.476654\n",
            "Loss training: 26.228092\n",
            "Loss training: 21.99646\n",
            "Loss training: 32.620914\n",
            "Loss training: 25.982498\n",
            "\n",
            "***************************\n",
            "Trained on 2570 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.413523\n",
            "Loss training: 19.541557\n",
            "Loss training: 27.536486\n",
            "Loss training: 27.516645\n",
            "Loss training: 43.997772\n",
            "Loss training: 35.115074\n",
            "Loss training: 52.960957\n",
            "Loss training: 26.58795\n",
            "Loss training: 29.339182\n",
            "Loss training: 22.918268\n",
            "\n",
            "***************************\n",
            "Trained on 2580 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.544716\n",
            "Loss training: 39.907803\n",
            "Loss training: 35.05402\n",
            "Loss training: 35.910706\n",
            "Loss training: 27.79997\n",
            "Loss training: 48.90006\n",
            "Loss training: 27.619616\n",
            "Loss training: 24.388834\n",
            "Loss training: 32.601322\n",
            "Loss training: 31.661097\n",
            "\n",
            "***************************\n",
            "Trained on 2590 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.297094\n",
            "Loss training: 25.325188\n",
            "Loss training: 38.000923\n",
            "Loss training: 45.360313\n",
            "Loss training: 27.273527\n",
            "Loss training: 35.12188\n",
            "Loss training: 43.67113\n",
            "Loss training: 34.86435\n",
            "Loss training: 45.138714\n",
            "Loss training: 59.711956\n",
            "\n",
            "***************************\n",
            "Trained on 2600 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.26693\n",
            "Loss training: 32.7814\n",
            "Loss training: 25.931286\n",
            "Loss training: 30.854212\n",
            "Loss training: 45.057774\n",
            "Loss training: 32.21476\n",
            "Loss training: 39.767593\n",
            "Loss training: 39.711147\n",
            "Loss training: 57.53712\n",
            "Loss training: 33.087116\n",
            "\n",
            "***************************\n",
            "Trained on 2610 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.22269\n",
            "Loss training: 32.793827\n",
            "Loss training: 55.39252\n",
            "Loss training: 26.642262\n",
            "Loss training: 45.886356\n",
            "Loss training: 46.87084\n",
            "Loss training: 33.974907\n",
            "Loss training: 27.90965\n",
            "Loss training: 25.810324\n",
            "Loss training: 25.628227\n",
            "\n",
            "***************************\n",
            "Trained on 2620 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.429554\n",
            "Loss training: 27.838898\n",
            "Loss training: 35.58861\n",
            "Loss training: 31.137428\n",
            "Loss training: 23.806406\n",
            "Loss training: 37.134323\n",
            "Loss training: 24.965294\n",
            "Loss training: 41.40426\n",
            "Loss training: 38.375614\n",
            "Loss training: 27.210825\n",
            "\n",
            "***************************\n",
            "Trained on 2630 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 16.782942\n",
            "Loss training: 27.232317\n",
            "Loss training: 18.700926\n",
            "Loss training: 22.658108\n",
            "Loss training: 15.295038\n",
            "Loss training: 22.387556\n",
            "Loss training: 35.854378\n",
            "Loss training: 18.230844\n",
            "Loss training: 30.095789\n",
            "Loss training: 35.367153\n",
            "\n",
            "***************************\n",
            "Trained on 2640 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.75555\n",
            "Loss training: 14.015194\n",
            "Loss training: 39.050438\n",
            "Loss training: 29.316395\n",
            "Loss training: 13.482946\n",
            "Loss training: 31.783453\n",
            "Loss training: 34.772495\n",
            "Loss training: 23.395441\n",
            "Loss training: 18.75257\n",
            "Loss training: 42.8915\n",
            "\n",
            "***************************\n",
            "Trained on 2650 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.023205\n",
            "Loss training: 28.86443\n",
            "Loss training: 42.04986\n",
            "Loss training: 46.433205\n",
            "Loss training: 28.688324\n",
            "Loss training: 56.236557\n",
            "Loss training: 29.423504\n",
            "Loss training: 35.949318\n",
            "Loss training: 40.340397\n",
            "Loss training: 28.017338\n",
            "\n",
            "***************************\n",
            "Trained on 2660 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.38163\n",
            "Loss training: 28.518864\n",
            "Loss training: 22.840158\n",
            "Loss training: 39.885628\n",
            "Loss training: 32.201576\n",
            "Loss training: 23.562468\n",
            "Loss training: 27.801512\n",
            "Loss training: 24.499998\n",
            "Loss training: 54.317993\n",
            "Loss training: 25.624086\n",
            "\n",
            "***************************\n",
            "Trained on 2670 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 15.022185\n",
            "Loss training: 39.738354\n",
            "Loss training: 40.070465\n",
            "Loss training: 41.744057\n",
            "Loss training: 27.192923\n",
            "Loss training: 34.003876\n",
            "Loss training: 41.215675\n",
            "Loss training: 29.662838\n",
            "Loss training: 25.144579\n",
            "Loss training: 29.415262\n",
            "\n",
            "***************************\n",
            "Trained on 2680 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.336605\n",
            "Loss training: 22.922745\n",
            "Loss training: 37.246124\n",
            "Loss training: 43.695446\n",
            "Loss training: 27.36796\n",
            "Loss training: 39.055706\n",
            "Loss training: 36.068584\n",
            "Loss training: 28.044827\n",
            "Loss training: 18.659428\n",
            "Loss training: 23.75188\n",
            "\n",
            "***************************\n",
            "Trained on 2690 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.618664\n",
            "Loss training: 49.664795\n",
            "Loss training: 26.792534\n",
            "Loss training: 32.71934\n",
            "Loss training: 56.669544\n",
            "Loss training: 55.58577\n",
            "Loss training: 29.169807\n",
            "Loss training: 27.573368\n",
            "Loss training: 34.306484\n",
            "Loss training: 33.84844\n",
            "\n",
            "***************************\n",
            "Trained on 2700 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.563072\n",
            "Loss training: 29.606606\n",
            "Loss training: 50.49781\n",
            "Loss training: 29.562382\n",
            "Loss training: 35.775017\n",
            "Loss training: 51.361523\n",
            "Loss training: 22.78718\n",
            "Loss training: 33.369083\n",
            "Loss training: 19.381498\n",
            "Loss training: 23.894728\n",
            "\n",
            "***************************\n",
            "Trained on 2710 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 49.90257\n",
            "Loss training: 18.518532\n",
            "Loss training: 43.849426\n",
            "Loss training: 42.16692\n",
            "Loss training: 58.3582\n",
            "Loss training: 29.57334\n",
            "Loss training: 45.93936\n",
            "Loss training: 33.460026\n",
            "Loss training: 26.8525\n",
            "Loss training: 39.386265\n",
            "\n",
            "***************************\n",
            "Trained on 2720 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.155903\n",
            "Loss training: 24.881824\n",
            "Loss training: 39.63547\n",
            "Loss training: 49.130333\n",
            "Loss training: 25.313202\n",
            "Loss training: 33.789757\n",
            "Loss training: 35.53044\n",
            "Loss training: 44.477406\n",
            "Loss training: 27.943836\n",
            "Loss training: 49.093735\n",
            "\n",
            "***************************\n",
            "Trained on 2730 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.958704\n",
            "Loss training: 45.271805\n",
            "Loss training: 29.089146\n",
            "Loss training: 35.235252\n",
            "Loss training: 18.022718\n",
            "Loss training: 17.230143\n",
            "Loss training: 27.720413\n",
            "Loss training: 35.7338\n",
            "Loss training: 35.88897\n",
            "Loss training: 34.948647\n",
            "\n",
            "***************************\n",
            "Trained on 2740 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.039896\n",
            "Loss training: 25.148373\n",
            "Loss training: 35.41915\n",
            "Loss training: 31.946672\n",
            "Loss training: 26.224844\n",
            "Loss training: 43.826363\n",
            "Loss training: 34.14916\n",
            "Loss training: 52.602993\n",
            "Loss training: 34.95015\n",
            "Loss training: 37.32377\n",
            "\n",
            "***************************\n",
            "Trained on 2750 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.655964\n",
            "Loss training: 31.664972\n",
            "Loss training: 45.68491\n",
            "Loss training: 33.527023\n",
            "Loss training: 25.567451\n",
            "Loss training: 51.61322\n",
            "Loss training: 28.781384\n",
            "Loss training: 23.694157\n",
            "Loss training: 39.938328\n",
            "Loss training: 29.55581\n",
            "\n",
            "***************************\n",
            "Trained on 2760 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.418167\n",
            "Loss training: 43.49724\n",
            "Loss training: 29.431908\n",
            "Loss training: 34.724674\n",
            "Loss training: 30.773134\n",
            "Loss training: 34.629063\n",
            "Loss training: 22.912252\n",
            "Loss training: 27.736975\n",
            "Loss training: 30.084982\n",
            "Loss training: 19.791727\n",
            "\n",
            "***************************\n",
            "Trained on 2770 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.042755\n",
            "Loss training: 40.936504\n",
            "Loss training: 18.59728\n",
            "Loss training: 63.535362\n",
            "Loss training: 19.104542\n",
            "Loss training: 46.763092\n",
            "Loss training: 32.37256\n",
            "Loss training: 23.835024\n",
            "Loss training: 36.580246\n",
            "Loss training: 40.152664\n",
            "\n",
            "***************************\n",
            "Trained on 2780 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.866814\n",
            "Loss training: 47.43589\n",
            "Loss training: 37.722546\n",
            "Loss training: 34.724567\n",
            "Loss training: 34.40976\n",
            "Loss training: 22.499165\n",
            "Loss training: 31.576735\n",
            "Loss training: 36.774307\n",
            "Loss training: 39.819336\n",
            "Loss training: 38.103855\n",
            "\n",
            "***************************\n",
            "Trained on 2790 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.941572\n",
            "Loss training: 22.687807\n",
            "Loss training: 22.455738\n",
            "Loss training: 28.044765\n",
            "Loss training: 51.314835\n",
            "Loss training: 30.149084\n",
            "Loss training: 45.05384\n",
            "Loss training: 34.290028\n",
            "Loss training: 41.963997\n",
            "Loss training: 32.3875\n",
            "\n",
            "***************************\n",
            "Trained on 2800 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.509674\n",
            "Loss training: 32.136578\n",
            "Loss training: 22.440506\n",
            "Loss training: 31.431154\n",
            "Loss training: 23.820545\n",
            "Loss training: 33.49307\n",
            "Loss training: 35.821297\n",
            "Loss training: 33.267387\n",
            "Loss training: 27.843142\n",
            "Loss training: 39.879623\n",
            "\n",
            "***************************\n",
            "Trained on 2810 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.096859\n",
            "Loss training: 35.605442\n",
            "Loss training: 28.68746\n",
            "Loss training: 25.89027\n",
            "Loss training: 32.961723\n",
            "Loss training: 29.413008\n",
            "Loss training: 24.514341\n",
            "Loss training: 34.21819\n",
            "Loss training: 34.101257\n",
            "Loss training: 18.71891\n",
            "\n",
            "***************************\n",
            "Trained on 2820 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 15.172812\n",
            "Loss training: 25.169346\n",
            "Loss training: 30.394289\n",
            "Loss training: 27.429834\n",
            "Loss training: 21.881952\n",
            "Loss training: 45.623512\n",
            "Loss training: 41.094334\n",
            "Loss training: 49.433105\n",
            "Loss training: 36.44879\n",
            "Loss training: 30.262972\n",
            "\n",
            "***************************\n",
            "Trained on 2830 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.636665\n",
            "Loss training: 13.441737\n",
            "Loss training: 46.92064\n",
            "Loss training: 15.200783\n",
            "Loss training: 64.342155\n",
            "Loss training: 40.2746\n",
            "Loss training: 28.184616\n",
            "Loss training: 22.052216\n",
            "Loss training: 32.985485\n",
            "Loss training: 22.003927\n",
            "\n",
            "***************************\n",
            "Trained on 2840 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 13.434497\n",
            "Loss training: 36.90556\n",
            "Loss training: 21.876644\n",
            "Loss training: 32.37155\n",
            "Loss training: 32.01601\n",
            "Loss training: 55.64929\n",
            "Loss training: 23.659445\n",
            "Loss training: 27.78168\n",
            "Loss training: 42.22846\n",
            "Loss training: 28.934847\n",
            "\n",
            "***************************\n",
            "Trained on 2850 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.663986\n",
            "Loss training: 27.674177\n",
            "Loss training: 25.908848\n",
            "Loss training: 23.245935\n",
            "Loss training: 45.42724\n",
            "Loss training: 34.528614\n",
            "Loss training: 18.53324\n",
            "Loss training: 22.228216\n",
            "Loss training: 30.471096\n",
            "Loss training: 38.703995\n",
            "\n",
            "***************************\n",
            "Trained on 2860 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.486673\n",
            "Loss training: 29.996973\n",
            "Loss training: 34.164207\n",
            "Loss training: 28.865784\n",
            "Loss training: 27.773087\n",
            "Loss training: 18.566605\n",
            "Loss training: 39.772793\n",
            "Loss training: 34.963753\n",
            "Loss training: 34.3828\n",
            "Loss training: 18.22798\n",
            "\n",
            "***************************\n",
            "Trained on 2870 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.813232\n",
            "Loss training: 43.473816\n",
            "Loss training: 24.011345\n",
            "Loss training: 44.91325\n",
            "Loss training: 44.604397\n",
            "Loss training: 13.803448\n",
            "Loss training: 45.017338\n",
            "Loss training: 32.387573\n",
            "Loss training: 39.17107\n",
            "Loss training: 43.9705\n",
            "\n",
            "***************************\n",
            "Trained on 2880 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.54558\n",
            "Loss training: 33.55146\n",
            "Loss training: 19.21867\n",
            "Loss training: 44.2458\n",
            "Loss training: 27.858534\n",
            "Loss training: 57.10714\n",
            "Loss training: 53.058212\n",
            "Loss training: 33.595596\n",
            "Loss training: 27.470882\n",
            "Loss training: 26.63253\n",
            "\n",
            "***************************\n",
            "Trained on 2890 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.512024\n",
            "Loss training: 41.61337\n",
            "Loss training: 39.941357\n",
            "Loss training: 30.00477\n",
            "Loss training: 40.931927\n",
            "Loss training: 37.41516\n",
            "Loss training: 40.025246\n",
            "Loss training: 36.810566\n",
            "Loss training: 25.998798\n",
            "Loss training: 33.556767\n",
            "\n",
            "***************************\n",
            "Trained on 2900 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.446867\n",
            "Loss training: 29.783514\n",
            "Loss training: 34.991623\n",
            "Loss training: 27.399424\n",
            "Loss training: 14.8374\n",
            "Loss training: 37.03332\n",
            "Loss training: 33.124058\n",
            "Loss training: 55.36638\n",
            "Loss training: 30.312212\n",
            "Loss training: 40.544582\n",
            "\n",
            "***************************\n",
            "Trained on 2910 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 18.51903\n",
            "Loss training: 60.09308\n",
            "Loss training: 31.86596\n",
            "Loss training: 33.438904\n",
            "Loss training: 35.12355\n",
            "Loss training: 26.866392\n",
            "Loss training: 49.95359\n",
            "Loss training: 35.748405\n",
            "Loss training: 44.934643\n",
            "Loss training: 30.962337\n",
            "\n",
            "***************************\n",
            "Trained on 2920 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.581013\n",
            "Loss training: 36.079567\n",
            "Loss training: 30.76011\n",
            "Loss training: 43.77653\n",
            "Loss training: 27.359316\n",
            "Loss training: 28.569239\n",
            "Loss training: 47.874714\n",
            "Loss training: 31.51043\n",
            "Loss training: 33.96508\n",
            "Loss training: 40.10263\n",
            "\n",
            "***************************\n",
            "Trained on 2930 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.971607\n",
            "Loss training: 33.79991\n",
            "Loss training: 25.91464\n",
            "Loss training: 27.800198\n",
            "Loss training: 31.453844\n",
            "Loss training: 33.129494\n",
            "Loss training: 27.590267\n",
            "Loss training: 26.615053\n",
            "Loss training: 32.795082\n",
            "Loss training: 23.784681\n",
            "\n",
            "***************************\n",
            "Trained on 2940 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.703228\n",
            "Loss training: 27.296104\n",
            "Loss training: 26.265915\n",
            "Loss training: 27.10519\n",
            "Loss training: 34.27761\n",
            "Loss training: 39.80235\n",
            "Loss training: 35.505405\n",
            "Loss training: 36.664368\n",
            "Loss training: 43.47868\n",
            "Loss training: 34.185966\n",
            "\n",
            "***************************\n",
            "Trained on 2950 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.374027\n",
            "Loss training: 32.458694\n",
            "Loss training: 30.100933\n",
            "Loss training: 36.51241\n",
            "Loss training: 30.665636\n",
            "Loss training: 12.993536\n",
            "Loss training: 22.578197\n",
            "Loss training: 25.735252\n",
            "Loss training: 28.828468\n",
            "Loss training: 32.316437\n",
            "\n",
            "***************************\n",
            "Trained on 2960 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 18.610949\n",
            "Loss training: 32.378605\n",
            "Loss training: 24.09416\n",
            "Loss training: 29.48976\n",
            "Loss training: 21.019705\n",
            "Loss training: 26.906687\n",
            "Loss training: 31.990551\n",
            "Loss training: 18.51199\n",
            "Loss training: 25.787037\n",
            "Loss training: 43.9028\n",
            "\n",
            "***************************\n",
            "Trained on 2970 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.062042\n",
            "Loss training: 27.528753\n",
            "Loss training: 22.205622\n",
            "Loss training: 26.046213\n",
            "Loss training: 32.792942\n",
            "Loss training: 25.94997\n",
            "Loss training: 33.612114\n",
            "Loss training: 28.278797\n",
            "Loss training: 22.61289\n",
            "Loss training: 26.947384\n",
            "\n",
            "***************************\n",
            "Trained on 2980 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.361847\n",
            "Loss training: 27.537964\n",
            "Loss training: 44.31753\n",
            "Loss training: 22.025026\n",
            "Loss training: 28.241009\n",
            "Loss training: 21.783485\n",
            "Loss training: 43.255703\n",
            "Loss training: 31.944172\n",
            "Loss training: 32.225594\n",
            "Loss training: 33.254093\n",
            "\n",
            "***************************\n",
            "Trained on 2990 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.869465\n",
            "Loss training: 34.95466\n",
            "Loss training: 43.914356\n",
            "Loss training: 34.244835\n",
            "Loss training: 34.843952\n",
            "Loss training: 59.81678\n",
            "Loss training: 43.434837\n",
            "Loss training: 28.800554\n",
            "Loss training: 40.70717\n",
            "Loss training: 18.474274\n",
            "\n",
            "***************************\n",
            "Trained on 3000 graphs\n",
            "***************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('/content/exp_2022-03-30-09:51:17/params_epochs_9000.pickle', 'rb') as f:\n",
        "#     loaded_params = pickle.load(f)\n",
        "\n",
        "loaded_params = final_params\n",
        "\n",
        "evaluate(loaded_params, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMmQkhHRy7Vf",
        "outputId": "4d9d6fc3-8303-4207-d3fa-2c446e1ce8de"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 27.584213256835938 | ROC: 0.7084962840761466\n",
            "Average test loss: 27.584213256835938 | Average ROC: 0.7084962840761466\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training the model for 5000 iterations on a single partition I was able to get the test loss down to around 2 (also got close to 0.8 during training but this increased later back to 14 then decreased again). The ROC here was around 99.9% on the single partition (when using the training mask for evaluation)."
      ],
      "metadata": {
        "id": "vXmArbXkZvcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_on_full_sets(params):\n",
        "  final_predictions = {}\n",
        "\n",
        "  for i in range(num_partitions):\n",
        "    node_ids = dgl_graph_metis_partition[i].ndata['_ID']\n",
        "    partition = processed_graphs[f'partition_{i}']\n",
        "    \n",
        "    predictions, _ = predict_on_graph(params, \n",
        "                                      partition['graph'], \n",
        "                                      partition['labels'], \n",
        "                                      partition['test_mask']  # Only used in the loss computation, does not affect predictions\n",
        "                                      )\n",
        "\n",
        "    predictions_after_masked_nodes_are_removed = remove_mask_from_data(\n",
        "        reshape_broadcasted_data(predictions),\n",
        "        reshape_broadcasted_data(partition['padding_mask'])\n",
        "        )\n",
        "\n",
        "    for index, node_id in enumerate(node_ids):\n",
        "      final_predictions[node_id] = predictions_after_masked_nodes_are_removed[index]\n",
        "\n",
        "    if (i + 1) % 10 == 0:\n",
        "      print(f'Evaluated {i + 1} / {num_partitions} subgraphs...')\n",
        "\n",
        "  # Sort the final predictions based on the node ids\n",
        "  predictions_in_order = dict(sorted(final_predictions.items()))\n",
        "\n",
        "  # Convert the values to a list to be able to slice based on the ids of the \n",
        "  # nodes in the test set\n",
        "  predictions_in_order = list(predictions_in_order.values())\n",
        "\n",
        "  final_roc_train = evaluator.eval({\n",
        "      \"y_true\": np.array(train_label), \n",
        "      \"y_pred\": np.array([predictions_in_order[x] for x in split_idx['train']])\n",
        "      })['rocauc']\n",
        "\n",
        "  final_roc_valid = evaluator.eval({\n",
        "      \"y_true\": np.array(valid_label), \n",
        "      \"y_pred\": np.array([predictions_in_order[x] for x in split_idx['valid']])\n",
        "      })['rocauc']\n",
        "\n",
        "  final_roc_test = evaluator.eval({\n",
        "      \"y_true\": np.array(test_label),\n",
        "      \"y_pred\": np.array([predictions_in_order[x] for x in split_idx['test']])\n",
        "      })['rocauc']\n",
        "\n",
        "  print()\n",
        "  print(f'Final ROC on the train set {final_roc_train}')\n",
        "  print(f'Final ROC on the validation set {final_roc_valid}')\n",
        "  print(f'Final ROC on the test set {final_roc_test}')\n",
        "\n",
        "evaluate_on_full_sets(loaded_params)\n",
        "\n",
        "'''\n",
        "  Previous runs:\n",
        "  (1) Configuration\n",
        "        learning_rate = 0.001\n",
        "        num_partitions = 50\n",
        "        hidden_dimension = 128\n",
        "        num_message_passing_steps = 5\n",
        "        num_training_steps = 1000\n",
        "    ROC on the train set 0.7348797273386144\n",
        "    ROC on the validation set 0.6025038939324504\n",
        "    ROC on the test set 0.5896861508337246\n",
        "\n",
        "  (2) Configuration\n",
        "        learning_rate = 0.001\n",
        "        num_partitions = 50\n",
        "        hidden_dimension = 128\n",
        "        num_message_passing_steps = 5\n",
        "        num_training_steps = 30000\n",
        "    ROC on the train set 0.8050085464161815\n",
        "    ROC on the validation set 0.6327603823722211\n",
        "    ROC on the test set 0.5078022533003436\n",
        "'''"
      ],
      "metadata": {
        "id": "aq4_r4M6VnzW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "1327385c-0eac-4ae2-989f-bb5bffe6d674"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluated 10 / 50 subgraphs...\n",
            "Evaluated 20 / 50 subgraphs...\n",
            "Evaluated 30 / 50 subgraphs...\n",
            "Evaluated 40 / 50 subgraphs...\n",
            "Evaluated 50 / 50 subgraphs...\n",
            "\n",
            "Final ROC on the train set 0.8050085464161815\n",
            "Final ROC on the validation set 0.6327603823722211\n",
            "Final ROC on the test set 0.5078022533003436\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  Previous runs:\\n  (1) Configuration\\n        learning_rate = 0.001\\n        num_partitions = 50\\n        hidden_dimension = 128\\n        num_message_passing_steps = 5\\n        num_training_steps = 1000\\n    ROC on the train set 0.7348797273386144\\n    ROC on the validation set 0.6025038939324504\\n    ROC on the test set 0.5896861508337246\\n\\n  (2) Configuration\\n        learning_rate = 0.001\\n        num_partitions = 50\\n        hidden_dimension = 128\\n        num_message_passing_steps = 5\\n        num_training_steps = 30000\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    }
  ]
}