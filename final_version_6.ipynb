{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final-version-2-sharded-networks.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bf319/Scaling_MPNNs/blob/main/final_version_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rLhMwiHHWbtK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a3af935-3ae0-4dcf-f81c-7ec5b9a41134"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for dm-haiku (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 75 kB 2.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 70 kB 3.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.6 MB/s \n",
            "\u001b[?25h  Building wheel for jaxline (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 78 kB 3.3 MB/s \n",
            "\u001b[?25h  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 136 kB 5.4 MB/s \n",
            "\u001b[?25h  Building wheel for metis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "--2022-03-29 16:49:59--  https://raw.githubusercontent.com/deepmind/jraph/master/jraph/experimental/sharded_graphnet.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22444 (22K) [text/plain]\n",
            "Saving to: ‘sharded_graphnet.py’\n",
            "\n",
            "sharded_graphnet.py 100%[===================>]  21.92K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2022-03-29 16:49:59 (16.5 MB/s) - ‘sharded_graphnet.py’ saved [22444/22444]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%pip install -q git+https://github.com/deepmind/dm-haiku\n",
        "%pip install -q jraph\n",
        "%pip install -q git+https://github.com/deepmind/jaxline\n",
        "%pip install -q ogb\n",
        "%pip install -q dgl\n",
        "%pip install -q optax\n",
        "%pip install -q metis\n",
        "\n",
        "!wget https://raw.githubusercontent.com/deepmind/jraph/master/jraph/experimental/sharded_graphnet.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "\n",
        "num_devices = jax.local_device_count()\n",
        "jax.devices()"
      ],
      "metadata": {
        "id": "iT2wqf76kIRp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1def6b40-f565-4f4e-b274-4eac5630cee7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ogb.nodeproppred import Evaluator\n",
        "from ogb.nodeproppred import DglNodePropPredDataset\n",
        "\n",
        "dataset = DglNodePropPredDataset(name = \"ogbn-proteins\")\n",
        "split_idx = dataset.get_idx_split()\n",
        "evaluator = Evaluator(name = 'ogbn-proteins')\n",
        "print(evaluator.expected_input_format)"
      ],
      "metadata": {
        "id": "xHClucOxWpAZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e53a800-d042-42c0-af9c-23a5f5dfd897"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Using backend: pytorch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/proteins.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 0.21 GB: 100%|██████████| 216/216 [00:06<00:00, 35.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/proteins.zip\n",
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.73s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into DGL objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  2.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n",
            "==== Expected input format of Evaluator for ogbn-proteins\n",
            "{'y_true': y_true, 'y_pred': y_pred}\n",
            "- y_true: numpy ndarray or torch tensor of shape (num_node, num_task)\n",
            "- y_pred: numpy ndarray or torch tensor of shape (num_node, num_task)\n",
            "where y_pred stores score values (for computing ROC-AUC),\n",
            "num_task is 112, and each row corresponds to one node.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "# There is only one graph in Node Property Prediction datasets\n",
        "ogbn_proteins_main_graph, ogbn_proteins_main_labels = dataset[0]\n",
        "\n",
        "'''\n",
        "  OGBN-Proteins\n",
        "    #Nodes = 132,534\n",
        "    #Edges = 39,561,252\n",
        "    #Diameter ~ 9 (https://cs.stanford.edu/people/jure/pubs/ogb-neurips20.pdf)\n",
        "    #Tasks = 112\n",
        "    #Split Type = Species\n",
        "    #Task Type = Binary classification\n",
        "    #Metric = ROC-AUC\n",
        "\n",
        "    Task:\n",
        "      The task is to predict the presence of protein functions in a multi-label binary classification setup,\n",
        "      where there are 112 kinds of labels to predict in total. \n",
        "      The performance is measured by the average of ROC-AUC scores across the 112 tasks.\n",
        "\n",
        "    #Others:\n",
        "      **undirected**\n",
        "      **weighted**\n",
        "      **typed (according to species)**\n",
        "\n",
        "  (1) Nodes represent proteins\n",
        "    (1.1) The proteins come from 8 species\n",
        "      len(set(graph.ndata['species'].reshape(-1).tolist())) == 8\n",
        "    (1.2) Each node has one feature associated with it (its species)\n",
        "      graph.ndata['species'].shape == (#nodes, 1)\n",
        "  \n",
        "  (2) Edges indicate different types of biologically meaningful associations between proteins\n",
        "    (2.1) All edges come with 8-dimensional features\n",
        "      graph.edata['feat'].shape == (2 * #edges, 8)\n",
        "\n",
        "'''\n",
        "# Get split labels\n",
        "train_label = dataset.labels[split_idx['train']]  # (86619, 112) -- binary values (presence of protein functions)\n",
        "valid_label = dataset.labels[split_idx['valid']]  # (21236, 112) -- binary values (presence of protein functions)\n",
        "test_label = dataset.labels[split_idx['test']]    # (24679, 112) -- binary values (presence of protein functions)\n",
        "\n",
        "# Create masks\n",
        "train_mask = jnp.zeros((ogbn_proteins_main_graph.num_nodes(), 1)).at[jnp.array(split_idx['train'])].set(1)\n",
        "valid_mask = jnp.zeros((ogbn_proteins_main_graph.num_nodes(), 1)).at[jnp.array(split_idx['valid'])].set(1)\n",
        "test_mask = jnp.zeros((ogbn_proteins_main_graph.num_nodes(), 1)).at[jnp.array(split_idx['test'])].set(1)"
      ],
      "metadata": {
        "id": "jCkzIEb4WsXU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jraph\n",
        "\n",
        "# From https://colab.research.google.com/github/deepmind/educational/blob/master/colabs/summer_schools/intro_to_graph_nets_tutorial_with_jraph.ipynb#scrollTo=7vEmAsr5bKN8\n",
        "def _nearest_bigger_power_of_two(x: int) -> int:\n",
        "  \"\"\"Computes the nearest power of two greater than x for padding.\"\"\"\n",
        "  y = 2\n",
        "  while y < x:\n",
        "    y *= 2\n",
        "  return y\n",
        "\n",
        "def pad_graph_to_nearest_power_of_two(\n",
        "    graphs_tuple: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
        "  \"\"\"Pads a batched `GraphsTuple` to the nearest power of two.\n",
        "  For example, if a `GraphsTuple` has 7 nodes, 5 edges and 3 graphs, this method\n",
        "  would pad the `GraphsTuple` nodes and edges:\n",
        "    7 nodes --> 8 nodes (2^3)\n",
        "    5 edges --> 8 edges (2^3)\n",
        "  And since padding is accomplished using `jraph.pad_with_graphs`, an extra\n",
        "  graph and node is added:\n",
        "    8 nodes --> 9 nodes\n",
        "    3 graphs --> 4 graphs\n",
        "  Args:\n",
        "    graphs_tuple: a batched `GraphsTuple` (can be batch size 1).\n",
        "  Returns:\n",
        "    A graphs_tuple batched to the nearest power of two.\n",
        "  \"\"\"\n",
        "  # Add 1 since we need at least one padding node for pad_with_graphs.\n",
        "  pad_nodes_to = _nearest_bigger_power_of_two(jnp.sum(graphs_tuple.n_node)) + 1\n",
        "  pad_edges_to = _nearest_bigger_power_of_two(jnp.sum(graphs_tuple.n_edge))\n",
        "  # Add 1 since we need at least one padding graph for pad_with_graphs.\n",
        "  # We do not pad to nearest power of two because the batch size is fixed.\n",
        "  pad_graphs_to = graphs_tuple.n_node.shape[0] + 1\n",
        "  return jraph.pad_with_graphs(graphs_tuple, pad_nodes_to, pad_edges_to,\n",
        "                               pad_graphs_to)"
      ],
      "metadata": {
        "id": "FSbePOUh2NBB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "from random import randint\n",
        "import numpy as np\n",
        "import sharded_graphnet\n",
        "\n",
        "def get_demo_training_graph():\n",
        "  num_nodes = 128\n",
        "  num_edges = 256\n",
        "\n",
        "  rand_dgl_graph = dgl.rand_graph(num_nodes = num_nodes, num_edges = num_edges)\n",
        "\n",
        "  node_features = jnp.array([[randint(0, 7)] for i in range(num_nodes)])\n",
        "  edge_features = jnp.array([[0.1 * randint(0, 10) for _ in range(8)] for i in range(num_edges)])\n",
        "\n",
        "  senders = jnp.array(rand_dgl_graph.edges()[0])\n",
        "  receivers = jnp.array(rand_dgl_graph.edges()[1])\n",
        "\n",
        "  in_tuple = jraph.GraphsTuple(\n",
        "            nodes = node_features.astype(np.float32),\n",
        "            edges = edge_features.astype(np.float32),  \n",
        "            senders = senders.astype(np.int32), \n",
        "            receivers = receivers.astype(np.int32),\n",
        "            n_node = jnp.array([num_nodes]), \n",
        "            n_edge = jnp.array([num_edges]),\n",
        "            globals = None  # No global features\n",
        "          )\n",
        "  \n",
        "  labels = jnp.array([[0 if j % 10 == 0 else 1 for j in range(112)] for i in range(num_nodes)])\n",
        "  train_mask = jnp.ones((num_nodes, 1))\n",
        "  \n",
        "  in_tuple = in_tuple._replace(\n",
        "      nodes = {\n",
        "          'inputs': in_tuple.nodes, \n",
        "          'targets': labels, \n",
        "          'train_mask': train_mask, \n",
        "          }\n",
        "  )\n",
        "\n",
        "  # in_tuple = pad_graph_to_nearest_power_of_two(in_tuple)\n",
        "  \n",
        "  return sharded_graphnet.graphs_tuple_to_broadcasted_sharded_graphs_tuple(\n",
        "      in_tuple,\n",
        "      num_shards = num_devices\n",
        "      )\n",
        "  \n",
        "demo_graph = get_demo_training_graph()\n",
        "demo_labels = demo_graph.nodes['targets']\n",
        "demo_mask = demo_graph.nodes['train_mask']\n",
        "demo_graph = demo_graph._replace(nodes = demo_graph.nodes['inputs']) "
      ],
      "metadata": {
        "id": "SKoX4h1z9ItW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import jraph\n",
        "import sharded_graphnet\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "enc = OneHotEncoder()\n",
        "enc.fit(ogbn_proteins_main_graph.ndata['species'])\n",
        "\n",
        "def dgl_graph_to_jraph(node_ids, labels, train_mask, valid_mask, test_mask):\n",
        "  # First add back the node and edge features\n",
        "  dgl_graph_with_features = dgl.node_subgraph(ogbn_proteins_main_graph, node_ids)\n",
        "\n",
        "  node_features = jnp.array(enc.transform(dgl_graph_with_features.ndata['species']).toarray())\n",
        "  senders = jnp.array(dgl_graph_with_features.edges()[0])\n",
        "  receivers = jnp.array(dgl_graph_with_features.edges()[1])\n",
        "\n",
        "  # Edges -- here we should include the 8-dimensional edge features\n",
        "  edges = jnp.array(dgl_graph_with_features.edata['feat'])\n",
        "\n",
        "  n_node = jnp.array([dgl_graph_with_features.num_nodes()])\n",
        "  n_edge = jnp.array([dgl_graph_with_features.num_edges()])\n",
        "\n",
        "  in_tuple = jraph.GraphsTuple(\n",
        "            nodes = node_features.astype(np.float32),\n",
        "            senders = senders.astype(np.int32), \n",
        "            receivers = receivers.astype(np.int32),\n",
        "            edges = edges.astype(np.float32),  \n",
        "            n_node = n_node, \n",
        "            n_edge = n_edge,\n",
        "            globals = None  # No global features\n",
        "          )\n",
        "  \n",
        "  in_tuple = in_tuple._replace(\n",
        "      nodes = {\n",
        "          'inputs': in_tuple.nodes, \n",
        "          'targets': labels, \n",
        "          'train_mask': train_mask, \n",
        "          'valid_mask': valid_mask, \n",
        "          'test_mask': test_mask,\n",
        "          'padding_mask': jnp.ones((in_tuple.nodes.shape[0], 1)) \n",
        "                                                        # TODO: Check this above\n",
        "                                                        # Adding this mask so that we can remove the nodes added after padding \n",
        "                                                        # for the final ROC computations on the full train / valid / test splits\n",
        "                                                        # This is because I want to pass the predictions on the true nodes to the \n",
        "                                                        # ogbn-evaluator, so I would first need to remove the predictions that come from padding.\n",
        "          }\n",
        "  )\n",
        "\n",
        "  in_tuple = pad_graph_to_nearest_power_of_two(in_tuple)\n",
        "  \n",
        "  return sharded_graphnet.graphs_tuple_to_broadcasted_sharded_graphs_tuple(\n",
        "      in_tuple,\n",
        "      num_shards = num_devices\n",
        "      )\n",
        "  \n",
        "def get_labels_for_subgraph(node_ids):\n",
        "  return jnp.array(ogbn_proteins_main_labels.index_select(0, node_ids))"
      ],
      "metadata": {
        "id": "fvH_XRJVWuLw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "\n",
        "'''\n",
        "  Generate graph partition using metis, with balanced number of edges in each partition.\n",
        "  Note: \n",
        "    The subgraphs do not contain the node/edge data in the input graph (https://docs.dgl.ai/generated/dgl.metis_partition.html)\n",
        "'''\n",
        "num_partitions = 50  ## TODO: Find some way to decrease this to something reasonable (< 50)\n",
        "\n",
        "dgl_graph_metis_partition = dgl.metis_partition(ogbn_proteins_main_graph, num_partitions, balance_edges = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUI9s4-0mPz9",
        "outputId": "6663d49e-e9ca-494a-e7d7-dbc0f77ffbbd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convert a graph into a bidirected graph: 2.300 seconds\n",
            "Construct multi-constraint weights: 0.014 seconds\n",
            "Metis partitioning: 28.399 seconds\n",
            "Split the graph: 0.688 seconds\n",
            "Construct subgraphs: 0.052 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert graphs to Jraph GraphsTuple\n",
        "processed_graphs = {}\n",
        "\n",
        "for idx in range(num_partitions):\n",
        "  node_ids = dgl_graph_metis_partition[idx].ndata['_ID']\n",
        "\n",
        "  labels = get_labels_for_subgraph(node_ids)\n",
        "  graph = dgl_graph_to_jraph(node_ids, \n",
        "                             labels, \n",
        "                             train_mask = train_mask.at[jnp.array(node_ids)].get(),\n",
        "                             valid_mask = valid_mask.at[jnp.array(node_ids)].get(),\n",
        "                             test_mask = test_mask.at[jnp.array(node_ids)].get()\n",
        "                             )\n",
        "\n",
        "  processed_graphs[f'partition_{idx}'] = {\n",
        "      'graph': graph._replace(nodes = graph.nodes['inputs']), \n",
        "      'labels': graph.nodes['targets'],\n",
        "      'train_mask': graph.nodes['train_mask'],\n",
        "      'valid_mask': graph.nodes['valid_mask'],\n",
        "      'test_mask': graph.nodes['test_mask'],\n",
        "      'padding_mask': graph.nodes['padding_mask']\n",
        "      }"
      ],
      "metadata": {
        "id": "s8-Ln58I_Fwp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import haiku as hk\n",
        "import jax\n",
        "import optax\n",
        "\n",
        "from typing import Sequence\n",
        "\n",
        "# See https://github.com/YuxuanXie/mcl/blob/5f7ee92e2a6bc89736263873a4ba9c14d1a676ff/glassy_dynamics/train_using_jax.py for alternative to using GraphMapFeatures\n",
        "# From https://github.com/YuxuanXie/mcl/blob/5f7ee92e2a6bc89736263873a4ba9c14d1a676ff/glassy_dynamics/train_using_jax.py\n",
        "\n",
        "hidden_dimension = 128\n",
        "num_message_passing_steps = 5 # Question: (256, 4) fails / (128, 6) works\n",
        "\n",
        "@jraph.concatenated_args\n",
        "def node_update_fn(feats: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Node update function for graph net.\"\"\"\n",
        "  net = hk.Sequential([hk.nets.MLP(output_sizes = [hidden_dimension, hidden_dimension], activation = jax.nn.relu, activate_final = True), hk.LayerNorm(axis = -1, create_scale = True, create_offset = True)])\n",
        "  return net(feats)\n",
        "\n",
        "@jraph.concatenated_args\n",
        "def edge_update_fn(feats: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Edge update function for graph net.\"\"\"\n",
        "  net = hk.Sequential([hk.nets.MLP(output_sizes = [hidden_dimension, hidden_dimension], activation = jax.nn.relu, activate_final = True), hk.LayerNorm(axis = -1, create_scale = True, create_offset = True)])\n",
        "  return net(feats)\n",
        "\n",
        "@hk.without_apply_rng\n",
        "@hk.transform\n",
        "def network_definition(graph):\n",
        "  \"\"\"Defines a graph neural network.\n",
        "  Args:\n",
        "    graph: Graphstuple the network processes.\n",
        "  Returns:\n",
        "    Decoded nodes.\n",
        "  \"\"\"\n",
        "  graph = graph._replace(\n",
        "      nodes = hk.Linear(hidden_dimension)(graph.nodes),\n",
        "      device_edges = hk.Linear(hidden_dimension)(graph.device_edges)\n",
        "  )\n",
        "  \n",
        "  sharded_gn = sharded_graphnet.ShardedEdgesGraphNetwork(\n",
        "      update_node_fn = node_update_fn,\n",
        "      update_edge_fn = edge_update_fn,\n",
        "      num_shards = num_devices\n",
        "      )\n",
        "\n",
        "  for _ in range(num_message_passing_steps):\n",
        "    residual_graph = sharded_gn(graph)\n",
        "    graph = graph._replace(\n",
        "        nodes = graph.nodes + residual_graph.nodes,\n",
        "        device_edges = graph.device_edges + residual_graph.device_edges\n",
        "    )\n",
        "\n",
        "    # graph = sharded_gn(graph)\n",
        "\n",
        "  graph = graph._replace(\n",
        "      nodes = hk.Sequential([hk.Linear(hidden_dimension), jax.nn.relu, hk.Linear(112)])(graph.nodes)\n",
        "  )\n",
        "  return graph.nodes"
      ],
      "metadata": {
        "id": "gPg7ph7sWyOn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bcast_local_devices(value):\n",
        "    \"\"\"Broadcasts an object to all local devices.\"\"\"\n",
        "    devices = jax.local_devices()\n",
        "\n",
        "    def _replicate(x):\n",
        "      \"\"\"Replicate an object on each device.\"\"\"\n",
        "      x = jnp.array(x)\n",
        "      return jax.device_put_sharded(len(devices) * [x], devices)\n",
        "\n",
        "    return jax.tree_util.tree_map(_replicate, value)"
      ],
      "metadata": {
        "id": "z6Qh75qxQfii"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reshape_broadcasted_data(data):\n",
        "  '''\n",
        "    Node predictions / Labels / Masks are identical on all the devices so we only take\n",
        "    one of them in order to remove the leading axis.\n",
        "  '''\n",
        "  return np.array(data)[0]\n",
        "  \n",
        "def remove_mask_from_data(data, mask):\n",
        "  '''\n",
        "    data.shape = [num_nodes, 112]\n",
        "    mask.shape = [num_nodes, 1]\n",
        "\n",
        "    We want to only return the data where mask == True\n",
        "  '''\n",
        "  sliced_data = np.compress(np.array(mask).reshape(-1).astype(bool), data, axis = 0)\n",
        "  return np.array(sliced_data)"
      ],
      "metadata": {
        "id": "oJ5T_oplbg_t"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import haiku as hk\n",
        "\n",
        "from random import randint\n",
        "\n",
        "# Try to follow this tutorial https://github.com/YuxuanXie/mcl/blob/5f7ee92e2a6bc89736263873a4ba9c14d1a676ff/glassy_dynamics/train_using_jax.py\n",
        "def compute_loss(params, graph, label, mask):\n",
        "  predictions = network_definition.apply(params, graph)\n",
        "\n",
        "  # use optax here (https://github.com/deepmind/optax/blob/master/optax/_src/loss.py#L116#L139)\n",
        "  loss = optax.sigmoid_binary_cross_entropy(predictions, label)  # shape [num_nodes, num_classes]\n",
        "  loss = loss * mask\n",
        "  loss = jnp.sum(loss) / jnp.sum(mask) # loss = mean_with_mask(loss, mask)\n",
        "\n",
        "  return loss\n",
        "\n",
        "def train(num_training_steps):\n",
        "  replicated_params = jax.pmap(network_definition.init, axis_name = 'i')(bcast_local_devices(jax.random.PRNGKey(42)), processed_graphs['partition_0']['graph'])\n",
        "\n",
        "  opt_init, opt_update = optax.adam(learning_rate = 0.01)  ## TODO: Maybe modify the learning rate (0.01 used in some paper for ogbn-proteins) https://arxiv.org/pdf/1609.02907.pdf  \n",
        "  replicated_opt_state = jax.pmap(opt_init, axis_name = 'i')(replicated_params)\n",
        "\n",
        "  @functools.partial(jax.pmap, axis_name='i')\n",
        "  def update(params, opt_state, graph, targets, mask):\n",
        "    # Compute the gradients on the given minibatch (individually on each device).\n",
        "    loss, grads = jax.value_and_grad(compute_loss)(params, graph, targets, mask)\n",
        "\n",
        "    # Combine the gradient across all devices (by taking their mean).\n",
        "    grads = jax.lax.pmean(grads, axis_name='i')\n",
        "\n",
        "    # Also combine the loss. Unnecessary for the update, but useful for logging.\n",
        "    loss = jax.lax.pmean(loss, axis_name='i')\n",
        "\n",
        "    updates, opt_state = opt_update(grads, opt_state)\n",
        "\n",
        "    return optax.apply_updates(params, updates), opt_state, loss\n",
        "\n",
        "  # Train\n",
        "  for idx in range(num_training_steps):\n",
        "    random_partition_idx = randint(0, num_partitions - 1)\n",
        "    random_partition = processed_graphs[f'partition_{random_partition_idx}']\n",
        "\n",
        "    graph = random_partition['graph']\n",
        "    labels = random_partition['labels']   # Automatically broadcasted by the sharded graph net\n",
        "    mask = random_partition['train_mask'] # Automatically broadcasted by the sharded graph net\n",
        "\n",
        "    replicated_params, replicated_opt_state, loss = update(\n",
        "        replicated_params, \n",
        "        replicated_opt_state, \n",
        "        graph, \n",
        "        labels,\n",
        "        mask\n",
        "        )\n",
        "    \n",
        "    print('Loss training:', reshape_broadcasted_data(loss))\n",
        "\n",
        "    if (idx + 1) % 10 == 0:\n",
        "      print()\n",
        "      print(f'***************************')\n",
        "      print(f'Trained on {idx + 1} graphs')\n",
        "      print(f'***************************')\n",
        "      print()\n",
        "\n",
        "    # Write params and opt_state\n",
        "\n",
        "  return replicated_params\n",
        "\n",
        "def evaluate(params, num_graphs_eval):\n",
        "  # Evaluate\n",
        "  accumulated_loss = 0.0\n",
        "  accumulated_roc = 0\n",
        "  graphs_evaluated = 0\n",
        "\n",
        "  for idx in range(num_graphs_eval):\n",
        "    random_partition_idx = idx\n",
        "    random_partition = processed_graphs[f'partition_{random_partition_idx}']\n",
        "\n",
        "    graph = random_partition['graph']\n",
        "    labels = random_partition['labels']     # Automatically broadcasted by the sharded graph net\n",
        "    mask = random_partition['test_mask']   # Automatically broadcasted by the sharded graph net \n",
        "\n",
        "    predictions, loss = predict_on_graph(params, graph, labels, mask)\n",
        "    loss = reshape_broadcasted_data(loss)\n",
        "    \n",
        "    collected_labels = reshape_broadcasted_data(labels)\n",
        "    collected_predictions = reshape_broadcasted_data(predictions)\n",
        "    collected_mask = reshape_broadcasted_data(mask)\n",
        "\n",
        "    try:\n",
        "      roc = evaluator.eval({\n",
        "          \"y_true\": remove_mask_from_data(collected_labels, collected_mask), \n",
        "          \"y_pred\": remove_mask_from_data(collected_predictions, collected_mask)\n",
        "          })['rocauc']\n",
        "\n",
        "      accumulated_loss += loss\n",
        "      accumulated_roc += roc\n",
        "      graphs_evaluated += 1\n",
        "\n",
        "      print(f'Test loss: {loss} | ROC: {roc}')\n",
        "    except Exception as err:\n",
        "      print(f'Could not compute ROC for partition {idx}')\n",
        "      print('Most likely all of the nodes are hidden at test / validation')\n",
        "      print('Check counts in mask')\n",
        "      print(np.unique(collected_mask, return_counts = True))\n",
        "      print('Check counts in labels after removing the test / validation mask')\n",
        "      print(np.unique(remove_mask_from_data(collected_labels, collected_mask), return_counts = True))\n",
        "      print(f'Error message: {str(err)}')\n",
        "      print()\n",
        "\n",
        "    if (idx + 1) % 10 == 0:\n",
        "      print()\n",
        "      print(f'***************************')\n",
        "      print(f'Evaluated on {idx + 1} graphs')\n",
        "      print(f'***************************')\n",
        "      print()\n",
        "\n",
        "  print(f'Average test loss: {accumulated_loss / graphs_evaluated} | Average ROC: {accumulated_roc / graphs_evaluated}')\n",
        "\n",
        "@functools.partial(jax.pmap, axis_name='i')\n",
        "def predict_on_graph(params, graph, label, mask):\n",
        "  decoded_nodes = network_definition.apply(params, graph)\n",
        "\n",
        "  compute_loss_fn = functools.partial(compute_loss)\n",
        "  loss = compute_loss_fn(params, graph, label, mask)\n",
        "\n",
        "  return jax.nn.sigmoid(decoded_nodes), loss\n",
        "\n",
        "final_params = train(num_training_steps = num_partitions * 100)\n",
        "evaluate(final_params, num_graphs_eval = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYVzddNITMSv",
        "outputId": "355c5164-bc57-40ee-9a50-3e2f941aa3b8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Loss training: 36.105026\n",
            "Loss training: 45.509426\n",
            "Loss training: 47.236496\n",
            "Loss training: 54.69067\n",
            "Loss training: 15.859834\n",
            "Loss training: 53.79707\n",
            "Loss training: 35.013588\n",
            "Loss training: 52.59075\n",
            "Loss training: 61.495735\n",
            "\n",
            "***************************\n",
            "Trained on 1680 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.977425\n",
            "Loss training: 48.256935\n",
            "Loss training: 52.792313\n",
            "Loss training: 39.036076\n",
            "Loss training: 49.041153\n",
            "Loss training: 40.468155\n",
            "Loss training: 58.44184\n",
            "Loss training: 48.392525\n",
            "Loss training: 56.93954\n",
            "Loss training: 53.41401\n",
            "\n",
            "***************************\n",
            "Trained on 1690 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 51.420235\n",
            "Loss training: 33.376278\n",
            "Loss training: 36.16233\n",
            "Loss training: 48.944775\n",
            "Loss training: 25.666996\n",
            "Loss training: 53.72586\n",
            "Loss training: 34.390244\n",
            "Loss training: 27.445341\n",
            "Loss training: 50.126774\n",
            "Loss training: 35.7336\n",
            "\n",
            "***************************\n",
            "Trained on 1700 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.579823\n",
            "Loss training: 47.177692\n",
            "Loss training: 46.395737\n",
            "Loss training: 53.594833\n",
            "Loss training: 45.479443\n",
            "Loss training: 37.65717\n",
            "Loss training: 46.881317\n",
            "Loss training: 28.751032\n",
            "Loss training: 45.777836\n",
            "Loss training: 34.37052\n",
            "\n",
            "***************************\n",
            "Trained on 1710 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 56.386288\n",
            "Loss training: 28.689583\n",
            "Loss training: 36.792076\n",
            "Loss training: 43.101635\n",
            "Loss training: 40.98996\n",
            "Loss training: 32.03662\n",
            "Loss training: 32.186836\n",
            "Loss training: 43.971333\n",
            "Loss training: 43.89314\n",
            "Loss training: 56.115433\n",
            "\n",
            "***************************\n",
            "Trained on 1720 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.894617\n",
            "Loss training: 38.087578\n",
            "Loss training: 31.942917\n",
            "Loss training: 48.268253\n",
            "Loss training: 35.170097\n",
            "Loss training: 37.99279\n",
            "Loss training: 46.408016\n",
            "Loss training: 47.64841\n",
            "Loss training: 30.028236\n",
            "Loss training: 27.627665\n",
            "\n",
            "***************************\n",
            "Trained on 1730 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.176327\n",
            "Loss training: 36.11801\n",
            "Loss training: 32.00761\n",
            "Loss training: 51.480762\n",
            "Loss training: 27.38725\n",
            "Loss training: 14.534008\n",
            "Loss training: 39.181263\n",
            "Loss training: 45.135452\n",
            "Loss training: 39.08202\n",
            "Loss training: 38.180065\n",
            "\n",
            "***************************\n",
            "Trained on 1740 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.14453\n",
            "Loss training: 31.189785\n",
            "Loss training: 33.49492\n",
            "Loss training: 43.843853\n",
            "Loss training: 27.279778\n",
            "Loss training: 34.293896\n",
            "Loss training: 43.02878\n",
            "Loss training: 37.70936\n",
            "Loss training: 47.185837\n",
            "Loss training: 14.149749\n",
            "\n",
            "***************************\n",
            "Trained on 1750 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.098057\n",
            "Loss training: 34.767\n",
            "Loss training: 36.99687\n",
            "Loss training: 30.528318\n",
            "Loss training: 41.59789\n",
            "Loss training: 31.991116\n",
            "Loss training: 15.428477\n",
            "Loss training: 68.36799\n",
            "Loss training: 38.510902\n",
            "Loss training: 35.145546\n",
            "\n",
            "***************************\n",
            "Trained on 1760 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.20905\n",
            "Loss training: 44.22086\n",
            "Loss training: 33.401577\n",
            "Loss training: 47.431126\n",
            "Loss training: 51.405518\n",
            "Loss training: 36.33202\n",
            "Loss training: 30.40902\n",
            "Loss training: 46.88662\n",
            "Loss training: 34.42139\n",
            "Loss training: 33.71903\n",
            "\n",
            "***************************\n",
            "Trained on 1770 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.76287\n",
            "Loss training: 28.5704\n",
            "Loss training: 35.261\n",
            "Loss training: 35.598793\n",
            "Loss training: 31.598213\n",
            "Loss training: 51.045334\n",
            "Loss training: 16.80046\n",
            "Loss training: 47.05312\n",
            "Loss training: 45.112392\n",
            "Loss training: 57.23739\n",
            "\n",
            "***************************\n",
            "Trained on 1780 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 46.558556\n",
            "Loss training: 39.072594\n",
            "Loss training: 47.453762\n",
            "Loss training: 44.256035\n",
            "Loss training: 32.296383\n",
            "Loss training: 35.31052\n",
            "Loss training: 37.155087\n",
            "Loss training: 37.166656\n",
            "Loss training: 36.916454\n",
            "Loss training: 52.428123\n",
            "\n",
            "***************************\n",
            "Trained on 1790 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.54351\n",
            "Loss training: 35.928207\n",
            "Loss training: 52.427692\n",
            "Loss training: 43.550232\n",
            "Loss training: 30.682798\n",
            "Loss training: 51.002\n",
            "Loss training: 35.199028\n",
            "Loss training: 36.664486\n",
            "Loss training: 32.389465\n",
            "Loss training: 30.558905\n",
            "\n",
            "***************************\n",
            "Trained on 1800 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.7559\n",
            "Loss training: 59.639076\n",
            "Loss training: 31.250774\n",
            "Loss training: 36.084286\n",
            "Loss training: 66.107925\n",
            "Loss training: 32.807903\n",
            "Loss training: 53.22041\n",
            "Loss training: 41.91334\n",
            "Loss training: 44.97228\n",
            "Loss training: 35.47621\n",
            "\n",
            "***************************\n",
            "Trained on 1810 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.616436\n",
            "Loss training: 39.198936\n",
            "Loss training: 32.61715\n",
            "Loss training: 49.903423\n",
            "Loss training: 35.58531\n",
            "Loss training: 39.210255\n",
            "Loss training: 43.346176\n",
            "Loss training: 34.624733\n",
            "Loss training: 16.75067\n",
            "Loss training: 33.608295\n",
            "\n",
            "***************************\n",
            "Trained on 1820 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 52.047443\n",
            "Loss training: 33.17627\n",
            "Loss training: 45.5842\n",
            "Loss training: 37.857525\n",
            "Loss training: 38.1815\n",
            "Loss training: 31.542418\n",
            "Loss training: 43.705997\n",
            "Loss training: 31.417282\n",
            "Loss training: 43.12903\n",
            "Loss training: 35.462193\n",
            "\n",
            "***************************\n",
            "Trained on 1830 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.402086\n",
            "Loss training: 33.38312\n",
            "Loss training: 26.84478\n",
            "Loss training: 26.773802\n",
            "Loss training: 36.510544\n",
            "Loss training: 47.665066\n",
            "Loss training: 26.506453\n",
            "Loss training: 33.47331\n",
            "Loss training: 15.825344\n",
            "Loss training: 31.731112\n",
            "\n",
            "***************************\n",
            "Trained on 1840 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 52.526978\n",
            "Loss training: 31.031326\n",
            "Loss training: 30.94164\n",
            "Loss training: 46.47997\n",
            "Loss training: 44.34941\n",
            "Loss training: 48.25823\n",
            "Loss training: 35.52907\n",
            "Loss training: 35.57813\n",
            "Loss training: 28.254925\n",
            "Loss training: 49.185486\n",
            "\n",
            "***************************\n",
            "Trained on 1850 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 52.37915\n",
            "Loss training: 15.043976\n",
            "Loss training: 43.11917\n",
            "Loss training: 29.907936\n",
            "Loss training: 45.414852\n",
            "Loss training: 32.73999\n",
            "Loss training: 32.99248\n",
            "Loss training: 47.45524\n",
            "Loss training: 34.560547\n",
            "Loss training: 52.38061\n",
            "\n",
            "***************************\n",
            "Trained on 1860 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.125994\n",
            "Loss training: 27.057188\n",
            "Loss training: 30.906382\n",
            "Loss training: 31.410604\n",
            "Loss training: 25.400196\n",
            "Loss training: 12.763906\n",
            "Loss training: 34.13647\n",
            "Loss training: 41.12001\n",
            "Loss training: 39.421425\n",
            "Loss training: 26.292885\n",
            "\n",
            "***************************\n",
            "Trained on 1870 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 46.710243\n",
            "Loss training: 42.717957\n",
            "Loss training: 45.664223\n",
            "Loss training: 34.808994\n",
            "Loss training: 26.036308\n",
            "Loss training: 30.761683\n",
            "Loss training: 56.411324\n",
            "Loss training: 53.181072\n",
            "Loss training: 38.841064\n",
            "Loss training: 30.733309\n",
            "\n",
            "***************************\n",
            "Trained on 1880 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.64942\n",
            "Loss training: 34.403522\n",
            "Loss training: 20.21644\n",
            "Loss training: 41.703777\n",
            "Loss training: 49.237404\n",
            "Loss training: 46.120644\n",
            "Loss training: 16.087736\n",
            "Loss training: 34.99893\n",
            "Loss training: 41.087826\n",
            "Loss training: 40.817123\n",
            "\n",
            "***************************\n",
            "Trained on 1890 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 56.99187\n",
            "Loss training: 37.122253\n",
            "Loss training: 35.244053\n",
            "Loss training: 57.104366\n",
            "Loss training: 29.87268\n",
            "Loss training: 53.846355\n",
            "Loss training: 41.45124\n",
            "Loss training: 33.145756\n",
            "Loss training: 26.94854\n",
            "Loss training: 42.264816\n",
            "\n",
            "***************************\n",
            "Trained on 1900 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.18132\n",
            "Loss training: 53.067554\n",
            "Loss training: 46.79568\n",
            "Loss training: 62.660557\n",
            "Loss training: 30.795988\n",
            "Loss training: 42.273552\n",
            "Loss training: 42.96339\n",
            "Loss training: 47.171867\n",
            "Loss training: 28.729927\n",
            "Loss training: 41.96509\n",
            "\n",
            "***************************\n",
            "Trained on 1910 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.187244\n",
            "Loss training: 46.923332\n",
            "Loss training: 34.39193\n",
            "Loss training: 54.371212\n",
            "Loss training: 51.001953\n",
            "Loss training: 42.677963\n",
            "Loss training: 34.871384\n",
            "Loss training: 45.55374\n",
            "Loss training: 61.295494\n",
            "Loss training: 51.973362\n",
            "\n",
            "***************************\n",
            "Trained on 1920 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 47.632214\n",
            "Loss training: 36.40067\n",
            "Loss training: 33.633984\n",
            "Loss training: 42.84154\n",
            "Loss training: 45.208145\n",
            "Loss training: 40.19352\n",
            "Loss training: 41.857697\n",
            "Loss training: 44.57429\n",
            "Loss training: 27.921501\n",
            "Loss training: 45.78955\n",
            "\n",
            "***************************\n",
            "Trained on 1930 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.094355\n",
            "Loss training: 43.32724\n",
            "Loss training: 39.561058\n",
            "Loss training: 34.76686\n",
            "Loss training: 44.73809\n",
            "Loss training: 39.923935\n",
            "Loss training: 49.309837\n",
            "Loss training: 42.77226\n",
            "Loss training: 46.90703\n",
            "Loss training: 52.436684\n",
            "\n",
            "***************************\n",
            "Trained on 1940 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.49165\n",
            "Loss training: 33.49819\n",
            "Loss training: 53.91513\n",
            "Loss training: 37.45803\n",
            "Loss training: 39.857464\n",
            "Loss training: 52.812073\n",
            "Loss training: 43.234562\n",
            "Loss training: 37.341537\n",
            "Loss training: 45.070778\n",
            "Loss training: 34.30958\n",
            "\n",
            "***************************\n",
            "Trained on 1950 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 68.76034\n",
            "Loss training: 36.412315\n",
            "Loss training: 38.304676\n",
            "Loss training: 33.1403\n",
            "Loss training: 51.30553\n",
            "Loss training: 31.115717\n",
            "Loss training: 45.013866\n",
            "Loss training: 30.616053\n",
            "Loss training: 42.034374\n",
            "Loss training: 30.354038\n",
            "\n",
            "***************************\n",
            "Trained on 1960 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 53.830948\n",
            "Loss training: 53.97883\n",
            "Loss training: 46.97354\n",
            "Loss training: 29.407099\n",
            "Loss training: 38.786926\n",
            "Loss training: 36.32781\n",
            "Loss training: 17.534262\n",
            "Loss training: 43.2759\n",
            "Loss training: 39.678436\n",
            "Loss training: 34.79086\n",
            "\n",
            "***************************\n",
            "Trained on 1970 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.05856\n",
            "Loss training: 36.29083\n",
            "Loss training: 50.053368\n",
            "Loss training: 46.249195\n",
            "Loss training: 28.065304\n",
            "Loss training: 63.367386\n",
            "Loss training: 32.70928\n",
            "Loss training: 31.346792\n",
            "Loss training: 45.496502\n",
            "Loss training: 29.180178\n",
            "\n",
            "***************************\n",
            "Trained on 1980 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.061314\n",
            "Loss training: 47.78444\n",
            "Loss training: 45.5063\n",
            "Loss training: 52.075428\n",
            "Loss training: 47.355503\n",
            "Loss training: 43.895996\n",
            "Loss training: 69.14486\n",
            "Loss training: 27.692852\n",
            "Loss training: 32.39562\n",
            "Loss training: 60.406384\n",
            "\n",
            "***************************\n",
            "Trained on 1990 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 48.6002\n",
            "Loss training: 31.869183\n",
            "Loss training: 38.531334\n",
            "Loss training: 46.932396\n",
            "Loss training: 46.876186\n",
            "Loss training: 54.20546\n",
            "Loss training: 31.932661\n",
            "Loss training: 17.241806\n",
            "Loss training: 32.30482\n",
            "Loss training: 32.256275\n",
            "\n",
            "***************************\n",
            "Trained on 2000 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.317827\n",
            "Loss training: 71.17382\n",
            "Loss training: 31.922682\n",
            "Loss training: 28.411787\n",
            "Loss training: 28.626043\n",
            "Loss training: 46.06757\n",
            "Loss training: 54.003265\n",
            "Loss training: 35.95649\n",
            "Loss training: 64.12827\n",
            "Loss training: 36.245296\n",
            "\n",
            "***************************\n",
            "Trained on 2010 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.674103\n",
            "Loss training: 34.082825\n",
            "Loss training: 57.04388\n",
            "Loss training: 34.91189\n",
            "Loss training: 24.54885\n",
            "Loss training: 42.04638\n",
            "Loss training: 38.31083\n",
            "Loss training: 39.53532\n",
            "Loss training: 15.126413\n",
            "Loss training: 47.301353\n",
            "\n",
            "***************************\n",
            "Trained on 2020 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 14.605621\n",
            "Loss training: 38.090187\n",
            "Loss training: 28.908108\n",
            "Loss training: 63.671387\n",
            "Loss training: 44.627747\n",
            "Loss training: 35.360603\n",
            "Loss training: 58.522434\n",
            "Loss training: 43.11978\n",
            "Loss training: 44.90407\n",
            "Loss training: 31.99403\n",
            "\n",
            "***************************\n",
            "Trained on 2030 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 55.00854\n",
            "Loss training: 55.085186\n",
            "Loss training: 34.355858\n",
            "Loss training: 38.234245\n",
            "Loss training: 32.11447\n",
            "Loss training: 32.095917\n",
            "Loss training: 52.22368\n",
            "Loss training: 42.223866\n",
            "Loss training: 41.70204\n",
            "Loss training: 29.033976\n",
            "\n",
            "***************************\n",
            "Trained on 2040 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.82808\n",
            "Loss training: 52.318516\n",
            "Loss training: 41.477753\n",
            "Loss training: 33.95413\n",
            "Loss training: 33.43057\n",
            "Loss training: 32.859028\n",
            "Loss training: 42.131817\n",
            "Loss training: 51.678684\n",
            "Loss training: 38.142536\n",
            "Loss training: 30.403372\n",
            "\n",
            "***************************\n",
            "Trained on 2050 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.186195\n",
            "Loss training: 27.982021\n",
            "Loss training: 48.178135\n",
            "Loss training: 41.29464\n",
            "Loss training: 30.48662\n",
            "Loss training: 52.051697\n",
            "Loss training: 44.78152\n",
            "Loss training: 33.3725\n",
            "Loss training: 39.049732\n",
            "Loss training: 44.165337\n",
            "\n",
            "***************************\n",
            "Trained on 2060 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.24932\n",
            "Loss training: 70.39868\n",
            "Loss training: 38.966064\n",
            "Loss training: 35.78813\n",
            "Loss training: 28.29688\n",
            "Loss training: 37.285046\n",
            "Loss training: 38.265167\n",
            "Loss training: 36.03847\n",
            "Loss training: 37.132523\n",
            "Loss training: 36.617737\n",
            "\n",
            "***************************\n",
            "Trained on 2070 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.99729\n",
            "Loss training: 31.093193\n",
            "Loss training: 42.630936\n",
            "Loss training: 36.7549\n",
            "Loss training: 65.051414\n",
            "Loss training: 37.666344\n",
            "Loss training: 49.525494\n",
            "Loss training: 34.395565\n",
            "Loss training: 31.370682\n",
            "Loss training: 48.33266\n",
            "\n",
            "***************************\n",
            "Trained on 2080 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 52.784615\n",
            "Loss training: 46.770576\n",
            "Loss training: 48.328606\n",
            "Loss training: 28.142971\n",
            "Loss training: 37.40904\n",
            "Loss training: 37.69755\n",
            "Loss training: 36.182716\n",
            "Loss training: 33.19562\n",
            "Loss training: 59.829548\n",
            "Loss training: 38.15329\n",
            "\n",
            "***************************\n",
            "Trained on 2090 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.717876\n",
            "Loss training: 29.781963\n",
            "Loss training: 59.243305\n",
            "Loss training: 46.80801\n",
            "Loss training: 14.386185\n",
            "Loss training: 28.740849\n",
            "Loss training: 44.015205\n",
            "Loss training: 45.41514\n",
            "Loss training: 29.855972\n",
            "Loss training: 59.748768\n",
            "\n",
            "***************************\n",
            "Trained on 2100 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 13.816205\n",
            "Loss training: 32.98401\n",
            "Loss training: 58.75725\n",
            "Loss training: 76.659424\n",
            "Loss training: 49.236675\n",
            "Loss training: 57.18229\n",
            "Loss training: 33.32121\n",
            "Loss training: 37.181263\n",
            "Loss training: 28.843037\n",
            "Loss training: 47.072063\n",
            "\n",
            "***************************\n",
            "Trained on 2110 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.457275\n",
            "Loss training: 43.63084\n",
            "Loss training: 31.230368\n",
            "Loss training: 35.908504\n",
            "Loss training: 38.558105\n",
            "Loss training: 43.639336\n",
            "Loss training: 36.99085\n",
            "Loss training: 40.848854\n",
            "Loss training: 36.31638\n",
            "Loss training: 31.739494\n",
            "\n",
            "***************************\n",
            "Trained on 2120 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.355297\n",
            "Loss training: 33.27856\n",
            "Loss training: 53.285595\n",
            "Loss training: 44.000633\n",
            "Loss training: 33.301456\n",
            "Loss training: 45.998886\n",
            "Loss training: 46.423622\n",
            "Loss training: 44.467716\n",
            "Loss training: 24.749939\n",
            "Loss training: 33.994457\n",
            "\n",
            "***************************\n",
            "Trained on 2130 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.47061\n",
            "Loss training: 49.34725\n",
            "Loss training: 35.176826\n",
            "Loss training: 35.629417\n",
            "Loss training: 39.879086\n",
            "Loss training: 34.752583\n",
            "Loss training: 26.236723\n",
            "Loss training: 34.791737\n",
            "Loss training: 49.950283\n",
            "Loss training: 57.86244\n",
            "\n",
            "***************************\n",
            "Trained on 2140 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.00156\n",
            "Loss training: 31.291037\n",
            "Loss training: 51.820354\n",
            "Loss training: 31.019209\n",
            "Loss training: 26.457615\n",
            "Loss training: 51.32747\n",
            "Loss training: 32.843105\n",
            "Loss training: 34.117832\n",
            "Loss training: 28.837284\n",
            "Loss training: 57.089417\n",
            "\n",
            "***************************\n",
            "Trained on 2150 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.931454\n",
            "Loss training: 45.82569\n",
            "Loss training: 37.57227\n",
            "Loss training: 32.417656\n",
            "Loss training: 58.049007\n",
            "Loss training: 35.51773\n",
            "Loss training: 43.405556\n",
            "Loss training: 49.847427\n",
            "Loss training: 30.266808\n",
            "Loss training: 52.648136\n",
            "\n",
            "***************************\n",
            "Trained on 2160 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 46.69011\n",
            "Loss training: 17.310314\n",
            "Loss training: 57.088913\n",
            "Loss training: 46.584354\n",
            "Loss training: 35.90494\n",
            "Loss training: 51.993214\n",
            "Loss training: 49.437523\n",
            "Loss training: 42.60415\n",
            "Loss training: 54.874706\n",
            "Loss training: 48.816086\n",
            "\n",
            "***************************\n",
            "Trained on 2170 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 21.86891\n",
            "Loss training: 53.493576\n",
            "Loss training: 31.840239\n",
            "Loss training: 27.975992\n",
            "Loss training: 28.945513\n",
            "Loss training: 46.09222\n",
            "Loss training: 21.667057\n",
            "Loss training: 45.167274\n",
            "Loss training: 31.45863\n",
            "Loss training: 28.386656\n",
            "\n",
            "***************************\n",
            "Trained on 2180 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.7631\n",
            "Loss training: 41.722843\n",
            "Loss training: 40.2919\n",
            "Loss training: 24.443916\n",
            "Loss training: 32.96843\n",
            "Loss training: 54.820683\n",
            "Loss training: 38.426743\n",
            "Loss training: 32.36427\n",
            "Loss training: 46.285995\n",
            "Loss training: 47.202526\n",
            "\n",
            "***************************\n",
            "Trained on 2190 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 56.554977\n",
            "Loss training: 27.980516\n",
            "Loss training: 35.044308\n",
            "Loss training: 33.187428\n",
            "Loss training: 48.01467\n",
            "Loss training: 39.865314\n",
            "Loss training: 45.492073\n",
            "Loss training: 37.062065\n",
            "Loss training: 59.309067\n",
            "Loss training: 30.920555\n",
            "\n",
            "***************************\n",
            "Trained on 2200 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.659954\n",
            "Loss training: 33.52709\n",
            "Loss training: 30.816822\n",
            "Loss training: 45.82788\n",
            "Loss training: 51.119427\n",
            "Loss training: 32.353607\n",
            "Loss training: 39.654697\n",
            "Loss training: 56.36045\n",
            "Loss training: 50.82357\n",
            "Loss training: 46.020306\n",
            "\n",
            "***************************\n",
            "Trained on 2210 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 16.046724\n",
            "Loss training: 49.048065\n",
            "Loss training: 54.244663\n",
            "Loss training: 36.818256\n",
            "Loss training: 46.87466\n",
            "Loss training: 31.059559\n",
            "Loss training: 48.864937\n",
            "Loss training: 15.241736\n",
            "Loss training: 15.114757\n",
            "Loss training: 55.675987\n",
            "\n",
            "***************************\n",
            "Trained on 2220 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 54.754883\n",
            "Loss training: 35.939934\n",
            "Loss training: 14.41371\n",
            "Loss training: 43.017162\n",
            "Loss training: 43.36269\n",
            "Loss training: 35.044365\n",
            "Loss training: 30.13645\n",
            "Loss training: 33.251873\n",
            "Loss training: 58.67011\n",
            "Loss training: 37.76612\n",
            "\n",
            "***************************\n",
            "Trained on 2230 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 47.2507\n",
            "Loss training: 28.711956\n",
            "Loss training: 42.27572\n",
            "Loss training: 44.41981\n",
            "Loss training: 33.121326\n",
            "Loss training: 36.89007\n",
            "Loss training: 45.848354\n",
            "Loss training: 45.401157\n",
            "Loss training: 21.2319\n",
            "Loss training: 36.570457\n",
            "\n",
            "***************************\n",
            "Trained on 2240 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.161877\n",
            "Loss training: 36.23764\n",
            "Loss training: 29.981936\n",
            "Loss training: 31.91719\n",
            "Loss training: 36.188286\n",
            "Loss training: 29.205027\n",
            "Loss training: 17.398516\n",
            "Loss training: 35.631042\n",
            "Loss training: 30.885925\n",
            "Loss training: 31.192232\n",
            "\n",
            "***************************\n",
            "Trained on 2250 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.505583\n",
            "Loss training: 32.685886\n",
            "Loss training: 47.920338\n",
            "Loss training: 46.511433\n",
            "Loss training: 28.342325\n",
            "Loss training: 29.227736\n",
            "Loss training: 32.15422\n",
            "Loss training: 51.787727\n",
            "Loss training: 32.182686\n",
            "Loss training: 21.314413\n",
            "\n",
            "***************************\n",
            "Trained on 2260 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 50.67898\n",
            "Loss training: 28.789461\n",
            "Loss training: 16.31591\n",
            "Loss training: 37.46438\n",
            "Loss training: 51.97705\n",
            "Loss training: 46.59247\n",
            "Loss training: 52.290676\n",
            "Loss training: 33.86713\n",
            "Loss training: 57.67499\n",
            "Loss training: 47.78229\n",
            "\n",
            "***************************\n",
            "Trained on 2270 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.970203\n",
            "Loss training: 34.11442\n",
            "Loss training: 37.967323\n",
            "Loss training: 34.098198\n",
            "Loss training: 35.65288\n",
            "Loss training: 49.940826\n",
            "Loss training: 33.377552\n",
            "Loss training: 13.7085085\n",
            "Loss training: 46.44906\n",
            "Loss training: 47.0379\n",
            "\n",
            "***************************\n",
            "Trained on 2280 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.66927\n",
            "Loss training: 37.482124\n",
            "Loss training: 58.535896\n",
            "Loss training: 28.952484\n",
            "Loss training: 46.264374\n",
            "Loss training: 33.91636\n",
            "Loss training: 38.24778\n",
            "Loss training: 45.451656\n",
            "Loss training: 36.250107\n",
            "Loss training: 45.17908\n",
            "\n",
            "***************************\n",
            "Trained on 2290 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 61.616257\n",
            "Loss training: 27.374723\n",
            "Loss training: 50.861633\n",
            "Loss training: 36.942204\n",
            "Loss training: 47.522446\n",
            "Loss training: 38.063095\n",
            "Loss training: 32.771454\n",
            "Loss training: 45.623474\n",
            "Loss training: 33.85193\n",
            "Loss training: 32.226814\n",
            "\n",
            "***************************\n",
            "Trained on 2300 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.215736\n",
            "Loss training: 35.50522\n",
            "Loss training: 28.38224\n",
            "Loss training: 57.36865\n",
            "Loss training: 57.37667\n",
            "Loss training: 40.02171\n",
            "Loss training: 43.56398\n",
            "Loss training: 32.18048\n",
            "Loss training: 44.44479\n",
            "Loss training: 35.27917\n",
            "\n",
            "***************************\n",
            "Trained on 2310 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.162846\n",
            "Loss training: 34.457394\n",
            "Loss training: 21.939264\n",
            "Loss training: 43.40384\n",
            "Loss training: 41.352497\n",
            "Loss training: 32.41488\n",
            "Loss training: 43.79935\n",
            "Loss training: 30.12598\n",
            "Loss training: 36.46877\n",
            "Loss training: 29.519695\n",
            "\n",
            "***************************\n",
            "Trained on 2320 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 53.97812\n",
            "Loss training: 31.35086\n",
            "Loss training: 35.614513\n",
            "Loss training: 16.330479\n",
            "Loss training: 43.68628\n",
            "Loss training: 25.77181\n",
            "Loss training: 25.654493\n",
            "Loss training: 27.37538\n",
            "Loss training: 45.90022\n",
            "Loss training: 58.523544\n",
            "\n",
            "***************************\n",
            "Trained on 2330 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 43.37639\n",
            "Loss training: 49.04253\n",
            "Loss training: 24.023462\n",
            "Loss training: 37.549423\n",
            "Loss training: 29.835194\n",
            "Loss training: 25.309477\n",
            "Loss training: 38.411953\n",
            "Loss training: 34.209965\n",
            "Loss training: 27.553257\n",
            "Loss training: 32.079937\n",
            "\n",
            "***************************\n",
            "Trained on 2340 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 46.034687\n",
            "Loss training: 47.82172\n",
            "Loss training: 32.646862\n",
            "Loss training: 44.491123\n",
            "Loss training: 31.748482\n",
            "Loss training: 49.61155\n",
            "Loss training: 30.715939\n",
            "Loss training: 37.92188\n",
            "Loss training: 34.59687\n",
            "Loss training: 30.372555\n",
            "\n",
            "***************************\n",
            "Trained on 2350 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 46.160706\n",
            "Loss training: 45.800865\n",
            "Loss training: 45.644917\n",
            "Loss training: 37.579357\n",
            "Loss training: 40.87762\n",
            "Loss training: 30.384567\n",
            "Loss training: 49.69248\n",
            "Loss training: 33.034885\n",
            "Loss training: 30.888956\n",
            "Loss training: 32.39485\n",
            "\n",
            "***************************\n",
            "Trained on 2360 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 55.418194\n",
            "Loss training: 25.893515\n",
            "Loss training: 43.6102\n",
            "Loss training: 28.387762\n",
            "Loss training: 33.33914\n",
            "Loss training: 37.60963\n",
            "Loss training: 35.64897\n",
            "Loss training: 21.98978\n",
            "Loss training: 52.29843\n",
            "Loss training: 35.891617\n",
            "\n",
            "***************************\n",
            "Trained on 2370 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.759348\n",
            "Loss training: 35.027233\n",
            "Loss training: 61.038074\n",
            "Loss training: 13.708362\n",
            "Loss training: 37.121243\n",
            "Loss training: 30.6627\n",
            "Loss training: 35.38258\n",
            "Loss training: 26.206057\n",
            "Loss training: 30.573206\n",
            "Loss training: 45.74359\n",
            "\n",
            "***************************\n",
            "Trained on 2380 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.448727\n",
            "Loss training: 38.030537\n",
            "Loss training: 37.8689\n",
            "Loss training: 53.97425\n",
            "Loss training: 32.43456\n",
            "Loss training: 31.543331\n",
            "Loss training: 34.81867\n",
            "Loss training: 35.63878\n",
            "Loss training: 35.11038\n",
            "Loss training: 27.976137\n",
            "\n",
            "***************************\n",
            "Trained on 2390 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 46.393215\n",
            "Loss training: 26.371601\n",
            "Loss training: 31.037497\n",
            "Loss training: 32.490273\n",
            "Loss training: 38.459335\n",
            "Loss training: 34.878468\n",
            "Loss training: 43.885\n",
            "Loss training: 45.869938\n",
            "Loss training: 31.768038\n",
            "Loss training: 43.65316\n",
            "\n",
            "***************************\n",
            "Trained on 2400 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 57.081554\n",
            "Loss training: 21.037836\n",
            "Loss training: 37.468826\n",
            "Loss training: 33.220352\n",
            "Loss training: 40.94309\n",
            "Loss training: 40.08695\n",
            "Loss training: 31.962711\n",
            "Loss training: 33.140854\n",
            "Loss training: 30.525143\n",
            "Loss training: 25.9618\n",
            "\n",
            "***************************\n",
            "Trained on 2410 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.336666\n",
            "Loss training: 31.31004\n",
            "Loss training: 30.9476\n",
            "Loss training: 31.468866\n",
            "Loss training: 66.00002\n",
            "Loss training: 46.24503\n",
            "Loss training: 46.631657\n",
            "Loss training: 30.287903\n",
            "Loss training: 59.57679\n",
            "Loss training: 29.215635\n",
            "\n",
            "***************************\n",
            "Trained on 2420 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.46382\n",
            "Loss training: 57.09778\n",
            "Loss training: 35.875507\n",
            "Loss training: 38.753468\n",
            "Loss training: 32.113636\n",
            "Loss training: 53.46202\n",
            "Loss training: 37.31364\n",
            "Loss training: 60.414787\n",
            "Loss training: 55.127846\n",
            "Loss training: 14.958371\n",
            "\n",
            "***************************\n",
            "Trained on 2430 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.65004\n",
            "Loss training: 29.993675\n",
            "Loss training: 34.209206\n",
            "Loss training: 26.980795\n",
            "Loss training: 44.146133\n",
            "Loss training: 30.691639\n",
            "Loss training: 53.109917\n",
            "Loss training: 28.83635\n",
            "Loss training: 52.079098\n",
            "Loss training: 46.186844\n",
            "\n",
            "***************************\n",
            "Trained on 2440 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.476995\n",
            "Loss training: 32.207104\n",
            "Loss training: 34.30629\n",
            "Loss training: 39.271862\n",
            "Loss training: 20.21201\n",
            "Loss training: 33.91181\n",
            "Loss training: 42.519527\n",
            "Loss training: 29.156603\n",
            "Loss training: 66.2842\n",
            "Loss training: 45.115326\n",
            "\n",
            "***************************\n",
            "Trained on 2450 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.98092\n",
            "Loss training: 32.63108\n",
            "Loss training: 35.47432\n",
            "Loss training: 47.945843\n",
            "Loss training: 54.60467\n",
            "Loss training: 47.1267\n",
            "Loss training: 47.072327\n",
            "Loss training: 37.27381\n",
            "Loss training: 46.646122\n",
            "Loss training: 31.189444\n",
            "\n",
            "***************************\n",
            "Trained on 2460 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.614378\n",
            "Loss training: 34.65105\n",
            "Loss training: 25.710772\n",
            "Loss training: 27.29696\n",
            "Loss training: 55.41123\n",
            "Loss training: 39.543797\n",
            "Loss training: 38.947212\n",
            "Loss training: 30.958323\n",
            "Loss training: 27.327082\n",
            "Loss training: 41.08761\n",
            "\n",
            "***************************\n",
            "Trained on 2470 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.623268\n",
            "Loss training: 29.834959\n",
            "Loss training: 55.8747\n",
            "Loss training: 38.184376\n",
            "Loss training: 32.191925\n",
            "Loss training: 49.86539\n",
            "Loss training: 49.184036\n",
            "Loss training: 16.634638\n",
            "Loss training: 39.940266\n",
            "Loss training: 32.705494\n",
            "\n",
            "***************************\n",
            "Trained on 2480 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 53.39641\n",
            "Loss training: 36.072994\n",
            "Loss training: 49.850258\n",
            "Loss training: 51.348835\n",
            "Loss training: 47.404297\n",
            "Loss training: 41.888966\n",
            "Loss training: 38.68269\n",
            "Loss training: 48.81711\n",
            "Loss training: 37.71758\n",
            "Loss training: 13.732038\n",
            "\n",
            "***************************\n",
            "Trained on 2490 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.718472\n",
            "Loss training: 34.71784\n",
            "Loss training: 13.026124\n",
            "Loss training: 29.471624\n",
            "Loss training: 30.495056\n",
            "Loss training: 51.378544\n",
            "Loss training: 42.70421\n",
            "Loss training: 35.965015\n",
            "Loss training: 44.49905\n",
            "Loss training: 34.279335\n",
            "\n",
            "***************************\n",
            "Trained on 2500 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 52.003414\n",
            "Loss training: 32.913765\n",
            "Loss training: 25.995584\n",
            "Loss training: 46.92066\n",
            "Loss training: 44.4749\n",
            "Loss training: 35.53112\n",
            "Loss training: 28.67732\n",
            "Loss training: 26.58539\n",
            "Loss training: 46.605427\n",
            "Loss training: 65.551414\n",
            "\n",
            "***************************\n",
            "Trained on 2510 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.140053\n",
            "Loss training: 24.753956\n",
            "Loss training: 46.23469\n",
            "Loss training: 38.27927\n",
            "Loss training: 38.518883\n",
            "Loss training: 38.300663\n",
            "Loss training: 28.648815\n",
            "Loss training: 44.940834\n",
            "Loss training: 34.996414\n",
            "Loss training: 33.330452\n",
            "\n",
            "***************************\n",
            "Trained on 2520 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.498863\n",
            "Loss training: 12.409941\n",
            "Loss training: 32.82592\n",
            "Loss training: 73.58244\n",
            "Loss training: 34.79807\n",
            "Loss training: 19.421112\n",
            "Loss training: 31.097092\n",
            "Loss training: 27.272516\n",
            "Loss training: 42.44567\n",
            "Loss training: 59.77306\n",
            "\n",
            "***************************\n",
            "Trained on 2530 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 13.530221\n",
            "Loss training: 63.271896\n",
            "Loss training: 21.75611\n",
            "Loss training: 56.88097\n",
            "Loss training: 39.47117\n",
            "Loss training: 49.14886\n",
            "Loss training: 30.88841\n",
            "Loss training: 39.916634\n",
            "Loss training: 43.94994\n",
            "Loss training: 37.20829\n",
            "\n",
            "***************************\n",
            "Trained on 2540 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.664524\n",
            "Loss training: 32.0943\n",
            "Loss training: 59.54686\n",
            "Loss training: 53.76147\n",
            "Loss training: 36.201347\n",
            "Loss training: 44.532562\n",
            "Loss training: 50.570118\n",
            "Loss training: 33.850742\n",
            "Loss training: 26.543497\n",
            "Loss training: 46.445934\n",
            "\n",
            "***************************\n",
            "Trained on 2550 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.972202\n",
            "Loss training: 33.515762\n",
            "Loss training: 31.596386\n",
            "Loss training: 20.07648\n",
            "Loss training: 44.01308\n",
            "Loss training: 46.50672\n",
            "Loss training: 40.04047\n",
            "Loss training: 39.557873\n",
            "Loss training: 32.410908\n",
            "Loss training: 41.517387\n",
            "\n",
            "***************************\n",
            "Trained on 2560 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.52754\n",
            "Loss training: 45.67399\n",
            "Loss training: 28.44194\n",
            "Loss training: 55.205723\n",
            "Loss training: 45.429604\n",
            "Loss training: 40.22419\n",
            "Loss training: 44.979034\n",
            "Loss training: 35.85156\n",
            "Loss training: 52.26315\n",
            "Loss training: 32.90923\n",
            "\n",
            "***************************\n",
            "Trained on 2570 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.028343\n",
            "Loss training: 52.754852\n",
            "Loss training: 52.71499\n",
            "Loss training: 48.65465\n",
            "Loss training: 47.69356\n",
            "Loss training: 51.795704\n",
            "Loss training: 47.252422\n",
            "Loss training: 47.61817\n",
            "Loss training: 34.316673\n",
            "Loss training: 46.25172\n",
            "\n",
            "***************************\n",
            "Trained on 2580 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 15.1294775\n",
            "Loss training: 32.613888\n",
            "Loss training: 20.471397\n",
            "Loss training: 28.859108\n",
            "Loss training: 55.604233\n",
            "Loss training: 19.728006\n",
            "Loss training: 25.838985\n",
            "Loss training: 15.182933\n",
            "Loss training: 12.716921\n",
            "Loss training: 30.171612\n",
            "\n",
            "***************************\n",
            "Trained on 2590 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 53.22537\n",
            "Loss training: 24.138721\n",
            "Loss training: 37.19639\n",
            "Loss training: 14.9598875\n",
            "Loss training: 35.759567\n",
            "Loss training: 40.8957\n",
            "Loss training: 30.267685\n",
            "Loss training: 34.515118\n",
            "Loss training: 54.383076\n",
            "Loss training: 46.7086\n",
            "\n",
            "***************************\n",
            "Trained on 2600 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 46.85325\n",
            "Loss training: 36.2947\n",
            "Loss training: 32.35408\n",
            "Loss training: 45.27624\n",
            "Loss training: 36.84925\n",
            "Loss training: 33.491196\n",
            "Loss training: 52.333027\n",
            "Loss training: 30.782967\n",
            "Loss training: 26.041143\n",
            "Loss training: 44.263878\n",
            "\n",
            "***************************\n",
            "Trained on 2610 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.909523\n",
            "Loss training: 38.58295\n",
            "Loss training: 67.402435\n",
            "Loss training: 65.63367\n",
            "Loss training: 25.253548\n",
            "Loss training: 48.042973\n",
            "Loss training: 41.862522\n",
            "Loss training: 31.52414\n",
            "Loss training: 31.355743\n",
            "Loss training: 37.448048\n",
            "\n",
            "***************************\n",
            "Trained on 2620 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.268562\n",
            "Loss training: 30.822226\n",
            "Loss training: 37.15783\n",
            "Loss training: 31.02277\n",
            "Loss training: 32.4568\n",
            "Loss training: 31.690502\n",
            "Loss training: 53.242836\n",
            "Loss training: 41.522488\n",
            "Loss training: 53.839336\n",
            "Loss training: 44.24019\n",
            "\n",
            "***************************\n",
            "Trained on 2630 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 46.06056\n",
            "Loss training: 59.54483\n",
            "Loss training: 35.556057\n",
            "Loss training: 32.21895\n",
            "Loss training: 43.528492\n",
            "Loss training: 32.02641\n",
            "Loss training: 13.558038\n",
            "Loss training: 28.228067\n",
            "Loss training: 26.412342\n",
            "Loss training: 45.467842\n",
            "\n",
            "***************************\n",
            "Trained on 2640 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 15.986576\n",
            "Loss training: 33.38199\n",
            "Loss training: 54.052856\n",
            "Loss training: 50.86379\n",
            "Loss training: 52.003963\n",
            "Loss training: 35.98703\n",
            "Loss training: 37.320156\n",
            "Loss training: 34.499153\n",
            "Loss training: 38.5013\n",
            "Loss training: 32.72925\n",
            "\n",
            "***************************\n",
            "Trained on 2650 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.201828\n",
            "Loss training: 37.20437\n",
            "Loss training: 44.908794\n",
            "Loss training: 37.57499\n",
            "Loss training: 30.702835\n",
            "Loss training: 20.91329\n",
            "Loss training: 20.956541\n",
            "Loss training: 44.524376\n",
            "Loss training: 44.2177\n",
            "Loss training: 27.639513\n",
            "\n",
            "***************************\n",
            "Trained on 2660 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.274815\n",
            "Loss training: 39.082043\n",
            "Loss training: 36.162914\n",
            "Loss training: 28.135212\n",
            "Loss training: 54.079884\n",
            "Loss training: 12.771105\n",
            "Loss training: 33.663586\n",
            "Loss training: 52.93241\n",
            "Loss training: 36.91125\n",
            "Loss training: 38.661423\n",
            "\n",
            "***************************\n",
            "Trained on 2670 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.18958\n",
            "Loss training: 53.257347\n",
            "Loss training: 43.586185\n",
            "Loss training: 30.680367\n",
            "Loss training: 38.92787\n",
            "Loss training: 28.703365\n",
            "Loss training: 26.573689\n",
            "Loss training: 43.23621\n",
            "Loss training: 28.162361\n",
            "Loss training: 45.009212\n",
            "\n",
            "***************************\n",
            "Trained on 2680 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 62.596012\n",
            "Loss training: 25.927069\n",
            "Loss training: 20.106726\n",
            "Loss training: 32.491726\n",
            "Loss training: 32.41003\n",
            "Loss training: 47.093132\n",
            "Loss training: 53.090652\n",
            "Loss training: 31.99661\n",
            "Loss training: 45.317467\n",
            "Loss training: 51.09379\n",
            "\n",
            "***************************\n",
            "Trained on 2690 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 43.03339\n",
            "Loss training: 30.855652\n",
            "Loss training: 43.878506\n",
            "Loss training: 35.472607\n",
            "Loss training: 33.807877\n",
            "Loss training: 43.60893\n",
            "Loss training: 32.309525\n",
            "Loss training: 19.058386\n",
            "Loss training: 46.25443\n",
            "Loss training: 36.77277\n",
            "\n",
            "***************************\n",
            "Trained on 2700 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.123909\n",
            "Loss training: 48.01112\n",
            "Loss training: 50.6618\n",
            "Loss training: 25.504972\n",
            "Loss training: 30.244757\n",
            "Loss training: 33.342922\n",
            "Loss training: 25.647385\n",
            "Loss training: 35.89417\n",
            "Loss training: 32.998634\n",
            "Loss training: 42.2058\n",
            "\n",
            "***************************\n",
            "Trained on 2710 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.548422\n",
            "Loss training: 39.59403\n",
            "Loss training: 54.203754\n",
            "Loss training: 28.314318\n",
            "Loss training: 44.510464\n",
            "Loss training: 15.28898\n",
            "Loss training: 35.519035\n",
            "Loss training: 27.198696\n",
            "Loss training: 39.37992\n",
            "Loss training: 52.41378\n",
            "\n",
            "***************************\n",
            "Trained on 2720 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.83028\n",
            "Loss training: 59.95857\n",
            "Loss training: 28.695269\n",
            "Loss training: 35.240414\n",
            "Loss training: 44.391293\n",
            "Loss training: 35.168396\n",
            "Loss training: 38.4252\n",
            "Loss training: 45.198456\n",
            "Loss training: 14.85156\n",
            "Loss training: 25.238352\n",
            "\n",
            "***************************\n",
            "Trained on 2730 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.729214\n",
            "Loss training: 46.83186\n",
            "Loss training: 19.563492\n",
            "Loss training: 53.76816\n",
            "Loss training: 25.533346\n",
            "Loss training: 61.778805\n",
            "Loss training: 39.112198\n",
            "Loss training: 25.54402\n",
            "Loss training: 30.26378\n",
            "Loss training: 43.950245\n",
            "\n",
            "***************************\n",
            "Trained on 2740 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.702797\n",
            "Loss training: 37.238403\n",
            "Loss training: 28.87247\n",
            "Loss training: 34.864944\n",
            "Loss training: 30.523783\n",
            "Loss training: 43.543434\n",
            "Loss training: 33.90061\n",
            "Loss training: 29.760427\n",
            "Loss training: 12.535315\n",
            "Loss training: 53.459045\n",
            "\n",
            "***************************\n",
            "Trained on 2750 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 12.593211\n",
            "Loss training: 28.889713\n",
            "Loss training: 25.361137\n",
            "Loss training: 42.422714\n",
            "Loss training: 40.772453\n",
            "Loss training: 34.398872\n",
            "Loss training: 34.188683\n",
            "Loss training: 53.05763\n",
            "Loss training: 38.527817\n",
            "Loss training: 30.70838\n",
            "\n",
            "***************************\n",
            "Trained on 2760 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.87808\n",
            "Loss training: 32.585278\n",
            "Loss training: 31.408577\n",
            "Loss training: 30.078083\n",
            "Loss training: 15.709031\n",
            "Loss training: 32.330093\n",
            "Loss training: 44.259224\n",
            "Loss training: 30.07649\n",
            "Loss training: 28.183626\n",
            "Loss training: 46.549458\n",
            "\n",
            "***************************\n",
            "Trained on 2770 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.554785\n",
            "Loss training: 38.79713\n",
            "Loss training: 30.17819\n",
            "Loss training: 37.33572\n",
            "Loss training: 43.558403\n",
            "Loss training: 51.966827\n",
            "Loss training: 19.913984\n",
            "Loss training: 32.29932\n",
            "Loss training: 42.431618\n",
            "Loss training: 26.018112\n",
            "\n",
            "***************************\n",
            "Trained on 2780 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 64.56031\n",
            "Loss training: 34.603535\n",
            "Loss training: 35.98368\n",
            "Loss training: 37.12317\n",
            "Loss training: 33.349655\n",
            "Loss training: 43.001427\n",
            "Loss training: 53.585335\n",
            "Loss training: 42.699387\n",
            "Loss training: 34.16592\n",
            "Loss training: 44.243393\n",
            "\n",
            "***************************\n",
            "Trained on 2790 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.26314\n",
            "Loss training: 34.606388\n",
            "Loss training: 32.33328\n",
            "Loss training: 30.729315\n",
            "Loss training: 38.754604\n",
            "Loss training: 32.15788\n",
            "Loss training: 42.606373\n",
            "Loss training: 30.187292\n",
            "Loss training: 43.946274\n",
            "Loss training: 51.27869\n",
            "\n",
            "***************************\n",
            "Trained on 2800 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.334385\n",
            "Loss training: 46.572563\n",
            "Loss training: 54.852547\n",
            "Loss training: 32.207424\n",
            "Loss training: 28.252731\n",
            "Loss training: 44.232025\n",
            "Loss training: 42.692657\n",
            "Loss training: 41.38373\n",
            "Loss training: 37.98988\n",
            "Loss training: 12.705627\n",
            "\n",
            "***************************\n",
            "Trained on 2810 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.374165\n",
            "Loss training: 35.99282\n",
            "Loss training: 27.490988\n",
            "Loss training: 30.000008\n",
            "Loss training: 35.028706\n",
            "Loss training: 34.03584\n",
            "Loss training: 51.662758\n",
            "Loss training: 26.031755\n",
            "Loss training: 47.640507\n",
            "Loss training: 34.426285\n",
            "\n",
            "***************************\n",
            "Trained on 2820 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.167324\n",
            "Loss training: 38.393734\n",
            "Loss training: 50.105965\n",
            "Loss training: 38.302544\n",
            "Loss training: 54.97576\n",
            "Loss training: 28.676802\n",
            "Loss training: 50.98446\n",
            "Loss training: 53.76217\n",
            "Loss training: 43.02935\n",
            "Loss training: 43.811985\n",
            "\n",
            "***************************\n",
            "Trained on 2830 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.382885\n",
            "Loss training: 37.78707\n",
            "Loss training: 21.107735\n",
            "Loss training: 61.172493\n",
            "Loss training: 36.81909\n",
            "Loss training: 58.857086\n",
            "Loss training: 13.660567\n",
            "Loss training: 52.139397\n",
            "Loss training: 44.116768\n",
            "Loss training: 60.890938\n",
            "\n",
            "***************************\n",
            "Trained on 2840 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.51042\n",
            "Loss training: 27.225122\n",
            "Loss training: 56.908474\n",
            "Loss training: 32.57042\n",
            "Loss training: 45.62585\n",
            "Loss training: 20.57447\n",
            "Loss training: 15.065649\n",
            "Loss training: 38.454704\n",
            "Loss training: 40.484283\n",
            "Loss training: 47.626736\n",
            "\n",
            "***************************\n",
            "Trained on 2850 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.350594\n",
            "Loss training: 31.89248\n",
            "Loss training: 44.084126\n",
            "Loss training: 46.514263\n",
            "Loss training: 30.006664\n",
            "Loss training: 38.259163\n",
            "Loss training: 60.19167\n",
            "Loss training: 13.75278\n",
            "Loss training: 37.203293\n",
            "Loss training: 59.490726\n",
            "\n",
            "***************************\n",
            "Trained on 2860 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.219196\n",
            "Loss training: 28.117558\n",
            "Loss training: 47.615967\n",
            "Loss training: 28.475004\n",
            "Loss training: 28.790928\n",
            "Loss training: 56.832424\n",
            "Loss training: 28.73722\n",
            "Loss training: 27.673761\n",
            "Loss training: 64.87563\n",
            "Loss training: 31.226414\n",
            "\n",
            "***************************\n",
            "Trained on 2870 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.66316\n",
            "Loss training: 41.86055\n",
            "Loss training: 32.50587\n",
            "Loss training: 36.32726\n",
            "Loss training: 35.519524\n",
            "Loss training: 34.45665\n",
            "Loss training: 14.52279\n",
            "Loss training: 28.098717\n",
            "Loss training: 53.923355\n",
            "Loss training: 32.91215\n",
            "\n",
            "***************************\n",
            "Trained on 2880 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 53.82201\n",
            "Loss training: 52.154026\n",
            "Loss training: 30.616175\n",
            "Loss training: 45.244072\n",
            "Loss training: 64.32394\n",
            "Loss training: 52.315388\n",
            "Loss training: 46.89057\n",
            "Loss training: 33.306892\n",
            "Loss training: 15.875478\n",
            "Loss training: 15.090895\n",
            "\n",
            "***************************\n",
            "Trained on 2890 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 51.270817\n",
            "Loss training: 34.690098\n",
            "Loss training: 38.051323\n",
            "Loss training: 33.264378\n",
            "Loss training: 56.56567\n",
            "Loss training: 49.470993\n",
            "Loss training: 36.403606\n",
            "Loss training: 35.97322\n",
            "Loss training: 42.578968\n",
            "Loss training: 36.07659\n",
            "\n",
            "***************************\n",
            "Trained on 2900 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.609528\n",
            "Loss training: 38.94547\n",
            "Loss training: 35.32175\n",
            "Loss training: 46.3994\n",
            "Loss training: 39.088013\n",
            "Loss training: 37.39295\n",
            "Loss training: 43.33027\n",
            "Loss training: 43.663536\n",
            "Loss training: 46.12377\n",
            "Loss training: 30.857115\n",
            "\n",
            "***************************\n",
            "Trained on 2910 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.670965\n",
            "Loss training: 33.012295\n",
            "Loss training: 12.000071\n",
            "Loss training: 35.289932\n",
            "Loss training: 19.64409\n",
            "Loss training: 33.854645\n",
            "Loss training: 41.02582\n",
            "Loss training: 15.7261915\n",
            "Loss training: 38.1127\n",
            "Loss training: 113.928\n",
            "\n",
            "***************************\n",
            "Trained on 2920 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 50.983322\n",
            "Loss training: 53.79329\n",
            "Loss training: 32.512707\n",
            "Loss training: 35.06527\n",
            "Loss training: 45.514027\n",
            "Loss training: 45.314827\n",
            "Loss training: 30.20699\n",
            "Loss training: 30.639864\n",
            "Loss training: 62.69925\n",
            "Loss training: 15.575835\n",
            "\n",
            "***************************\n",
            "Trained on 2930 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.79918\n",
            "Loss training: 52.24244\n",
            "Loss training: 28.667152\n",
            "Loss training: 36.67112\n",
            "Loss training: 38.671192\n",
            "Loss training: 20.685205\n",
            "Loss training: 28.25675\n",
            "Loss training: 47.92552\n",
            "Loss training: 34.669262\n",
            "Loss training: 47.059597\n",
            "\n",
            "***************************\n",
            "Trained on 2940 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.238525\n",
            "Loss training: 31.836554\n",
            "Loss training: 38.99674\n",
            "Loss training: 44.260815\n",
            "Loss training: 35.901775\n",
            "Loss training: 30.477772\n",
            "Loss training: 36.479755\n",
            "Loss training: 52.6275\n",
            "Loss training: 29.608662\n",
            "Loss training: 58.147533\n",
            "\n",
            "***************************\n",
            "Trained on 2950 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 60.927025\n",
            "Loss training: 49.227734\n",
            "Loss training: 37.451225\n",
            "Loss training: 38.53271\n",
            "Loss training: 55.36727\n",
            "Loss training: 37.039078\n",
            "Loss training: 54.812675\n",
            "Loss training: 42.773754\n",
            "Loss training: 45.379196\n",
            "Loss training: 27.239628\n",
            "\n",
            "***************************\n",
            "Trained on 2960 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.739464\n",
            "Loss training: 34.91827\n",
            "Loss training: 61.9931\n",
            "Loss training: 43.608253\n",
            "Loss training: 35.302578\n",
            "Loss training: 31.61029\n",
            "Loss training: 33.684765\n",
            "Loss training: 59.57161\n",
            "Loss training: 32.43169\n",
            "Loss training: 37.99403\n",
            "\n",
            "***************************\n",
            "Trained on 2970 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.55704\n",
            "Loss training: 51.34888\n",
            "Loss training: 16.418198\n",
            "Loss training: 46.591717\n",
            "Loss training: 47.137257\n",
            "Loss training: 41.637383\n",
            "Loss training: 20.728424\n",
            "Loss training: 35.452606\n",
            "Loss training: 19.814756\n",
            "Loss training: 34.80374\n",
            "\n",
            "***************************\n",
            "Trained on 2980 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 48.602448\n",
            "Loss training: 35.338196\n",
            "Loss training: 32.80102\n",
            "Loss training: 41.004997\n",
            "Loss training: 42.850613\n",
            "Loss training: 29.558353\n",
            "Loss training: 47.70072\n",
            "Loss training: 47.138245\n",
            "Loss training: 32.014454\n",
            "Loss training: 44.844925\n",
            "\n",
            "***************************\n",
            "Trained on 2990 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.371174\n",
            "Loss training: 16.22543\n",
            "Loss training: 30.978577\n",
            "Loss training: 38.55134\n",
            "Loss training: 30.645802\n",
            "Loss training: 41.685207\n",
            "Loss training: 52.85541\n",
            "Loss training: 34.10969\n",
            "Loss training: 38.87775\n",
            "Loss training: 31.853735\n",
            "\n",
            "***************************\n",
            "Trained on 3000 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.289658\n",
            "Loss training: 30.146278\n",
            "Loss training: 47.33799\n",
            "Loss training: 46.908554\n",
            "Loss training: 33.69511\n",
            "Loss training: 32.607033\n",
            "Loss training: 32.369606\n",
            "Loss training: 37.74868\n",
            "Loss training: 32.992405\n",
            "Loss training: 24.59099\n",
            "\n",
            "***************************\n",
            "Trained on 3010 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 47.47225\n",
            "Loss training: 35.19632\n",
            "Loss training: 40.601013\n",
            "Loss training: 57.20786\n",
            "Loss training: 44.4299\n",
            "Loss training: 63.71382\n",
            "Loss training: 43.631416\n",
            "Loss training: 24.253769\n",
            "Loss training: 42.234257\n",
            "Loss training: 36.943928\n",
            "\n",
            "***************************\n",
            "Trained on 3020 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 55.57647\n",
            "Loss training: 37.81565\n",
            "Loss training: 39.962334\n",
            "Loss training: 56.117393\n",
            "Loss training: 34.997746\n",
            "Loss training: 44.06644\n",
            "Loss training: 33.263287\n",
            "Loss training: 54.4487\n",
            "Loss training: 17.355564\n",
            "Loss training: 48.313328\n",
            "\n",
            "***************************\n",
            "Trained on 3030 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.483341\n",
            "Loss training: 31.079557\n",
            "Loss training: 49.498905\n",
            "Loss training: 30.348995\n",
            "Loss training: 32.263447\n",
            "Loss training: 46.482025\n",
            "Loss training: 35.07982\n",
            "Loss training: 40.040867\n",
            "Loss training: 68.80421\n",
            "Loss training: 39.01722\n",
            "\n",
            "***************************\n",
            "Trained on 3040 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 15.807078\n",
            "Loss training: 31.524782\n",
            "Loss training: 16.407515\n",
            "Loss training: 36.508648\n",
            "Loss training: 39.637566\n",
            "Loss training: 43.261936\n",
            "Loss training: 36.434296\n",
            "Loss training: 54.908585\n",
            "Loss training: 32.627693\n",
            "Loss training: 46.408794\n",
            "\n",
            "***************************\n",
            "Trained on 3050 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.126308\n",
            "Loss training: 54.759705\n",
            "Loss training: 45.080845\n",
            "Loss training: 15.365744\n",
            "Loss training: 32.61192\n",
            "Loss training: 26.757736\n",
            "Loss training: 37.819344\n",
            "Loss training: 43.76389\n",
            "Loss training: 56.732864\n",
            "Loss training: 42.269978\n",
            "\n",
            "***************************\n",
            "Trained on 3060 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.865814\n",
            "Loss training: 57.02773\n",
            "Loss training: 53.70079\n",
            "Loss training: 43.01413\n",
            "Loss training: 26.118258\n",
            "Loss training: 43.548096\n",
            "Loss training: 35.26813\n",
            "Loss training: 46.72327\n",
            "Loss training: 53.901897\n",
            "Loss training: 28.950323\n",
            "\n",
            "***************************\n",
            "Trained on 3070 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 56.422832\n",
            "Loss training: 41.117695\n",
            "Loss training: 50.59456\n",
            "Loss training: 47.9634\n",
            "Loss training: 55.29903\n",
            "Loss training: 37.45701\n",
            "Loss training: 55.751076\n",
            "Loss training: 36.802082\n",
            "Loss training: 59.14867\n",
            "Loss training: 52.535778\n",
            "\n",
            "***************************\n",
            "Trained on 3080 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.023613\n",
            "Loss training: 50.770367\n",
            "Loss training: 32.728745\n",
            "Loss training: 51.02861\n",
            "Loss training: 47.896957\n",
            "Loss training: 36.298744\n",
            "Loss training: 28.24559\n",
            "Loss training: 29.198496\n",
            "Loss training: 20.806519\n",
            "Loss training: 44.867798\n",
            "\n",
            "***************************\n",
            "Trained on 3090 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 48.33229\n",
            "Loss training: 32.265495\n",
            "Loss training: 26.239132\n",
            "Loss training: 12.35369\n",
            "Loss training: 59.704254\n",
            "Loss training: 45.889286\n",
            "Loss training: 46.011074\n",
            "Loss training: 38.818455\n",
            "Loss training: 44.37909\n",
            "Loss training: 51.64574\n",
            "\n",
            "***************************\n",
            "Trained on 3100 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.77532\n",
            "Loss training: 45.66964\n",
            "Loss training: 32.56405\n",
            "Loss training: 47.55085\n",
            "Loss training: 12.358296\n",
            "Loss training: 64.05676\n",
            "Loss training: 31.386707\n",
            "Loss training: 52.66779\n",
            "Loss training: 44.698196\n",
            "Loss training: 37.187984\n",
            "\n",
            "***************************\n",
            "Trained on 3110 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 12.875194\n",
            "Loss training: 27.242188\n",
            "Loss training: 43.73704\n",
            "Loss training: 52.558243\n",
            "Loss training: 59.145622\n",
            "Loss training: 26.204195\n",
            "Loss training: 37.6043\n",
            "Loss training: 34.90831\n",
            "Loss training: 51.99501\n",
            "Loss training: 43.56857\n",
            "\n",
            "***************************\n",
            "Trained on 3120 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 62.078606\n",
            "Loss training: 49.962147\n",
            "Loss training: 39.20028\n",
            "Loss training: 50.773636\n",
            "Loss training: 33.7565\n",
            "Loss training: 47.375317\n",
            "Loss training: 49.321682\n",
            "Loss training: 51.886547\n",
            "Loss training: 51.44633\n",
            "Loss training: 57.494713\n",
            "\n",
            "***************************\n",
            "Trained on 3130 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 51.32817\n",
            "Loss training: 37.541916\n",
            "Loss training: 38.45072\n",
            "Loss training: 42.925636\n",
            "Loss training: 53.0122\n",
            "Loss training: 55.298977\n",
            "Loss training: 31.526012\n",
            "Loss training: 39.497925\n",
            "Loss training: 46.951298\n",
            "Loss training: 26.054655\n",
            "\n",
            "***************************\n",
            "Trained on 3140 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 53.612953\n",
            "Loss training: 12.488871\n",
            "Loss training: 53.166485\n",
            "Loss training: 35.108116\n",
            "Loss training: 48.730515\n",
            "Loss training: 51.062244\n",
            "Loss training: 51.89162\n",
            "Loss training: 55.942993\n",
            "Loss training: 34.5277\n",
            "Loss training: 36.899673\n",
            "\n",
            "***************************\n",
            "Trained on 3150 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.044018\n",
            "Loss training: 44.425617\n",
            "Loss training: 38.984653\n",
            "Loss training: 40.363155\n",
            "Loss training: 31.994005\n",
            "Loss training: 41.337498\n",
            "Loss training: 39.603348\n",
            "Loss training: 47.1447\n",
            "Loss training: 29.970392\n",
            "Loss training: 51.210262\n",
            "\n",
            "***************************\n",
            "Trained on 3160 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.36428\n",
            "Loss training: 51.424538\n",
            "Loss training: 16.826397\n",
            "Loss training: 37.446304\n",
            "Loss training: 41.874756\n",
            "Loss training: 27.641146\n",
            "Loss training: 34.18533\n",
            "Loss training: 49.561695\n",
            "Loss training: 47.761703\n",
            "Loss training: 43.43634\n",
            "\n",
            "***************************\n",
            "Trained on 3170 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.45935\n",
            "Loss training: 52.240757\n",
            "Loss training: 45.38591\n",
            "Loss training: 35.38018\n",
            "Loss training: 37.076427\n",
            "Loss training: 28.914911\n",
            "Loss training: 34.78994\n",
            "Loss training: 41.44043\n",
            "Loss training: 29.66029\n",
            "Loss training: 30.918806\n",
            "\n",
            "***************************\n",
            "Trained on 3180 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.098312\n",
            "Loss training: 45.686718\n",
            "Loss training: 73.210175\n",
            "Loss training: 43.02506\n",
            "Loss training: 27.608713\n",
            "Loss training: 30.674356\n",
            "Loss training: 48.03448\n",
            "Loss training: 50.47258\n",
            "Loss training: 46.04882\n",
            "Loss training: 42.975826\n",
            "\n",
            "***************************\n",
            "Trained on 3190 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.816042\n",
            "Loss training: 16.243116\n",
            "Loss training: 16.064838\n",
            "Loss training: 45.844128\n",
            "Loss training: 26.975159\n",
            "Loss training: 43.367878\n",
            "Loss training: 38.38592\n",
            "Loss training: 28.160742\n",
            "Loss training: 19.78337\n",
            "Loss training: 53.9888\n",
            "\n",
            "***************************\n",
            "Trained on 3200 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.736904\n",
            "Loss training: 38.373188\n",
            "Loss training: 30.241507\n",
            "Loss training: 31.889793\n",
            "Loss training: 33.39881\n",
            "Loss training: 45.349308\n",
            "Loss training: 34.97522\n",
            "Loss training: 37.541164\n",
            "Loss training: 30.890884\n",
            "Loss training: 67.25499\n",
            "\n",
            "***************************\n",
            "Trained on 3210 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.36754\n",
            "Loss training: 58.98629\n",
            "Loss training: 42.656254\n",
            "Loss training: 58.041492\n",
            "Loss training: 57.073013\n",
            "Loss training: 36.887856\n",
            "Loss training: 30.51543\n",
            "Loss training: 50.16671\n",
            "Loss training: 50.730858\n",
            "Loss training: 50.503124\n",
            "\n",
            "***************************\n",
            "Trained on 3220 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 50.105705\n",
            "Loss training: 34.102196\n",
            "Loss training: 31.290087\n",
            "Loss training: 47.10319\n",
            "Loss training: 36.833927\n",
            "Loss training: 35.435146\n",
            "Loss training: 47.15072\n",
            "Loss training: 36.782795\n",
            "Loss training: 30.793686\n",
            "Loss training: 50.697113\n",
            "\n",
            "***************************\n",
            "Trained on 3230 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.707598\n",
            "Loss training: 38.932667\n",
            "Loss training: 42.62841\n",
            "Loss training: 29.173382\n",
            "Loss training: 50.739494\n",
            "Loss training: 32.557156\n",
            "Loss training: 42.52678\n",
            "Loss training: 56.144672\n",
            "Loss training: 31.115097\n",
            "Loss training: 48.40817\n",
            "\n",
            "***************************\n",
            "Trained on 3240 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.873337\n",
            "Loss training: 41.05676\n",
            "Loss training: 47.431282\n",
            "Loss training: 30.417175\n",
            "Loss training: 32.207478\n",
            "Loss training: 35.413628\n",
            "Loss training: 58.21609\n",
            "Loss training: 59.007572\n",
            "Loss training: 20.10335\n",
            "Loss training: 44.317696\n",
            "\n",
            "***************************\n",
            "Trained on 3250 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 51.171852\n",
            "Loss training: 53.133396\n",
            "Loss training: 43.420753\n",
            "Loss training: 44.9671\n",
            "Loss training: 42.534027\n",
            "Loss training: 49.906425\n",
            "Loss training: 36.904144\n",
            "Loss training: 32.118183\n",
            "Loss training: 49.7731\n",
            "Loss training: 53.19382\n",
            "\n",
            "***************************\n",
            "Trained on 3260 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.34757\n",
            "Loss training: 32.928703\n",
            "Loss training: 20.639042\n",
            "Loss training: 50.957775\n",
            "Loss training: 73.4125\n",
            "Loss training: 38.405685\n",
            "Loss training: 14.941498\n",
            "Loss training: 29.409887\n",
            "Loss training: 46.817715\n",
            "Loss training: 36.439514\n",
            "\n",
            "***************************\n",
            "Trained on 3270 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.57575\n",
            "Loss training: 36.936\n",
            "Loss training: 29.851727\n",
            "Loss training: 43.30635\n",
            "Loss training: 51.557274\n",
            "Loss training: 49.909306\n",
            "Loss training: 47.48697\n",
            "Loss training: 26.51965\n",
            "Loss training: 34.541775\n",
            "Loss training: 44.20324\n",
            "\n",
            "***************************\n",
            "Trained on 3280 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.58032\n",
            "Loss training: 39.077053\n",
            "Loss training: 30.465443\n",
            "Loss training: 49.4098\n",
            "Loss training: 26.403343\n",
            "Loss training: 49.20882\n",
            "Loss training: 42.59953\n",
            "Loss training: 26.567698\n",
            "Loss training: 44.793915\n",
            "Loss training: 57.65423\n",
            "\n",
            "***************************\n",
            "Trained on 3290 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 48.362003\n",
            "Loss training: 35.656322\n",
            "Loss training: 31.367407\n",
            "Loss training: 46.00368\n",
            "Loss training: 31.57721\n",
            "Loss training: 47.074123\n",
            "Loss training: 31.45376\n",
            "Loss training: 44.792114\n",
            "Loss training: 36.83786\n",
            "Loss training: 32.49843\n",
            "\n",
            "***************************\n",
            "Trained on 3300 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 66.58882\n",
            "Loss training: 31.114998\n",
            "Loss training: 37.988594\n",
            "Loss training: 30.527193\n",
            "Loss training: 35.57976\n",
            "Loss training: 36.386993\n",
            "Loss training: 44.2345\n",
            "Loss training: 49.937805\n",
            "Loss training: 26.912935\n",
            "Loss training: 26.841717\n",
            "\n",
            "***************************\n",
            "Trained on 3310 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.057533\n",
            "Loss training: 34.63863\n",
            "Loss training: 31.79075\n",
            "Loss training: 33.11251\n",
            "Loss training: 37.902447\n",
            "Loss training: 30.403803\n",
            "Loss training: 40.071056\n",
            "Loss training: 42.412975\n",
            "Loss training: 42.16521\n",
            "Loss training: 46.029873\n",
            "\n",
            "***************************\n",
            "Trained on 3320 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.08175\n",
            "Loss training: 66.4899\n",
            "Loss training: 37.43729\n",
            "Loss training: 44.754395\n",
            "Loss training: 46.31451\n",
            "Loss training: 38.82045\n",
            "Loss training: 49.624786\n",
            "Loss training: 49.4329\n",
            "Loss training: 51.823082\n",
            "Loss training: 26.026472\n",
            "\n",
            "***************************\n",
            "Trained on 3330 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 43.972492\n",
            "Loss training: 37.64474\n",
            "Loss training: 30.90899\n",
            "Loss training: 28.469051\n",
            "Loss training: 14.384932\n",
            "Loss training: 37.641327\n",
            "Loss training: 34.352085\n",
            "Loss training: 44.478786\n",
            "Loss training: 25.84315\n",
            "Loss training: 32.17974\n",
            "\n",
            "***************************\n",
            "Trained on 3340 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.09245\n",
            "Loss training: 25.86853\n",
            "Loss training: 30.357054\n",
            "Loss training: 45.689262\n",
            "Loss training: 44.04686\n",
            "Loss training: 70.9184\n",
            "Loss training: 31.573833\n",
            "Loss training: 51.207546\n",
            "Loss training: 15.45254\n",
            "Loss training: 32.304832\n",
            "\n",
            "***************************\n",
            "Trained on 3350 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 15.54229\n",
            "Loss training: 33.581985\n",
            "Loss training: 56.496407\n",
            "Loss training: 35.915947\n",
            "Loss training: 14.359274\n",
            "Loss training: 35.692596\n",
            "Loss training: 37.070053\n",
            "Loss training: 39.39505\n",
            "Loss training: 31.100273\n",
            "Loss training: 26.347506\n",
            "\n",
            "***************************\n",
            "Trained on 3360 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.261045\n",
            "Loss training: 48.21111\n",
            "Loss training: 31.408829\n",
            "Loss training: 21.18788\n",
            "Loss training: 50.61209\n",
            "Loss training: 56.648308\n",
            "Loss training: 37.798695\n",
            "Loss training: 56.061314\n",
            "Loss training: 35.659016\n",
            "Loss training: 26.170818\n",
            "\n",
            "***************************\n",
            "Trained on 3370 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.457844\n",
            "Loss training: 34.36382\n",
            "Loss training: 44.73999\n",
            "Loss training: 43.86075\n",
            "Loss training: 35.031456\n",
            "Loss training: 56.813046\n",
            "Loss training: 32.009552\n",
            "Loss training: 38.993427\n",
            "Loss training: 31.476711\n",
            "Loss training: 46.326057\n",
            "\n",
            "***************************\n",
            "Trained on 3380 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.38288\n",
            "Loss training: 41.7555\n",
            "Loss training: 54.477707\n",
            "Loss training: 66.2499\n",
            "Loss training: 35.102222\n",
            "Loss training: 41.937458\n",
            "Loss training: 38.55227\n",
            "Loss training: 47.224102\n",
            "Loss training: 36.267727\n",
            "Loss training: 36.17961\n",
            "\n",
            "***************************\n",
            "Trained on 3390 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.422554\n",
            "Loss training: 48.531944\n",
            "Loss training: 50.565804\n",
            "Loss training: 28.90879\n",
            "Loss training: 26.254341\n",
            "Loss training: 49.65191\n",
            "Loss training: 28.442936\n",
            "Loss training: 35.822697\n",
            "Loss training: 50.535187\n",
            "Loss training: 41.102867\n",
            "\n",
            "***************************\n",
            "Trained on 3400 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 65.17854\n",
            "Loss training: 46.530724\n",
            "Loss training: 46.38644\n",
            "Loss training: 30.304646\n",
            "Loss training: 34.636436\n",
            "Loss training: 15.182199\n",
            "Loss training: 35.492954\n",
            "Loss training: 44.59197\n",
            "Loss training: 31.210314\n",
            "Loss training: 51.30436\n",
            "\n",
            "***************************\n",
            "Trained on 3410 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.880438\n",
            "Loss training: 38.963127\n",
            "Loss training: 46.49837\n",
            "Loss training: 41.428368\n",
            "Loss training: 14.591421\n",
            "Loss training: 33.702515\n",
            "Loss training: 50.86535\n",
            "Loss training: 30.288187\n",
            "Loss training: 62.029648\n",
            "Loss training: 27.23657\n",
            "\n",
            "***************************\n",
            "Trained on 3420 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.641418\n",
            "Loss training: 29.653278\n",
            "Loss training: 38.140114\n",
            "Loss training: 38.032887\n",
            "Loss training: 45.236126\n",
            "Loss training: 50.377563\n",
            "Loss training: 25.710562\n",
            "Loss training: 47.824352\n",
            "Loss training: 34.36604\n",
            "Loss training: 35.47099\n",
            "\n",
            "***************************\n",
            "Trained on 3430 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 49.34078\n",
            "Loss training: 50.783173\n",
            "Loss training: 37.755672\n",
            "Loss training: 50.381115\n",
            "Loss training: 31.18227\n",
            "Loss training: 33.758816\n",
            "Loss training: 45.784256\n",
            "Loss training: 30.991148\n",
            "Loss training: 32.324615\n",
            "Loss training: 42.29873\n",
            "\n",
            "***************************\n",
            "Trained on 3440 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.256065\n",
            "Loss training: 36.188606\n",
            "Loss training: 47.804127\n",
            "Loss training: 31.137072\n",
            "Loss training: 41.306644\n",
            "Loss training: 42.50881\n",
            "Loss training: 29.289278\n",
            "Loss training: 26.766245\n",
            "Loss training: 33.349297\n",
            "Loss training: 51.80251\n",
            "\n",
            "***************************\n",
            "Trained on 3450 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 13.729313\n",
            "Loss training: 47.41533\n",
            "Loss training: 27.69694\n",
            "Loss training: 37.874035\n",
            "Loss training: 32.83342\n",
            "Loss training: 26.642754\n",
            "Loss training: 36.300404\n",
            "Loss training: 63.033\n",
            "Loss training: 35.95861\n",
            "Loss training: 32.355865\n",
            "\n",
            "***************************\n",
            "Trained on 3460 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 47.98943\n",
            "Loss training: 28.948582\n",
            "Loss training: 35.020874\n",
            "Loss training: 38.729053\n",
            "Loss training: 32.351227\n",
            "Loss training: 15.137216\n",
            "Loss training: 42.912876\n",
            "Loss training: 27.316301\n",
            "Loss training: 66.87663\n",
            "Loss training: 15.179221\n",
            "\n",
            "***************************\n",
            "Trained on 3470 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.873487\n",
            "Loss training: 60.349003\n",
            "Loss training: 27.550129\n",
            "Loss training: 56.318184\n",
            "Loss training: 46.97176\n",
            "Loss training: 57.362503\n",
            "Loss training: 30.898226\n",
            "Loss training: 42.523605\n",
            "Loss training: 44.193718\n",
            "Loss training: 14.148305\n",
            "\n",
            "***************************\n",
            "Trained on 3480 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.176655\n",
            "Loss training: 36.703625\n",
            "Loss training: 33.712692\n",
            "Loss training: 31.916039\n",
            "Loss training: 48.7511\n",
            "Loss training: 47.90327\n",
            "Loss training: 38.618587\n",
            "Loss training: 34.41332\n",
            "Loss training: 48.293583\n",
            "Loss training: 39.390865\n",
            "\n",
            "***************************\n",
            "Trained on 3490 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.078228\n",
            "Loss training: 31.433296\n",
            "Loss training: 33.33397\n",
            "Loss training: 46.70048\n",
            "Loss training: 52.002827\n",
            "Loss training: 35.87242\n",
            "Loss training: 45.176403\n",
            "Loss training: 28.58892\n",
            "Loss training: 33.036915\n",
            "Loss training: 46.33961\n",
            "\n",
            "***************************\n",
            "Trained on 3500 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.399986\n",
            "Loss training: 30.894163\n",
            "Loss training: 13.829045\n",
            "Loss training: 31.08777\n",
            "Loss training: 74.88905\n",
            "Loss training: 42.987225\n",
            "Loss training: 38.744312\n",
            "Loss training: 30.701424\n",
            "Loss training: 52.706898\n",
            "Loss training: 34.36088\n",
            "\n",
            "***************************\n",
            "Trained on 3510 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 46.25243\n",
            "Loss training: 46.61983\n",
            "Loss training: 29.79687\n",
            "Loss training: 28.286102\n",
            "Loss training: 43.856094\n",
            "Loss training: 51.377125\n",
            "Loss training: 34.985012\n",
            "Loss training: 32.47554\n",
            "Loss training: 43.9459\n",
            "Loss training: 26.889141\n",
            "\n",
            "***************************\n",
            "Trained on 3520 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.857578\n",
            "Loss training: 43.50474\n",
            "Loss training: 63.279312\n",
            "Loss training: 29.263357\n",
            "Loss training: 20.09015\n",
            "Loss training: 25.901892\n",
            "Loss training: 30.722351\n",
            "Loss training: 35.967613\n",
            "Loss training: 35.46043\n",
            "Loss training: 29.407824\n",
            "\n",
            "***************************\n",
            "Trained on 3530 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.997883\n",
            "Loss training: 30.710363\n",
            "Loss training: 45.095848\n",
            "Loss training: 33.417236\n",
            "Loss training: 51.317284\n",
            "Loss training: 43.00872\n",
            "Loss training: 39.768356\n",
            "Loss training: 43.096462\n",
            "Loss training: 46.125698\n",
            "Loss training: 44.907818\n",
            "\n",
            "***************************\n",
            "Trained on 3540 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.438793\n",
            "Loss training: 35.800236\n",
            "Loss training: 33.181873\n",
            "Loss training: 14.002931\n",
            "Loss training: 42.754776\n",
            "Loss training: 27.553993\n",
            "Loss training: 15.225057\n",
            "Loss training: 51.310093\n",
            "Loss training: 45.21775\n",
            "Loss training: 26.619907\n",
            "\n",
            "***************************\n",
            "Trained on 3550 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.450657\n",
            "Loss training: 20.23925\n",
            "Loss training: 49.029778\n",
            "Loss training: 46.07581\n",
            "Loss training: 27.809114\n",
            "Loss training: 31.84932\n",
            "Loss training: 27.781282\n",
            "Loss training: 25.940353\n",
            "Loss training: 34.211624\n",
            "Loss training: 42.776085\n",
            "\n",
            "***************************\n",
            "Trained on 3560 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.230835\n",
            "Loss training: 45.61133\n",
            "Loss training: 31.99329\n",
            "Loss training: 53.361473\n",
            "Loss training: 25.890167\n",
            "Loss training: 49.321377\n",
            "Loss training: 37.471096\n",
            "Loss training: 56.462128\n",
            "Loss training: 45.755608\n",
            "Loss training: 25.654701\n",
            "\n",
            "***************************\n",
            "Trained on 3570 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.193687\n",
            "Loss training: 39.471535\n",
            "Loss training: 43.624947\n",
            "Loss training: 12.865\n",
            "Loss training: 31.55324\n",
            "Loss training: 32.635044\n",
            "Loss training: 37.14666\n",
            "Loss training: 44.068546\n",
            "Loss training: 42.00987\n",
            "Loss training: 37.621742\n",
            "\n",
            "***************************\n",
            "Trained on 3580 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.87867\n",
            "Loss training: 30.143782\n",
            "Loss training: 33.224873\n",
            "Loss training: 56.27753\n",
            "Loss training: 29.86259\n",
            "Loss training: 45.443424\n",
            "Loss training: 29.806778\n",
            "Loss training: 51.98913\n",
            "Loss training: 50.8251\n",
            "Loss training: 45.005215\n",
            "\n",
            "***************************\n",
            "Trained on 3590 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.642357\n",
            "Loss training: 31.775217\n",
            "Loss training: 37.757915\n",
            "Loss training: 30.380495\n",
            "Loss training: 26.1625\n",
            "Loss training: 45.24449\n",
            "Loss training: 27.768187\n",
            "Loss training: 47.47605\n",
            "Loss training: 34.280323\n",
            "Loss training: 13.050525\n",
            "\n",
            "***************************\n",
            "Trained on 3600 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.008152\n",
            "Loss training: 33.305424\n",
            "Loss training: 40.662563\n",
            "Loss training: 42.643215\n",
            "Loss training: 38.590343\n",
            "Loss training: 19.494\n",
            "Loss training: 51.723305\n",
            "Loss training: 36.794456\n",
            "Loss training: 34.793324\n",
            "Loss training: 32.746376\n",
            "\n",
            "***************************\n",
            "Trained on 3610 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.32837\n",
            "Loss training: 26.005585\n",
            "Loss training: 43.254093\n",
            "Loss training: 12.817708\n",
            "Loss training: 64.56325\n",
            "Loss training: 49.516388\n",
            "Loss training: 31.592478\n",
            "Loss training: 42.43681\n",
            "Loss training: 51.86287\n",
            "Loss training: 38.42384\n",
            "\n",
            "***************************\n",
            "Trained on 3620 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.98568\n",
            "Loss training: 14.812377\n",
            "Loss training: 36.23106\n",
            "Loss training: 45.551224\n",
            "Loss training: 48.32397\n",
            "Loss training: 31.649925\n",
            "Loss training: 49.532734\n",
            "Loss training: 45.490982\n",
            "Loss training: 40.922283\n",
            "Loss training: 46.32056\n",
            "\n",
            "***************************\n",
            "Trained on 3630 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.84151\n",
            "Loss training: 37.99086\n",
            "Loss training: 26.086435\n",
            "Loss training: 42.341415\n",
            "Loss training: 45.342262\n",
            "Loss training: 33.946312\n",
            "Loss training: 62.97231\n",
            "Loss training: 37.465935\n",
            "Loss training: 29.996143\n",
            "Loss training: 41.406612\n",
            "\n",
            "***************************\n",
            "Trained on 3640 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 46.55056\n",
            "Loss training: 51.497765\n",
            "Loss training: 35.618046\n",
            "Loss training: 19.839409\n",
            "Loss training: 37.496403\n",
            "Loss training: 46.42158\n",
            "Loss training: 31.162481\n",
            "Loss training: 39.320736\n",
            "Loss training: 32.0962\n",
            "Loss training: 45.16454\n",
            "\n",
            "***************************\n",
            "Trained on 3650 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.961695\n",
            "Loss training: 63.42479\n",
            "Loss training: 32.116722\n",
            "Loss training: 43.543205\n",
            "Loss training: 33.512733\n",
            "Loss training: 51.89799\n",
            "Loss training: 57.336643\n",
            "Loss training: 43.357635\n",
            "Loss training: 38.962215\n",
            "Loss training: 34.91047\n",
            "\n",
            "***************************\n",
            "Trained on 3660 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.516708\n",
            "Loss training: 28.327044\n",
            "Loss training: 28.037958\n",
            "Loss training: 35.11654\n",
            "Loss training: 42.625877\n",
            "Loss training: 31.707123\n",
            "Loss training: 37.694904\n",
            "Loss training: 34.542725\n",
            "Loss training: 27.725399\n",
            "Loss training: 28.006245\n",
            "\n",
            "***************************\n",
            "Trained on 3670 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.985397\n",
            "Loss training: 61.08138\n",
            "Loss training: 28.091852\n",
            "Loss training: 48.32078\n",
            "Loss training: 61.26974\n",
            "Loss training: 37.182034\n",
            "Loss training: 27.986795\n",
            "Loss training: 31.440565\n",
            "Loss training: 36.064804\n",
            "Loss training: 37.92753\n",
            "\n",
            "***************************\n",
            "Trained on 3680 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.80287\n",
            "Loss training: 50.65303\n",
            "Loss training: 45.93488\n",
            "Loss training: 43.580124\n",
            "Loss training: 59.275772\n",
            "Loss training: 22.28367\n",
            "Loss training: 35.646248\n",
            "Loss training: 21.747868\n",
            "Loss training: 45.65681\n",
            "Loss training: 35.502125\n",
            "\n",
            "***************************\n",
            "Trained on 3690 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 54.038166\n",
            "Loss training: 48.281677\n",
            "Loss training: 51.845806\n",
            "Loss training: 38.76855\n",
            "Loss training: 48.09587\n",
            "Loss training: 35.02188\n",
            "Loss training: 55.851894\n",
            "Loss training: 49.63032\n",
            "Loss training: 45.599358\n",
            "Loss training: 42.11221\n",
            "\n",
            "***************************\n",
            "Trained on 3700 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.546133\n",
            "Loss training: 134.52522\n",
            "Loss training: 44.43101\n",
            "Loss training: 29.099339\n",
            "Loss training: 26.634434\n",
            "Loss training: 25.76771\n",
            "Loss training: 51.20588\n",
            "Loss training: 46.23511\n",
            "Loss training: 30.011688\n",
            "Loss training: 33.728535\n",
            "\n",
            "***************************\n",
            "Trained on 3710 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 57.15099\n",
            "Loss training: 36.93705\n",
            "Loss training: 102.7148\n",
            "Loss training: 40.07906\n",
            "Loss training: 44.774635\n",
            "Loss training: 21.252348\n",
            "Loss training: 36.208908\n",
            "Loss training: 20.231745\n",
            "Loss training: 52.519012\n",
            "Loss training: 34.0848\n",
            "\n",
            "***************************\n",
            "Trained on 3720 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 43.03441\n",
            "Loss training: 47.241135\n",
            "Loss training: 54.22912\n",
            "Loss training: 46.566692\n",
            "Loss training: 27.523434\n",
            "Loss training: 58.184113\n",
            "Loss training: 46.515263\n",
            "Loss training: 50.813007\n",
            "Loss training: 46.16351\n",
            "Loss training: 45.720863\n",
            "\n",
            "***************************\n",
            "Trained on 3730 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 54.864624\n",
            "Loss training: 52.101295\n",
            "Loss training: 52.51205\n",
            "Loss training: 48.667606\n",
            "Loss training: 28.233719\n",
            "Loss training: 70.30809\n",
            "Loss training: 46.245842\n",
            "Loss training: 31.693266\n",
            "Loss training: 32.202564\n",
            "Loss training: 45.923973\n",
            "\n",
            "***************************\n",
            "Trained on 3740 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.520994\n",
            "Loss training: 32.90906\n",
            "Loss training: 53.05979\n",
            "Loss training: 32.553852\n",
            "Loss training: 46.037216\n",
            "Loss training: 58.589878\n",
            "Loss training: 39.576138\n",
            "Loss training: 46.258873\n",
            "Loss training: 43.106026\n",
            "Loss training: 50.836567\n",
            "\n",
            "***************************\n",
            "Trained on 3750 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 56.399895\n",
            "Loss training: 51.587234\n",
            "Loss training: 24.304482\n",
            "Loss training: 59.250286\n",
            "Loss training: 50.13419\n",
            "Loss training: 44.059525\n",
            "Loss training: 32.11284\n",
            "Loss training: 47.654167\n",
            "Loss training: 56.50225\n",
            "Loss training: 37.332394\n",
            "\n",
            "***************************\n",
            "Trained on 3760 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 46.143032\n",
            "Loss training: 34.039318\n",
            "Loss training: 36.481026\n",
            "Loss training: 32.97872\n",
            "Loss training: 42.676224\n",
            "Loss training: 64.27153\n",
            "Loss training: 46.941795\n",
            "Loss training: 50.1129\n",
            "Loss training: 32.33994\n",
            "Loss training: 45.240986\n",
            "\n",
            "***************************\n",
            "Trained on 3770 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.056675\n",
            "Loss training: 51.652096\n",
            "Loss training: 42.27458\n",
            "Loss training: 53.397495\n",
            "Loss training: 50.82254\n",
            "Loss training: 33.157146\n",
            "Loss training: 44.7299\n",
            "Loss training: 46.093826\n",
            "Loss training: 39.57764\n",
            "Loss training: 50.552853\n",
            "\n",
            "***************************\n",
            "Trained on 3780 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.918907\n",
            "Loss training: 36.23911\n",
            "Loss training: 16.515783\n",
            "Loss training: 39.805313\n",
            "Loss training: 47.917526\n",
            "Loss training: 58.476303\n",
            "Loss training: 28.199465\n",
            "Loss training: 55.871708\n",
            "Loss training: 45.873615\n",
            "Loss training: 34.28872\n",
            "\n",
            "***************************\n",
            "Trained on 3790 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.36872\n",
            "Loss training: 45.567574\n",
            "Loss training: 26.998901\n",
            "Loss training: 50.375256\n",
            "Loss training: 42.918137\n",
            "Loss training: 33.06817\n",
            "Loss training: 46.290134\n",
            "Loss training: 30.76447\n",
            "Loss training: 25.920408\n",
            "Loss training: 51.370346\n",
            "\n",
            "***************************\n",
            "Trained on 3800 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.574492\n",
            "Loss training: 48.265354\n",
            "Loss training: 55.908073\n",
            "Loss training: 12.604768\n",
            "Loss training: 41.647556\n",
            "Loss training: 45.670906\n",
            "Loss training: 13.26149\n",
            "Loss training: 44.247833\n",
            "Loss training: 47.89283\n",
            "Loss training: 51.65533\n",
            "\n",
            "***************************\n",
            "Trained on 3810 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.54373\n",
            "Loss training: 31.8453\n",
            "Loss training: 53.53876\n",
            "Loss training: 26.944088\n",
            "Loss training: 14.178171\n",
            "Loss training: 35.819355\n",
            "Loss training: 26.676456\n",
            "Loss training: 45.22273\n",
            "Loss training: 37.686802\n",
            "Loss training: 49.311592\n",
            "\n",
            "***************************\n",
            "Trained on 3820 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 48.9967\n",
            "Loss training: 35.22237\n",
            "Loss training: 14.771668\n",
            "Loss training: 37.448532\n",
            "Loss training: 43.120094\n",
            "Loss training: 36.091354\n",
            "Loss training: 36.18976\n",
            "Loss training: 47.358463\n",
            "Loss training: 43.001167\n",
            "Loss training: 31.69489\n",
            "\n",
            "***************************\n",
            "Trained on 3830 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.24636\n",
            "Loss training: 28.325077\n",
            "Loss training: 50.194004\n",
            "Loss training: 36.749783\n",
            "Loss training: 46.48593\n",
            "Loss training: 50.446\n",
            "Loss training: 35.102226\n",
            "Loss training: 53.523994\n",
            "Loss training: 39.61119\n",
            "Loss training: 30.749434\n",
            "\n",
            "***************************\n",
            "Trained on 3840 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 57.911297\n",
            "Loss training: 31.299788\n",
            "Loss training: 41.775833\n",
            "Loss training: 43.62918\n",
            "Loss training: 35.37145\n",
            "Loss training: 46.8664\n",
            "Loss training: 30.98781\n",
            "Loss training: 45.48511\n",
            "Loss training: 33.142487\n",
            "Loss training: 43.70694\n",
            "\n",
            "***************************\n",
            "Trained on 3850 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 46.415073\n",
            "Loss training: 55.95532\n",
            "Loss training: 46.237373\n",
            "Loss training: 33.67309\n",
            "Loss training: 26.344643\n",
            "Loss training: 50.209812\n",
            "Loss training: 20.687567\n",
            "Loss training: 34.952217\n",
            "Loss training: 33.34815\n",
            "Loss training: 70.486855\n",
            "\n",
            "***************************\n",
            "Trained on 3860 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.99015\n",
            "Loss training: 14.588086\n",
            "Loss training: 41.553616\n",
            "Loss training: 37.355385\n",
            "Loss training: 26.281078\n",
            "Loss training: 49.40133\n",
            "Loss training: 42.998222\n",
            "Loss training: 51.046307\n",
            "Loss training: 31.187248\n",
            "Loss training: 58.245472\n",
            "\n",
            "***************************\n",
            "Trained on 3870 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.565857\n",
            "Loss training: 27.696419\n",
            "Loss training: 43.329098\n",
            "Loss training: 56.90076\n",
            "Loss training: 43.02527\n",
            "Loss training: 26.504156\n",
            "Loss training: 34.400173\n",
            "Loss training: 26.62167\n",
            "Loss training: 46.861797\n",
            "Loss training: 30.865004\n",
            "\n",
            "***************************\n",
            "Trained on 3880 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.909973\n",
            "Loss training: 38.757156\n",
            "Loss training: 25.654932\n",
            "Loss training: 66.759705\n",
            "Loss training: 38.6176\n",
            "Loss training: 63.77385\n",
            "Loss training: 35.930737\n",
            "Loss training: 53.585976\n",
            "Loss training: 36.55778\n",
            "Loss training: 39.051716\n",
            "\n",
            "***************************\n",
            "Trained on 3890 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.31389\n",
            "Loss training: 38.938446\n",
            "Loss training: 12.410696\n",
            "Loss training: 27.73945\n",
            "Loss training: 11.973889\n",
            "Loss training: 32.075085\n",
            "Loss training: 32.401154\n",
            "Loss training: 30.864517\n",
            "Loss training: 16.472267\n",
            "Loss training: 36.003086\n",
            "\n",
            "***************************\n",
            "Trained on 3900 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.273373\n",
            "Loss training: 31.120752\n",
            "Loss training: 15.728114\n",
            "Loss training: 36.169014\n",
            "Loss training: 51.201984\n",
            "Loss training: 50.13859\n",
            "Loss training: 44.780758\n",
            "Loss training: 30.721836\n",
            "Loss training: 26.75783\n",
            "Loss training: 32.783234\n",
            "\n",
            "***************************\n",
            "Trained on 3910 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.117416\n",
            "Loss training: 47.619793\n",
            "Loss training: 21.717724\n",
            "Loss training: 20.936829\n",
            "Loss training: 32.707138\n",
            "Loss training: 57.99548\n",
            "Loss training: 25.66024\n",
            "Loss training: 56.90506\n",
            "Loss training: 34.349\n",
            "Loss training: 30.84134\n",
            "\n",
            "***************************\n",
            "Trained on 3920 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 70.5368\n",
            "Loss training: 30.676275\n",
            "Loss training: 45.127537\n",
            "Loss training: 54.224903\n",
            "Loss training: 44.302265\n",
            "Loss training: 35.65408\n",
            "Loss training: 30.784918\n",
            "Loss training: 31.90703\n",
            "Loss training: 51.49595\n",
            "Loss training: 44.28676\n",
            "\n",
            "***************************\n",
            "Trained on 3930 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 51.70113\n",
            "Loss training: 30.656816\n",
            "Loss training: 51.31066\n",
            "Loss training: 28.974371\n",
            "Loss training: 34.109615\n",
            "Loss training: 54.82149\n",
            "Loss training: 51.35108\n",
            "Loss training: 32.805424\n",
            "Loss training: 36.285778\n",
            "Loss training: 28.901924\n",
            "\n",
            "***************************\n",
            "Trained on 3940 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.777866\n",
            "Loss training: 44.15886\n",
            "Loss training: 46.399406\n",
            "Loss training: 35.82015\n",
            "Loss training: 41.225754\n",
            "Loss training: 38.692574\n",
            "Loss training: 44.357567\n",
            "Loss training: 53.230797\n",
            "Loss training: 43.134262\n",
            "Loss training: 30.569084\n",
            "\n",
            "***************************\n",
            "Trained on 3950 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.048521\n",
            "Loss training: 38.843777\n",
            "Loss training: 31.32837\n",
            "Loss training: 38.373703\n",
            "Loss training: 57.92805\n",
            "Loss training: 34.588486\n",
            "Loss training: 59.04957\n",
            "Loss training: 64.785446\n",
            "Loss training: 25.747599\n",
            "Loss training: 33.65678\n",
            "\n",
            "***************************\n",
            "Trained on 3960 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.087215\n",
            "Loss training: 50.80939\n",
            "Loss training: 34.25394\n",
            "Loss training: 54.376408\n",
            "Loss training: 55.415394\n",
            "Loss training: 54.885788\n",
            "Loss training: 35.795536\n",
            "Loss training: 39.22847\n",
            "Loss training: 50.201694\n",
            "Loss training: 36.72029\n",
            "\n",
            "***************************\n",
            "Trained on 3970 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.65953\n",
            "Loss training: 36.341396\n",
            "Loss training: 49.70817\n",
            "Loss training: 31.42483\n",
            "Loss training: 52.1803\n",
            "Loss training: 50.933\n",
            "Loss training: 35.564594\n",
            "Loss training: 63.784126\n",
            "Loss training: 47.103416\n",
            "Loss training: 42.392128\n",
            "\n",
            "***************************\n",
            "Trained on 3980 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.687946\n",
            "Loss training: 47.24365\n",
            "Loss training: 60.14371\n",
            "Loss training: 26.675564\n",
            "Loss training: 30.446321\n",
            "Loss training: 43.439064\n",
            "Loss training: 26.913357\n",
            "Loss training: 38.35202\n",
            "Loss training: 28.804487\n",
            "Loss training: 45.953136\n",
            "\n",
            "***************************\n",
            "Trained on 3990 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.621738\n",
            "Loss training: 30.990192\n",
            "Loss training: 37.492558\n",
            "Loss training: 31.828749\n",
            "Loss training: 44.283756\n",
            "Loss training: 51.096767\n",
            "Loss training: 32.828476\n",
            "Loss training: 30.387768\n",
            "Loss training: 49.81373\n",
            "Loss training: 31.126535\n",
            "\n",
            "***************************\n",
            "Trained on 4000 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.436882\n",
            "Loss training: 37.82295\n",
            "Loss training: 30.81048\n",
            "Loss training: 49.508038\n",
            "Loss training: 48.593895\n",
            "Loss training: 32.36234\n",
            "Loss training: 68.944\n",
            "Loss training: 44.6074\n",
            "Loss training: 35.18868\n",
            "Loss training: 35.46296\n",
            "\n",
            "***************************\n",
            "Trained on 4010 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.508116\n",
            "Loss training: 30.510601\n",
            "Loss training: 26.017921\n",
            "Loss training: 49.43508\n",
            "Loss training: 12.566633\n",
            "Loss training: 45.819065\n",
            "Loss training: 33.23669\n",
            "Loss training: 30.503386\n",
            "Loss training: 47.57502\n",
            "Loss training: 30.387577\n",
            "\n",
            "***************************\n",
            "Trained on 4020 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 49.523666\n",
            "Loss training: 25.334238\n",
            "Loss training: 59.749073\n",
            "Loss training: 46.748207\n",
            "Loss training: 37.207672\n",
            "Loss training: 50.359104\n",
            "Loss training: 29.06417\n",
            "Loss training: 50.011505\n",
            "Loss training: 59.583614\n",
            "Loss training: 54.100227\n",
            "\n",
            "***************************\n",
            "Trained on 4030 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 57.997135\n",
            "Loss training: 31.967949\n",
            "Loss training: 44.185055\n",
            "Loss training: 46.185516\n",
            "Loss training: 35.582428\n",
            "Loss training: 41.28262\n",
            "Loss training: 46.325737\n",
            "Loss training: 35.682747\n",
            "Loss training: 46.658733\n",
            "Loss training: 28.221725\n",
            "\n",
            "***************************\n",
            "Trained on 4040 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 43.032448\n",
            "Loss training: 26.347359\n",
            "Loss training: 30.557217\n",
            "Loss training: 37.06568\n",
            "Loss training: 77.70702\n",
            "Loss training: 57.245026\n",
            "Loss training: 51.848015\n",
            "Loss training: 66.5505\n",
            "Loss training: 55.655552\n",
            "Loss training: 35.530434\n",
            "\n",
            "***************************\n",
            "Trained on 4050 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.453682\n",
            "Loss training: 55.12858\n",
            "Loss training: 39.50718\n",
            "Loss training: 49.54815\n",
            "Loss training: 36.172916\n",
            "Loss training: 44.20583\n",
            "Loss training: 40.530193\n",
            "Loss training: 52.3886\n",
            "Loss training: 13.223338\n",
            "Loss training: 50.595432\n",
            "\n",
            "***************************\n",
            "Trained on 4060 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.701668\n",
            "Loss training: 47.28116\n",
            "Loss training: 48.939957\n",
            "Loss training: 34.641617\n",
            "Loss training: 26.538868\n",
            "Loss training: 44.33192\n",
            "Loss training: 24.95922\n",
            "Loss training: 37.10376\n",
            "Loss training: 53.876366\n",
            "Loss training: 46.807854\n",
            "\n",
            "***************************\n",
            "Trained on 4070 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 61.488396\n",
            "Loss training: 35.40396\n",
            "Loss training: 27.876417\n",
            "Loss training: 59.299515\n",
            "Loss training: 46.24242\n",
            "Loss training: 48.79043\n",
            "Loss training: 49.236206\n",
            "Loss training: 44.856163\n",
            "Loss training: 24.921604\n",
            "Loss training: 61.821636\n",
            "\n",
            "***************************\n",
            "Trained on 4080 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.01062\n",
            "Loss training: 33.73048\n",
            "Loss training: 44.411865\n",
            "Loss training: 33.690765\n",
            "Loss training: 31.349638\n",
            "Loss training: 28.160711\n",
            "Loss training: 63.404972\n",
            "Loss training: 26.195276\n",
            "Loss training: 26.11169\n",
            "Loss training: 53.263443\n",
            "\n",
            "***************************\n",
            "Trained on 4090 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.198833\n",
            "Loss training: 37.857162\n",
            "Loss training: 42.028316\n",
            "Loss training: 32.355698\n",
            "Loss training: 15.455503\n",
            "Loss training: 37.83316\n",
            "Loss training: 36.14241\n",
            "Loss training: 31.016638\n",
            "Loss training: 44.659836\n",
            "Loss training: 33.729984\n",
            "\n",
            "***************************\n",
            "Trained on 4100 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 54.122044\n",
            "Loss training: 57.055195\n",
            "Loss training: 33.890194\n",
            "Loss training: 28.032913\n",
            "Loss training: 55.28892\n",
            "Loss training: 31.729366\n",
            "Loss training: 38.912956\n",
            "Loss training: 39.83371\n",
            "Loss training: 35.86728\n",
            "Loss training: 39.208862\n",
            "\n",
            "***************************\n",
            "Trained on 4110 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.53287\n",
            "Loss training: 42.35374\n",
            "Loss training: 13.235062\n",
            "Loss training: 54.64083\n",
            "Loss training: 31.313526\n",
            "Loss training: 36.011192\n",
            "Loss training: 26.760817\n",
            "Loss training: 35.98539\n",
            "Loss training: 40.214424\n",
            "Loss training: 40.196674\n",
            "\n",
            "***************************\n",
            "Trained on 4120 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.68067\n",
            "Loss training: 34.71039\n",
            "Loss training: 44.239513\n",
            "Loss training: 35.056286\n",
            "Loss training: 54.52586\n",
            "Loss training: 36.59904\n",
            "Loss training: 30.682661\n",
            "Loss training: 45.89534\n",
            "Loss training: 40.182693\n",
            "Loss training: 34.69494\n",
            "\n",
            "***************************\n",
            "Trained on 4130 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 46.148872\n",
            "Loss training: 28.836462\n",
            "Loss training: 50.276836\n",
            "Loss training: 26.112951\n",
            "Loss training: 41.09564\n",
            "Loss training: 40.56411\n",
            "Loss training: 43.844326\n",
            "Loss training: 34.74463\n",
            "Loss training: 30.278954\n",
            "Loss training: 30.968138\n",
            "\n",
            "***************************\n",
            "Trained on 4140 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 51.717625\n",
            "Loss training: 43.71034\n",
            "Loss training: 44.581573\n",
            "Loss training: 31.174822\n",
            "Loss training: 46.59261\n",
            "Loss training: 35.65504\n",
            "Loss training: 47.006382\n",
            "Loss training: 42.35639\n",
            "Loss training: 37.398163\n",
            "Loss training: 20.14483\n",
            "\n",
            "***************************\n",
            "Trained on 4150 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.36208\n",
            "Loss training: 43.9967\n",
            "Loss training: 27.178772\n",
            "Loss training: 71.63565\n",
            "Loss training: 67.256645\n",
            "Loss training: 30.68356\n",
            "Loss training: 37.524376\n",
            "Loss training: 58.75761\n",
            "Loss training: 35.560837\n",
            "Loss training: 31.001684\n",
            "\n",
            "***************************\n",
            "Trained on 4160 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.86701\n",
            "Loss training: 40.545242\n",
            "Loss training: 52.174473\n",
            "Loss training: 28.100512\n",
            "Loss training: 35.97273\n",
            "Loss training: 44.050858\n",
            "Loss training: 58.785366\n",
            "Loss training: 28.03415\n",
            "Loss training: 42.478325\n",
            "Loss training: 48.609684\n",
            "\n",
            "***************************\n",
            "Trained on 4170 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.27984\n",
            "Loss training: 36.866432\n",
            "Loss training: 29.840242\n",
            "Loss training: 31.054996\n",
            "Loss training: 29.42027\n",
            "Loss training: 37.624035\n",
            "Loss training: 20.501455\n",
            "Loss training: 33.973286\n",
            "Loss training: 48.132816\n",
            "Loss training: 44.521534\n",
            "\n",
            "***************************\n",
            "Trained on 4180 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.62205\n",
            "Loss training: 27.596613\n",
            "Loss training: 47.621468\n",
            "Loss training: 61.021893\n",
            "Loss training: 60.31405\n",
            "Loss training: 29.525835\n",
            "Loss training: 42.88145\n",
            "Loss training: 47.107624\n",
            "Loss training: 25.9344\n",
            "Loss training: 28.722849\n",
            "\n",
            "***************************\n",
            "Trained on 4190 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 47.312057\n",
            "Loss training: 36.4659\n",
            "Loss training: 26.127745\n",
            "Loss training: 57.119545\n",
            "Loss training: 39.4964\n",
            "Loss training: 52.98864\n",
            "Loss training: 50.8639\n",
            "Loss training: 32.710293\n",
            "Loss training: 28.048386\n",
            "Loss training: 45.496445\n",
            "\n",
            "***************************\n",
            "Trained on 4200 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 51.286068\n",
            "Loss training: 39.063313\n",
            "Loss training: 30.446754\n",
            "Loss training: 36.192112\n",
            "Loss training: 38.206333\n",
            "Loss training: 35.56987\n",
            "Loss training: 49.836365\n",
            "Loss training: 52.279125\n",
            "Loss training: 45.769573\n",
            "Loss training: 55.703243\n",
            "\n",
            "***************************\n",
            "Trained on 4210 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.641006\n",
            "Loss training: 31.536087\n",
            "Loss training: 42.116768\n",
            "Loss training: 13.491792\n",
            "Loss training: 44.27437\n",
            "Loss training: 62.465065\n",
            "Loss training: 43.897743\n",
            "Loss training: 37.030823\n",
            "Loss training: 13.164042\n",
            "Loss training: 34.034214\n",
            "\n",
            "***************************\n",
            "Trained on 4220 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.027702\n",
            "Loss training: 32.19712\n",
            "Loss training: 37.25398\n",
            "Loss training: 40.557392\n",
            "Loss training: 51.828754\n",
            "Loss training: 34.190903\n",
            "Loss training: 46.241993\n",
            "Loss training: 31.96151\n",
            "Loss training: 33.98492\n",
            "Loss training: 43.290527\n",
            "\n",
            "***************************\n",
            "Trained on 4230 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 19.601547\n",
            "Loss training: 48.728893\n",
            "Loss training: 37.314774\n",
            "Loss training: 38.839603\n",
            "Loss training: 61.49078\n",
            "Loss training: 13.410852\n",
            "Loss training: 36.20177\n",
            "Loss training: 45.5142\n",
            "Loss training: 48.948788\n",
            "Loss training: 46.848087\n",
            "\n",
            "***************************\n",
            "Trained on 4240 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.425312\n",
            "Loss training: 28.226131\n",
            "Loss training: 51.004906\n",
            "Loss training: 27.54132\n",
            "Loss training: 28.290047\n",
            "Loss training: 29.560833\n",
            "Loss training: 44.84337\n",
            "Loss training: 19.637299\n",
            "Loss training: 56.711205\n",
            "Loss training: 114.45824\n",
            "\n",
            "***************************\n",
            "Trained on 4250 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.840614\n",
            "Loss training: 29.906723\n",
            "Loss training: 33.23762\n",
            "Loss training: 39.050434\n",
            "Loss training: 33.60539\n",
            "Loss training: 31.786598\n",
            "Loss training: 34.981373\n",
            "Loss training: 43.94278\n",
            "Loss training: 31.41723\n",
            "Loss training: 43.377842\n",
            "\n",
            "***************************\n",
            "Trained on 4260 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 52.43712\n",
            "Loss training: 64.74723\n",
            "Loss training: 17.13204\n",
            "Loss training: 45.85025\n",
            "Loss training: 35.408573\n",
            "Loss training: 36.78053\n",
            "Loss training: 70.13368\n",
            "Loss training: 57.24862\n",
            "Loss training: 42.479286\n",
            "Loss training: 41.844765\n",
            "\n",
            "***************************\n",
            "Trained on 4270 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 89.90127\n",
            "Loss training: 40.980286\n",
            "Loss training: 29.862204\n",
            "Loss training: 50.54188\n",
            "Loss training: 25.535849\n",
            "Loss training: 32.076782\n",
            "Loss training: 39.871094\n",
            "Loss training: 31.696445\n",
            "Loss training: 16.604286\n",
            "Loss training: 35.792152\n",
            "\n",
            "***************************\n",
            "Trained on 4280 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.832947\n",
            "Loss training: 53.172672\n",
            "Loss training: 41.608246\n",
            "Loss training: 52.288593\n",
            "Loss training: 50.597652\n",
            "Loss training: 51.17027\n",
            "Loss training: 43.433807\n",
            "Loss training: 43.251812\n",
            "Loss training: 50.755898\n",
            "Loss training: 51.358204\n",
            "\n",
            "***************************\n",
            "Trained on 4290 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.043236\n",
            "Loss training: 53.213776\n",
            "Loss training: 54.574093\n",
            "Loss training: 46.006245\n",
            "Loss training: 36.16747\n",
            "Loss training: 45.848442\n",
            "Loss training: 35.13627\n",
            "Loss training: 13.507212\n",
            "Loss training: 49.033226\n",
            "Loss training: 34.333733\n",
            "\n",
            "***************************\n",
            "Trained on 4300 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.341896\n",
            "Loss training: 37.04365\n",
            "Loss training: 51.688976\n",
            "Loss training: 38.81847\n",
            "Loss training: 35.714176\n",
            "Loss training: 27.116293\n",
            "Loss training: 36.42276\n",
            "Loss training: 35.62685\n",
            "Loss training: 47.70088\n",
            "Loss training: 53.483143\n",
            "\n",
            "***************************\n",
            "Trained on 4310 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 56.681187\n",
            "Loss training: 46.431583\n",
            "Loss training: 31.278734\n",
            "Loss training: 36.132042\n",
            "Loss training: 47.51631\n",
            "Loss training: 34.886303\n",
            "Loss training: 46.74353\n",
            "Loss training: 33.933197\n",
            "Loss training: 45.5902\n",
            "Loss training: 36.597153\n",
            "\n",
            "***************************\n",
            "Trained on 4320 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.464659\n",
            "Loss training: 31.610363\n",
            "Loss training: 51.014565\n",
            "Loss training: 29.470829\n",
            "Loss training: 17.521845\n",
            "Loss training: 44.93602\n",
            "Loss training: 37.862106\n",
            "Loss training: 40.695827\n",
            "Loss training: 46.75685\n",
            "Loss training: 45.872074\n",
            "\n",
            "***************************\n",
            "Trained on 4330 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.164406\n",
            "Loss training: 37.83747\n",
            "Loss training: 34.90658\n",
            "Loss training: 57.72368\n",
            "Loss training: 49.084797\n",
            "Loss training: 29.273783\n",
            "Loss training: 29.948423\n",
            "Loss training: 45.47076\n",
            "Loss training: 67.38442\n",
            "Loss training: 36.43528\n",
            "\n",
            "***************************\n",
            "Trained on 4340 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 59.448666\n",
            "Loss training: 26.21759\n",
            "Loss training: 30.903826\n",
            "Loss training: 33.80266\n",
            "Loss training: 49.157707\n",
            "Loss training: 33.260254\n",
            "Loss training: 38.89618\n",
            "Loss training: 35.939053\n",
            "Loss training: 35.276714\n",
            "Loss training: 26.010826\n",
            "\n",
            "***************************\n",
            "Trained on 4350 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.381674\n",
            "Loss training: 41.319744\n",
            "Loss training: 36.291527\n",
            "Loss training: 41.9977\n",
            "Loss training: 47.192364\n",
            "Loss training: 38.837852\n",
            "Loss training: 32.59282\n",
            "Loss training: 29.415405\n",
            "Loss training: 30.964071\n",
            "Loss training: 31.552366\n",
            "\n",
            "***************************\n",
            "Trained on 4360 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.94818\n",
            "Loss training: 36.14908\n",
            "Loss training: 61.36569\n",
            "Loss training: 36.803055\n",
            "Loss training: 38.575172\n",
            "Loss training: 50.498566\n",
            "Loss training: 46.779488\n",
            "Loss training: 45.97952\n",
            "Loss training: 13.990662\n",
            "Loss training: 60.06505\n",
            "\n",
            "***************************\n",
            "Trained on 4370 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 50.732624\n",
            "Loss training: 47.18918\n",
            "Loss training: 35.911938\n",
            "Loss training: 36.01661\n",
            "Loss training: 13.098858\n",
            "Loss training: 58.214855\n",
            "Loss training: 45.874653\n",
            "Loss training: 58.572556\n",
            "Loss training: 50.961956\n",
            "Loss training: 41.96101\n",
            "\n",
            "***************************\n",
            "Trained on 4380 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.467323\n",
            "Loss training: 48.378273\n",
            "Loss training: 34.26548\n",
            "Loss training: 41.7561\n",
            "Loss training: 36.71424\n",
            "Loss training: 40.660835\n",
            "Loss training: 51.507328\n",
            "Loss training: 64.201004\n",
            "Loss training: 43.071842\n",
            "Loss training: 41.429695\n",
            "\n",
            "***************************\n",
            "Trained on 4390 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.467102\n",
            "Loss training: 36.16465\n",
            "Loss training: 31.339296\n",
            "Loss training: 29.166079\n",
            "Loss training: 31.96083\n",
            "Loss training: 35.516426\n",
            "Loss training: 30.779596\n",
            "Loss training: 32.67341\n",
            "Loss training: 66.54853\n",
            "Loss training: 47.603573\n",
            "\n",
            "***************************\n",
            "Trained on 4400 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 48.35299\n",
            "Loss training: 32.00807\n",
            "Loss training: 49.86184\n",
            "Loss training: 30.445795\n",
            "Loss training: 27.11117\n",
            "Loss training: 47.0388\n",
            "Loss training: 43.23007\n",
            "Loss training: 32.777546\n",
            "Loss training: 37.149193\n",
            "Loss training: 37.47918\n",
            "\n",
            "***************************\n",
            "Trained on 4410 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.97442\n",
            "Loss training: 42.629356\n",
            "Loss training: 37.54059\n",
            "Loss training: 22.53016\n",
            "Loss training: 32.390903\n",
            "Loss training: 36.22907\n",
            "Loss training: 36.381214\n",
            "Loss training: 60.3164\n",
            "Loss training: 34.659676\n",
            "Loss training: 34.319168\n",
            "\n",
            "***************************\n",
            "Trained on 4420 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.089127\n",
            "Loss training: 49.28641\n",
            "Loss training: 69.81115\n",
            "Loss training: 34.919865\n",
            "Loss training: 45.529045\n",
            "Loss training: 15.195333\n",
            "Loss training: 44.197964\n",
            "Loss training: 32.18483\n",
            "Loss training: 35.000065\n",
            "Loss training: 32.26308\n",
            "\n",
            "***************************\n",
            "Trained on 4430 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 58.75254\n",
            "Loss training: 31.968075\n",
            "Loss training: 36.070633\n",
            "Loss training: 45.65633\n",
            "Loss training: 42.012604\n",
            "Loss training: 25.185734\n",
            "Loss training: 43.758915\n",
            "Loss training: 25.954817\n",
            "Loss training: 56.357872\n",
            "Loss training: 47.11487\n",
            "\n",
            "***************************\n",
            "Trained on 4440 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 56.543972\n",
            "Loss training: 13.569209\n",
            "Loss training: 47.482445\n",
            "Loss training: 36.461884\n",
            "Loss training: 42.991215\n",
            "Loss training: 39.66476\n",
            "Loss training: 39.51643\n",
            "Loss training: 34.341446\n",
            "Loss training: 46.95178\n",
            "Loss training: 49.788452\n",
            "\n",
            "***************************\n",
            "Trained on 4450 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.406937\n",
            "Loss training: 34.664665\n",
            "Loss training: 36.309437\n",
            "Loss training: 32.54597\n",
            "Loss training: 35.83486\n",
            "Loss training: 26.626717\n",
            "Loss training: 47.027664\n",
            "Loss training: 35.885834\n",
            "Loss training: 32.04852\n",
            "Loss training: 52.57641\n",
            "\n",
            "***************************\n",
            "Trained on 4460 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 58.43032\n",
            "Loss training: 36.058605\n",
            "Loss training: 41.67317\n",
            "Loss training: 30.427256\n",
            "Loss training: 55.789886\n",
            "Loss training: 26.586044\n",
            "Loss training: 35.258263\n",
            "Loss training: 34.57184\n",
            "Loss training: 26.821743\n",
            "Loss training: 14.195768\n",
            "\n",
            "***************************\n",
            "Trained on 4470 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.043137\n",
            "Loss training: 37.91385\n",
            "Loss training: 49.75724\n",
            "Loss training: 36.156944\n",
            "Loss training: 28.109734\n",
            "Loss training: 64.553345\n",
            "Loss training: 44.98917\n",
            "Loss training: 38.40432\n",
            "Loss training: 30.656557\n",
            "Loss training: 44.923878\n",
            "\n",
            "***************************\n",
            "Trained on 4480 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 51.65405\n",
            "Loss training: 35.648205\n",
            "Loss training: 47.767887\n",
            "Loss training: 59.483738\n",
            "Loss training: 45.08612\n",
            "Loss training: 35.008575\n",
            "Loss training: 44.93731\n",
            "Loss training: 39.297012\n",
            "Loss training: 37.096264\n",
            "Loss training: 38.332367\n",
            "\n",
            "***************************\n",
            "Trained on 4490 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 49.14902\n",
            "Loss training: 50.99116\n",
            "Loss training: 38.70839\n",
            "Loss training: 46.99814\n",
            "Loss training: 36.377426\n",
            "Loss training: 30.28925\n",
            "Loss training: 34.13532\n",
            "Loss training: 30.555323\n",
            "Loss training: 28.493216\n",
            "Loss training: 43.214893\n",
            "\n",
            "***************************\n",
            "Trained on 4500 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.508114\n",
            "Loss training: 41.651188\n",
            "Loss training: 34.641422\n",
            "Loss training: 30.75522\n",
            "Loss training: 28.349817\n",
            "Loss training: 59.081875\n",
            "Loss training: 41.84432\n",
            "Loss training: 40.577343\n",
            "Loss training: 55.101467\n",
            "Loss training: 26.466587\n",
            "\n",
            "***************************\n",
            "Trained on 4510 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.93014\n",
            "Loss training: 51.466988\n",
            "Loss training: 52.805\n",
            "Loss training: 46.555595\n",
            "Loss training: 26.998804\n",
            "Loss training: 36.894848\n",
            "Loss training: 31.767624\n",
            "Loss training: 39.587975\n",
            "Loss training: 49.471386\n",
            "Loss training: 35.96371\n",
            "\n",
            "***************************\n",
            "Trained on 4520 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.969343\n",
            "Loss training: 32.83581\n",
            "Loss training: 32.62935\n",
            "Loss training: 55.85999\n",
            "Loss training: 39.240185\n",
            "Loss training: 28.628136\n",
            "Loss training: 46.752903\n",
            "Loss training: 30.722902\n",
            "Loss training: 51.290222\n",
            "Loss training: 44.82933\n",
            "\n",
            "***************************\n",
            "Trained on 4530 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.132053\n",
            "Loss training: 29.391903\n",
            "Loss training: 52.402397\n",
            "Loss training: 41.01279\n",
            "Loss training: 57.185646\n",
            "Loss training: 14.3899\n",
            "Loss training: 44.967216\n",
            "Loss training: 35.12049\n",
            "Loss training: 45.88982\n",
            "Loss training: 53.096004\n",
            "\n",
            "***************************\n",
            "Trained on 4540 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.42219\n",
            "Loss training: 40.44653\n",
            "Loss training: 32.325512\n",
            "Loss training: 53.55561\n",
            "Loss training: 31.418434\n",
            "Loss training: 36.644196\n",
            "Loss training: 58.94387\n",
            "Loss training: 28.655054\n",
            "Loss training: 33.82644\n",
            "Loss training: 31.05555\n",
            "\n",
            "***************************\n",
            "Trained on 4550 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 13.989633\n",
            "Loss training: 36.650246\n",
            "Loss training: 62.633606\n",
            "Loss training: 47.095272\n",
            "Loss training: 37.417492\n",
            "Loss training: 36.786057\n",
            "Loss training: 54.831123\n",
            "Loss training: 34.542706\n",
            "Loss training: 38.529602\n",
            "Loss training: 46.251476\n",
            "\n",
            "***************************\n",
            "Trained on 4560 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.943817\n",
            "Loss training: 47.197754\n",
            "Loss training: 29.423971\n",
            "Loss training: 61.2706\n",
            "Loss training: 26.819895\n",
            "Loss training: 35.432953\n",
            "Loss training: 44.877525\n",
            "Loss training: 34.874664\n",
            "Loss training: 53.42973\n",
            "Loss training: 41.616695\n",
            "\n",
            "***************************\n",
            "Trained on 4570 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.050123\n",
            "Loss training: 61.53888\n",
            "Loss training: 36.948063\n",
            "Loss training: 46.11518\n",
            "Loss training: 28.237036\n",
            "Loss training: 34.802326\n",
            "Loss training: 45.971554\n",
            "Loss training: 29.194736\n",
            "Loss training: 34.340492\n",
            "Loss training: 33.94499\n",
            "\n",
            "***************************\n",
            "Trained on 4580 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.6119\n",
            "Loss training: 36.56956\n",
            "Loss training: 28.198095\n",
            "Loss training: 33.96985\n",
            "Loss training: 60.76123\n",
            "Loss training: 32.015007\n",
            "Loss training: 46.042084\n",
            "Loss training: 36.701717\n",
            "Loss training: 58.811977\n",
            "Loss training: 32.011936\n",
            "\n",
            "***************************\n",
            "Trained on 4590 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.69424\n",
            "Loss training: 34.41935\n",
            "Loss training: 27.405342\n",
            "Loss training: 46.22064\n",
            "Loss training: 30.401682\n",
            "Loss training: 52.44069\n",
            "Loss training: 38.381966\n",
            "Loss training: 35.348446\n",
            "Loss training: 42.055244\n",
            "Loss training: 47.889534\n",
            "\n",
            "***************************\n",
            "Trained on 4600 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.20644\n",
            "Loss training: 26.743973\n",
            "Loss training: 33.29332\n",
            "Loss training: 12.649419\n",
            "Loss training: 30.176521\n",
            "Loss training: 43.735023\n",
            "Loss training: 33.182575\n",
            "Loss training: 12.108628\n",
            "Loss training: 30.945883\n",
            "Loss training: 42.13691\n",
            "\n",
            "***************************\n",
            "Trained on 4610 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.48213\n",
            "Loss training: 53.34531\n",
            "Loss training: 65.797134\n",
            "Loss training: 35.416393\n",
            "Loss training: 31.91398\n",
            "Loss training: 33.174786\n",
            "Loss training: 33.34114\n",
            "Loss training: 46.302128\n",
            "Loss training: 43.27371\n",
            "Loss training: 38.277863\n",
            "\n",
            "***************************\n",
            "Trained on 4620 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.54486\n",
            "Loss training: 35.74911\n",
            "Loss training: 31.678709\n",
            "Loss training: 58.13271\n",
            "Loss training: 31.47075\n",
            "Loss training: 45.89357\n",
            "Loss training: 32.029762\n",
            "Loss training: 27.897223\n",
            "Loss training: 34.73982\n",
            "Loss training: 44.579777\n",
            "\n",
            "***************************\n",
            "Trained on 4630 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 61.34165\n",
            "Loss training: 12.369663\n",
            "Loss training: 44.540344\n",
            "Loss training: 52.708908\n",
            "Loss training: 46.86142\n",
            "Loss training: 46.71314\n",
            "Loss training: 32.491604\n",
            "Loss training: 46.57798\n",
            "Loss training: 28.360744\n",
            "Loss training: 28.087826\n",
            "\n",
            "***************************\n",
            "Trained on 4640 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.57607\n",
            "Loss training: 46.018784\n",
            "Loss training: 52.729336\n",
            "Loss training: 63.130344\n",
            "Loss training: 32.779892\n",
            "Loss training: 52.409336\n",
            "Loss training: 36.75317\n",
            "Loss training: 38.78347\n",
            "Loss training: 33.641357\n",
            "Loss training: 37.41488\n",
            "\n",
            "***************************\n",
            "Trained on 4650 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 48.923794\n",
            "Loss training: 42.467228\n",
            "Loss training: 22.284042\n",
            "Loss training: 42.38213\n",
            "Loss training: 57.645405\n",
            "Loss training: 30.611704\n",
            "Loss training: 46.1788\n",
            "Loss training: 30.474022\n",
            "Loss training: 45.72931\n",
            "Loss training: 27.895794\n",
            "\n",
            "***************************\n",
            "Trained on 4660 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.166843\n",
            "Loss training: 51.3026\n",
            "Loss training: 32.881573\n",
            "Loss training: 37.705334\n",
            "Loss training: 51.656307\n",
            "Loss training: 32.024895\n",
            "Loss training: 27.764378\n",
            "Loss training: 37.74572\n",
            "Loss training: 35.612907\n",
            "Loss training: 35.61978\n",
            "\n",
            "***************************\n",
            "Trained on 4670 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.496025\n",
            "Loss training: 34.598812\n",
            "Loss training: 32.202393\n",
            "Loss training: 61.246803\n",
            "Loss training: 44.85926\n",
            "Loss training: 32.793156\n",
            "Loss training: 59.178772\n",
            "Loss training: 36.09722\n",
            "Loss training: 28.732347\n",
            "Loss training: 29.715136\n",
            "\n",
            "***************************\n",
            "Trained on 4680 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 48.249985\n",
            "Loss training: 43.68974\n",
            "Loss training: 48.171104\n",
            "Loss training: 47.255573\n",
            "Loss training: 27.822454\n",
            "Loss training: 28.054964\n",
            "Loss training: 45.608463\n",
            "Loss training: 40.81171\n",
            "Loss training: 31.950565\n",
            "Loss training: 25.820444\n",
            "\n",
            "***************************\n",
            "Trained on 4690 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.744656\n",
            "Loss training: 46.929325\n",
            "Loss training: 28.159512\n",
            "Loss training: 66.87765\n",
            "Loss training: 41.326107\n",
            "Loss training: 50.896244\n",
            "Loss training: 40.489582\n",
            "Loss training: 50.62401\n",
            "Loss training: 29.68477\n",
            "Loss training: 48.185925\n",
            "\n",
            "***************************\n",
            "Trained on 4700 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 55.873634\n",
            "Loss training: 41.33624\n",
            "Loss training: 29.739206\n",
            "Loss training: 30.027252\n",
            "Loss training: 31.595863\n",
            "Loss training: 52.803856\n",
            "Loss training: 35.060844\n",
            "Loss training: 45.487892\n",
            "Loss training: 35.51037\n",
            "Loss training: 38.43653\n",
            "\n",
            "***************************\n",
            "Trained on 4710 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.35744\n",
            "Loss training: 44.003513\n",
            "Loss training: 66.96524\n",
            "Loss training: 37.245556\n",
            "Loss training: 42.83168\n",
            "Loss training: 28.12509\n",
            "Loss training: 46.96132\n",
            "Loss training: 46.434875\n",
            "Loss training: 31.520817\n",
            "Loss training: 44.801\n",
            "\n",
            "***************************\n",
            "Trained on 4720 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.01574\n",
            "Loss training: 28.435995\n",
            "Loss training: 32.8912\n",
            "Loss training: 44.06885\n",
            "Loss training: 44.02194\n",
            "Loss training: 33.49261\n",
            "Loss training: 25.586967\n",
            "Loss training: 60.944492\n",
            "Loss training: 37.899956\n",
            "Loss training: 53.013523\n",
            "\n",
            "***************************\n",
            "Trained on 4730 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.070137\n",
            "Loss training: 43.871685\n",
            "Loss training: 43.752384\n",
            "Loss training: 35.772964\n",
            "Loss training: 45.856533\n",
            "Loss training: 25.445707\n",
            "Loss training: 42.684002\n",
            "Loss training: 37.40863\n",
            "Loss training: 45.950512\n",
            "Loss training: 45.94437\n",
            "\n",
            "***************************\n",
            "Trained on 4740 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.357405\n",
            "Loss training: 31.433683\n",
            "Loss training: 43.249294\n",
            "Loss training: 31.107397\n",
            "Loss training: 24.107296\n",
            "Loss training: 32.06369\n",
            "Loss training: 40.049885\n",
            "Loss training: 35.02491\n",
            "Loss training: 35.23238\n",
            "Loss training: 28.079603\n",
            "\n",
            "***************************\n",
            "Trained on 4750 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 43.143864\n",
            "Loss training: 42.960903\n",
            "Loss training: 24.498373\n",
            "Loss training: 15.8582945\n",
            "Loss training: 53.27546\n",
            "Loss training: 44.211796\n",
            "Loss training: 33.037445\n",
            "Loss training: 56.96802\n",
            "Loss training: 35.6925\n",
            "Loss training: 59.17795\n",
            "\n",
            "***************************\n",
            "Trained on 4760 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.40776\n",
            "Loss training: 47.613266\n",
            "Loss training: 34.241013\n",
            "Loss training: 48.748497\n",
            "Loss training: 39.35613\n",
            "Loss training: 34.16262\n",
            "Loss training: 46.15842\n",
            "Loss training: 33.58907\n",
            "Loss training: 38.233696\n",
            "Loss training: 30.445578\n",
            "\n",
            "***************************\n",
            "Trained on 4770 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.55464\n",
            "Loss training: 45.35188\n",
            "Loss training: 25.666742\n",
            "Loss training: 35.39375\n",
            "Loss training: 35.79028\n",
            "Loss training: 62.446957\n",
            "Loss training: 36.13958\n",
            "Loss training: 40.86636\n",
            "Loss training: 65.887054\n",
            "Loss training: 61.34056\n",
            "\n",
            "***************************\n",
            "Trained on 4780 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.0485\n",
            "Loss training: 14.153441\n",
            "Loss training: 34.820827\n",
            "Loss training: 56.034428\n",
            "Loss training: 40.577076\n",
            "Loss training: 46.87183\n",
            "Loss training: 32.812153\n",
            "Loss training: 31.692368\n",
            "Loss training: 36.60479\n",
            "Loss training: 37.90422\n",
            "\n",
            "***************************\n",
            "Trained on 4790 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 16.00827\n",
            "Loss training: 41.764973\n",
            "Loss training: 30.860691\n",
            "Loss training: 35.23912\n",
            "Loss training: 43.901817\n",
            "Loss training: 54.594257\n",
            "Loss training: 53.28311\n",
            "Loss training: 31.865843\n",
            "Loss training: 49.796143\n",
            "Loss training: 46.346054\n",
            "\n",
            "***************************\n",
            "Trained on 4800 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.321754\n",
            "Loss training: 30.558926\n",
            "Loss training: 30.631067\n",
            "Loss training: 43.42221\n",
            "Loss training: 43.34001\n",
            "Loss training: 44.348267\n",
            "Loss training: 33.5081\n",
            "Loss training: 44.115364\n",
            "Loss training: 59.49223\n",
            "Loss training: 33.07058\n",
            "\n",
            "***************************\n",
            "Trained on 4810 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.158676\n",
            "Loss training: 48.659817\n",
            "Loss training: 39.040783\n",
            "Loss training: 54.51475\n",
            "Loss training: 48.51496\n",
            "Loss training: 30.457323\n",
            "Loss training: 45.962486\n",
            "Loss training: 36.954136\n",
            "Loss training: 12.775416\n",
            "Loss training: 44.75238\n",
            "\n",
            "***************************\n",
            "Trained on 4820 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 12.409156\n",
            "Loss training: 30.718777\n",
            "Loss training: 44.09646\n",
            "Loss training: 62.942226\n",
            "Loss training: 36.155342\n",
            "Loss training: 60.407055\n",
            "Loss training: 51.40925\n",
            "Loss training: 32.164574\n",
            "Loss training: 34.832256\n",
            "Loss training: 36.58767\n",
            "\n",
            "***************************\n",
            "Trained on 4830 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 12.005206\n",
            "Loss training: 56.500156\n",
            "Loss training: 34.26631\n",
            "Loss training: 34.917377\n",
            "Loss training: 30.666166\n",
            "Loss training: 32.37296\n",
            "Loss training: 32.210815\n",
            "Loss training: 37.345383\n",
            "Loss training: 28.73776\n",
            "Loss training: 32.81746\n",
            "\n",
            "***************************\n",
            "Trained on 4840 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.338543\n",
            "Loss training: 57.53478\n",
            "Loss training: 31.905094\n",
            "Loss training: 56.4671\n",
            "Loss training: 28.361046\n",
            "Loss training: 36.03514\n",
            "Loss training: 52.200172\n",
            "Loss training: 35.852768\n",
            "Loss training: 59.283386\n",
            "Loss training: 36.244564\n",
            "\n",
            "***************************\n",
            "Trained on 4850 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.987057\n",
            "Loss training: 52.86511\n",
            "Loss training: 29.40566\n",
            "Loss training: 37.04199\n",
            "Loss training: 32.26473\n",
            "Loss training: 51.61452\n",
            "Loss training: 13.569883\n",
            "Loss training: 31.202057\n",
            "Loss training: 46.809723\n",
            "Loss training: 30.6648\n",
            "\n",
            "***************************\n",
            "Trained on 4860 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.31145\n",
            "Loss training: 39.340843\n",
            "Loss training: 44.14597\n",
            "Loss training: 36.202034\n",
            "Loss training: 44.15825\n",
            "Loss training: 36.968952\n",
            "Loss training: 19.531715\n",
            "Loss training: 16.099379\n",
            "Loss training: 39.724163\n",
            "Loss training: 46.521355\n",
            "\n",
            "***************************\n",
            "Trained on 4870 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 12.10563\n",
            "Loss training: 52.79975\n",
            "Loss training: 34.574566\n",
            "Loss training: 28.11682\n",
            "Loss training: 52.08727\n",
            "Loss training: 35.272\n",
            "Loss training: 49.353977\n",
            "Loss training: 36.072323\n",
            "Loss training: 54.782814\n",
            "Loss training: 33.229866\n",
            "\n",
            "***************************\n",
            "Trained on 4880 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.242025\n",
            "Loss training: 46.35645\n",
            "Loss training: 46.253345\n",
            "Loss training: 34.020344\n",
            "Loss training: 43.0488\n",
            "Loss training: 28.595417\n",
            "Loss training: 38.226524\n",
            "Loss training: 41.921577\n",
            "Loss training: 61.403217\n",
            "Loss training: 36.62282\n",
            "\n",
            "***************************\n",
            "Trained on 4890 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 48.813255\n",
            "Loss training: 33.187824\n",
            "Loss training: 15.1759\n",
            "Loss training: 31.495855\n",
            "Loss training: 35.238205\n",
            "Loss training: 33.89346\n",
            "Loss training: 35.07329\n",
            "Loss training: 42.785892\n",
            "Loss training: 32.928734\n",
            "Loss training: 36.863113\n",
            "\n",
            "***************************\n",
            "Trained on 4900 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.559174\n",
            "Loss training: 38.02719\n",
            "Loss training: 34.404648\n",
            "Loss training: 34.658203\n",
            "Loss training: 30.16134\n",
            "Loss training: 59.150707\n",
            "Loss training: 34.639957\n",
            "Loss training: 39.198055\n",
            "Loss training: 27.054152\n",
            "Loss training: 47.29414\n",
            "\n",
            "***************************\n",
            "Trained on 4910 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.02421\n",
            "Loss training: 42.596176\n",
            "Loss training: 26.013607\n",
            "Loss training: 28.71596\n",
            "Loss training: 46.59396\n",
            "Loss training: 42.878292\n",
            "Loss training: 28.500792\n",
            "Loss training: 24.695963\n",
            "Loss training: 37.87\n",
            "Loss training: 42.443024\n",
            "\n",
            "***************************\n",
            "Trained on 4920 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.72017\n",
            "Loss training: 34.8256\n",
            "Loss training: 32.114773\n",
            "Loss training: 44.757618\n",
            "Loss training: 33.818882\n",
            "Loss training: 26.004957\n",
            "Loss training: 44.956043\n",
            "Loss training: 46.81964\n",
            "Loss training: 56.940327\n",
            "Loss training: 30.032429\n",
            "\n",
            "***************************\n",
            "Trained on 4930 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.410767\n",
            "Loss training: 34.617867\n",
            "Loss training: 19.823523\n",
            "Loss training: 30.351469\n",
            "Loss training: 38.747234\n",
            "Loss training: 33.762\n",
            "Loss training: 43.814377\n",
            "Loss training: 52.59746\n",
            "Loss training: 42.32013\n",
            "Loss training: 60.673615\n",
            "\n",
            "***************************\n",
            "Trained on 4940 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.233543\n",
            "Loss training: 55.680466\n",
            "Loss training: 37.25841\n",
            "Loss training: 53.193977\n",
            "Loss training: 48.869957\n",
            "Loss training: 46.858257\n",
            "Loss training: 33.70996\n",
            "Loss training: 36.87451\n",
            "Loss training: 36.435905\n",
            "Loss training: 13.72464\n",
            "\n",
            "***************************\n",
            "Trained on 4950 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 46.602722\n",
            "Loss training: 32.15654\n",
            "Loss training: 50.053074\n",
            "Loss training: 31.410378\n",
            "Loss training: 32.18024\n",
            "Loss training: 42.143265\n",
            "Loss training: 35.882942\n",
            "Loss training: 32.930492\n",
            "Loss training: 31.308723\n",
            "Loss training: 30.719286\n",
            "\n",
            "***************************\n",
            "Trained on 4960 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.75461\n",
            "Loss training: 30.219257\n",
            "Loss training: 32.005997\n",
            "Loss training: 35.330788\n",
            "Loss training: 46.42285\n",
            "Loss training: 15.565948\n",
            "Loss training: 36.662632\n",
            "Loss training: 44.092552\n",
            "Loss training: 45.797504\n",
            "Loss training: 33.579468\n",
            "\n",
            "***************************\n",
            "Trained on 4970 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.62131\n",
            "Loss training: 35.900932\n",
            "Loss training: 33.300663\n",
            "Loss training: 45.834072\n",
            "Loss training: 44.78276\n",
            "Loss training: 31.968662\n",
            "Loss training: 44.365932\n",
            "Loss training: 30.473585\n",
            "Loss training: 52.74064\n",
            "Loss training: 38.66829\n",
            "\n",
            "***************************\n",
            "Trained on 4980 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 59.869278\n",
            "Loss training: 31.600883\n",
            "Loss training: 38.207985\n",
            "Loss training: 30.506697\n",
            "Loss training: 39.384537\n",
            "Loss training: 49.30392\n",
            "Loss training: 45.74935\n",
            "Loss training: 27.938625\n",
            "Loss training: 31.21624\n",
            "Loss training: 30.071056\n",
            "\n",
            "***************************\n",
            "Trained on 4990 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 51.510765\n",
            "Loss training: 12.643276\n",
            "Loss training: 29.992184\n",
            "Loss training: 32.094097\n",
            "Loss training: 64.734276\n",
            "Loss training: 34.57795\n",
            "Loss training: 35.038506\n",
            "Loss training: 12.170798\n",
            "Loss training: 44.52529\n",
            "Loss training: 34.878967\n",
            "\n",
            "***************************\n",
            "Trained on 5000 graphs\n",
            "***************************\n",
            "\n",
            "Test loss: 29.00429916381836 | ROC: 0.600329997916495\n",
            "Test loss: 35.42852783203125 | ROC: 0.5307935352707324\n",
            "Test loss: 33.83931350708008 | ROC: 0.5166870117532643\n",
            "Test loss: 27.41913414001465 | ROC: 0.4005040250841687\n",
            "Test loss: 36.61497116088867 | ROC: 0.5622902411792355\n",
            "Average test loss: 32.4612491607666 | Average ROC: 0.5221209622407792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_on_full_sets(params):\n",
        "  final_predictions = {}\n",
        "\n",
        "  for i in range(num_partitions):\n",
        "    node_ids = dgl_graph_metis_partition[i].ndata['_ID']\n",
        "    partition = processed_graphs[f'partition_{i}']\n",
        "    \n",
        "    predictions, _ = predict_on_graph(params, \n",
        "                                      partition['graph'], \n",
        "                                      partition['labels'], \n",
        "                                      partition['test_mask']  # Only used in the loss computation, does not affect predictions\n",
        "                                      )\n",
        "\n",
        "    predictions_after_masked_nodes_are_removed = remove_mask_from_data(\n",
        "        reshape_broadcasted_data(predictions),\n",
        "        reshape_broadcasted_data(partition['padding_mask'])\n",
        "        )\n",
        "\n",
        "    for index, node_id in enumerate(node_ids):\n",
        "      final_predictions[node_id] = predictions_after_masked_nodes_are_removed[index]\n",
        "\n",
        "    if (i + 1) % 10 == 0:\n",
        "      print(f'Evaluated {i + 1} / {num_partitions} subgraphs...')\n",
        "\n",
        "  # Sort the final predictions based on the node ids\n",
        "  predictions_in_order = dict(sorted(final_predictions.items()))\n",
        "\n",
        "  # Convert the values to a list to be able to slice based on the ids of the \n",
        "  # nodes in the test set\n",
        "  predictions_in_order = list(predictions_in_order.values())\n",
        "\n",
        "  final_roc_train = evaluator.eval({\n",
        "      \"y_true\": np.array(train_label), \n",
        "      \"y_pred\": np.array([predictions_in_order[x] for x in split_idx['train']])\n",
        "      })['rocauc']\n",
        "\n",
        "  final_roc_valid = evaluator.eval({\n",
        "      \"y_true\": np.array(valid_label), \n",
        "      \"y_pred\": np.array([predictions_in_order[x] for x in split_idx['valid']])\n",
        "      })['rocauc']\n",
        "\n",
        "  final_roc_test = evaluator.eval({\n",
        "      \"y_true\": np.array(test_label),\n",
        "      \"y_pred\": np.array([predictions_in_order[x] for x in split_idx['test']])\n",
        "      })['rocauc']\n",
        "\n",
        "  print()\n",
        "  print(f'Final ROC on the train set {final_roc_train}')\n",
        "  print(f'Final ROC on the validation set {final_roc_valid}')\n",
        "  print(f'Final ROC on the test set {final_roc_test}')\n",
        "\n",
        "evaluate_on_full_sets(final_params)"
      ],
      "metadata": {
        "id": "aq4_r4M6VnzW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67ef2486-2260-4e36-9d7a-3e7850f999c1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluated 10 / 50 subgraphs...\n",
            "Evaluated 20 / 50 subgraphs...\n",
            "Evaluated 30 / 50 subgraphs...\n",
            "Evaluated 40 / 50 subgraphs...\n",
            "Evaluated 50 / 50 subgraphs...\n",
            "\n",
            "Final ROC on the train set 0.74613213676143\n",
            "Final ROC on the validation set 0.554611312736839\n",
            "Final ROC on the test set 0.5321309322083977\n"
          ]
        }
      ]
    }
  ]
}