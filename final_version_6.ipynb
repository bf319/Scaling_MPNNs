{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final-version-2-sharded-networks.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bf319/Scaling_MPNNs/blob/main/final_version_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rLhMwiHHWbtK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "331658f0-49e2-42bd-e657-e0c75c11becf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for dm-haiku (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 75 kB 2.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 70 kB 3.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 77 kB 6.5 MB/s \n",
            "\u001b[?25h  Building wheel for jaxline (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 78 kB 3.3 MB/s \n",
            "\u001b[?25h  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 136 kB 5.1 MB/s \n",
            "\u001b[?25h  Building wheel for metis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "--2022-03-29 11:32:09--  https://raw.githubusercontent.com/deepmind/jraph/master/jraph/experimental/sharded_graphnet.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22444 (22K) [text/plain]\n",
            "Saving to: ‘sharded_graphnet.py’\n",
            "\n",
            "sharded_graphnet.py 100%[===================>]  21.92K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2022-03-29 11:32:09 (15.2 MB/s) - ‘sharded_graphnet.py’ saved [22444/22444]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%pip install -q git+https://github.com/deepmind/dm-haiku\n",
        "%pip install -q jraph\n",
        "%pip install -q git+https://github.com/deepmind/jaxline\n",
        "%pip install -q ogb\n",
        "%pip install -q dgl\n",
        "%pip install -q optax\n",
        "%pip install -q metis\n",
        "\n",
        "!wget https://raw.githubusercontent.com/deepmind/jraph/master/jraph/experimental/sharded_graphnet.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "\n",
        "num_devices = jax.local_device_count()\n",
        "jax.devices()"
      ],
      "metadata": {
        "id": "iT2wqf76kIRp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be135fd7-0c05-497e-8967-5c27aaae860d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ogb.nodeproppred import Evaluator\n",
        "from ogb.nodeproppred import DglNodePropPredDataset\n",
        "\n",
        "dataset = DglNodePropPredDataset(name = \"ogbn-proteins\")\n",
        "split_idx = dataset.get_idx_split()\n",
        "evaluator = Evaluator(name = 'ogbn-proteins')"
      ],
      "metadata": {
        "id": "xHClucOxWpAZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82f39c2d-fbd9-4b04-fa2f-d7d232cd92b5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n",
            "Using backend: pytorch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/proteins.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 0.21 GB: 100%|██████████| 216/216 [00:06<00:00, 34.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/proteins.zip\n",
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:02<00:00,  2.30s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into DGL objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  2.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "# There is only one graph in Node Property Prediction datasets\n",
        "ogbn_proteins_main_graph, ogbn_proteins_main_labels = dataset[0]\n",
        "\n",
        "'''\n",
        "  OGBN-Proteins\n",
        "    #Nodes = 132,534\n",
        "    #Edges = 39,561,252\n",
        "    #Tasks = 112\n",
        "    #Split Type = Species\n",
        "    #Task Type = Binary classification\n",
        "    #Metric = ROC-AUC\n",
        "\n",
        "    Task:\n",
        "      The task is to predict the presence of protein functions in a multi-label binary classification setup,\n",
        "      where there are 112 kinds of labels to predict in total. \n",
        "      The performance is measured by the average of ROC-AUC scores across the 112 tasks.\n",
        "\n",
        "    #Others:\n",
        "      **undirected**\n",
        "      **weighted**\n",
        "      **typed (according to species)**\n",
        "\n",
        "  (1) Nodes represent proteins\n",
        "    (1.1) The proteins come from 8 species\n",
        "      len(set(graph.ndata['species'].reshape(-1).tolist())) == 8\n",
        "    (1.2) Each node has one feature associated with it (its species)\n",
        "      graph.ndata['species'].shape == (#nodes, 1)\n",
        "  \n",
        "  (2) Edges indicate different types of biologically meaningful associations between proteins\n",
        "    (2.1) All edges come with 8-dimensional features\n",
        "      graph.edata['feat'].shape == (2 * #edges, 8)\n",
        "\n",
        "'''\n",
        "# Get split labels\n",
        "train_label = dataset.labels[split_idx['train']]  # (86619, 112) -- binary values (presence of protein functions)\n",
        "valid_label = dataset.labels[split_idx['valid']]  # (21236, 112) -- binary values (presence of protein functions)\n",
        "test_label = dataset.labels[split_idx['test']]    # (24679, 112) -- binary values (presence of protein functions)\n",
        "\n",
        "# Create masks\n",
        "train_mask = jnp.zeros((ogbn_proteins_main_graph.num_nodes(), 1)).at[jnp.array(split_idx['train'])].set(1)\n",
        "valid_mask = jnp.zeros((ogbn_proteins_main_graph.num_nodes(), 1)).at[jnp.array(split_idx['valid'])].set(1)\n",
        "test_mask = jnp.zeros((ogbn_proteins_main_graph.num_nodes(), 1)).at[jnp.array(split_idx['test'])].set(1)"
      ],
      "metadata": {
        "id": "jCkzIEb4WsXU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jraph\n",
        "\n",
        "# From https://colab.research.google.com/github/deepmind/educational/blob/master/colabs/summer_schools/intro_to_graph_nets_tutorial_with_jraph.ipynb#scrollTo=7vEmAsr5bKN8\n",
        "def _nearest_bigger_power_of_two(x: int) -> int:\n",
        "  \"\"\"Computes the nearest power of two greater than x for padding.\"\"\"\n",
        "  y = 2\n",
        "  while y < x:\n",
        "    y *= 2\n",
        "  return y\n",
        "\n",
        "def pad_graph_to_nearest_power_of_two(\n",
        "    graphs_tuple: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
        "  \"\"\"Pads a batched `GraphsTuple` to the nearest power of two.\n",
        "  For example, if a `GraphsTuple` has 7 nodes, 5 edges and 3 graphs, this method\n",
        "  would pad the `GraphsTuple` nodes and edges:\n",
        "    7 nodes --> 8 nodes (2^3)\n",
        "    5 edges --> 8 edges (2^3)\n",
        "  And since padding is accomplished using `jraph.pad_with_graphs`, an extra\n",
        "  graph and node is added:\n",
        "    8 nodes --> 9 nodes\n",
        "    3 graphs --> 4 graphs\n",
        "  Args:\n",
        "    graphs_tuple: a batched `GraphsTuple` (can be batch size 1).\n",
        "  Returns:\n",
        "    A graphs_tuple batched to the nearest power of two.\n",
        "  \"\"\"\n",
        "  # Add 1 since we need at least one padding node for pad_with_graphs.\n",
        "  pad_nodes_to = _nearest_bigger_power_of_two(jnp.sum(graphs_tuple.n_node)) + 1\n",
        "  pad_edges_to = _nearest_bigger_power_of_two(jnp.sum(graphs_tuple.n_edge))\n",
        "  # Add 1 since we need at least one padding graph for pad_with_graphs.\n",
        "  # We do not pad to nearest power of two because the batch size is fixed.\n",
        "  pad_graphs_to = graphs_tuple.n_node.shape[0] + 1\n",
        "  return jraph.pad_with_graphs(graphs_tuple, pad_nodes_to, pad_edges_to,\n",
        "                               pad_graphs_to)"
      ],
      "metadata": {
        "id": "FSbePOUh2NBB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_demo_training_graph():\n",
        "  num_nodes = 100\n",
        "  num_edges = 200\n",
        "\n",
        "  rand_dgl_graph = dgl.rand_graph(num_nodes = num_nodes, num_edges = num_edges)\n",
        "\n",
        "  node_features = jnp.array([[randint(0, 7)] for i in range(num_nodes)])\n",
        "  edge_features = jnp.array([[0.1 * randint(0, 10) for _ in range(8)] for i in range(num_edges)])\n",
        "\n",
        "  senders = jnp.array(rand_dgl_graph.edges()[0])\n",
        "  receivers = jnp.array(rand_dgl_graph.edges()[1])\n",
        "\n",
        "  in_tuple = jraph.GraphsTuple(\n",
        "            nodes = node_features.astype(np.float32),\n",
        "            edges = edge_features.astype(np.float32),  \n",
        "            senders = senders.astype(np.int32), \n",
        "            receivers = receivers.astype(np.int32),\n",
        "            n_node = jnp.array([num_nodes]), \n",
        "            n_edge = jnp.array([num_edges]),\n",
        "            globals = None  # No global features\n",
        "          )\n",
        "  \n",
        "  labels = jnp.array([[randint(0, 1) for _ in range(112)] for i in range(num_nodes)])\n",
        "  train_mask = jnp.ones((100, 1))\n",
        "  \n",
        "  in_tuple = in_tuple._replace(\n",
        "      nodes = {\n",
        "          'inputs': in_tuple.nodes, \n",
        "          'targets': labels, \n",
        "          'train_mask': train_mask, \n",
        "          }\n",
        "  )\n",
        "\n",
        "  in_tuple = pad_graph_to_nearest_power_of_two(in_tuple)\n",
        "  \n",
        "  return sharded_graphnet.graphs_tuple_to_broadcasted_sharded_graphs_tuple(\n",
        "      in_tuple,\n",
        "      num_shards = num_devices\n",
        "      )\n",
        "  \n",
        "demo_graph = get_demo_training_graph()\n",
        "demo_labels = demo_graph.nodes['targets']\n",
        "demo_mask = demo_graph.nodes['train_mask']\n",
        "demo_graph = demo_graph._replace(nodes = demo_graph.nodes['inputs']) "
      ],
      "metadata": {
        "id": "SKoX4h1z9ItW"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import jraph\n",
        "\n",
        "import sharded_graphnet\n",
        "\n",
        "def dgl_graph_to_jraph(node_ids, labels, train_mask, valid_mask, test_mask):\n",
        "  # First add back the node and edge features\n",
        "  dgl_graph_with_features = dgl.node_subgraph(ogbn_proteins_main_graph, node_ids)\n",
        "\n",
        "  node_features = jnp.array(dgl_graph_with_features.ndata['species']) ## TODO: One hot encoding\n",
        "  senders = jnp.array(dgl_graph_with_features.edges()[0])\n",
        "  receivers = jnp.array(dgl_graph_with_features.edges()[1])\n",
        "\n",
        "  # Edges -- here we should include the 8-dimensional edge features\n",
        "  edges = jnp.array(dgl_graph_with_features.edata['feat'])\n",
        "\n",
        "  n_node = jnp.array([dgl_graph_with_features.num_nodes()])\n",
        "  n_edge = jnp.array([dgl_graph_with_features.num_edges()])\n",
        "\n",
        "  in_tuple = jraph.GraphsTuple(\n",
        "            nodes = node_features.astype(np.float32),\n",
        "            senders = senders.astype(np.int32), \n",
        "            receivers = receivers.astype(np.int32),\n",
        "            edges = edges.astype(np.float32),  \n",
        "            n_node = n_node, \n",
        "            n_edge = n_edge,\n",
        "            globals = None  # No global features\n",
        "          )\n",
        "  \n",
        "  in_tuple = in_tuple._replace(\n",
        "      nodes = {\n",
        "          'inputs': in_tuple.nodes, \n",
        "          'targets': labels, \n",
        "          'train_mask': train_mask, \n",
        "          'valid_mask': valid_mask, \n",
        "          'test_mask': test_mask,\n",
        "          'padding_mask': jnp.ones_like(in_tuple.nodes) # Adding thmis mask so that we can remove the nodes added after padding \n",
        "                                                        # for the final ROC computations on the full train / valid / test splits\n",
        "                                                        # This is because I want to pass the predictions on the true nodes to the \n",
        "                                                        # ogbn-evaluator, so I would first need to remove the predictions that come from padding.\n",
        "          }\n",
        "  )\n",
        "\n",
        "  in_tuple = pad_graph_to_nearest_power_of_two(in_tuple)\n",
        "  \n",
        "  return sharded_graphnet.graphs_tuple_to_broadcasted_sharded_graphs_tuple(\n",
        "      in_tuple,\n",
        "      num_shards = num_devices\n",
        "      )\n",
        "  \n",
        "def get_labels_for_subgraph(node_ids):\n",
        "  return jnp.array(ogbn_proteins_main_labels.index_select(0, node_ids))"
      ],
      "metadata": {
        "id": "fvH_XRJVWuLw"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "\n",
        "'''\n",
        "  Generate graph partition using metis, with balanced number of edges in each partition.\n",
        "  Note: \n",
        "    The subgraphs do not contain the node/edge data in the input graph (https://docs.dgl.ai/generated/dgl.metis_partition.html)\n",
        "'''\n",
        "num_partitions = 50  ## TODO: Find some way to decrease this to something reasonable (< 50)\n",
        "\n",
        "dgl_graph_metis_partition = dgl.metis_partition(ogbn_proteins_main_graph, num_partitions, balance_edges = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUI9s4-0mPz9",
        "outputId": "8d7e071a-1f26-4f74-d1ac-19464989329f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convert a graph into a bidirected graph: 2.157 seconds\n",
            "Construct multi-constraint weights: 0.010 seconds\n",
            "Metis partitioning: 25.412 seconds\n",
            "Split the graph: 0.640 seconds\n",
            "Construct subgraphs: 0.031 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert graphs to Jraph GraphsTuple\n",
        "processed_graphs = {}\n",
        "\n",
        "for idx in range(num_partitions):\n",
        "  node_ids = dgl_graph_metis_partition[idx].ndata['_ID']\n",
        "\n",
        "  labels = get_labels_for_subgraph(node_ids)\n",
        "  graph = dgl_graph_to_jraph(node_ids, \n",
        "                             labels, \n",
        "                             train_mask = train_mask.at[jnp.array(node_ids)].get(),\n",
        "                             valid_mask = valid_mask.at[jnp.array(node_ids)].get(),\n",
        "                             test_mask = test_mask.at[jnp.array(node_ids)].get()\n",
        "                             )\n",
        "\n",
        "  processed_graphs[f'partition_{idx}'] = {\n",
        "      'graph': graph._replace(nodes = graph.nodes['inputs']), \n",
        "      'labels': graph.nodes['targets'],\n",
        "      'train_mask': graph.nodes['train_mask'],\n",
        "      'valid_mask': graph.nodes['valid_mask'],\n",
        "      'test_mask': graph.nodes['test_mask'],\n",
        "      'padding_mask': graph.nodes['padding_mask']\n",
        "      }"
      ],
      "metadata": {
        "id": "s8-Ln58I_Fwp"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import haiku as hk\n",
        "import jax\n",
        "import optax\n",
        "\n",
        "from typing import Sequence\n",
        "\n",
        "# See https://github.com/YuxuanXie/mcl/blob/5f7ee92e2a6bc89736263873a4ba9c14d1a676ff/glassy_dynamics/train_using_jax.py for alternative to using GraphMapFeatures\n",
        "# From https://github.com/YuxuanXie/mcl/blob/5f7ee92e2a6bc89736263873a4ba9c14d1a676ff/glassy_dynamics/train_using_jax.py\n",
        "\n",
        "hidden_dimension = 128\n",
        "num_message_passing_steps = 6 # Question: (256, 4) fails / (128, 6) works\n",
        "\n",
        "@jraph.concatenated_args\n",
        "def node_update_fn(feats: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Node update function for graph net.\"\"\"\n",
        "  net = hk.Sequential([hk.nets.MLP(output_sizes = [hidden_dimension, hidden_dimension], activation = jax.nn.relu), hk.LayerNorm(axis = -1, create_scale = True, create_offset = True)])\n",
        "  return net(feats)\n",
        "\n",
        "@jraph.concatenated_args\n",
        "def edge_update_fn(feats: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Edge update function for graph net.\"\"\"\n",
        "  net = hk.Sequential([hk.nets.MLP(output_sizes = [hidden_dimension, hidden_dimension], activation = jax.nn.relu), hk.LayerNorm(axis = -1, create_scale = True, create_offset = True)])\n",
        "  return net(feats)\n",
        "\n",
        "@hk.without_apply_rng\n",
        "@hk.transform\n",
        "def network_definition(graph):\n",
        "  \"\"\"Defines a graph neural network.\n",
        "  Args:\n",
        "    graph: Graphstuple the network processes.\n",
        "  Returns:\n",
        "    Decoded nodes.\n",
        "  \"\"\"\n",
        "  graph = graph._replace(\n",
        "      nodes = hk.Linear(hidden_dimension)(graph.nodes),\n",
        "      device_edges = hk.Linear(hidden_dimension)(graph.device_edges)\n",
        "  )\n",
        "  \n",
        "  sharded_gn = sharded_graphnet.ShardedEdgesGraphNetwork(\n",
        "      update_node_fn = node_update_fn,\n",
        "      update_edge_fn = edge_update_fn,\n",
        "      num_shards = num_devices\n",
        "      )\n",
        "\n",
        "  for _ in range(num_message_passing_steps):\n",
        "    residual_graph = sharded_gn(graph)\n",
        "    graph = graph._replace(\n",
        "        nodes = graph.nodes + residual_graph.nodes,\n",
        "        device_edges = graph.device_edges + residual_graph.device_edges\n",
        "    )\n",
        "\n",
        "    # graph = sharded_gn(graph)\n",
        "\n",
        "  graph = graph._replace(\n",
        "      nodes = hk.Sequential([hk.Linear(hidden_dimension), jax.nn.relu, hk.Linear(112)])(graph.nodes)\n",
        "  )\n",
        "  return graph.nodes"
      ],
      "metadata": {
        "id": "gPg7ph7sWyOn"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bcast_local_devices(value):\n",
        "    \"\"\"Broadcasts an object to all local devices.\"\"\"\n",
        "    devices = jax.local_devices()\n",
        "\n",
        "    def _replicate(x):\n",
        "      \"\"\"Replicate an object on each device.\"\"\"\n",
        "      x = jnp.array(x)\n",
        "      return jax.device_put_sharded(len(devices) * [x], devices)\n",
        "\n",
        "    return jax.tree_util.tree_map(_replicate, value)"
      ],
      "metadata": {
        "id": "z6Qh75qxQfii"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reshape_broadcasted_data(data):\n",
        "  '''\n",
        "    Node predictions / Labels / Masks are identical on all the devices so we only take\n",
        "    one of them in order to remove the leading axis.\n",
        "  '''\n",
        "  return np.array(data)[0]\n",
        "  \n",
        "def remove_mask_from_data(data, mask):\n",
        "  '''\n",
        "    data.shape = [num_nodes, 112]\n",
        "    mask.shape = [num_nodes, 1]\n",
        "\n",
        "    We want to only return the data where mask == True\n",
        "  '''\n",
        "  sliced_data = np.compress(np.array(mask).reshape(-1).astype(bool), data, axis = 0)\n",
        "  return np.array(sliced_data)"
      ],
      "metadata": {
        "id": "oJ5T_oplbg_t"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import haiku as hk\n",
        "\n",
        "from random import randint\n",
        "\n",
        "# Try to follow this tutorial https://github.com/YuxuanXie/mcl/blob/5f7ee92e2a6bc89736263873a4ba9c14d1a676ff/glassy_dynamics/train_using_jax.py\n",
        "def compute_loss(params, graph, label, mask):\n",
        "  predictions = network_definition.apply(params, graph)\n",
        "\n",
        "  # use optax here (https://github.com/deepmind/optax/blob/master/optax/_src/loss.py#L116#L139)\n",
        "  loss = optax.sigmoid_binary_cross_entropy(predictions, label)  # shape [num_nodes, num_classes]\n",
        "  loss = loss * mask\n",
        "  loss = jnp.sum(loss) / jnp.sum(mask) # loss = mean_with_mask(loss, mask)\n",
        "\n",
        "  return loss\n",
        "\n",
        "def train(num_training_steps):\n",
        "  # replicated_params = jax.pmap(network_definition.init, axis_name = 'i')(bcast_local_devices(jax.random.PRNGKey(42)), processed_graphs['partition_0']['graph'])\n",
        "  replicated_params = jax.pmap(network_definition.init, axis_name = 'i')(bcast_local_devices(jax.random.PRNGKey(42)), demo_graph)\n",
        "\n",
        "  opt_init, opt_update = optax.adam(learning_rate = 0.1)  \n",
        "  replicated_opt_state = jax.pmap(opt_init, axis_name = 'i')(replicated_params)\n",
        "\n",
        "  @functools.partial(jax.pmap, axis_name='i')\n",
        "  def update(params, opt_state, graph, targets, mask):\n",
        "    # Compute the gradients on the given minibatch (individually on each device).\n",
        "    loss, grads = jax.value_and_grad(compute_loss)(params, graph, targets, mask)\n",
        "\n",
        "    # Combine the gradient across all devices (by taking their mean).\n",
        "    grads = jax.lax.pmean(grads, axis_name='i')\n",
        "\n",
        "    # Also combine the loss. Unnecessary for the update, but useful for logging.\n",
        "    loss = jax.lax.pmean(loss, axis_name='i')\n",
        "\n",
        "    updates, opt_state = opt_update(grads, opt_state)\n",
        "\n",
        "    return optax.apply_updates(params, updates), opt_state, loss\n",
        "\n",
        "  # Train\n",
        "  print(f'*** Trying to overfit on partition {0} ***')\n",
        "  print(f'*** Trying to overfit on partition {0} ***')\n",
        "  print(f'*** Trying to overfit on partition {0} ***')\n",
        "\n",
        "  for idx in range(num_training_steps):\n",
        "    # random_partition_idx = randint(0, num_partitions - 1)\n",
        "    # random_partition = processed_graphs[f'partition_{random_partition_idx}']\n",
        "\n",
        "    # graph = random_partition['graph']\n",
        "    # labels = random_partition['labels']   # Automatically broadcasted by the sharded graph net\n",
        "    # mask = random_partition['train_mask'] # Automatically broadcasted by the sharded graph net\n",
        "\n",
        "    graph = demo_graph\n",
        "    labels = demo_labels\n",
        "    mask = demo_mask\n",
        "\n",
        "    replicated_params, replicated_opt_state, loss = update(\n",
        "        replicated_params, \n",
        "        replicated_opt_state, \n",
        "        graph, \n",
        "        labels,\n",
        "        mask\n",
        "        )\n",
        "    \n",
        "    print('Loss training:', reshape_broadcasted_data(loss))\n",
        "\n",
        "    if (idx + 1) % 10 == 0:\n",
        "      print()\n",
        "      print(f'***************************')\n",
        "      print(f'Trained on {idx + 1} graphs')\n",
        "      print(f'***************************')\n",
        "      print()\n",
        "\n",
        "    # Write params and opt_state\n",
        "\n",
        "  return replicated_params\n",
        "\n",
        "def evaluate(params, num_graphs_eval):\n",
        "  # Evaluate\n",
        "  accumulated_loss = 0.0\n",
        "  accumulated_roc = 0\n",
        "  graphs_evaluated = 0\n",
        "\n",
        "  for idx in range(num_graphs_eval):\n",
        "    # random_partition_idx = idx\n",
        "    # random_partition = processed_graphs[f'partition_{random_partition_idx}']\n",
        "\n",
        "    # graph = random_partition['graph']\n",
        "    # labels = random_partition['labels']     # Automatically broadcasted by the sharded graph net\n",
        "    # mask = random_partition['train_mask']   # Automatically broadcasted by the sharded graph net \n",
        "    #                                         # TODO: Replace with test_mask\n",
        "\n",
        "    graph = demo_graph\n",
        "    labels = demo_labels\n",
        "    mask = demo_mask\n",
        "\n",
        "    predictions, loss = predict_on_graph(params, graph, labels, mask)\n",
        "    loss = reshape_broadcasted_data(loss)\n",
        "    \n",
        "    collected_labels = reshape_broadcasted_data(labels)\n",
        "    collected_predictions = reshape_broadcasted_data(predictions)\n",
        "    collected_mask = reshape_broadcasted_data(mask)\n",
        "\n",
        "    try:\n",
        "      roc = evaluator.eval({\n",
        "          \"y_true\": remove_mask_from_data(collected_labels, collected_mask), \n",
        "          \"y_pred\": remove_mask_from_data(collected_predictions, collected_mask)\n",
        "          })['rocauc']\n",
        "\n",
        "      accumulated_loss += loss\n",
        "      accumulated_roc += roc\n",
        "      graphs_evaluated += 1\n",
        "\n",
        "      print(f'Test loss: {loss} | ROC: {roc}')\n",
        "    except Exception as err:\n",
        "      print(f'Could not compute ROC for partition {idx}')\n",
        "      print('Most likely all of the nodes are hidden at test / validation')\n",
        "      print('Check counts in mask')\n",
        "      print(np.unique(collected_mask, return_counts = True))\n",
        "      print('Check counts in labels after removing the test / validation mask')\n",
        "      print(np.unique(remove_mask_from_data(collected_labels, collected_mask), return_counts = True))\n",
        "      print(f'Error message: {str(err)}')\n",
        "      print()\n",
        "\n",
        "    if (idx + 1) % 10 == 0:\n",
        "      print()\n",
        "      print(f'***************************')\n",
        "      print(f'Evaluated on {idx + 1} graphs')\n",
        "      print(f'***************************')\n",
        "      print()\n",
        "\n",
        "  print(f'Average test loss: {accumulated_loss / graphs_evaluated} | Average ROC: {accumulated_roc / graphs_evaluated}')\n",
        "\n",
        "@functools.partial(jax.pmap, axis_name='i')\n",
        "def predict_on_graph(params, graph, label, mask):\n",
        "  decoded_nodes = network_definition.apply(params, graph)\n",
        "\n",
        "  compute_loss_fn = functools.partial(compute_loss)\n",
        "  loss = compute_loss_fn(params, graph, label, mask)\n",
        "\n",
        "  return jax.nn.sigmoid(decoded_nodes).astype(int), loss\n",
        "\n",
        "final_params = train(num_training_steps = num_partitions * 1000)\n",
        "evaluate(final_params, num_graphs_eval = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYVzddNITMSv",
        "outputId": "487d097b-ce0d-454f-e864-844e99a2ce8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Trying to overfit on partition 0 ***\n",
            "*** Trying to overfit on partition 0 ***\n",
            "*** Trying to overfit on partition 0 ***\n",
            "Loss training: 132.64075\n",
            "Loss training: 2490.6733\n",
            "Loss training: 981.73645\n",
            "Loss training: 1430.4204\n",
            "Loss training: 334.303\n",
            "Loss training: 77.58702\n",
            "Loss training: 77.56461\n",
            "Loss training: 133.23216\n",
            "Loss training: 77.55504\n",
            "Loss training: 77.56648\n",
            "\n",
            "***************************\n",
            "Trained on 10 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.575455\n",
            "Loss training: 77.57991\n",
            "Loss training: 77.57868\n",
            "Loss training: 77.571236\n",
            "Loss training: 77.55751\n",
            "Loss training: 77.53799\n",
            "Loss training: 77.51336\n",
            "Loss training: 77.484535\n",
            "Loss training: 77.45258\n",
            "Loss training: 77.41871\n",
            "\n",
            "***************************\n",
            "Trained on 20 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.38378\n",
            "Loss training: 77.34884\n",
            "Loss training: 77.314865\n",
            "Loss training: 77.28259\n",
            "Loss training: 77.25268\n",
            "Loss training: 77.22552\n",
            "Loss training: 77.20152\n",
            "Loss training: 77.18086\n",
            "Loss training: 77.16348\n",
            "Loss training: 77.14927\n",
            "\n",
            "***************************\n",
            "Trained on 30 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.13805\n",
            "Loss training: 77.12951\n",
            "Loss training: 77.12335\n",
            "Loss training: 77.11906\n",
            "Loss training: 77.1163\n",
            "Loss training: 77.114815\n",
            "Loss training: 77.114235\n",
            "Loss training: 77.11419\n",
            "Loss training: 77.114334\n",
            "Loss training: 77.11452\n",
            "\n",
            "***************************\n",
            "Trained on 40 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.11453\n",
            "Loss training: 77.114365\n",
            "Loss training: 77.11384\n",
            "Loss training: 77.11293\n",
            "Loss training: 77.11171\n",
            "Loss training: 77.11023\n",
            "Loss training: 77.108475\n",
            "Loss training: 77.10655\n",
            "Loss training: 77.10453\n",
            "Loss training: 77.10251\n",
            "\n",
            "***************************\n",
            "Trained on 50 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.10052\n",
            "Loss training: 77.09868\n",
            "Loss training: 77.09698\n",
            "Loss training: 77.095535\n",
            "Loss training: 77.09427\n",
            "Loss training: 77.09322\n",
            "Loss training: 77.0924\n",
            "Loss training: 77.09176\n",
            "Loss training: 77.09128\n",
            "Loss training: 77.09094\n",
            "\n",
            "***************************\n",
            "Trained on 60 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.09072\n",
            "Loss training: 77.090576\n",
            "Loss training: 77.09048\n",
            "Loss training: 77.09041\n",
            "Loss training: 77.09038\n",
            "Loss training: 77.09035\n",
            "Loss training: 77.09028\n",
            "Loss training: 77.09022\n",
            "Loss training: 77.09012\n",
            "Loss training: 77.09\n",
            "\n",
            "***************************\n",
            "Trained on 70 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.089874\n",
            "Loss training: 77.08968\n",
            "Loss training: 77.08952\n",
            "Loss training: 77.089386\n",
            "Loss training: 77.08926\n",
            "Loss training: 77.08909\n",
            "Loss training: 77.08895\n",
            "Loss training: 77.088844\n",
            "Loss training: 77.08874\n",
            "Loss training: 77.08865\n",
            "\n",
            "***************************\n",
            "Trained on 80 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08857\n",
            "Loss training: 77.08855\n",
            "Loss training: 77.08852\n",
            "Loss training: 77.08852\n",
            "Loss training: 77.088486\n",
            "Loss training: 77.088486\n",
            "Loss training: 77.08848\n",
            "Loss training: 77.08848\n",
            "Loss training: 77.08846\n",
            "Loss training: 77.088425\n",
            "\n",
            "***************************\n",
            "Trained on 90 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08841\n",
            "Loss training: 77.088455\n",
            "Loss training: 77.08844\n",
            "Loss training: 77.08841\n",
            "Loss training: 77.088425\n",
            "Loss training: 77.0884\n",
            "Loss training: 77.08838\n",
            "Loss training: 77.08836\n",
            "Loss training: 77.08836\n",
            "Loss training: 77.088356\n",
            "\n",
            "***************************\n",
            "Trained on 100 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08833\n",
            "Loss training: 77.08832\n",
            "Loss training: 77.08832\n",
            "Loss training: 77.0883\n",
            "Loss training: 77.0883\n",
            "Loss training: 77.08831\n",
            "Loss training: 77.0883\n",
            "Loss training: 77.0883\n",
            "Loss training: 77.08829\n",
            "Loss training: 77.08829\n",
            "\n",
            "***************************\n",
            "Trained on 110 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08829\n",
            "Loss training: 77.08829\n",
            "Loss training: 77.08828\n",
            "Loss training: 77.08829\n",
            "Loss training: 77.08829\n",
            "Loss training: 77.08828\n",
            "Loss training: 77.08829\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08828\n",
            "\n",
            "***************************\n",
            "Trained on 120 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08828\n",
            "Loss training: 77.08828\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08828\n",
            "\n",
            "***************************\n",
            "Trained on 130 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08828\n",
            "Loss training: 77.08828\n",
            "Loss training: 77.08829\n",
            "Loss training: 77.08829\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08828\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 140 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08829\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08828\n",
            "Loss training: 77.08827\n",
            "\n",
            "***************************\n",
            "Trained on 150 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08829\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08827\n",
            "\n",
            "***************************\n",
            "Trained on 160 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08829\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08828\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08827\n",
            "\n",
            "***************************\n",
            "Trained on 170 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08828\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08827\n",
            "\n",
            "***************************\n",
            "Trained on 180 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08829\n",
            "Loss training: 77.08828\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08828\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08827\n",
            "\n",
            "***************************\n",
            "Trained on 190 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08827\n",
            "\n",
            "***************************\n",
            "Trained on 200 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 210 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 220 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 230 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08825\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 240 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08825\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 250 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 260 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 270 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 280 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08827\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08825\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 290 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 300 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 310 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 320 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 330 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 340 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 350 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 360 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 370 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 380 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 390 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 400 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 410 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 420 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 430 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 440 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 450 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 460 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 470 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 480 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 490 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 500 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 510 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 520 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 530 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 540 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 550 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 560 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 570 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 580 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 590 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 600 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 610 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 620 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 630 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 640 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 650 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 660 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 670 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 680 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 690 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 700 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 710 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 720 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 730 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 740 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 750 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 760 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 770 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 780 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 790 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 800 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 810 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 820 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 830 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 840 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 850 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 860 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 870 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 880 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 890 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 900 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 910 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 920 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 930 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 940 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 950 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 960 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 970 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 980 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 990 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 1000 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1010 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1020 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1030 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1040 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1050 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1060 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1070 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1080 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1090 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1100 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1110 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1120 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1130 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1140 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1150 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1160 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1170 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1180 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1190 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1200 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1210 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1220 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1230 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1240 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1250 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1260 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1270 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1280 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1290 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1300 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1310 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1320 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1330 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1340 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1350 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1360 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1370 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1380 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1390 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1400 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1410 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1420 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1430 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1440 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1450 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1460 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1470 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1480 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1490 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 1500 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1510 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1520 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1530 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1540 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1550 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 1560 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1570 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1580 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 1590 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1600 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1610 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 1620 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1630 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1640 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1650 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1660 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1670 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1680 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1690 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 1700 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1710 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1720 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1730 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1740 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1750 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1760 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1770 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1780 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 1790 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1800 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1810 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1820 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1830 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 1840 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 1850 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1860 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 1870 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1880 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 1890 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1900 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1910 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1920 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1930 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1940 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1950 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1960 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1970 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 1980 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 1990 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 2000 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 2010 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 2020 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 2030 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 2040 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 2050 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 2060 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 2070 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 2080 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "\n",
            "***************************\n",
            "Trained on 2090 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 2100 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 2110 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 2120 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 2130 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 2140 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.088264\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "Loss training: 77.08826\n",
            "\n",
            "***************************\n",
            "Trained on 2150 graphs\n",
            "***************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_on_full_sets(params):\n",
        "  final_predictions = {}\n",
        "\n",
        "  for i in range(num_partitions):\n",
        "    node_ids = dgl_graph_metis_partition[i].ndata['_ID']\n",
        "    partition = processed_graphs[f'partition_{i}']\n",
        "    padding_mask = reshape_broadcasted_data(partition['padding_mask'])\n",
        "    \n",
        "    predictions, _ = predict_on_graph(params, partition['graph'], partition['labels'], partition['test_mask'])\n",
        "    predictions_after_masked_nodes_are_removed = reshape_broadcasted_data(predictions)\n",
        "    predictions_after_masked_nodes_are_removed = remove_mask_from_data(\n",
        "        predictions_after_masked_nodes_are_removed,\n",
        "        padding_mask\n",
        "        )\n",
        "\n",
        "    for index, node_id in enumerate(node_ids):\n",
        "      final_predictions[node_id] = predictions_after_masked_nodes_are_removed[index]\n",
        "\n",
        "    if (i + 1) % 10 == 0:\n",
        "      print(f'Evaluated {i + 1} / {num_partitions} subgraphs...')\n",
        "\n",
        "  # Sort the final predictions based on the node ids\n",
        "  predictions_in_order = dict(sorted(final_predictions.items()))\n",
        "\n",
        "  # Convert the values to a list to be able to slice based on the ids of the \n",
        "  # nodes in the test set\n",
        "  predictions_in_order = list(predictions_in_order.values())\n",
        "\n",
        "  final_roc_train = evaluator.eval({\n",
        "      \"y_true\": np.array(train_label), \n",
        "      \"y_pred\": np.array([predictions_in_order[x] for x in split_idx['train']])\n",
        "      })['rocauc']\n",
        "\n",
        "  final_roc_valid = evaluator.eval({\n",
        "      \"y_true\": np.array(valid_label), \n",
        "      \"y_pred\": np.array([predictions_in_order[x] for x in split_idx['valid']])\n",
        "      })['rocauc']\n",
        "\n",
        "  final_roc_test = evaluator.eval({\n",
        "      \"y_true\": np.array(test_label),\n",
        "      \"y_pred\": np.array([predictions_in_order[x] for x in split_idx['test']])\n",
        "      })['rocauc']\n",
        "\n",
        "  print()\n",
        "  print(f'Final ROC on the train set {final_roc_train}')\n",
        "  print(f'Final ROC on the validation set {final_roc_valid}')\n",
        "  print(f'Final ROC on the test set {final_roc_test}')\n",
        "\n",
        "evaluate_on_full_sets(final_params)"
      ],
      "metadata": {
        "id": "aq4_r4M6VnzW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}