{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final-version-2-sharded-networks.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bf319/Scaling_MPNNs/blob/main/final_version_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rLhMwiHHWbtK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ff7711e-b146-49e9-f896-ea8b5edc4322"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-31 09:46:43--  https://raw.githubusercontent.com/deepmind/jraph/master/jraph/experimental/sharded_graphnet.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22444 (22K) [text/plain]\n",
            "Saving to: ‘sharded_graphnet.py.2’\n",
            "\n",
            "\rsharded_graphnet.py   0%[                    ]       0  --.-KB/s               \rsharded_graphnet.py 100%[===================>]  21.92K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2022-03-31 09:46:43 (15.3 MB/s) - ‘sharded_graphnet.py.2’ saved [22444/22444]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%pip install -q git+https://github.com/deepmind/dm-haiku\n",
        "%pip install -q jraph\n",
        "%pip install -q git+https://github.com/deepmind/jaxline\n",
        "%pip install -q ogb\n",
        "%pip install -q dgl\n",
        "%pip install -q optax\n",
        "%pip install -q metis\n",
        "\n",
        "!wget https://raw.githubusercontent.com/deepmind/jraph/master/jraph/experimental/sharded_graphnet.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "\n",
        "num_devices = jax.local_device_count()\n",
        "jax.devices()"
      ],
      "metadata": {
        "id": "iT2wqf76kIRp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af0a7cf2-61d3-4d82-8f70-3f37d7b78d63"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ogb.nodeproppred import Evaluator\n",
        "from ogb.nodeproppred import DglNodePropPredDataset\n",
        "\n",
        "dataset = DglNodePropPredDataset(name = \"ogbn-proteins\")\n",
        "split_idx = dataset.get_idx_split()\n",
        "evaluator = Evaluator(name = 'ogbn-proteins')\n",
        "print(evaluator.expected_input_format)"
      ],
      "metadata": {
        "id": "xHClucOxWpAZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38697c01-b8c8-4164-9156-4c492a44812d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using backend: pytorch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== Expected input format of Evaluator for ogbn-proteins\n",
            "{'y_true': y_true, 'y_pred': y_pred}\n",
            "- y_true: numpy ndarray or torch tensor of shape (num_node, num_task)\n",
            "- y_pred: numpy ndarray or torch tensor of shape (num_node, num_task)\n",
            "where y_pred stores score values (for computing ROC-AUC),\n",
            "num_task is 112, and each row corresponds to one node.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "# There is only one graph in Node Property Prediction datasets\n",
        "ogbn_proteins_main_graph, ogbn_proteins_main_labels = dataset[0]\n",
        "\n",
        "'''\n",
        "  OGBN-Proteins\n",
        "    #Nodes = 132,534\n",
        "    #Edges = 39,561,252\n",
        "    #Diameter ~ 9 (https://cs.stanford.edu/people/jure/pubs/ogb-neurips20.pdf)\n",
        "    #Tasks = 112\n",
        "    #Split Type = Species\n",
        "    #Task Type = Binary classification\n",
        "    #Metric = ROC-AUC\n",
        "\n",
        "    Task:\n",
        "      The task is to predict the presence of protein functions in a multi-label binary classification setup,\n",
        "      where there are 112 kinds of labels to predict in total. \n",
        "      The performance is measured by the average of ROC-AUC scores across the 112 tasks.\n",
        "\n",
        "    #Others:\n",
        "      **undirected**\n",
        "      **weighted**\n",
        "      **typed (according to species)**\n",
        "\n",
        "  (1) Nodes represent proteins\n",
        "    (1.1) The proteins come from 8 species\n",
        "      len(set(graph.ndata['species'].reshape(-1).tolist())) == 8\n",
        "    (1.2) Each node has one feature associated with it (its species)\n",
        "      graph.ndata['species'].shape == (#nodes, 1)\n",
        "  \n",
        "  (2) Edges indicate different types of biologically meaningful associations between proteins\n",
        "    (2.1) All edges come with 8-dimensional features\n",
        "      graph.edata['feat'].shape == (2 * #edges, 8)\n",
        "\n",
        "'''\n",
        "# Get split labels\n",
        "train_label = dataset.labels[split_idx['train']]  # (86619, 112) -- binary values (presence of protein functions)\n",
        "valid_label = dataset.labels[split_idx['valid']]  # (21236, 112) -- binary values (presence of protein functions)\n",
        "test_label = dataset.labels[split_idx['test']]    # (24679, 112) -- binary values (presence of protein functions)\n",
        "\n",
        "# Create masks\n",
        "train_mask = jnp.zeros((ogbn_proteins_main_graph.num_nodes(), 1)).at[jnp.array(split_idx['train'])].set(1)\n",
        "valid_mask = jnp.zeros((ogbn_proteins_main_graph.num_nodes(), 1)).at[jnp.array(split_idx['valid'])].set(1)\n",
        "test_mask = jnp.zeros((ogbn_proteins_main_graph.num_nodes(), 1)).at[jnp.array(split_idx['test'])].set(1)"
      ],
      "metadata": {
        "id": "jCkzIEb4WsXU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jraph\n",
        "\n",
        "# From https://colab.research.google.com/github/deepmind/educational/blob/master/colabs/summer_schools/intro_to_graph_nets_tutorial_with_jraph.ipynb#scrollTo=7vEmAsr5bKN8\n",
        "def _nearest_bigger_power_of_two(x: int) -> int:\n",
        "  \"\"\"Computes the nearest power of two greater than x for padding.\"\"\"\n",
        "  y = 2\n",
        "  while y < x:\n",
        "    y *= 2\n",
        "  return y\n",
        "\n",
        "def pad_graph_to_nearest_power_of_two(\n",
        "    graphs_tuple: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
        "  \"\"\"Pads a batched `GraphsTuple` to the nearest power of two.\n",
        "  For example, if a `GraphsTuple` has 7 nodes, 5 edges and 3 graphs, this method\n",
        "  would pad the `GraphsTuple` nodes and edges:\n",
        "    7 nodes --> 8 nodes (2^3)\n",
        "    5 edges --> 8 edges (2^3)\n",
        "  And since padding is accomplished using `jraph.pad_with_graphs`, an extra\n",
        "  graph and node is added:\n",
        "    8 nodes --> 9 nodes\n",
        "    3 graphs --> 4 graphs\n",
        "  Args:\n",
        "    graphs_tuple: a batched `GraphsTuple` (can be batch size 1).\n",
        "  Returns:\n",
        "    A graphs_tuple batched to the nearest power of two.\n",
        "  \"\"\"\n",
        "  # Add 1 since we need at least one padding node for pad_with_graphs.\n",
        "  pad_nodes_to = _nearest_bigger_power_of_two(jnp.sum(graphs_tuple.n_node)) + 1\n",
        "  pad_edges_to = _nearest_bigger_power_of_two(jnp.sum(graphs_tuple.n_edge))\n",
        "  # Add 1 since we need at least one padding graph for pad_with_graphs.\n",
        "  # We do not pad to nearest power of two because the batch size is fixed.\n",
        "  pad_graphs_to = graphs_tuple.n_node.shape[0] + 1\n",
        "  return jraph.pad_with_graphs(graphs_tuple, pad_nodes_to, pad_edges_to,\n",
        "                               pad_graphs_to)"
      ],
      "metadata": {
        "id": "FSbePOUh2NBB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "from random import randint\n",
        "import numpy as np\n",
        "import sharded_graphnet\n",
        "\n",
        "def get_demo_training_graph():\n",
        "  num_nodes = 16\n",
        "  num_edges = 8\n",
        "\n",
        "  rand_dgl_graph = dgl.rand_graph(num_nodes = num_nodes, num_edges = num_edges)\n",
        "\n",
        "  node_features = jnp.array([[randint(0, 7)] for i in range(num_nodes)])\n",
        "  edge_features = jnp.array([[0.1 * randint(0, 10) for _ in range(8)] for i in range(num_edges)])\n",
        "\n",
        "  senders = jnp.array(rand_dgl_graph.edges()[0])\n",
        "  receivers = jnp.array(rand_dgl_graph.edges()[1])\n",
        "\n",
        "  in_tuple = jraph.GraphsTuple(\n",
        "            nodes = node_features.astype(np.float32),\n",
        "            edges = edge_features.astype(np.float32),  \n",
        "            senders = senders.astype(np.int32), \n",
        "            receivers = receivers.astype(np.int32),\n",
        "            n_node = jnp.array([num_nodes]), \n",
        "            n_edge = jnp.array([num_edges]),\n",
        "            globals = None  # No global features\n",
        "          )\n",
        "  \n",
        "  labels = jnp.array([[randint(0, 1) for j in range(112)] for i in range(num_nodes)])\n",
        "  train_mask = jnp.ones((num_nodes, 1))\n",
        "  \n",
        "  in_tuple = in_tuple._replace(\n",
        "      nodes = {\n",
        "          'inputs': in_tuple.nodes, \n",
        "          'targets': labels, \n",
        "          'train_mask': train_mask, \n",
        "          }\n",
        "  )\n",
        "\n",
        "  # in_tuple = pad_graph_to_nearest_power_of_two(in_tuple)\n",
        "  \n",
        "  return sharded_graphnet.graphs_tuple_to_broadcasted_sharded_graphs_tuple(\n",
        "      in_tuple,\n",
        "      num_shards = num_devices\n",
        "      )\n",
        "  \n",
        "demo_graph = get_demo_training_graph()\n",
        "demo_labels = demo_graph.nodes['targets']\n",
        "demo_mask = demo_graph.nodes['train_mask']\n",
        "demo_graph = demo_graph._replace(nodes = demo_graph.nodes['inputs']) "
      ],
      "metadata": {
        "id": "SKoX4h1z9ItW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import jraph\n",
        "import sharded_graphnet\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "enc = OneHotEncoder()\n",
        "enc.fit(ogbn_proteins_main_graph.ndata['species'])\n",
        "\n",
        "def dgl_graph_to_jraph(node_ids, labels, train_mask, valid_mask, test_mask):\n",
        "  # First add back the node and edge features\n",
        "  dgl_graph_with_features = dgl.node_subgraph(ogbn_proteins_main_graph, node_ids)\n",
        "\n",
        "  node_features = jnp.array(enc.transform(dgl_graph_with_features.ndata['species']).toarray())\n",
        "  senders = jnp.array(dgl_graph_with_features.edges()[0])\n",
        "  receivers = jnp.array(dgl_graph_with_features.edges()[1])\n",
        "\n",
        "  # Edges -- here we should include the 8-dimensional edge features\n",
        "  edges = jnp.array(dgl_graph_with_features.edata['feat'])\n",
        "\n",
        "  n_node = jnp.array([dgl_graph_with_features.num_nodes()])\n",
        "  n_edge = jnp.array([dgl_graph_with_features.num_edges()])\n",
        "\n",
        "  in_tuple = jraph.GraphsTuple(\n",
        "            nodes = node_features.astype(np.float32),\n",
        "            senders = senders.astype(np.int32), \n",
        "            receivers = receivers.astype(np.int32),\n",
        "            edges = edges.astype(np.float32),  \n",
        "            n_node = n_node, \n",
        "            n_edge = n_edge,\n",
        "            globals = None  # No global features\n",
        "          )\n",
        "  \n",
        "  in_tuple = in_tuple._replace(\n",
        "      nodes = {\n",
        "          'inputs': in_tuple.nodes, \n",
        "          'targets': labels, \n",
        "          'train_mask': train_mask, \n",
        "          'valid_mask': valid_mask, \n",
        "          'test_mask': test_mask,\n",
        "          'padding_mask': jnp.ones((in_tuple.nodes.shape[0], 1)) \n",
        "                                                        # TODO: Check this above\n",
        "                                                        # Adding this mask so that we can remove the nodes added after padding \n",
        "                                                        # for the final ROC computations on the full train / valid / test splits\n",
        "                                                        # This is because I want to pass the predictions on the true nodes to the \n",
        "                                                        # ogbn-evaluator, so I would first need to remove the predictions that come from padding.\n",
        "          }\n",
        "  )\n",
        "\n",
        "  in_tuple = pad_graph_to_nearest_power_of_two(in_tuple)\n",
        "  \n",
        "  return sharded_graphnet.graphs_tuple_to_broadcasted_sharded_graphs_tuple(\n",
        "      in_tuple,\n",
        "      num_shards = num_devices\n",
        "      )\n",
        "  \n",
        "def get_labels_for_subgraph(node_ids):\n",
        "  return jnp.array(ogbn_proteins_main_labels.index_select(0, node_ids))"
      ],
      "metadata": {
        "id": "fvH_XRJVWuLw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "\n",
        "'''\n",
        "  Generate graph partition using metis, with balanced number of edges in each partition.\n",
        "  Note: \n",
        "    The subgraphs do not contain the node/edge data in the input graph (https://docs.dgl.ai/generated/dgl.metis_partition.html)\n",
        "'''\n",
        "num_partitions = 100  ## TODO: Find some way to decrease this to something reasonable (< 50)\n",
        "\n",
        "dgl_graph_metis_partition = dgl.metis_partition(ogbn_proteins_main_graph, num_partitions, balance_edges = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUI9s4-0mPz9",
        "outputId": "6e0b549e-da57-40d9-8b93-5c493ffa946e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convert a graph into a bidirected graph: 3.603 seconds\n",
            "Construct multi-constraint weights: 0.003 seconds\n",
            "Metis partitioning: 32.929 seconds\n",
            "Split the graph: 0.536 seconds\n",
            "Construct subgraphs: 0.112 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert graphs to Jraph GraphsTuple\n",
        "processed_graphs = {}\n",
        "\n",
        "for idx in range(num_partitions):\n",
        "  node_ids = dgl_graph_metis_partition[idx].ndata['_ID']\n",
        "\n",
        "  labels = get_labels_for_subgraph(node_ids)\n",
        "  graph = dgl_graph_to_jraph(node_ids, \n",
        "                             labels, \n",
        "                             train_mask = train_mask.at[jnp.array(node_ids)].get(),\n",
        "                             valid_mask = valid_mask.at[jnp.array(node_ids)].get(),\n",
        "                             test_mask = test_mask.at[jnp.array(node_ids)].get()\n",
        "                             )\n",
        "\n",
        "  processed_graphs[f'partition_{idx}'] = {\n",
        "      'graph': graph._replace(nodes = graph.nodes['inputs']), \n",
        "      'labels': graph.nodes['targets'],\n",
        "      'train_mask': graph.nodes['train_mask'],\n",
        "      'valid_mask': graph.nodes['valid_mask'],\n",
        "      'test_mask': graph.nodes['test_mask'],\n",
        "      'padding_mask': graph.nodes['padding_mask']\n",
        "      }"
      ],
      "metadata": {
        "id": "s8-Ln58I_Fwp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import haiku as hk\n",
        "import jax\n",
        "import optax\n",
        "\n",
        "from typing import Sequence\n",
        "\n",
        "# See https://github.com/YuxuanXie/mcl/blob/5f7ee92e2a6bc89736263873a4ba9c14d1a676ff/glassy_dynamics/train_using_jax.py for alternative to using GraphMapFeatures\n",
        "# From https://github.com/YuxuanXie/mcl/blob/5f7ee92e2a6bc89736263873a4ba9c14d1a676ff/glassy_dynamics/train_using_jax.py\n",
        "\n",
        "hidden_dimension = 128\n",
        "num_message_passing_steps = 5 # Question: (256, 4) fails / (128, 6) works\n",
        "\n",
        "@jraph.concatenated_args\n",
        "def node_update_fn(feats: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Node update function for graph net.\"\"\"\n",
        "  net = hk.Sequential([hk.nets.MLP(output_sizes = [hidden_dimension, hidden_dimension], activation = jax.nn.relu, activate_final = False), hk.LayerNorm(axis = -1, create_scale = True, create_offset = True)])\n",
        "  return net(feats)\n",
        "\n",
        "@jraph.concatenated_args\n",
        "def edge_update_fn(feats: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Edge update function for graph net.\"\"\"\n",
        "  net = hk.Sequential([hk.nets.MLP(output_sizes = [hidden_dimension, hidden_dimension], activation = jax.nn.relu, activate_final = False), hk.LayerNorm(axis = -1, create_scale = True, create_offset = True)])\n",
        "  return net(feats)\n",
        "\n",
        "@hk.without_apply_rng\n",
        "@hk.transform\n",
        "def network_definition(graph):\n",
        "  \"\"\"Defines a graph neural network.\n",
        "  Args:\n",
        "    graph: Graphstuple the network processes.\n",
        "  Returns:\n",
        "    Decoded nodes.\n",
        "  \"\"\"\n",
        "  graph = graph._replace(\n",
        "      nodes = hk.Linear(hidden_dimension)(graph.nodes),\n",
        "      device_edges = hk.Linear(hidden_dimension)(graph.device_edges)\n",
        "  )\n",
        "  \n",
        "  sharded_gn = sharded_graphnet.ShardedEdgesGraphNetwork(\n",
        "      update_node_fn = node_update_fn,\n",
        "      update_edge_fn = edge_update_fn,\n",
        "      num_shards = num_devices\n",
        "      )\n",
        "\n",
        "  for _ in range(num_message_passing_steps):\n",
        "    residual_graph = sharded_gn(graph)\n",
        "    graph = graph._replace(\n",
        "        nodes = graph.nodes + residual_graph.nodes, # Question: Should I be using relu afterwards?\n",
        "        device_edges = graph.device_edges + residual_graph.device_edges\n",
        "    )\n",
        "\n",
        "  graph = graph._replace(\n",
        "      nodes = hk.Sequential([hk.Linear(hidden_dimension), jax.nn.relu, hk.Linear(112)])(graph.nodes)\n",
        "  )\n",
        "  return graph.nodes"
      ],
      "metadata": {
        "id": "gPg7ph7sWyOn"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bcast_local_devices(value):\n",
        "    \"\"\"Broadcasts an object to all local devices.\"\"\"\n",
        "    devices = jax.local_devices()\n",
        "\n",
        "    def _replicate(x):\n",
        "      \"\"\"Replicate an object on each device.\"\"\"\n",
        "      x = jnp.array(x)\n",
        "      return jax.device_put_sharded(len(devices) * [x], devices)\n",
        "\n",
        "    return jax.tree_util.tree_map(_replicate, value)"
      ],
      "metadata": {
        "id": "z6Qh75qxQfii"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reshape_broadcasted_data(data):\n",
        "  '''\n",
        "    Node predictions / Labels / Masks are identical on all the devices so we only take\n",
        "    one of them in order to remove the leading axis.\n",
        "  '''\n",
        "  return np.array(data)[0]\n",
        "  \n",
        "def remove_mask_from_data(data, mask):\n",
        "  '''\n",
        "    data.shape = [num_nodes, 112]\n",
        "    mask.shape = [num_nodes, 1]\n",
        "\n",
        "    We want to only return the data where mask == True\n",
        "  '''\n",
        "  sliced_data = np.compress(np.array(mask).reshape(-1).astype(bool), data, axis = 0)\n",
        "  return np.array(sliced_data)"
      ],
      "metadata": {
        "id": "oJ5T_oplbg_t"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import functools\n",
        "import haiku as hk\n",
        "\n",
        "from random import randint\n",
        "from google.colab import files\n",
        "\n",
        "# Try to follow this tutorial https://github.com/YuxuanXie/mcl/blob/5f7ee92e2a6bc89736263873a4ba9c14d1a676ff/glassy_dynamics/train_using_jax.py\n",
        "def compute_loss(params, graph, label, mask):\n",
        "  predictions = network_definition.apply(params, graph)\n",
        "\n",
        "  # use optax here (https://github.com/deepmind/optax/blob/master/optax/_src/loss.py#L116#L139)\n",
        "  loss = optax.sigmoid_binary_cross_entropy(predictions, label)  # shape [num_nodes, num_classes]\n",
        "  loss = loss * mask\n",
        "  loss = jnp.sum(loss) / jnp.sum(mask) # loss = mean_with_mask(loss, mask)\n",
        "\n",
        "  return loss\n",
        "\n",
        "def train(num_training_steps, learning_rate, results_path):\n",
        "  replicated_params = jax.pmap(network_definition.init, axis_name = 'i')(bcast_local_devices(jax.random.PRNGKey(42)), processed_graphs['partition_0']['graph'])\n",
        "\n",
        "  opt_init, opt_update = optax.adam(learning_rate = learning_rate)  \n",
        "  replicated_opt_state = jax.pmap(opt_init, axis_name = 'i')(replicated_params)\n",
        "\n",
        "  @functools.partial(jax.pmap, axis_name='i')\n",
        "  def update(params, opt_state, graph, targets, mask):\n",
        "    # Compute the gradients on the given minibatch (individually on each device).\n",
        "    loss, grads = jax.value_and_grad(compute_loss)(params, graph, targets, mask)\n",
        "\n",
        "    # Combine the gradient across all devices (by taking their mean).\n",
        "    grads = jax.lax.pmean(grads, axis_name='i')\n",
        "\n",
        "    # Also combine the loss. Unnecessary for the update, but useful for logging.\n",
        "    loss = jax.lax.pmean(loss, axis_name='i')\n",
        "\n",
        "    updates, opt_state = opt_update(grads, opt_state)\n",
        "\n",
        "    return optax.apply_updates(params, updates), opt_state, loss\n",
        "\n",
        "  # Train\n",
        "  for idx in range(num_training_steps):\n",
        "    random_partition_idx = randint(0, num_partitions - 1)\n",
        "    random_partition = processed_graphs[f'partition_{random_partition_idx}']\n",
        "\n",
        "    graph = random_partition['graph']\n",
        "    labels = random_partition['labels']   # Automatically broadcasted by the sharded graph net\n",
        "    mask = random_partition['train_mask'] # Automatically broadcasted by the sharded graph net\n",
        "\n",
        "    replicated_params, replicated_opt_state, loss = update(\n",
        "        replicated_params, \n",
        "        replicated_opt_state, \n",
        "        graph, \n",
        "        labels,\n",
        "        mask\n",
        "        )\n",
        "    \n",
        "    print('Loss training:', reshape_broadcasted_data(loss))\n",
        "\n",
        "    if (idx + 1) % 10 == 0:\n",
        "      print()\n",
        "      print(f'***************************')\n",
        "      print(f'Trained on {idx + 1} graphs')\n",
        "      print(f'***************************')\n",
        "      print()\n",
        "\n",
        "    if (idx + 1) % 2000 == 0:\n",
        "      # Save parameters every 1000 epochs\n",
        "      params_file = f'{results_path}/params_epochs_{idx + 1}.pickle'\n",
        "      opt_state_file = f'{results_path}/opt_state_epochs_{idx + 1}.pickle'\n",
        "\n",
        "      with open(params_file, 'wb') as f:\n",
        "        # Save parameters to file\n",
        "        pickle.dump(replicated_params, f)\n",
        "\n",
        "        # Download in case workspace gets restarted\n",
        "        files.download(params_file)\n",
        "\n",
        "      with open(opt_state_file, 'wb') as f:\n",
        "        # Save optimiser state to file\n",
        "        pickle.dump(replicated_opt_state, f)\n",
        "\n",
        "        # Download in case workspace gets restarted\n",
        "        files.download(opt_state_file)\n",
        "\n",
        "  return replicated_params\n",
        "\n",
        "def evaluate(params, num_graphs_eval):\n",
        "  # Evaluate\n",
        "  accumulated_loss = 0.0\n",
        "  accumulated_roc = 0\n",
        "  graphs_evaluated = 0\n",
        "\n",
        "  print('**** Note that this uses the train_mask ****')\n",
        "  print('**** Note that this uses the train_mask ****')\n",
        "  print('**** Note that this uses the train_mask ****')\n",
        "\n",
        "  for idx in range(num_graphs_eval):\n",
        "    random_partition_idx = idx\n",
        "    random_partition = processed_graphs[f'partition_{random_partition_idx}']\n",
        "\n",
        "    graph = random_partition['graph']\n",
        "    labels = random_partition['labels']     # Automatically broadcasted by the sharded graph net\n",
        "    mask = random_partition['test_mask']    # Automatically broadcasted by the sharded graph net\n",
        "\n",
        "    predictions, loss = predict_on_graph(params, graph, labels, mask)\n",
        "    loss = reshape_broadcasted_data(loss)\n",
        "    \n",
        "    collected_labels = reshape_broadcasted_data(labels)\n",
        "    collected_predictions = reshape_broadcasted_data(predictions)\n",
        "    collected_mask = reshape_broadcasted_data(mask)\n",
        "\n",
        "    try:\n",
        "      roc = evaluator.eval({\n",
        "          \"y_true\": remove_mask_from_data(collected_labels, collected_mask), \n",
        "          \"y_pred\": remove_mask_from_data(collected_predictions, collected_mask)\n",
        "          })['rocauc']\n",
        "\n",
        "      accumulated_loss += loss\n",
        "      accumulated_roc += roc\n",
        "      graphs_evaluated += 1\n",
        "\n",
        "      print(f'Test loss: {loss} | ROC: {roc}')\n",
        "    except Exception as err:\n",
        "      print(f'Error message: \\n{str(err)}')\n",
        "      print()\n",
        "\n",
        "    if (idx + 1) % 10 == 0:\n",
        "      print()\n",
        "      print(f'***************************')\n",
        "      print(f'Evaluated on {idx + 1} graphs')\n",
        "      print(f'***************************')\n",
        "      print()\n",
        "\n",
        "  print(f'Average test loss: {accumulated_loss / graphs_evaluated} | Average ROC: {accumulated_roc / graphs_evaluated}')\n",
        "\n",
        "@functools.partial(jax.pmap, axis_name='i')\n",
        "def predict_on_graph(params, graph, label, mask):\n",
        "  decoded_nodes = network_definition.apply(params, graph)\n",
        "\n",
        "  compute_loss_fn = functools.partial(compute_loss)\n",
        "  loss = compute_loss_fn(params, graph, label, mask)\n",
        "\n",
        "  return jax.nn.sigmoid(decoded_nodes), loss"
      ],
      "metadata": {
        "id": "xYVzddNITMSv"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_on_full_sets(params):\n",
        "  final_predictions = {}\n",
        "\n",
        "  for i in range(num_partitions):\n",
        "    node_ids = dgl_graph_metis_partition[i].ndata['_ID']\n",
        "    partition = processed_graphs[f'partition_{i}']\n",
        "    \n",
        "    predictions, _ = predict_on_graph(params, \n",
        "                                      partition['graph'], \n",
        "                                      partition['labels'], \n",
        "                                      partition['test_mask']  # Only used in the loss computation, does not affect predictions\n",
        "                                      )\n",
        "\n",
        "    predictions_after_masked_nodes_are_removed = remove_mask_from_data(\n",
        "        reshape_broadcasted_data(predictions),\n",
        "        reshape_broadcasted_data(partition['padding_mask'])\n",
        "        )\n",
        "\n",
        "    for index, node_id in enumerate(node_ids):\n",
        "      final_predictions[node_id] = predictions_after_masked_nodes_are_removed[index]\n",
        "\n",
        "    if (i + 1) % 10 == 0:\n",
        "      print(f'Evaluated {i + 1} / {num_partitions} subgraphs...')\n",
        "\n",
        "  # Sort the final predictions based on the node ids\n",
        "  predictions_in_order = dict(sorted(final_predictions.items()))\n",
        "\n",
        "  # Convert the values to a list to be able to slice based on the ids of the \n",
        "  # nodes in the test set\n",
        "  predictions_in_order = list(predictions_in_order.values())\n",
        "\n",
        "  final_roc_train = evaluator.eval({\n",
        "      \"y_true\": np.array(train_label), \n",
        "      \"y_pred\": np.array([predictions_in_order[x] for x in split_idx['train']])\n",
        "      })['rocauc']\n",
        "\n",
        "  final_roc_valid = evaluator.eval({\n",
        "      \"y_true\": np.array(valid_label), \n",
        "      \"y_pred\": np.array([predictions_in_order[x] for x in split_idx['valid']])\n",
        "      })['rocauc']\n",
        "\n",
        "  final_roc_test = evaluator.eval({\n",
        "      \"y_true\": np.array(test_label),\n",
        "      \"y_pred\": np.array([predictions_in_order[x] for x in split_idx['test']])\n",
        "      })['rocauc']\n",
        "\n",
        "  print()\n",
        "  print(f'Final ROC on the train set {final_roc_train}')\n",
        "  print(f'Final ROC on the validation set {final_roc_valid}')\n",
        "  print(f'Final ROC on the test set {final_roc_test}')"
      ],
      "metadata": {
        "id": "Xt-6IXF8U5vs"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "current_time = datetime.today().strftime('%Y-%m-%d-%H:%M:%S')\n",
        "exp_path = f'/content/exp_{current_time}/'\n",
        "os.makedirs(exp_path, exist_ok = False)\n",
        "\n",
        "# Main training loop\n",
        "final_params = train(\n",
        "    num_training_steps = 500, \n",
        "    learning_rate = 0.01, ## TODO: Maybe modify the learning rate? \n",
        "    results_path = exp_path\n",
        "    )\n",
        "\n",
        "# with open('/content/exp_2022-03-30-09:51:17/params_epochs_9000.pickle', 'rb') as f:\n",
        "#     loaded_params = pickle.load(f)\n",
        "loaded_params = final_params\n",
        "evaluate_on_full_sets(loaded_params)\n",
        "\n",
        "'''\n",
        "  Previous runs:\n",
        "  (1) Configuration\n",
        "        learning_rate = 0.001\n",
        "        num_partitions = 50\n",
        "        hidden_dimension = 128\n",
        "        num_message_passing_steps = 5\n",
        "        num_training_steps = 1000\n",
        "    ROC on the train set 0.7348797273386144\n",
        "    ROC on the validation set 0.6025038939324504\n",
        "    ROC on the test set 0.5896861508337246\n",
        "\n",
        "  (2) Configuration\n",
        "        learning_rate = 0.001\n",
        "        num_partitions = 50\n",
        "        hidden_dimension = 128\n",
        "        num_message_passing_steps = 5\n",
        "        num_training_steps = 3000\n",
        "    ROC on the train set 0.8050085464161815\n",
        "    ROC on the validation set 0.6327603823722211\n",
        "    ROC on the test set 0.5078022533003436\n",
        "\n",
        "  (3) Configuration\n",
        "        learning_rate = 0.1 (Question: I think this might be too high -- based on the results in (5) with lower number of epochs)\n",
        "        num_partitions = 100\n",
        "        hidden_dimension = 128\n",
        "        num_message_passing_steps = 5\n",
        "        num_training_steps = 1000\n",
        "    ROC on the train set 0.5\n",
        "    ROC on the validation set 0.5 \n",
        "    ROC on the test set 0.5\n",
        "\n",
        "  (4) Configuration\n",
        "        learning_rate = 0.01\n",
        "        num_partitions = 100\n",
        "        hidden_dimension = 128\n",
        "        num_message_passing_steps = 5\n",
        "        num_training_steps = 100\n",
        "    ROC on the train set 0.6501172261188106\n",
        "    ROC on the validation set 0.5281974299591566\n",
        "    ROC on the test set 0.47652056321124514\n",
        "\n",
        "  (5) Configuration\n",
        "        learning_rate = 0.01\n",
        "        num_partitions = 100\n",
        "        hidden_dimension = 128\n",
        "        num_message_passing_steps = 5\n",
        "        num_training_steps = 500\n",
        "    ROC on the train set 0.6939371049645034\n",
        "    ROC on the validation set 0.559224577731843\n",
        "    ROC on the test set 0.5488968392833208\n",
        "'''"
      ],
      "metadata": {
        "id": "N4sdL1RSWurH",
        "outputId": "358169e9-341a-413a-d43c-4b326c833682",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss training: 98.99076\n",
            "Loss training: 66.68706\n",
            "Loss training: 49.040108\n",
            "Loss training: 37.073647\n",
            "Loss training: 55.795807\n",
            "Loss training: 55.865505\n",
            "Loss training: 40.578808\n",
            "Loss training: 47.454693\n",
            "Loss training: 32.199127\n",
            "Loss training: 43.01808\n",
            "\n",
            "***************************\n",
            "Trained on 10 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 47.017223\n",
            "Loss training: 44.602627\n",
            "Loss training: 38.274002\n",
            "Loss training: 36.411457\n",
            "Loss training: 34.30842\n",
            "Loss training: 15.22694\n",
            "Loss training: 35.10764\n",
            "Loss training: 38.523518\n",
            "Loss training: 42.649315\n",
            "Loss training: 43.31335\n",
            "\n",
            "***************************\n",
            "Trained on 20 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.921654\n",
            "Loss training: 32.419365\n",
            "Loss training: 60.892456\n",
            "Loss training: 42.008472\n",
            "Loss training: 39.028446\n",
            "Loss training: 36.60769\n",
            "Loss training: 47.61043\n",
            "Loss training: 41.16031\n",
            "Loss training: 26.810614\n",
            "Loss training: 46.133453\n",
            "\n",
            "***************************\n",
            "Trained on 30 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.971418\n",
            "Loss training: 31.75446\n",
            "Loss training: 45.133583\n",
            "Loss training: 59.525448\n",
            "Loss training: 37.326237\n",
            "Loss training: 42.932156\n",
            "Loss training: 28.23426\n",
            "Loss training: 41.964317\n",
            "Loss training: 57.162117\n",
            "Loss training: 33.20786\n",
            "\n",
            "***************************\n",
            "Trained on 40 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 55.925823\n",
            "Loss training: 61.950775\n",
            "Loss training: 48.86738\n",
            "Loss training: 47.89349\n",
            "Loss training: 39.923637\n",
            "Loss training: 47.139313\n",
            "Loss training: 81.89056\n",
            "Loss training: 46.986004\n",
            "Loss training: 58.250923\n",
            "Loss training: 49.4398\n",
            "\n",
            "***************************\n",
            "Trained on 50 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 52.37734\n",
            "Loss training: 45.390205\n",
            "Loss training: 29.421755\n",
            "Loss training: 57.473614\n",
            "Loss training: 56.29921\n",
            "Loss training: 30.152752\n",
            "Loss training: 64.847176\n",
            "Loss training: 50.34948\n",
            "Loss training: 56.467766\n",
            "Loss training: 44.332176\n",
            "\n",
            "***************************\n",
            "Trained on 60 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 56.837276\n",
            "Loss training: 42.267815\n",
            "Loss training: 33.80771\n",
            "Loss training: 49.90525\n",
            "Loss training: 29.866823\n",
            "Loss training: 36.431705\n",
            "Loss training: 57.364597\n",
            "Loss training: 32.363922\n",
            "Loss training: 40.254646\n",
            "Loss training: 63.295933\n",
            "\n",
            "***************************\n",
            "Trained on 70 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 63.02922\n",
            "Loss training: 60.613876\n",
            "Loss training: 57.794556\n",
            "Loss training: 56.745052\n",
            "Loss training: 54.10465\n",
            "Loss training: 45.576942\n",
            "Loss training: 60.971943\n",
            "Loss training: 40.163155\n",
            "Loss training: 42.34078\n",
            "Loss training: 50.304356\n",
            "\n",
            "***************************\n",
            "Trained on 80 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 86.27437\n",
            "Loss training: 75.12579\n",
            "Loss training: 35.43786\n",
            "Loss training: 47.167603\n",
            "Loss training: 47.198574\n",
            "Loss training: 39.450287\n",
            "Loss training: 44.934937\n",
            "Loss training: 35.060444\n",
            "Loss training: 47.854683\n",
            "Loss training: 25.258303\n",
            "\n",
            "***************************\n",
            "Trained on 90 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.26956\n",
            "Loss training: 26.086384\n",
            "Loss training: 48.002834\n",
            "Loss training: 21.39298\n",
            "Loss training: 41.381504\n",
            "Loss training: 20.746965\n",
            "Loss training: 64.47025\n",
            "Loss training: 36.357635\n",
            "Loss training: 36.89672\n",
            "Loss training: 49.669724\n",
            "\n",
            "***************************\n",
            "Trained on 100 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 51.72278\n",
            "Loss training: 56.101837\n",
            "Loss training: 43.74825\n",
            "Loss training: 38.931465\n",
            "Loss training: 49.52923\n",
            "Loss training: 37.52256\n",
            "Loss training: 34.21427\n",
            "Loss training: 66.82855\n",
            "Loss training: 46.358307\n",
            "Loss training: 35.77731\n",
            "\n",
            "***************************\n",
            "Trained on 110 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.43344\n",
            "Loss training: 28.122229\n",
            "Loss training: 52.52851\n",
            "Loss training: 24.189844\n",
            "Loss training: 24.821264\n",
            "Loss training: 67.68427\n",
            "Loss training: 35.72285\n",
            "Loss training: 30.976938\n",
            "Loss training: 16.569319\n",
            "Loss training: 45.757126\n",
            "\n",
            "***************************\n",
            "Trained on 120 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.521378\n",
            "Loss training: 44.256897\n",
            "Loss training: 59.134796\n",
            "Loss training: 53.31854\n",
            "Loss training: 37.45703\n",
            "Loss training: 26.939053\n",
            "Loss training: 39.601345\n",
            "Loss training: 44.08967\n",
            "Loss training: 35.791718\n",
            "Loss training: 46.627766\n",
            "\n",
            "***************************\n",
            "Trained on 130 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 73.85063\n",
            "Loss training: 44.731895\n",
            "Loss training: 49.10638\n",
            "Loss training: 46.599884\n",
            "Loss training: 40.019135\n",
            "Loss training: 45.23047\n",
            "Loss training: 46.02772\n",
            "Loss training: 44.676155\n",
            "Loss training: 45.09109\n",
            "Loss training: 45.08737\n",
            "\n",
            "***************************\n",
            "Trained on 140 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.687513\n",
            "Loss training: 30.364138\n",
            "Loss training: 61.463417\n",
            "Loss training: 49.920662\n",
            "Loss training: 55.780006\n",
            "Loss training: 57.40188\n",
            "Loss training: 25.043835\n",
            "Loss training: 52.369434\n",
            "Loss training: 55.127087\n",
            "Loss training: 52.34842\n",
            "\n",
            "***************************\n",
            "Trained on 150 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.436966\n",
            "Loss training: 29.180187\n",
            "Loss training: 26.268026\n",
            "Loss training: 24.474604\n",
            "Loss training: 47.603794\n",
            "Loss training: 41.574047\n",
            "Loss training: 11.381262\n",
            "Loss training: 10.212584\n",
            "Loss training: 47.410114\n",
            "Loss training: 44.975967\n",
            "\n",
            "***************************\n",
            "Trained on 160 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 66.925674\n",
            "Loss training: 48.82515\n",
            "Loss training: 47.291298\n",
            "Loss training: 34.499714\n",
            "Loss training: 49.36983\n",
            "Loss training: 18.38036\n",
            "Loss training: 53.528152\n",
            "Loss training: 51.099464\n",
            "Loss training: 28.322876\n",
            "Loss training: 32.467194\n",
            "\n",
            "***************************\n",
            "Trained on 170 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 43.83213\n",
            "Loss training: 27.95359\n",
            "Loss training: 64.945694\n",
            "Loss training: 51.428726\n",
            "Loss training: 54.083416\n",
            "Loss training: 36.610107\n",
            "Loss training: 43.073467\n",
            "Loss training: 47.204266\n",
            "Loss training: 40.837074\n",
            "Loss training: 31.62215\n",
            "\n",
            "***************************\n",
            "Trained on 180 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.170048\n",
            "Loss training: 49.186237\n",
            "Loss training: 40.53441\n",
            "Loss training: 33.376514\n",
            "Loss training: 84.72078\n",
            "Loss training: 41.744797\n",
            "Loss training: 52.09906\n",
            "Loss training: 35.615063\n",
            "Loss training: 34.850212\n",
            "Loss training: 43.157833\n",
            "\n",
            "***************************\n",
            "Trained on 190 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 60.64505\n",
            "Loss training: 40.545\n",
            "Loss training: 40.34132\n",
            "Loss training: 46.51676\n",
            "Loss training: 25.129345\n",
            "Loss training: 44.91466\n",
            "Loss training: 37.898567\n",
            "Loss training: 37.274532\n",
            "Loss training: 29.010662\n",
            "Loss training: 39.736504\n",
            "\n",
            "***************************\n",
            "Trained on 200 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 43.613457\n",
            "Loss training: 53.326797\n",
            "Loss training: 47.339195\n",
            "Loss training: 50.487225\n",
            "Loss training: 61.262432\n",
            "Loss training: 25.979818\n",
            "Loss training: 53.53198\n",
            "Loss training: 32.62863\n",
            "Loss training: 58.85061\n",
            "Loss training: 51.280323\n",
            "\n",
            "***************************\n",
            "Trained on 210 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 53.161556\n",
            "Loss training: 31.697935\n",
            "Loss training: 42.78106\n",
            "Loss training: 52.191708\n",
            "Loss training: 34.23399\n",
            "Loss training: 28.089352\n",
            "Loss training: 30.325966\n",
            "Loss training: 82.34734\n",
            "Loss training: 53.791386\n",
            "Loss training: 38.05432\n",
            "\n",
            "***************************\n",
            "Trained on 220 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.07962\n",
            "Loss training: 34.163273\n",
            "Loss training: 48.289337\n",
            "Loss training: 53.239346\n",
            "Loss training: 46.418304\n",
            "Loss training: 44.568607\n",
            "Loss training: 36.733295\n",
            "Loss training: 35.757282\n",
            "Loss training: 75.00107\n",
            "Loss training: 51.330795\n",
            "\n",
            "***************************\n",
            "Trained on 230 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.577984\n",
            "Loss training: 38.815193\n",
            "Loss training: 33.238247\n",
            "Loss training: 46.620693\n",
            "Loss training: 37.067993\n",
            "Loss training: 37.262592\n",
            "Loss training: 48.119156\n",
            "Loss training: 22.21386\n",
            "Loss training: 35.346523\n",
            "Loss training: 47.81735\n",
            "\n",
            "***************************\n",
            "Trained on 240 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 54.86092\n",
            "Loss training: 45.496326\n",
            "Loss training: 34.337162\n",
            "Loss training: 37.64653\n",
            "Loss training: 73.2405\n",
            "Loss training: 46.356377\n",
            "Loss training: 48.56615\n",
            "Loss training: 41.38785\n",
            "Loss training: 47.003216\n",
            "Loss training: 37.30664\n",
            "\n",
            "***************************\n",
            "Trained on 250 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 46.11856\n",
            "Loss training: 18.32931\n",
            "Loss training: 49.67386\n",
            "Loss training: 40.48656\n",
            "Loss training: 23.077124\n",
            "Loss training: 44.017822\n",
            "Loss training: 70.99738\n",
            "Loss training: 40.047104\n",
            "Loss training: 74.93592\n",
            "Loss training: 41.45428\n",
            "\n",
            "***************************\n",
            "Trained on 260 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 43.29144\n",
            "Loss training: 44.30341\n",
            "Loss training: 35.297256\n",
            "Loss training: 43.681995\n",
            "Loss training: 45.389793\n",
            "Loss training: 37.801777\n",
            "Loss training: 28.858582\n",
            "Loss training: 67.75503\n",
            "Loss training: 50.221615\n",
            "Loss training: 70.84533\n",
            "\n",
            "***************************\n",
            "Trained on 270 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.548878\n",
            "Loss training: 16.80183\n",
            "Loss training: 41.90588\n",
            "Loss training: 35.4301\n",
            "Loss training: 36.02618\n",
            "Loss training: 45.36489\n",
            "Loss training: 32.284306\n",
            "Loss training: 45.205425\n",
            "Loss training: 37.562824\n",
            "Loss training: 44.704865\n",
            "\n",
            "***************************\n",
            "Trained on 280 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 16.562094\n",
            "Loss training: 22.03963\n",
            "Loss training: 53.30762\n",
            "Loss training: 35.469387\n",
            "Loss training: 38.01844\n",
            "Loss training: 38.99065\n",
            "Loss training: 40.20759\n",
            "Loss training: 36.815968\n",
            "Loss training: 20.367325\n",
            "Loss training: 32.498924\n",
            "\n",
            "***************************\n",
            "Trained on 290 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.746525\n",
            "Loss training: 37.763252\n",
            "Loss training: 55.230698\n",
            "Loss training: 55.37173\n",
            "Loss training: 28.91055\n",
            "Loss training: 35.428013\n",
            "Loss training: 48.163525\n",
            "Loss training: 53.421574\n",
            "Loss training: 44.221397\n",
            "Loss training: 39.64301\n",
            "\n",
            "***************************\n",
            "Trained on 300 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 18.20584\n",
            "Loss training: 47.89057\n",
            "Loss training: 38.93115\n",
            "Loss training: 17.725756\n",
            "Loss training: 39.992203\n",
            "Loss training: 51.211548\n",
            "Loss training: 58.588905\n",
            "Loss training: 53.939342\n",
            "Loss training: 33.486504\n",
            "Loss training: 40.561775\n",
            "\n",
            "***************************\n",
            "Trained on 310 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 46.013474\n",
            "Loss training: 51.259586\n",
            "Loss training: 68.04085\n",
            "Loss training: 48.139454\n",
            "Loss training: 41.92312\n",
            "Loss training: 48.98726\n",
            "Loss training: 58.52704\n",
            "Loss training: 46.563816\n",
            "Loss training: 39.172646\n",
            "Loss training: 44.035492\n",
            "\n",
            "***************************\n",
            "Trained on 320 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.079895\n",
            "Loss training: 38.496483\n",
            "Loss training: 31.420225\n",
            "Loss training: 54.78148\n",
            "Loss training: 59.6455\n",
            "Loss training: 43.248318\n",
            "Loss training: 27.499857\n",
            "Loss training: 57.363144\n",
            "Loss training: 48.26669\n",
            "Loss training: 43.8224\n",
            "\n",
            "***************************\n",
            "Trained on 330 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 66.21633\n",
            "Loss training: 48.078533\n",
            "Loss training: 47.20949\n",
            "Loss training: 50.69278\n",
            "Loss training: 46.320786\n",
            "Loss training: 37.313892\n",
            "Loss training: 48.57103\n",
            "Loss training: 39.341568\n",
            "Loss training: 38.301506\n",
            "Loss training: 41.8237\n",
            "\n",
            "***************************\n",
            "Trained on 340 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 48.65799\n",
            "Loss training: 40.95638\n",
            "Loss training: 53.301968\n",
            "Loss training: 68.08584\n",
            "Loss training: 64.296005\n",
            "Loss training: 21.196245\n",
            "Loss training: 40.526337\n",
            "Loss training: 50.8736\n",
            "Loss training: 50.780422\n",
            "Loss training: 59.144005\n",
            "\n",
            "***************************\n",
            "Trained on 350 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.353607\n",
            "Loss training: 47.19472\n",
            "Loss training: 33.40786\n",
            "Loss training: 26.425053\n",
            "Loss training: 99.13144\n",
            "Loss training: 23.586424\n",
            "Loss training: 36.124344\n",
            "Loss training: 37.79378\n",
            "Loss training: 20.523787\n",
            "Loss training: 67.40056\n",
            "\n",
            "***************************\n",
            "Trained on 360 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.74798\n",
            "Loss training: 38.100376\n",
            "Loss training: 67.17053\n",
            "Loss training: 41.0928\n",
            "Loss training: 51.924473\n",
            "Loss training: 40.38509\n",
            "Loss training: 38.077515\n",
            "Loss training: 18.830929\n",
            "Loss training: 73.146805\n",
            "Loss training: 55.99234\n",
            "\n",
            "***************************\n",
            "Trained on 370 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 21.781902\n",
            "Loss training: 45.358227\n",
            "Loss training: 56.01675\n",
            "Loss training: 36.04074\n",
            "Loss training: 53.53008\n",
            "Loss training: 36.788406\n",
            "Loss training: 40.20492\n",
            "Loss training: 18.818277\n",
            "Loss training: 47.87734\n",
            "Loss training: 45.410885\n",
            "\n",
            "***************************\n",
            "Trained on 380 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 63.70301\n",
            "Loss training: 57.704906\n",
            "Loss training: 41.143703\n",
            "Loss training: 40.886745\n",
            "Loss training: 46.759647\n",
            "Loss training: 41.28082\n",
            "Loss training: 44.375607\n",
            "Loss training: 46.449062\n",
            "Loss training: 39.978474\n",
            "Loss training: 46.623585\n",
            "\n",
            "***************************\n",
            "Trained on 390 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.297657\n",
            "Loss training: 19.490442\n",
            "Loss training: 34.031826\n",
            "Loss training: 35.51231\n",
            "Loss training: 49.07432\n",
            "Loss training: 39.825153\n",
            "Loss training: 41.00698\n",
            "Loss training: 29.455898\n",
            "Loss training: 43.645054\n",
            "Loss training: 49.31822\n",
            "\n",
            "***************************\n",
            "Trained on 400 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 51.492924\n",
            "Loss training: 44.305706\n",
            "Loss training: 56.659203\n",
            "Loss training: 43.82579\n",
            "Loss training: 56.681084\n",
            "Loss training: 37.363598\n",
            "Loss training: 27.650627\n",
            "Loss training: 34.875607\n",
            "Loss training: 46.688522\n",
            "Loss training: 25.261768\n",
            "\n",
            "***************************\n",
            "Trained on 410 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.89761\n",
            "Loss training: 32.570927\n",
            "Loss training: 48.62898\n",
            "Loss training: 44.383575\n",
            "Loss training: 42.979656\n",
            "Loss training: 27.996685\n",
            "Loss training: 14.912477\n",
            "Loss training: 83.86831\n",
            "Loss training: 35.421005\n",
            "Loss training: 43.0531\n",
            "\n",
            "***************************\n",
            "Trained on 420 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 43.547417\n",
            "Loss training: 57.251904\n",
            "Loss training: 42.904644\n",
            "Loss training: 33.967308\n",
            "Loss training: 21.736639\n",
            "Loss training: 43.07906\n",
            "Loss training: 50.535515\n",
            "Loss training: 37.573215\n",
            "Loss training: 65.3597\n",
            "Loss training: 44.569897\n",
            "\n",
            "***************************\n",
            "Trained on 430 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 49.960003\n",
            "Loss training: 51.54317\n",
            "Loss training: 43.390827\n",
            "Loss training: 49.10341\n",
            "Loss training: 43.749195\n",
            "Loss training: 46.62806\n",
            "Loss training: 37.74991\n",
            "Loss training: 14.207483\n",
            "Loss training: 65.25326\n",
            "Loss training: 45.019123\n",
            "\n",
            "***************************\n",
            "Trained on 440 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.216085\n",
            "Loss training: 55.119488\n",
            "Loss training: 54.808907\n",
            "Loss training: 45.421314\n",
            "Loss training: 39.005474\n",
            "Loss training: 23.715136\n",
            "Loss training: 54.56458\n",
            "Loss training: 52.256367\n",
            "Loss training: 43.656124\n",
            "Loss training: 46.598106\n",
            "\n",
            "***************************\n",
            "Trained on 450 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 16.274778\n",
            "Loss training: 34.3929\n",
            "Loss training: 50.24677\n",
            "Loss training: 59.861862\n",
            "Loss training: 31.651323\n",
            "Loss training: 24.582697\n",
            "Loss training: 32.921837\n",
            "Loss training: 56.153866\n",
            "Loss training: 20.846512\n",
            "Loss training: 41.921402\n",
            "\n",
            "***************************\n",
            "Trained on 460 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.435944\n",
            "Loss training: 63.419064\n",
            "Loss training: 9.557984\n",
            "Loss training: 35.2448\n",
            "Loss training: 37.676994\n",
            "Loss training: 37.11093\n",
            "Loss training: 30.680109\n",
            "Loss training: 52.251617\n",
            "Loss training: 52.46148\n",
            "Loss training: 44.840614\n",
            "\n",
            "***************************\n",
            "Trained on 470 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 48.01269\n",
            "Loss training: 25.34943\n",
            "Loss training: 61.47993\n",
            "Loss training: 16.561045\n",
            "Loss training: 42.62525\n",
            "Loss training: 61.66104\n",
            "Loss training: 30.134228\n",
            "Loss training: 43.35604\n",
            "Loss training: 44.6547\n",
            "Loss training: 48.1542\n",
            "\n",
            "***************************\n",
            "Trained on 480 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 51.428257\n",
            "Loss training: 50.96827\n",
            "Loss training: 53.672432\n",
            "Loss training: 37.146477\n",
            "Loss training: 38.582767\n",
            "Loss training: 38.76708\n",
            "Loss training: 31.45574\n",
            "Loss training: 32.340885\n",
            "Loss training: 38.26227\n",
            "Loss training: 47.21308\n",
            "\n",
            "***************************\n",
            "Trained on 490 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.906185\n",
            "Loss training: 13.217115\n",
            "Loss training: 73.1315\n",
            "Loss training: 42.33447\n",
            "Loss training: 34.689083\n",
            "Loss training: 51.83025\n",
            "Loss training: 32.892235\n",
            "Loss training: 67.01885\n",
            "Loss training: 56.632374\n",
            "Loss training: 39.314537\n",
            "\n",
            "***************************\n",
            "Trained on 500 graphs\n",
            "***************************\n",
            "\n",
            "Evaluated 10 / 100 subgraphs...\n",
            "Evaluated 20 / 100 subgraphs...\n",
            "Evaluated 30 / 100 subgraphs...\n",
            "Evaluated 40 / 100 subgraphs...\n",
            "Evaluated 50 / 100 subgraphs...\n",
            "Evaluated 60 / 100 subgraphs...\n",
            "Evaluated 70 / 100 subgraphs...\n",
            "Evaluated 80 / 100 subgraphs...\n",
            "Evaluated 90 / 100 subgraphs...\n",
            "Evaluated 100 / 100 subgraphs...\n",
            "\n",
            "Final ROC on the train set 0.6939371049645034\n",
            "Final ROC on the validation set 0.559224577731843\n",
            "Final ROC on the test set 0.5488968392833208\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  Previous runs:\\n  (1) Configuration\\n        learning_rate = 0.001\\n        num_partitions = 50\\n        hidden_dimension = 128\\n        num_message_passing_steps = 5\\n        num_training_steps = 1000\\n    ROC on the train set 0.7348797273386144\\n    ROC on the validation set 0.6025038939324504\\n    ROC on the test set 0.5896861508337246\\n\\n  (2) Configuration\\n        learning_rate = 0.001\\n        num_partitions = 50\\n        hidden_dimension = 128\\n        num_message_passing_steps = 5\\n        num_training_steps = 3000\\n    ROC on the train set 0.8050085464161815\\n    ROC on the validation set 0.6327603823722211\\n    ROC on the test set 0.5078022533003436\\n\\n  (3) Configuration\\n        learning_rate = 0.1\\n        num_partitions = 100\\n        hidden_dimension = 128\\n        num_message_passing_steps = 5\\n        num_training_steps = 1000\\n    ROC on the train set 0.5\\n    ROC on the validation set 0.5 \\n    ROC on the test set 0.5\\n\\n  (4) Configuration\\n        learning_rate = 0.01\\n        num_partitions = 100\\n        hidden_dimension = 128\\n        num_message_passing_steps = 5\\n        num_training_steps = 100\\n    ROC on the train set 0.6501172261188106\\n    ROC on the validation set 0.5281974299591566\\n    ROC on the test set 0.47652056321124514\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###################################### FUNCTIONS FOR TESTING ######################################\n",
        "run_overfit_on_single_partition = True\n",
        "run_overfit_on_demo_graph = False"
      ],
      "metadata": {
        "id": "yK0Qy8CdU6tl"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def overfit_on_single_graph(\n",
        "    num_training_steps, \n",
        "    learning_rate,\n",
        "    graph,\n",
        "    labels,\n",
        "    mask\n",
        "    ):\n",
        "  replicated_params = jax.pmap(network_definition.init, axis_name = 'i')(bcast_local_devices(jax.random.PRNGKey(42)), graph)\n",
        "\n",
        "  opt_init, opt_update = optax.adam(learning_rate = learning_rate)  \n",
        "  replicated_opt_state = jax.pmap(opt_init, axis_name = 'i')(replicated_params)\n",
        "\n",
        "  @functools.partial(jax.pmap, axis_name='i')\n",
        "  def update(params, opt_state, graph, targets, mask):\n",
        "    # Compute the gradients on the given minibatch (individually on each device).\n",
        "    loss, grads = jax.value_and_grad(compute_loss)(params, graph, targets, mask)\n",
        "\n",
        "    # Combine the gradient across all devices (by taking their mean).\n",
        "    grads = jax.lax.pmean(grads, axis_name='i')\n",
        "\n",
        "    # Also combine the loss. Unnecessary for the update, but useful for logging.\n",
        "    loss = jax.lax.pmean(loss, axis_name='i')\n",
        "\n",
        "    updates, opt_state = opt_update(grads, opt_state)\n",
        "\n",
        "    return optax.apply_updates(params, updates), opt_state, loss\n",
        "\n",
        "  # Train on a single partition\n",
        "  for idx in range(num_training_steps):\n",
        "    replicated_params, replicated_opt_state, loss = update(\n",
        "      replicated_params, \n",
        "      replicated_opt_state, \n",
        "      graph, \n",
        "      labels,\n",
        "      mask\n",
        "      ) \n",
        "\n",
        "    print('Loss training:', reshape_broadcasted_data(loss))\n",
        "\n",
        "    if (idx + 1) % 10 == 0:\n",
        "      print()\n",
        "      print(f'***************************')\n",
        "      print(f'Trained on {idx + 1} graphs')\n",
        "      print(f'***************************')\n",
        "      print()"
      ],
      "metadata": {
        "id": "DNchIrnxPk1P"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_demo_training_graph(num_nodes, num_edges):\n",
        "  rand_dgl_graph = dgl.rand_graph(num_nodes = num_nodes, num_edges = num_edges)\n",
        "\n",
        "  node_features = jnp.array([[randint(0, 7)] for i in range(num_nodes)])\n",
        "  edge_features = jnp.array([[0.1 * randint(0, 10) for _ in range(8)] for i in range(num_edges)])\n",
        "\n",
        "  senders = jnp.array(rand_dgl_graph.edges()[0])\n",
        "  receivers = jnp.array(rand_dgl_graph.edges()[1])\n",
        "\n",
        "  in_tuple = jraph.GraphsTuple(\n",
        "            nodes = node_features.astype(np.float32),\n",
        "            edges = edge_features.astype(np.float32),  \n",
        "            senders = senders.astype(np.int32), \n",
        "            receivers = receivers.astype(np.int32),\n",
        "            n_node = jnp.array([num_nodes]), \n",
        "            n_edge = jnp.array([num_edges]),\n",
        "            globals = None  # No global features\n",
        "          )\n",
        "  \n",
        "  labels = jnp.array([[randint(0, 1) for j in range(112)] for i in range(num_nodes)])\n",
        "  \n",
        "  in_tuple = in_tuple._replace(\n",
        "      nodes = {\n",
        "          'inputs': in_tuple.nodes, \n",
        "          'targets': labels, \n",
        "          'train_mask': jnp.ones((num_nodes, 1)), # No nodes are masked \n",
        "          }\n",
        "  )\n",
        "\n",
        "  # in_tuple = pad_graph_to_nearest_power_of_two(in_tuple)\n",
        "  \n",
        "  return sharded_graphnet.graphs_tuple_to_broadcasted_sharded_graphs_tuple(\n",
        "      in_tuple,\n",
        "      num_shards = num_devices\n",
        "      )\n",
        "\n",
        "def overfit_on_demo_graph(num_training_steps, learning_rate):\n",
        "  demo_graph = get_demo_training_graph(num_nodes = 16, num_edges = 8)\n",
        "  demo_labels = demo_graph.nodes['targets']\n",
        "  demo_mask = demo_graph.nodes['train_mask']\n",
        "  demo_graph = demo_graph._replace(nodes = demo_graph.nodes['inputs'])\n",
        "\n",
        "  overfit_on_single_graph(\n",
        "      num_training_steps = num_training_steps,\n",
        "      learning_rate = learning_rate,\n",
        "      graph = demo_graph,\n",
        "      labels = demo_labels,\n",
        "      mask = demo_mask\n",
        "  )"
      ],
      "metadata": {
        "id": "zQdvmYQETQNr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Overfit on an existing partition\n",
        "if run_overfit_on_single_partition:\n",
        "  print('*** Trying to overfit on a single partition ***')\n",
        "  overfit_on_single_graph(\n",
        "      num_training_steps = 5000,\n",
        "      learning_rate = 0.001,\n",
        "      graph = processed_graphs['partition_0']['graph'],\n",
        "      labels = processed_graphs['partition_0']['labels'],\n",
        "      mask = processed_graphs['partition_0']['train_mask']\n",
        "      )"
      ],
      "metadata": {
        "id": "xfmlIPlpUfNZ",
        "outputId": "1c100424-98a0-4919-a2f3-478fc18792b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Trying to overfit on a single partition ***\n",
            "Loss training: 84.53167\n",
            "Loss training: 61.270084\n",
            "Loss training: 65.28496\n",
            "Loss training: 53.758053\n",
            "Loss training: 49.95763\n",
            "Loss training: 48.81813\n",
            "Loss training: 47.50673\n",
            "Loss training: 46.31217\n",
            "Loss training: 45.886826\n",
            "Loss training: 45.490566\n",
            "\n",
            "***************************\n",
            "Trained on 10 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.992046\n",
            "Loss training: 44.392914\n",
            "Loss training: 43.967613\n",
            "Loss training: 43.618076\n",
            "Loss training: 43.2789\n",
            "Loss training: 42.873016\n",
            "Loss training: 42.43755\n",
            "Loss training: 42.049526\n",
            "Loss training: 41.68623\n",
            "Loss training: 41.376934\n",
            "\n",
            "***************************\n",
            "Trained on 20 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.520893\n",
            "Loss training: 46.484684\n",
            "Loss training: 41.232147\n",
            "Loss training: 44.456116\n",
            "Loss training: 41.955597\n",
            "Loss training: 41.558895\n",
            "Loss training: 42.799366\n",
            "Loss training: 41.38099\n",
            "Loss training: 41.004505\n",
            "Loss training: 41.952812\n",
            "\n",
            "***************************\n",
            "Trained on 30 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.02756\n",
            "Loss training: 40.525074\n",
            "Loss training: 41.01577\n",
            "Loss training: 40.57745\n",
            "Loss training: 39.872925\n",
            "Loss training: 40.18617\n",
            "Loss training: 39.890514\n",
            "Loss training: 39.34341\n",
            "Loss training: 39.63167\n",
            "Loss training: 39.17948\n",
            "\n",
            "***************************\n",
            "Trained on 40 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.053383\n",
            "Loss training: 39.100086\n",
            "Loss training: 38.648525\n",
            "Loss training: 38.870872\n",
            "Loss training: 38.395836\n",
            "Loss training: 38.609955\n",
            "Loss training: 38.172035\n",
            "Loss training: 38.38015\n",
            "Loss training: 37.987366\n",
            "Loss training: 38.12744\n",
            "\n",
            "***************************\n",
            "Trained on 50 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.782497\n",
            "Loss training: 37.893703\n",
            "Loss training: 37.675667\n",
            "Loss training: 37.57875\n",
            "Loss training: 37.594532\n",
            "Loss training: 37.33206\n",
            "Loss training: 37.34269\n",
            "Loss training: 37.279125\n",
            "Loss training: 37.078484\n",
            "Loss training: 37.02912\n",
            "\n",
            "***************************\n",
            "Trained on 60 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.052334\n",
            "Loss training: 36.991367\n",
            "Loss training: 36.80397\n",
            "Loss training: 36.65107\n",
            "Loss training: 36.591137\n",
            "Loss training: 36.61406\n",
            "Loss training: 36.926834\n",
            "Loss training: 37.330276\n",
            "Loss training: 37.74951\n",
            "Loss training: 36.518097\n",
            "\n",
            "***************************\n",
            "Trained on 70 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.50418\n",
            "Loss training: 37.027245\n",
            "Loss training: 36.094826\n",
            "Loss training: 36.54288\n",
            "Loss training: 36.38104\n",
            "Loss training: 35.9215\n",
            "Loss training: 36.352016\n",
            "Loss training: 35.733482\n",
            "Loss training: 35.8301\n",
            "Loss training: 35.87203\n",
            "\n",
            "***************************\n",
            "Trained on 80 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.409565\n",
            "Loss training: 35.604595\n",
            "Loss training: 35.642452\n",
            "Loss training: 35.245464\n",
            "Loss training: 35.09833\n",
            "Loss training: 35.297104\n",
            "Loss training: 35.322563\n",
            "Loss training: 34.943184\n",
            "Loss training: 34.634624\n",
            "Loss training: 34.68636\n",
            "\n",
            "***************************\n",
            "Trained on 90 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.942406\n",
            "Loss training: 35.464394\n",
            "Loss training: 35.516045\n",
            "Loss training: 34.716103\n",
            "Loss training: 34.281002\n",
            "Loss training: 34.636765\n",
            "Loss training: 34.99433\n",
            "Loss training: 34.48051\n",
            "Loss training: 33.839783\n",
            "Loss training: 34.273067\n",
            "\n",
            "***************************\n",
            "Trained on 100 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.611744\n",
            "Loss training: 34.061035\n",
            "Loss training: 33.532455\n",
            "Loss training: 34.0775\n",
            "Loss training: 34.285336\n",
            "Loss training: 33.792397\n",
            "Loss training: 33.67403\n",
            "Loss training: 34.717266\n",
            "Loss training: 34.641743\n",
            "Loss training: 33.088493\n",
            "\n",
            "***************************\n",
            "Trained on 110 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.804893\n",
            "Loss training: 33.837166\n",
            "Loss training: 33.256855\n",
            "Loss training: 32.971203\n",
            "Loss training: 33.208324\n",
            "Loss training: 32.897106\n",
            "Loss training: 32.589626\n",
            "Loss training: 32.73764\n",
            "Loss training: 32.7325\n",
            "Loss training: 32.105324\n",
            "\n",
            "***************************\n",
            "Trained on 120 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.22133\n",
            "Loss training: 32.41552\n",
            "Loss training: 31.822731\n",
            "Loss training: 31.947376\n",
            "Loss training: 31.430492\n",
            "Loss training: 31.788252\n",
            "Loss training: 31.498013\n",
            "Loss training: 31.41596\n",
            "Loss training: 31.669773\n",
            "Loss training: 30.975092\n",
            "\n",
            "***************************\n",
            "Trained on 130 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.95599\n",
            "Loss training: 30.66282\n",
            "Loss training: 30.85104\n",
            "Loss training: 30.520084\n",
            "Loss training: 30.342505\n",
            "Loss training: 30.665773\n",
            "Loss training: 32.264984\n",
            "Loss training: 31.54997\n",
            "Loss training: 32.25864\n",
            "Loss training: 31.88266\n",
            "\n",
            "***************************\n",
            "Trained on 140 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.821037\n",
            "Loss training: 31.222397\n",
            "Loss training: 30.530205\n",
            "Loss training: 30.816664\n",
            "Loss training: 29.984886\n",
            "Loss training: 30.39251\n",
            "Loss training: 29.637827\n",
            "Loss training: 29.823757\n",
            "Loss training: 29.500418\n",
            "Loss training: 29.42676\n",
            "\n",
            "***************************\n",
            "Trained on 150 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.86769\n",
            "Loss training: 29.098906\n",
            "Loss training: 28.970335\n",
            "Loss training: 29.456863\n",
            "Loss training: 28.91221\n",
            "Loss training: 29.339687\n",
            "Loss training: 28.422647\n",
            "Loss training: 28.427504\n",
            "Loss training: 27.931784\n",
            "Loss training: 28.21911\n",
            "\n",
            "***************************\n",
            "Trained on 160 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.837326\n",
            "Loss training: 28.25668\n",
            "Loss training: 27.973133\n",
            "Loss training: 28.480886\n",
            "Loss training: 29.374361\n",
            "Loss training: 30.368263\n",
            "Loss training: 30.127893\n",
            "Loss training: 29.687632\n",
            "Loss training: 29.264135\n",
            "Loss training: 29.084225\n",
            "\n",
            "***************************\n",
            "Trained on 170 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.498884\n",
            "Loss training: 28.719957\n",
            "Loss training: 28.327337\n",
            "Loss training: 28.225616\n",
            "Loss training: 27.6224\n",
            "Loss training: 27.47515\n",
            "Loss training: 27.315884\n",
            "Loss training: 27.054768\n",
            "Loss training: 26.495054\n",
            "Loss training: 26.54461\n",
            "\n",
            "***************************\n",
            "Trained on 180 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.12674\n",
            "Loss training: 26.932558\n",
            "Loss training: 30.889458\n",
            "Loss training: 28.561935\n",
            "Loss training: 28.30965\n",
            "Loss training: 27.773916\n",
            "Loss training: 27.82573\n",
            "Loss training: 26.957693\n",
            "Loss training: 27.192062\n",
            "Loss training: 26.34612\n",
            "\n",
            "***************************\n",
            "Trained on 190 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.606995\n",
            "Loss training: 27.008173\n",
            "Loss training: 31.447319\n",
            "Loss training: 30.895641\n",
            "Loss training: 29.900267\n",
            "Loss training: 29.36853\n",
            "Loss training: 27.942713\n",
            "Loss training: 27.707678\n",
            "Loss training: 27.206509\n",
            "Loss training: 26.81335\n",
            "\n",
            "***************************\n",
            "Trained on 200 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.986294\n",
            "Loss training: 26.154982\n",
            "Loss training: 26.689222\n",
            "Loss training: 25.880947\n",
            "Loss training: 25.581043\n",
            "Loss training: 25.125032\n",
            "Loss training: 24.899368\n",
            "Loss training: 24.303907\n",
            "Loss training: 24.72474\n",
            "Loss training: 25.371952\n",
            "\n",
            "***************************\n",
            "Trained on 210 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.643187\n",
            "Loss training: 24.886358\n",
            "Loss training: 25.064299\n",
            "Loss training: 24.99135\n",
            "Loss training: 23.440681\n",
            "Loss training: 26.42208\n",
            "Loss training: 28.648907\n",
            "Loss training: 30.170797\n",
            "Loss training: 29.054039\n",
            "Loss training: 29.484184\n",
            "\n",
            "***************************\n",
            "Trained on 220 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.321585\n",
            "Loss training: 27.989744\n",
            "Loss training: 27.724173\n",
            "Loss training: 26.938116\n",
            "Loss training: 26.51281\n",
            "Loss training: 25.733255\n",
            "Loss training: 25.582813\n",
            "Loss training: 25.127033\n",
            "Loss training: 24.17121\n",
            "Loss training: 23.800676\n",
            "\n",
            "***************************\n",
            "Trained on 230 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.596176\n",
            "Loss training: 23.14074\n",
            "Loss training: 23.120003\n",
            "Loss training: 22.99663\n",
            "Loss training: 23.983261\n",
            "Loss training: 24.960827\n",
            "Loss training: 25.480059\n",
            "Loss training: 23.89493\n",
            "Loss training: 23.922216\n",
            "Loss training: 23.228848\n",
            "\n",
            "***************************\n",
            "Trained on 240 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.787678\n",
            "Loss training: 22.717514\n",
            "Loss training: 22.180525\n",
            "Loss training: 21.97587\n",
            "Loss training: 21.222397\n",
            "Loss training: 21.395546\n",
            "Loss training: 20.680975\n",
            "Loss training: 20.839544\n",
            "Loss training: 20.457987\n",
            "Loss training: 21.489777\n",
            "\n",
            "***************************\n",
            "Trained on 250 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.64666\n",
            "Loss training: 21.739664\n",
            "Loss training: 23.043259\n",
            "Loss training: 21.454676\n",
            "Loss training: 22.235466\n",
            "Loss training: 20.985567\n",
            "Loss training: 20.660826\n",
            "Loss training: 20.70635\n",
            "Loss training: 20.44241\n",
            "Loss training: 19.909346\n",
            "\n",
            "***************************\n",
            "Trained on 260 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 19.729124\n",
            "Loss training: 19.58635\n",
            "Loss training: 19.37991\n",
            "Loss training: 19.08901\n",
            "Loss training: 18.573902\n",
            "Loss training: 18.982214\n",
            "Loss training: 19.974398\n",
            "Loss training: 23.836878\n",
            "Loss training: 24.121511\n",
            "Loss training: 25.436817\n",
            "\n",
            "***************************\n",
            "Trained on 270 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.52683\n",
            "Loss training: 22.965405\n",
            "Loss training: 22.978699\n",
            "Loss training: 22.082354\n",
            "Loss training: 21.442278\n",
            "Loss training: 20.980888\n",
            "Loss training: 20.619621\n",
            "Loss training: 20.466936\n",
            "Loss training: 19.55619\n",
            "Loss training: 19.54174\n",
            "\n",
            "***************************\n",
            "Trained on 280 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 18.97438\n",
            "Loss training: 18.660542\n",
            "Loss training: 18.230934\n",
            "Loss training: 18.197727\n",
            "Loss training: 17.577345\n",
            "Loss training: 17.422953\n",
            "Loss training: 17.426798\n",
            "Loss training: 18.927385\n",
            "Loss training: 19.608099\n",
            "Loss training: 20.827423\n",
            "\n",
            "***************************\n",
            "Trained on 290 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 19.23986\n",
            "Loss training: 19.087963\n",
            "Loss training: 18.277472\n",
            "Loss training: 18.728594\n",
            "Loss training: 18.418427\n",
            "Loss training: 19.253168\n",
            "Loss training: 17.531792\n",
            "Loss training: 18.491259\n",
            "Loss training: 16.70769\n",
            "Loss training: 17.39522\n",
            "\n",
            "***************************\n",
            "Trained on 300 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 16.669725\n",
            "Loss training: 16.53869\n",
            "Loss training: 16.031763\n",
            "Loss training: 15.903436\n",
            "Loss training: 15.625674\n",
            "Loss training: 15.409126\n",
            "Loss training: 15.021212\n",
            "Loss training: 14.867345\n",
            "Loss training: 14.564456\n",
            "Loss training: 14.573071\n",
            "\n",
            "***************************\n",
            "Trained on 310 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 14.136451\n",
            "Loss training: 14.321297\n",
            "Loss training: 13.921203\n",
            "Loss training: 14.009292\n",
            "Loss training: 15.653734\n",
            "Loss training: 15.904859\n",
            "Loss training: 15.109787\n",
            "Loss training: 14.423553\n",
            "Loss training: 16.229015\n",
            "Loss training: 15.852266\n",
            "\n",
            "***************************\n",
            "Trained on 320 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 15.399059\n",
            "Loss training: 14.690984\n",
            "Loss training: 14.4161215\n",
            "Loss training: 14.040204\n",
            "Loss training: 13.748749\n",
            "Loss training: 13.69443\n",
            "Loss training: 12.964588\n",
            "Loss training: 13.377867\n",
            "Loss training: 12.635628\n",
            "Loss training: 12.8287945\n",
            "\n",
            "***************************\n",
            "Trained on 330 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 13.049658\n",
            "Loss training: 13.37941\n",
            "Loss training: 13.766943\n",
            "Loss training: 15.626503\n",
            "Loss training: 15.158383\n",
            "Loss training: 16.146923\n",
            "Loss training: 16.731478\n",
            "Loss training: 14.443412\n",
            "Loss training: 14.799922\n",
            "Loss training: 14.112973\n",
            "\n",
            "***************************\n",
            "Trained on 340 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 14.450839\n",
            "Loss training: 13.44433\n",
            "Loss training: 13.832675\n",
            "Loss training: 12.626452\n",
            "Loss training: 12.84432\n",
            "Loss training: 12.173945\n",
            "Loss training: 12.327046\n",
            "Loss training: 11.774509\n",
            "Loss training: 11.919043\n",
            "Loss training: 11.418547\n",
            "\n",
            "***************************\n",
            "Trained on 350 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 11.330238\n",
            "Loss training: 10.915245\n",
            "Loss training: 10.907916\n",
            "Loss training: 10.574043\n",
            "Loss training: 10.525161\n",
            "Loss training: 10.399809\n",
            "Loss training: 10.132112\n",
            "Loss training: 10.073211\n",
            "Loss training: 10.139828\n",
            "Loss training: 10.653944\n",
            "\n",
            "***************************\n",
            "Trained on 360 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 11.405753\n",
            "Loss training: 10.642822\n",
            "Loss training: 10.487242\n",
            "Loss training: 10.682711\n",
            "Loss training: 10.127359\n",
            "Loss training: 10.3606\n",
            "Loss training: 10.246221\n",
            "Loss training: 10.955766\n",
            "Loss training: 11.155837\n",
            "Loss training: 11.04488\n",
            "\n",
            "***************************\n",
            "Trained on 370 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 10.981146\n",
            "Loss training: 10.498701\n",
            "Loss training: 10.151754\n",
            "Loss training: 9.894869\n",
            "Loss training: 10.063346\n",
            "Loss training: 9.765696\n",
            "Loss training: 9.692129\n",
            "Loss training: 9.72341\n",
            "Loss training: 8.999135\n",
            "Loss training: 9.330902\n",
            "\n",
            "***************************\n",
            "Trained on 380 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 8.870091\n",
            "Loss training: 9.196214\n",
            "Loss training: 10.035473\n",
            "Loss training: 10.766783\n",
            "Loss training: 12.4169445\n",
            "Loss training: 10.316337\n",
            "Loss training: 10.457223\n",
            "Loss training: 10.011927\n",
            "Loss training: 10.148168\n",
            "Loss training: 9.255919\n",
            "\n",
            "***************************\n",
            "Trained on 390 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 9.476597\n",
            "Loss training: 9.156062\n",
            "Loss training: 8.861798\n",
            "Loss training: 8.66367\n",
            "Loss training: 8.6266365\n",
            "Loss training: 8.308346\n",
            "Loss training: 8.421997\n",
            "Loss training: 8.339691\n",
            "Loss training: 8.791132\n",
            "Loss training: 8.150301\n",
            "\n",
            "***************************\n",
            "Trained on 400 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 8.035049\n",
            "Loss training: 8.589755\n",
            "Loss training: 7.7945986\n",
            "Loss training: 7.901042\n",
            "Loss training: 8.315416\n",
            "Loss training: 7.5799327\n",
            "Loss training: 8.062318\n",
            "Loss training: 8.129357\n",
            "Loss training: 7.6604195\n",
            "Loss training: 7.844283\n",
            "\n",
            "***************************\n",
            "Trained on 410 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 7.9308424\n",
            "Loss training: 7.721191\n",
            "Loss training: 7.945541\n",
            "Loss training: 7.5249696\n",
            "Loss training: 7.5374007\n",
            "Loss training: 7.355427\n",
            "Loss training: 7.6707644\n",
            "Loss training: 8.953728\n",
            "Loss training: 8.623062\n",
            "Loss training: 7.8875256\n",
            "\n",
            "***************************\n",
            "Trained on 420 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 8.423702\n",
            "Loss training: 12.728096\n",
            "Loss training: 34.378315\n",
            "Loss training: 48.852856\n",
            "Loss training: 53.256878\n",
            "Loss training: 40.32836\n",
            "Loss training: 39.897793\n",
            "Loss training: 36.313454\n",
            "Loss training: 36.006756\n",
            "Loss training: 34.67292\n",
            "\n",
            "***************************\n",
            "Trained on 430 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.32159\n",
            "Loss training: 32.634617\n",
            "Loss training: 32.036674\n",
            "Loss training: 31.359901\n",
            "Loss training: 30.72584\n",
            "Loss training: 29.984877\n",
            "Loss training: 29.638004\n",
            "Loss training: 28.91253\n",
            "Loss training: 28.551859\n",
            "Loss training: 28.281134\n",
            "\n",
            "***************************\n",
            "Trained on 440 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.73275\n",
            "Loss training: 27.152176\n",
            "Loss training: 26.748316\n",
            "Loss training: 26.470718\n",
            "Loss training: 26.456404\n",
            "Loss training: 26.006031\n",
            "Loss training: 25.245237\n",
            "Loss training: 24.654411\n",
            "Loss training: 24.572157\n",
            "Loss training: 24.632042\n",
            "\n",
            "***************************\n",
            "Trained on 450 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.856647\n",
            "Loss training: 23.175571\n",
            "Loss training: 22.861462\n",
            "Loss training: 22.774652\n",
            "Loss training: 22.691479\n",
            "Loss training: 21.76711\n",
            "Loss training: 21.67485\n",
            "Loss training: 21.413149\n",
            "Loss training: 21.230719\n",
            "Loss training: 21.289255\n",
            "\n",
            "***************************\n",
            "Trained on 460 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 20.473967\n",
            "Loss training: 19.92725\n",
            "Loss training: 19.865005\n",
            "Loss training: 19.591728\n",
            "Loss training: 19.356525\n",
            "Loss training: 19.311176\n",
            "Loss training: 19.12199\n",
            "Loss training: 18.838192\n",
            "Loss training: 19.774704\n",
            "Loss training: 19.56515\n",
            "\n",
            "***************************\n",
            "Trained on 470 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 21.959082\n",
            "Loss training: 20.674221\n",
            "Loss training: 20.049871\n",
            "Loss training: 20.009926\n",
            "Loss training: 19.082253\n",
            "Loss training: 18.957611\n",
            "Loss training: 18.752243\n",
            "Loss training: 18.10123\n",
            "Loss training: 17.72927\n",
            "Loss training: 17.58295\n",
            "\n",
            "***************************\n",
            "Trained on 480 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 17.18242\n",
            "Loss training: 17.004486\n",
            "Loss training: 16.908667\n",
            "Loss training: 16.666765\n",
            "Loss training: 16.577564\n",
            "Loss training: 16.98401\n",
            "Loss training: 17.512402\n",
            "Loss training: 17.417107\n",
            "Loss training: 16.372604\n",
            "Loss training: 16.472378\n",
            "\n",
            "***************************\n",
            "Trained on 490 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 16.25509\n",
            "Loss training: 15.968134\n",
            "Loss training: 15.674714\n",
            "Loss training: 15.203399\n",
            "Loss training: 15.1113615\n",
            "Loss training: 15.1591625\n",
            "Loss training: 14.836399\n",
            "Loss training: 14.723047\n",
            "Loss training: 14.481911\n",
            "Loss training: 14.554235\n",
            "\n",
            "***************************\n",
            "Trained on 500 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 14.470629\n",
            "Loss training: 14.368417\n",
            "Loss training: 14.077035\n",
            "Loss training: 13.869938\n",
            "Loss training: 13.924429\n",
            "Loss training: 14.304191\n",
            "Loss training: 14.567419\n",
            "Loss training: 14.604203\n",
            "Loss training: 13.669418\n",
            "Loss training: 14.147023\n",
            "\n",
            "***************************\n",
            "Trained on 510 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 15.134928\n",
            "Loss training: 14.124126\n",
            "Loss training: 15.5609455\n",
            "Loss training: 14.877449\n",
            "Loss training: 15.009813\n",
            "Loss training: 15.15016\n",
            "Loss training: 14.554482\n",
            "Loss training: 14.996351\n",
            "Loss training: 14.232226\n",
            "Loss training: 14.307709\n",
            "\n",
            "***************************\n",
            "Trained on 520 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 13.804396\n",
            "Loss training: 13.449805\n",
            "Loss training: 13.548869\n",
            "Loss training: 13.187713\n",
            "Loss training: 12.936136\n",
            "Loss training: 13.13884\n",
            "Loss training: 13.369119\n",
            "Loss training: 14.3528385\n",
            "Loss training: 15.358091\n",
            "Loss training: 14.0724325\n",
            "\n",
            "***************************\n",
            "Trained on 530 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 13.622779\n",
            "Loss training: 13.500701\n",
            "Loss training: 13.465966\n",
            "Loss training: 13.477591\n",
            "Loss training: 12.951084\n",
            "Loss training: 13.237839\n",
            "Loss training: 12.26196\n",
            "Loss training: 12.679839\n",
            "Loss training: 12.50275\n",
            "Loss training: 12.414637\n",
            "\n",
            "***************************\n",
            "Trained on 540 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 12.569231\n",
            "Loss training: 12.487216\n",
            "Loss training: 11.698152\n",
            "Loss training: 11.918751\n",
            "Loss training: 11.321155\n",
            "Loss training: 12.191992\n",
            "Loss training: 15.677899\n",
            "Loss training: 18.267729\n",
            "Loss training: 17.464941\n",
            "Loss training: 16.456127\n",
            "\n",
            "***************************\n",
            "Trained on 550 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 15.780781\n",
            "Loss training: 15.573131\n",
            "Loss training: 14.5995865\n",
            "Loss training: 15.020298\n",
            "Loss training: 14.608079\n",
            "Loss training: 13.926153\n",
            "Loss training: 14.138721\n",
            "Loss training: 13.5637045\n",
            "Loss training: 13.518298\n",
            "Loss training: 13.287425\n",
            "\n",
            "***************************\n",
            "Trained on 560 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 13.05157\n",
            "Loss training: 12.786134\n",
            "Loss training: 12.671967\n",
            "Loss training: 12.351928\n",
            "Loss training: 12.285012\n",
            "Loss training: 12.119627\n",
            "Loss training: 11.897849\n",
            "Loss training: 11.858216\n",
            "Loss training: 11.612892\n",
            "Loss training: 11.519041\n",
            "\n",
            "***************************\n",
            "Trained on 570 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 11.339295\n",
            "Loss training: 11.219018\n",
            "Loss training: 11.110944\n",
            "Loss training: 11.009344\n",
            "Loss training: 10.820644\n",
            "Loss training: 10.768106\n",
            "Loss training: 10.743524\n",
            "Loss training: 10.97046\n",
            "Loss training: 11.267471\n",
            "Loss training: 12.634183\n",
            "\n",
            "***************************\n",
            "Trained on 580 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 12.320899\n",
            "Loss training: 11.006689\n",
            "Loss training: 11.718962\n",
            "Loss training: 10.862627\n",
            "Loss training: 11.446253\n",
            "Loss training: 11.799984\n",
            "Loss training: 11.353266\n",
            "Loss training: 10.872934\n",
            "Loss training: 10.504688\n",
            "Loss training: 10.279065\n",
            "\n",
            "***************************\n",
            "Trained on 590 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 10.094995\n",
            "Loss training: 9.892459\n",
            "Loss training: 10.012135\n",
            "Loss training: 9.457052\n",
            "Loss training: 9.291835\n",
            "Loss training: 9.140546\n",
            "Loss training: 9.053726\n",
            "Loss training: 8.864449\n",
            "Loss training: 9.031714\n",
            "Loss training: 9.650032\n",
            "\n",
            "***************************\n",
            "Trained on 600 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 11.037686\n",
            "Loss training: 9.33546\n",
            "Loss training: 9.756997\n",
            "Loss training: 9.062993\n",
            "Loss training: 9.520949\n",
            "Loss training: 8.583698\n",
            "Loss training: 9.03074\n",
            "Loss training: 9.968267\n",
            "Loss training: 9.575676\n",
            "Loss training: 9.6148615\n",
            "\n",
            "***************************\n",
            "Trained on 610 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 9.29719\n",
            "Loss training: 8.712527\n",
            "Loss training: 8.892264\n",
            "Loss training: 8.509766\n",
            "Loss training: 8.381244\n",
            "Loss training: 8.167734\n",
            "Loss training: 8.0354395\n",
            "Loss training: 7.902154\n",
            "Loss training: 8.387519\n",
            "Loss training: 7.9135656\n",
            "\n",
            "***************************\n",
            "Trained on 620 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 7.4628954\n",
            "Loss training: 7.798832\n",
            "Loss training: 7.1016216\n",
            "Loss training: 7.157447\n",
            "Loss training: 6.935221\n",
            "Loss training: 6.946764\n",
            "Loss training: 6.840057\n",
            "Loss training: 6.6101127\n",
            "Loss training: 6.509475\n",
            "Loss training: 6.4250765\n",
            "\n",
            "***************************\n",
            "Trained on 630 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 6.244038\n",
            "Loss training: 6.307661\n",
            "Loss training: 6.1798215\n",
            "Loss training: 6.059769\n",
            "Loss training: 5.888494\n",
            "Loss training: 5.900567\n",
            "Loss training: 5.806049\n",
            "Loss training: 6.0172596\n",
            "Loss training: 6.942617\n",
            "Loss training: 8.09927\n",
            "\n",
            "***************************\n",
            "Trained on 640 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 10.791799\n",
            "Loss training: 10.988235\n",
            "Loss training: 18.14711\n",
            "Loss training: 14.804584\n",
            "Loss training: 14.514948\n",
            "Loss training: 13.932238\n",
            "Loss training: 14.924248\n",
            "Loss training: 12.660314\n",
            "Loss training: 13.693631\n",
            "Loss training: 12.52858\n",
            "\n",
            "***************************\n",
            "Trained on 650 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 11.141939\n",
            "Loss training: 11.964475\n",
            "Loss training: 10.683464\n",
            "Loss training: 10.507133\n",
            "Loss training: 9.8681555\n",
            "Loss training: 9.067585\n",
            "Loss training: 9.214883\n",
            "Loss training: 8.703871\n",
            "Loss training: 8.19705\n",
            "Loss training: 8.216752\n",
            "\n",
            "***************************\n",
            "Trained on 660 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 7.93437\n",
            "Loss training: 7.350903\n",
            "Loss training: 7.460674\n",
            "Loss training: 6.9760747\n",
            "Loss training: 6.924321\n",
            "Loss training: 6.730773\n",
            "Loss training: 6.5507336\n",
            "Loss training: 6.3468986\n",
            "Loss training: 6.3010244\n",
            "Loss training: 6.0853014\n",
            "\n",
            "***************************\n",
            "Trained on 670 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 5.9244156\n",
            "Loss training: 5.985885\n",
            "Loss training: 5.878152\n",
            "Loss training: 5.692846\n",
            "Loss training: 5.5549393\n",
            "Loss training: 5.5076675\n",
            "Loss training: 5.4822555\n",
            "Loss training: 5.635107\n",
            "Loss training: 5.399478\n",
            "Loss training: 5.2999873\n",
            "\n",
            "***************************\n",
            "Trained on 680 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 5.1114674\n",
            "Loss training: 5.0560775\n",
            "Loss training: 5.144629\n",
            "Loss training: 5.126527\n",
            "Loss training: 5.2534842\n",
            "Loss training: 5.3179007\n",
            "Loss training: 5.321349\n",
            "Loss training: 5.5389175\n",
            "Loss training: 5.3468375\n",
            "Loss training: 5.143606\n",
            "\n",
            "***************************\n",
            "Trained on 690 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 4.8093805\n",
            "Loss training: 5.0171638\n",
            "Loss training: 4.957676\n",
            "Loss training: 4.778829\n",
            "Loss training: 4.720837\n",
            "Loss training: 4.599347\n",
            "Loss training: 4.724474\n",
            "Loss training: 4.8495116\n",
            "Loss training: 4.903164\n",
            "Loss training: 4.9390044\n",
            "\n",
            "***************************\n",
            "Trained on 700 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 5.1396403\n",
            "Loss training: 6.3321185\n",
            "Loss training: 7.982167\n",
            "Loss training: 6.544853\n",
            "Loss training: 6.9582334\n",
            "Loss training: 6.3045263\n",
            "Loss training: 6.5288544\n",
            "Loss training: 5.8235054\n",
            "Loss training: 6.1416197\n",
            "Loss training: 5.707166\n",
            "\n",
            "***************************\n",
            "Trained on 710 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 6.187883\n",
            "Loss training: 5.280444\n",
            "Loss training: 5.2051964\n",
            "Loss training: 5.6709347\n",
            "Loss training: 5.4843903\n",
            "Loss training: 5.3530564\n",
            "Loss training: 4.966671\n",
            "Loss training: 5.025056\n",
            "Loss training: 4.830675\n",
            "Loss training: 4.8089395\n",
            "\n",
            "***************************\n",
            "Trained on 720 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 4.621582\n",
            "Loss training: 4.6749306\n",
            "Loss training: 4.5242243\n",
            "Loss training: 4.3319755\n",
            "Loss training: 4.433796\n",
            "Loss training: 4.294842\n",
            "Loss training: 4.2446027\n",
            "Loss training: 4.2609363\n",
            "Loss training: 4.690878\n",
            "Loss training: 4.410763\n",
            "\n",
            "***************************\n",
            "Trained on 730 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 4.029133\n",
            "Loss training: 4.2217126\n",
            "Loss training: 4.238844\n",
            "Loss training: 4.397085\n",
            "Loss training: 5.143107\n",
            "Loss training: 5.0256295\n",
            "Loss training: 4.4343777\n",
            "Loss training: 4.4079237\n",
            "Loss training: 4.3357015\n",
            "Loss training: 4.1010313\n",
            "\n",
            "***************************\n",
            "Trained on 740 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 4.1163573\n",
            "Loss training: 4.177472\n",
            "Loss training: 3.9482315\n",
            "Loss training: 3.8373647\n",
            "Loss training: 3.8210137\n",
            "Loss training: 3.774836\n",
            "Loss training: 3.7157738\n",
            "Loss training: 3.655985\n",
            "Loss training: 3.8963199\n",
            "Loss training: 3.8067336\n",
            "\n",
            "***************************\n",
            "Trained on 750 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 4.504953\n",
            "Loss training: 4.97494\n",
            "Loss training: 3.932557\n",
            "Loss training: 4.262163\n",
            "Loss training: 3.7725885\n",
            "Loss training: 3.9684675\n",
            "Loss training: 3.9327786\n",
            "Loss training: 3.8284125\n",
            "Loss training: 3.9633553\n",
            "Loss training: 4.407827\n",
            "\n",
            "***************************\n",
            "Trained on 760 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 4.5035872\n",
            "Loss training: 4.549807\n",
            "Loss training: 4.370952\n",
            "Loss training: 4.4163256\n",
            "Loss training: 4.4880066\n",
            "Loss training: 4.089677\n",
            "Loss training: 3.9612608\n",
            "Loss training: 4.232319\n",
            "Loss training: 5.3848763\n",
            "Loss training: 5.1425266\n",
            "\n",
            "***************************\n",
            "Trained on 770 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 4.574887\n",
            "Loss training: 4.296176\n",
            "Loss training: 4.308054\n",
            "Loss training: 4.313508\n",
            "Loss training: 4.2281466\n",
            "Loss training: 4.3578086\n",
            "Loss training: 4.3621206\n",
            "Loss training: 4.9084735\n",
            "Loss training: 4.949824\n",
            "Loss training: 6.8694024\n",
            "\n",
            "***************************\n",
            "Trained on 780 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 4.711469\n",
            "Loss training: 5.759213\n",
            "Loss training: 7.3551936\n",
            "Loss training: 6.114725\n",
            "Loss training: 5.504338\n",
            "Loss training: 5.735132\n",
            "Loss training: 5.3705845\n",
            "Loss training: 5.63912\n",
            "Loss training: 4.721323\n",
            "Loss training: 4.961805\n",
            "\n",
            "***************************\n",
            "Trained on 790 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 4.554322\n",
            "Loss training: 4.765732\n",
            "Loss training: 4.6294727\n",
            "Loss training: 4.11297\n",
            "Loss training: 4.3924108\n",
            "Loss training: 3.9803417\n",
            "Loss training: 3.8808963\n",
            "Loss training: 3.9209816\n",
            "Loss training: 3.9908507\n",
            "Loss training: 3.6951177\n",
            "\n",
            "***************************\n",
            "Trained on 800 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 3.7259507\n",
            "Loss training: 3.9720087\n",
            "Loss training: 3.4127655\n",
            "Loss training: 3.781263\n",
            "Loss training: 4.3241744\n",
            "Loss training: 3.4309447\n",
            "Loss training: 4.46081\n",
            "Loss training: 3.620615\n",
            "Loss training: 4.212235\n",
            "Loss training: 3.5645437\n",
            "\n",
            "***************************\n",
            "Trained on 810 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 3.699195\n",
            "Loss training: 3.4055822\n",
            "Loss training: 3.4608068\n",
            "Loss training: 3.3316593\n",
            "Loss training: 3.5193894\n",
            "Loss training: 3.2103624\n",
            "Loss training: 3.4276226\n",
            "Loss training: 4.6128397\n",
            "Loss training: 7.4332657\n",
            "Loss training: 4.3042254\n",
            "\n",
            "***************************\n",
            "Trained on 820 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 5.582068\n",
            "Loss training: 5.2610197\n",
            "Loss training: 5.3978925\n",
            "Loss training: 4.2892537\n",
            "Loss training: 4.9122853\n",
            "Loss training: 4.237324\n",
            "Loss training: 4.1782203\n",
            "Loss training: 4.471184\n",
            "Loss training: 3.8095348\n",
            "Loss training: 4.174888\n",
            "\n",
            "***************************\n",
            "Trained on 830 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 3.6205256\n",
            "Loss training: 3.649954\n",
            "Loss training: 3.6970294\n",
            "Loss training: 3.352344\n",
            "Loss training: 3.41166\n",
            "Loss training: 3.3389823\n",
            "Loss training: 3.2341096\n",
            "Loss training: 3.1998503\n",
            "Loss training: 3.0292392\n",
            "Loss training: 3.0428264\n",
            "\n",
            "***************************\n",
            "Trained on 840 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 2.9836988\n",
            "Loss training: 2.917248\n",
            "Loss training: 3.0313902\n",
            "Loss training: 2.9859836\n",
            "Loss training: 3.292683\n",
            "Loss training: 3.1704001\n",
            "Loss training: 3.2388744\n",
            "Loss training: 3.1745899\n",
            "Loss training: 3.3520765\n",
            "Loss training: 3.296427\n",
            "\n",
            "***************************\n",
            "Trained on 850 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 3.1083164\n",
            "Loss training: 2.935226\n",
            "Loss training: 3.0354054\n",
            "Loss training: 2.7068038\n",
            "Loss training: 2.8214371\n",
            "Loss training: 2.694765\n",
            "Loss training: 2.598902\n",
            "Loss training: 2.6209745\n",
            "Loss training: 2.5343485\n",
            "Loss training: 2.496126\n",
            "\n",
            "***************************\n",
            "Trained on 860 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 2.4303536\n",
            "Loss training: 2.3991597\n",
            "Loss training: 2.384882\n",
            "Loss training: 2.3163052\n",
            "Loss training: 2.2897747\n",
            "Loss training: 2.2860944\n",
            "Loss training: 2.1830146\n",
            "Loss training: 2.15203\n",
            "Loss training: 2.212411\n",
            "Loss training: 3.0542355\n",
            "\n",
            "***************************\n",
            "Trained on 870 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 10.358219\n",
            "Loss training: 13.482686\n",
            "Loss training: 20.04484\n",
            "Loss training: 27.122849\n",
            "Loss training: 46.580135\n",
            "Loss training: 30.713482\n",
            "Loss training: 31.904533\n",
            "Loss training: 27.011148\n",
            "Loss training: 23.835491\n",
            "Loss training: 23.624374\n",
            "\n",
            "***************************\n",
            "Trained on 880 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.906075\n",
            "Loss training: 21.748299\n",
            "Loss training: 20.769262\n",
            "Loss training: 20.224184\n",
            "Loss training: 19.4493\n",
            "Loss training: 18.706964\n",
            "Loss training: 17.880163\n",
            "Loss training: 17.082947\n",
            "Loss training: 16.42306\n",
            "Loss training: 15.695416\n",
            "\n",
            "***************************\n",
            "Trained on 890 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 14.901641\n",
            "Loss training: 14.208549\n",
            "Loss training: 13.569767\n",
            "Loss training: 13.057847\n",
            "Loss training: 12.610476\n",
            "Loss training: 12.087009\n",
            "Loss training: 11.586735\n",
            "Loss training: 11.191576\n",
            "Loss training: 10.721164\n",
            "Loss training: 10.263386\n",
            "\n",
            "***************************\n",
            "Trained on 900 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 9.866486\n",
            "Loss training: 9.484718\n",
            "Loss training: 9.2286625\n",
            "Loss training: 9.397533\n",
            "Loss training: 9.690028\n",
            "Loss training: 10.011451\n",
            "Loss training: 9.570029\n",
            "Loss training: 10.126469\n",
            "Loss training: 8.813967\n",
            "Loss training: 8.849738\n",
            "\n",
            "***************************\n",
            "Trained on 910 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 8.019892\n",
            "Loss training: 8.131266\n",
            "Loss training: 7.594972\n",
            "Loss training: 7.370607\n",
            "Loss training: 7.133842\n",
            "Loss training: 6.9378486\n",
            "Loss training: 6.6263304\n",
            "Loss training: 6.5736055\n",
            "Loss training: 6.3126006\n",
            "Loss training: 6.206428\n",
            "\n",
            "***************************\n",
            "Trained on 920 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 6.0894756\n",
            "Loss training: 5.96203\n",
            "Loss training: 5.791805\n",
            "Loss training: 5.6180034\n",
            "Loss training: 5.4898753\n",
            "Loss training: 5.311676\n",
            "Loss training: 5.2533674\n",
            "Loss training: 5.1026287\n",
            "Loss training: 4.9895754\n",
            "Loss training: 4.908899\n",
            "\n",
            "***************************\n",
            "Trained on 930 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 4.769782\n",
            "Loss training: 4.674139\n",
            "Loss training: 4.6308312\n",
            "Loss training: 4.7068286\n",
            "Loss training: 4.7043514\n",
            "Loss training: 5.538316\n",
            "Loss training: 6.0946913\n",
            "Loss training: 7.342881\n",
            "Loss training: 7.141457\n",
            "Loss training: 6.7208276\n",
            "\n",
            "***************************\n",
            "Trained on 940 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 6.469928\n",
            "Loss training: 5.9975877\n",
            "Loss training: 6.0455422\n",
            "Loss training: 5.5128098\n",
            "Loss training: 5.6145053\n",
            "Loss training: 4.937282\n",
            "Loss training: 5.187968\n",
            "Loss training: 4.800738\n",
            "Loss training: 4.7003627\n",
            "Loss training: 4.637237\n",
            "\n",
            "***************************\n",
            "Trained on 950 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 4.510809\n",
            "Loss training: 4.1319323\n",
            "Loss training: 4.2700186\n",
            "Loss training: 4.127601\n",
            "Loss training: 3.9485211\n",
            "Loss training: 3.8386905\n",
            "Loss training: 3.8460228\n",
            "Loss training: 3.6532109\n",
            "Loss training: 3.6145046\n",
            "Loss training: 3.5290027\n",
            "\n",
            "***************************\n",
            "Trained on 960 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 3.4237642\n",
            "Loss training: 3.4028895\n",
            "Loss training: 3.2394142\n",
            "Loss training: 3.2628236\n",
            "Loss training: 3.1460223\n",
            "Loss training: 3.0696282\n",
            "Loss training: 3.0469952\n",
            "Loss training: 2.982205\n",
            "Loss training: 2.90121\n",
            "Loss training: 2.895773\n",
            "\n",
            "***************************\n",
            "Trained on 970 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 2.8511906\n",
            "Loss training: 2.8244915\n",
            "Loss training: 2.8831186\n",
            "Loss training: 3.1414826\n",
            "Loss training: 3.3997092\n",
            "Loss training: 5.110272\n",
            "Loss training: 5.386957\n",
            "Loss training: 18.50858\n",
            "Loss training: 19.911085\n",
            "Loss training: 15.514752\n",
            "\n",
            "***************************\n",
            "Trained on 980 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 13.711878\n",
            "Loss training: 12.243081\n",
            "Loss training: 12.995967\n",
            "Loss training: 10.245493\n",
            "Loss training: 10.753917\n",
            "Loss training: 9.546867\n",
            "Loss training: 8.886427\n",
            "Loss training: 9.238581\n",
            "Loss training: 8.698955\n",
            "Loss training: 7.9566774\n",
            "\n",
            "***************************\n",
            "Trained on 990 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 7.9897175\n",
            "Loss training: 7.54619\n",
            "Loss training: 7.165532\n",
            "Loss training: 6.8954067\n",
            "Loss training: 6.468458\n",
            "Loss training: 6.3034635\n",
            "Loss training: 5.9595\n",
            "Loss training: 5.8169036\n",
            "Loss training: 5.502849\n",
            "Loss training: 5.3538213\n",
            "\n",
            "***************************\n",
            "Trained on 1000 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 5.078953\n",
            "Loss training: 4.9097557\n",
            "Loss training: 4.730926\n",
            "Loss training: 4.475534\n",
            "Loss training: 4.3464518\n",
            "Loss training: 4.208973\n",
            "Loss training: 4.0229983\n",
            "Loss training: 3.8626628\n",
            "Loss training: 3.728715\n",
            "Loss training: 3.5877607\n",
            "\n",
            "***************************\n",
            "Trained on 1010 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 3.4794946\n",
            "Loss training: 3.366157\n",
            "Loss training: 3.263966\n",
            "Loss training: 3.1706743\n",
            "Loss training: 3.0798442\n",
            "Loss training: 3.0015342\n",
            "Loss training: 2.9223633\n",
            "Loss training: 2.8476224\n",
            "Loss training: 2.7845783\n",
            "Loss training: 2.737576\n",
            "\n",
            "***************************\n",
            "Trained on 1020 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 2.6823444\n",
            "Loss training: 2.6435273\n",
            "Loss training: 2.6212287\n",
            "Loss training: 2.6715698\n",
            "Loss training: 2.7150507\n",
            "Loss training: 3.0520654\n",
            "Loss training: 2.5290096\n",
            "Loss training: 2.8171108\n",
            "Loss training: 3.7149549\n",
            "Loss training: 2.9994\n",
            "\n",
            "***************************\n",
            "Trained on 1030 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 5.9662566\n",
            "Loss training: 9.674436\n",
            "Loss training: 9.368236\n",
            "Loss training: 6.8738275\n",
            "Loss training: 7.2710524\n",
            "Loss training: 8.927267\n",
            "Loss training: 7.6481485\n",
            "Loss training: 6.526327\n",
            "Loss training: 6.022965\n",
            "Loss training: 5.601215\n",
            "\n",
            "***************************\n",
            "Trained on 1040 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 5.542096\n",
            "Loss training: 5.020012\n",
            "Loss training: 4.446311\n",
            "Loss training: 4.9706535\n",
            "Loss training: 4.2613993\n",
            "Loss training: 4.265629\n",
            "Loss training: 3.9673216\n",
            "Loss training: 4.0069013\n",
            "Loss training: 3.8131099\n",
            "Loss training: 3.5313683\n",
            "\n",
            "***************************\n",
            "Trained on 1050 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 3.4688203\n",
            "Loss training: 3.3941658\n",
            "Loss training: 3.1960154\n",
            "Loss training: 3.1170247\n",
            "Loss training: 3.0448844\n",
            "Loss training: 2.9002736\n",
            "Loss training: 2.8115194\n",
            "Loss training: 2.7342925\n",
            "Loss training: 2.6906965\n",
            "Loss training: 2.5863771\n",
            "\n",
            "***************************\n",
            "Trained on 1060 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 2.5279098\n",
            "Loss training: 2.4733472\n",
            "Loss training: 2.3935122\n",
            "Loss training: 2.3195734\n",
            "Loss training: 2.2623029\n",
            "Loss training: 2.2296598\n",
            "Loss training: 2.1659455\n",
            "Loss training: 2.105404\n",
            "Loss training: 2.061313\n",
            "Loss training: 2.0096796\n",
            "\n",
            "***************************\n",
            "Trained on 1070 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 1.9689505\n",
            "Loss training: 1.9282999\n",
            "Loss training: 1.8913505\n",
            "Loss training: 1.8513032\n",
            "Loss training: 1.8200182\n",
            "Loss training: 1.7950082\n",
            "Loss training: 1.759349\n",
            "Loss training: 1.7324848\n",
            "Loss training: 1.7146865\n",
            "Loss training: 1.7058054\n",
            "\n",
            "***************************\n",
            "Trained on 1080 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 1.7253925\n",
            "Loss training: 1.7251397\n",
            "Loss training: 1.8097993\n",
            "Loss training: 1.6806865\n",
            "Loss training: 1.6530007\n",
            "Loss training: 1.5704013\n",
            "Loss training: 1.5257958\n",
            "Loss training: 1.514423\n",
            "Loss training: 1.4990828\n",
            "Loss training: 1.480474\n",
            "\n",
            "***************************\n",
            "Trained on 1090 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 1.5147206\n",
            "Loss training: 1.8381606\n",
            "Loss training: 1.9212966\n",
            "Loss training: 3.0161378\n",
            "Loss training: 1.7828145\n",
            "Loss training: 4.487029\n",
            "Loss training: 4.558535\n",
            "Loss training: 5.1615644\n",
            "Loss training: 3.624175\n",
            "Loss training: 3.2407782\n",
            "\n",
            "***************************\n",
            "Trained on 1100 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 3.7847347\n",
            "Loss training: 3.7609491\n",
            "Loss training: 3.9107785\n",
            "Loss training: 4.3897576\n",
            "Loss training: 4.9349084\n",
            "Loss training: 3.437575\n",
            "Loss training: 3.434727\n",
            "Loss training: 5.3479447\n",
            "Loss training: 4.0067244\n",
            "Loss training: 4.28851\n",
            "\n",
            "***************************\n",
            "Trained on 1110 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 3.8343036\n",
            "Loss training: 3.429054\n",
            "Loss training: 3.4871864\n",
            "Loss training: 2.934287\n",
            "Loss training: 3.0227356\n",
            "Loss training: 2.9099991\n",
            "Loss training: 2.7763867\n",
            "Loss training: 2.683506\n",
            "Loss training: 2.6648066\n",
            "Loss training: 2.52395\n",
            "\n",
            "***************************\n",
            "Trained on 1120 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 2.36892\n",
            "Loss training: 2.3843143\n",
            "Loss training: 2.2485445\n",
            "Loss training: 2.183641\n",
            "Loss training: 2.2082663\n",
            "Loss training: 2.1253316\n",
            "Loss training: 2.0328999\n",
            "Loss training: 1.9823023\n",
            "Loss training: 1.9455909\n",
            "Loss training: 1.9315385\n",
            "\n",
            "***************************\n",
            "Trained on 1130 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 1.8892184\n",
            "Loss training: 1.8115747\n",
            "Loss training: 1.7693201\n",
            "Loss training: 1.7462869\n",
            "Loss training: 1.6961884\n",
            "Loss training: 1.6695474\n",
            "Loss training: 1.6272744\n",
            "Loss training: 1.6058183\n",
            "Loss training: 1.5859325\n",
            "Loss training: 1.5423265\n",
            "\n",
            "***************************\n",
            "Trained on 1140 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 1.5246238\n",
            "Loss training: 1.4993646\n",
            "Loss training: 1.477019\n",
            "Loss training: 1.4495362\n",
            "Loss training: 1.4293499\n",
            "Loss training: 1.4209673\n",
            "Loss training: 1.3798814\n",
            "Loss training: 1.3553513\n",
            "Loss training: 1.336362\n",
            "Loss training: 1.3205494\n",
            "\n",
            "***************************\n",
            "Trained on 1150 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 1.2986068\n",
            "Loss training: 1.2747102\n",
            "Loss training: 1.2995938\n",
            "Loss training: 1.2882593\n",
            "Loss training: 1.2417746\n",
            "Loss training: 1.2161651\n",
            "Loss training: 1.2238069\n",
            "Loss training: 1.2189258\n",
            "Loss training: 1.2061592\n",
            "Loss training: 1.1529739\n",
            "\n",
            "***************************\n",
            "Trained on 1160 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 1.1439353\n",
            "Loss training: 1.1822926\n",
            "Loss training: 1.2125014\n",
            "Loss training: 1.1858933\n",
            "Loss training: 1.1117288\n",
            "Loss training: 1.0910127\n",
            "Loss training: 1.1593728\n",
            "Loss training: 1.2619454\n",
            "Loss training: 1.37051\n",
            "Loss training: 1.1114902\n",
            "\n",
            "***************************\n",
            "Trained on 1170 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 1.1692835\n",
            "Loss training: 1.2672321\n",
            "Loss training: 1.1005619\n",
            "Loss training: 1.0666198\n",
            "Loss training: 1.1825782\n",
            "Loss training: 1.0578768\n",
            "Loss training: 1.0302467\n",
            "Loss training: 1.1340612\n",
            "Loss training: 1.1004709\n",
            "Loss training: 0.9744368\n",
            "\n",
            "***************************\n",
            "Trained on 1180 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.9928309\n",
            "Loss training: 1.0204221\n",
            "Loss training: 0.9777313\n",
            "Loss training: 0.9762723\n",
            "Loss training: 0.9985741\n",
            "Loss training: 1.0468417\n",
            "Loss training: 1.0928439\n",
            "Loss training: 1.1008029\n",
            "Loss training: 0.9968623\n",
            "Loss training: 0.92794687\n",
            "\n",
            "***************************\n",
            "Trained on 1190 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.9200523\n",
            "Loss training: 1.0536933\n",
            "Loss training: 1.1879994\n",
            "Loss training: 1.0259607\n",
            "Loss training: 0.94434506\n",
            "Loss training: 0.92169344\n",
            "Loss training: 0.9618935\n",
            "Loss training: 1.1304171\n",
            "Loss training: 1.030295\n",
            "Loss training: 0.95395255\n",
            "\n",
            "***************************\n",
            "Trained on 1200 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.87811637\n",
            "Loss training: 0.85116994\n",
            "Loss training: 0.97972965\n",
            "Loss training: 1.0829451\n",
            "Loss training: 1.0375053\n",
            "Loss training: 0.8898858\n",
            "Loss training: 0.93727154\n",
            "Loss training: 0.922621\n",
            "Loss training: 1.0769769\n",
            "Loss training: 1.2931331\n",
            "\n",
            "***************************\n",
            "Trained on 1210 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 1.0308949\n",
            "Loss training: 0.8688149\n",
            "Loss training: 1.0320843\n",
            "Loss training: 0.92352533\n",
            "Loss training: 0.9484747\n",
            "Loss training: 0.9113195\n",
            "Loss training: 1.0585483\n",
            "Loss training: 1.343085\n",
            "Loss training: 1.3706591\n",
            "Loss training: 1.5397911\n",
            "\n",
            "***************************\n",
            "Trained on 1220 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 1.7512083\n",
            "Loss training: 4.736796\n",
            "Loss training: 16.316927\n",
            "Loss training: 23.819225\n",
            "Loss training: 30.579758\n",
            "Loss training: 38.106113\n",
            "Loss training: 31.898968\n",
            "Loss training: 23.37401\n",
            "Loss training: 24.704813\n",
            "Loss training: 20.980028\n",
            "\n",
            "***************************\n",
            "Trained on 1230 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 19.587732\n",
            "Loss training: 17.714077\n",
            "Loss training: 17.067926\n",
            "Loss training: 15.696879\n",
            "Loss training: 14.892479\n",
            "Loss training: 14.13539\n",
            "Loss training: 13.625947\n",
            "Loss training: 13.131039\n",
            "Loss training: 12.254394\n",
            "Loss training: 11.341173\n",
            "\n",
            "***************************\n",
            "Trained on 1240 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 10.725888\n",
            "Loss training: 10.32234\n",
            "Loss training: 9.872075\n",
            "Loss training: 9.32916\n",
            "Loss training: 8.944403\n",
            "Loss training: 8.594503\n",
            "Loss training: 8.209488\n",
            "Loss training: 7.877873\n",
            "Loss training: 7.5855584\n",
            "Loss training: 7.27116\n",
            "\n",
            "***************************\n",
            "Trained on 1250 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 7.040976\n",
            "Loss training: 6.806733\n",
            "Loss training: 6.558811\n",
            "Loss training: 6.336745\n",
            "Loss training: 6.1124315\n",
            "Loss training: 5.938832\n",
            "Loss training: 5.756865\n",
            "Loss training: 5.588698\n",
            "Loss training: 5.4412255\n",
            "Loss training: 5.309555\n",
            "\n",
            "***************************\n",
            "Trained on 1260 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 5.175167\n",
            "Loss training: 5.04304\n",
            "Loss training: 4.911136\n",
            "Loss training: 4.806963\n",
            "Loss training: 4.679195\n",
            "Loss training: 4.5696034\n",
            "Loss training: 4.461479\n",
            "Loss training: 4.3585906\n",
            "Loss training: 4.246633\n",
            "Loss training: 4.1548038\n",
            "\n",
            "***************************\n",
            "Trained on 1270 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 4.076367\n",
            "Loss training: 3.9901464\n",
            "Loss training: 3.8917296\n",
            "Loss training: 3.7916367\n",
            "Loss training: 3.7157187\n",
            "Loss training: 3.7276452\n",
            "Loss training: 3.6536796\n",
            "Loss training: 3.65596\n",
            "Loss training: 3.4304955\n",
            "Loss training: 3.3845627\n",
            "\n",
            "***************************\n",
            "Trained on 1280 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 3.3989038\n",
            "Loss training: 3.181273\n",
            "Loss training: 3.2574902\n",
            "Loss training: 3.3340304\n",
            "Loss training: 3.0409777\n",
            "Loss training: 3.4269161\n",
            "Loss training: 4.7534614\n",
            "Loss training: 4.3288407\n",
            "Loss training: 4.3439126\n",
            "Loss training: 3.5333238\n",
            "\n",
            "***************************\n",
            "Trained on 1290 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 4.7370706\n",
            "Loss training: 3.281021\n",
            "Loss training: 3.871847\n",
            "Loss training: 3.2297115\n",
            "Loss training: 3.4027293\n",
            "Loss training: 3.1872983\n",
            "Loss training: 3.1637807\n",
            "Loss training: 3.1142335\n",
            "Loss training: 2.9273567\n",
            "Loss training: 2.9172845\n",
            "\n",
            "***************************\n",
            "Trained on 1300 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 2.868733\n",
            "Loss training: 2.9001696\n",
            "Loss training: 2.8277874\n",
            "Loss training: 2.8238554\n",
            "Loss training: 2.622425\n",
            "Loss training: 2.7247145\n",
            "Loss training: 2.330743\n",
            "Loss training: 2.8332257\n",
            "Loss training: 2.2215216\n",
            "Loss training: 2.5842795\n",
            "\n",
            "***************************\n",
            "Trained on 1310 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 2.3662019\n",
            "Loss training: 2.620092\n",
            "Loss training: 2.3255153\n",
            "Loss training: 2.2818892\n",
            "Loss training: 2.3365433\n",
            "Loss training: 2.0847952\n",
            "Loss training: 2.265892\n",
            "Loss training: 2.1580155\n",
            "Loss training: 1.8744291\n",
            "Loss training: 1.8685131\n",
            "\n",
            "***************************\n",
            "Trained on 1320 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 1.8721939\n",
            "Loss training: 1.8260776\n",
            "Loss training: 1.7031626\n",
            "Loss training: 1.7120373\n",
            "Loss training: 1.54598\n",
            "Loss training: 1.5725083\n",
            "Loss training: 1.5309267\n",
            "Loss training: 1.6175594\n",
            "Loss training: 1.8366137\n",
            "Loss training: 2.8846729\n",
            "\n",
            "***************************\n",
            "Trained on 1330 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 1.9031703\n",
            "Loss training: 1.635578\n",
            "Loss training: 1.3565184\n",
            "Loss training: 1.640588\n",
            "Loss training: 1.887585\n",
            "Loss training: 1.3359768\n",
            "Loss training: 1.872946\n",
            "Loss training: 1.992273\n",
            "Loss training: 1.5192811\n",
            "Loss training: 2.5925457\n",
            "\n",
            "***************************\n",
            "Trained on 1340 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 4.4475036\n",
            "Loss training: 3.8979542\n",
            "Loss training: 3.3585885\n",
            "Loss training: 3.4065871\n",
            "Loss training: 2.4065824\n",
            "Loss training: 3.1203918\n",
            "Loss training: 2.5432558\n",
            "Loss training: 2.4045255\n",
            "Loss training: 2.3850749\n",
            "Loss training: 2.1568413\n",
            "\n",
            "***************************\n",
            "Trained on 1350 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 2.127789\n",
            "Loss training: 2.2022579\n",
            "Loss training: 1.8009703\n",
            "Loss training: 1.8709446\n",
            "Loss training: 1.7449094\n",
            "Loss training: 1.7095262\n",
            "Loss training: 1.5671984\n",
            "Loss training: 1.557316\n",
            "Loss training: 1.4779624\n",
            "Loss training: 1.4169931\n",
            "\n",
            "***************************\n",
            "Trained on 1360 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 1.3961962\n",
            "Loss training: 1.329197\n",
            "Loss training: 1.3299557\n",
            "Loss training: 1.2045772\n",
            "Loss training: 1.198446\n",
            "Loss training: 1.1818452\n",
            "Loss training: 1.1287966\n",
            "Loss training: 1.0989854\n",
            "Loss training: 1.0838448\n",
            "Loss training: 1.0353602\n",
            "\n",
            "***************************\n",
            "Trained on 1370 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 1.0034753\n",
            "Loss training: 0.9960345\n",
            "Loss training: 0.97175264\n",
            "Loss training: 0.9370131\n",
            "Loss training: 0.90293634\n",
            "Loss training: 0.887961\n",
            "Loss training: 0.8642186\n",
            "Loss training: 0.83645105\n",
            "Loss training: 0.8300652\n",
            "Loss training: 0.8433286\n",
            "\n",
            "***************************\n",
            "Trained on 1380 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 1.0287143\n",
            "Loss training: 1.906124\n",
            "Loss training: 5.530554\n",
            "Loss training: 2.2493196\n",
            "Loss training: 4.706751\n",
            "Loss training: 2.5048547\n",
            "Loss training: 4.7176514\n",
            "Loss training: 4.6987677\n",
            "Loss training: 9.098864\n",
            "Loss training: 11.173288\n",
            "\n",
            "***************************\n",
            "Trained on 1390 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 8.271361\n",
            "Loss training: 7.4772987\n",
            "Loss training: 6.378332\n",
            "Loss training: 5.643021\n",
            "Loss training: 4.988136\n",
            "Loss training: 4.2184496\n",
            "Loss training: 4.7248716\n",
            "Loss training: 4.484204\n",
            "Loss training: 3.692612\n",
            "Loss training: 3.9300547\n",
            "\n",
            "***************************\n",
            "Trained on 1400 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 3.5046313\n",
            "Loss training: 3.533045\n",
            "Loss training: 3.2997863\n",
            "Loss training: 3.0199952\n",
            "Loss training: 2.7308795\n",
            "Loss training: 2.6910307\n",
            "Loss training: 2.5655122\n",
            "Loss training: 2.5247567\n",
            "Loss training: 2.3531377\n",
            "Loss training: 2.167549\n",
            "\n",
            "***************************\n",
            "Trained on 1410 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 2.1369157\n",
            "Loss training: 2.0087528\n",
            "Loss training: 1.9176532\n",
            "Loss training: 1.8501273\n",
            "Loss training: 1.7392551\n",
            "Loss training: 1.6566687\n",
            "Loss training: 1.5773191\n",
            "Loss training: 1.4968345\n",
            "Loss training: 1.4329636\n",
            "Loss training: 1.3790308\n",
            "\n",
            "***************************\n",
            "Trained on 1420 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 1.3093791\n",
            "Loss training: 1.2594101\n",
            "Loss training: 1.1961827\n",
            "Loss training: 1.1455706\n",
            "Loss training: 1.1133413\n",
            "Loss training: 1.063836\n",
            "Loss training: 1.0264091\n",
            "Loss training: 0.9948324\n",
            "Loss training: 0.96000916\n",
            "Loss training: 0.9369705\n",
            "\n",
            "***************************\n",
            "Trained on 1430 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.9080538\n",
            "Loss training: 0.8839788\n",
            "Loss training: 0.86274356\n",
            "Loss training: 0.83809114\n",
            "Loss training: 0.81400734\n",
            "Loss training: 0.79817307\n",
            "Loss training: 0.77912784\n",
            "Loss training: 0.7690953\n",
            "Loss training: 0.767446\n",
            "Loss training: 0.8363269\n",
            "\n",
            "***************************\n",
            "Trained on 1440 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 1.0499641\n",
            "Loss training: 1.4930577\n",
            "Loss training: 2.135732\n",
            "Loss training: 0.8533841\n",
            "Loss training: 1.2927042\n",
            "Loss training: 1.5498686\n",
            "Loss training: 1.0666829\n",
            "Loss training: 1.464595\n",
            "Loss training: 0.86875683\n",
            "Loss training: 1.1900048\n",
            "\n",
            "***************************\n",
            "Trained on 1450 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.9234545\n",
            "Loss training: 0.98836654\n",
            "Loss training: 0.9231693\n",
            "Loss training: 0.8830295\n",
            "Loss training: 0.9208837\n",
            "Loss training: 0.81320316\n",
            "Loss training: 0.8720854\n",
            "Loss training: 0.7934923\n",
            "Loss training: 0.81955075\n",
            "Loss training: 0.75405294\n",
            "\n",
            "***************************\n",
            "Trained on 1460 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.76209545\n",
            "Loss training: 0.72838044\n",
            "Loss training: 0.7121287\n",
            "Loss training: 0.70725214\n",
            "Loss training: 0.6836758\n",
            "Loss training: 0.67036825\n",
            "Loss training: 0.655151\n",
            "Loss training: 0.6264156\n",
            "Loss training: 0.64500856\n",
            "Loss training: 0.5908945\n",
            "\n",
            "***************************\n",
            "Trained on 1470 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.61990994\n",
            "Loss training: 0.57080275\n",
            "Loss training: 0.5876419\n",
            "Loss training: 0.5558382\n",
            "Loss training: 0.56356275\n",
            "Loss training: 0.53709596\n",
            "Loss training: 0.54208183\n",
            "Loss training: 0.524102\n",
            "Loss training: 0.52119505\n",
            "Loss training: 0.50902694\n",
            "\n",
            "***************************\n",
            "Trained on 1480 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.502569\n",
            "Loss training: 0.4935992\n",
            "Loss training: 0.48870948\n",
            "Loss training: 0.47712848\n",
            "Loss training: 0.47537503\n",
            "Loss training: 0.4618556\n",
            "Loss training: 0.46178633\n",
            "Loss training: 0.44774398\n",
            "Loss training: 0.4471811\n",
            "Loss training: 0.43779263\n",
            "\n",
            "***************************\n",
            "Trained on 1490 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.43195552\n",
            "Loss training: 0.42829812\n",
            "Loss training: 0.4189823\n",
            "Loss training: 0.41674396\n",
            "Loss training: 0.4081194\n",
            "Loss training: 0.40327245\n",
            "Loss training: 0.3993019\n",
            "Loss training: 0.39202508\n",
            "Loss training: 0.3869661\n",
            "Loss training: 0.3818783\n",
            "\n",
            "***************************\n",
            "Trained on 1500 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.37685657\n",
            "Loss training: 0.37033588\n",
            "Loss training: 0.36344472\n",
            "Loss training: 0.3580759\n",
            "Loss training: 0.35477042\n",
            "Loss training: 0.35022476\n",
            "Loss training: 0.34495726\n",
            "Loss training: 0.34088293\n",
            "Loss training: 0.33834317\n",
            "Loss training: 0.33729383\n",
            "\n",
            "***************************\n",
            "Trained on 1510 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.33942235\n",
            "Loss training: 0.34384733\n",
            "Loss training: 0.36875552\n",
            "Loss training: 0.4202052\n",
            "Loss training: 0.54551566\n",
            "Loss training: 0.8809389\n",
            "Loss training: 3.1953666\n",
            "Loss training: 7.6615796\n",
            "Loss training: 6.477351\n",
            "Loss training: 8.087493\n",
            "\n",
            "***************************\n",
            "Trained on 1520 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 14.795\n",
            "Loss training: 8.738609\n",
            "Loss training: 10.782654\n",
            "Loss training: 7.0953693\n",
            "Loss training: 8.094165\n",
            "Loss training: 5.242951\n",
            "Loss training: 6.2180214\n",
            "Loss training: 5.211203\n",
            "Loss training: 4.6282907\n",
            "Loss training: 4.3743787\n",
            "\n",
            "***************************\n",
            "Trained on 1530 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 4.250359\n",
            "Loss training: 3.7570622\n",
            "Loss training: 3.5578523\n",
            "Loss training: 3.5128467\n",
            "Loss training: 3.0706327\n",
            "Loss training: 2.7747505\n",
            "Loss training: 2.6844356\n",
            "Loss training: 2.5966153\n",
            "Loss training: 2.4116354\n",
            "Loss training: 2.1913977\n",
            "\n",
            "***************************\n",
            "Trained on 1540 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 2.090962\n",
            "Loss training: 1.9784626\n",
            "Loss training: 1.8623906\n",
            "Loss training: 1.7946646\n",
            "Loss training: 1.7116783\n",
            "Loss training: 1.6148655\n",
            "Loss training: 1.55858\n",
            "Loss training: 1.4872137\n",
            "Loss training: 1.4153938\n",
            "Loss training: 1.3552324\n",
            "\n",
            "***************************\n",
            "Trained on 1550 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 1.2950759\n",
            "Loss training: 1.2591565\n",
            "Loss training: 1.211531\n",
            "Loss training: 1.1668494\n",
            "Loss training: 1.137081\n",
            "Loss training: 1.0979912\n",
            "Loss training: 1.0604779\n",
            "Loss training: 1.0299886\n",
            "Loss training: 1.0060331\n",
            "Loss training: 0.98193353\n",
            "\n",
            "***************************\n",
            "Trained on 1560 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.94848007\n",
            "Loss training: 0.927192\n",
            "Loss training: 0.90621257\n",
            "Loss training: 0.88740987\n",
            "Loss training: 0.8637009\n",
            "Loss training: 0.84853095\n",
            "Loss training: 0.8306323\n",
            "Loss training: 0.813044\n",
            "Loss training: 0.7976641\n",
            "Loss training: 0.7808206\n",
            "\n",
            "***************************\n",
            "Trained on 1570 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.7661662\n",
            "Loss training: 0.75336564\n",
            "Loss training: 0.7401898\n",
            "Loss training: 0.7274881\n",
            "Loss training: 0.71545905\n",
            "Loss training: 0.7049106\n",
            "Loss training: 0.69381016\n",
            "Loss training: 0.6853619\n",
            "Loss training: 0.6736732\n",
            "Loss training: 0.66381186\n",
            "\n",
            "***************************\n",
            "Trained on 1580 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.6532589\n",
            "Loss training: 0.6442969\n",
            "Loss training: 0.6338549\n",
            "Loss training: 0.6271141\n",
            "Loss training: 0.6182849\n",
            "Loss training: 0.6102541\n",
            "Loss training: 0.600731\n",
            "Loss training: 0.59245044\n",
            "Loss training: 0.5865092\n",
            "Loss training: 0.5785512\n",
            "\n",
            "***************************\n",
            "Trained on 1590 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.5710807\n",
            "Loss training: 0.56387365\n",
            "Loss training: 0.5570263\n",
            "Loss training: 0.55105096\n",
            "Loss training: 0.5442746\n",
            "Loss training: 0.53682196\n",
            "Loss training: 0.5316015\n",
            "Loss training: 0.52435\n",
            "Loss training: 0.5186407\n",
            "Loss training: 0.51289296\n",
            "\n",
            "***************************\n",
            "Trained on 1600 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.50582427\n",
            "Loss training: 0.5011491\n",
            "Loss training: 0.49455908\n",
            "Loss training: 0.4890745\n",
            "Loss training: 0.48304853\n",
            "Loss training: 0.47859347\n",
            "Loss training: 0.4717712\n",
            "Loss training: 0.46680698\n",
            "Loss training: 0.46245733\n",
            "Loss training: 0.4569368\n",
            "\n",
            "***************************\n",
            "Trained on 1610 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.45205992\n",
            "Loss training: 0.4473962\n",
            "Loss training: 0.442951\n",
            "Loss training: 0.43845308\n",
            "Loss training: 0.43247053\n",
            "Loss training: 0.42857867\n",
            "Loss training: 0.4240655\n",
            "Loss training: 0.41953424\n",
            "Loss training: 0.41488495\n",
            "Loss training: 0.41025648\n",
            "\n",
            "***************************\n",
            "Trained on 1620 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.40629283\n",
            "Loss training: 0.40248075\n",
            "Loss training: 0.398704\n",
            "Loss training: 0.3932741\n",
            "Loss training: 0.38966152\n",
            "Loss training: 0.38705248\n",
            "Loss training: 0.38263613\n",
            "Loss training: 0.37829867\n",
            "Loss training: 0.37526712\n",
            "Loss training: 0.37020957\n",
            "\n",
            "***************************\n",
            "Trained on 1630 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.36758617\n",
            "Loss training: 0.3654599\n",
            "Loss training: 0.36152643\n",
            "Loss training: 0.35664496\n",
            "Loss training: 0.35459608\n",
            "Loss training: 0.35158294\n",
            "Loss training: 0.34773403\n",
            "Loss training: 0.34422612\n",
            "Loss training: 0.34205115\n",
            "Loss training: 0.33833468\n",
            "\n",
            "***************************\n",
            "Trained on 1640 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.33578736\n",
            "Loss training: 0.333217\n",
            "Loss training: 0.3302156\n",
            "Loss training: 0.3275763\n",
            "Loss training: 0.32438594\n",
            "Loss training: 0.3237697\n",
            "Loss training: 0.3206848\n",
            "Loss training: 0.31634587\n",
            "Loss training: 0.31107533\n",
            "Loss training: 0.30854416\n",
            "\n",
            "***************************\n",
            "Trained on 1650 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.3047657\n",
            "Loss training: 0.30208322\n",
            "Loss training: 0.29980683\n",
            "Loss training: 0.29660195\n",
            "Loss training: 0.2981817\n",
            "Loss training: 0.29705068\n",
            "Loss training: 0.29993194\n",
            "Loss training: 0.29263428\n",
            "Loss training: 0.28611398\n",
            "Loss training: 0.27803865\n",
            "\n",
            "***************************\n",
            "Trained on 1660 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.27561095\n",
            "Loss training: 0.27191564\n",
            "Loss training: 0.2722741\n",
            "Loss training: 0.2677855\n",
            "Loss training: 0.2690514\n",
            "Loss training: 0.27750668\n",
            "Loss training: 0.30184293\n",
            "Loss training: 0.39147076\n",
            "Loss training: 0.64827085\n",
            "Loss training: 2.5750148\n",
            "\n",
            "***************************\n",
            "Trained on 1670 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 7.739625\n",
            "Loss training: 5.9413834\n",
            "Loss training: 8.079876\n",
            "Loss training: 11.907938\n",
            "Loss training: 23.50459\n",
            "Loss training: 21.23533\n",
            "Loss training: 23.29355\n",
            "Loss training: 17.883934\n",
            "Loss training: 11.881085\n",
            "Loss training: 11.286929\n",
            "\n",
            "***************************\n",
            "Trained on 1680 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 11.253703\n",
            "Loss training: 10.224484\n",
            "Loss training: 8.268541\n",
            "Loss training: 8.127896\n",
            "Loss training: 7.8099494\n",
            "Loss training: 7.2861824\n",
            "Loss training: 6.6173716\n",
            "Loss training: 6.123732\n",
            "Loss training: 5.6257486\n",
            "Loss training: 5.189118\n",
            "\n",
            "***************************\n",
            "Trained on 1690 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 4.8674083\n",
            "Loss training: 4.5045834\n",
            "Loss training: 4.104807\n",
            "Loss training: 3.8238833\n",
            "Loss training: 3.6182454\n",
            "Loss training: 3.372857\n",
            "Loss training: 3.1348805\n",
            "Loss training: 2.9248865\n",
            "Loss training: 2.744326\n",
            "Loss training: 2.5689964\n",
            "\n",
            "***************************\n",
            "Trained on 1700 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 2.4256263\n",
            "Loss training: 2.2953775\n",
            "Loss training: 2.1803672\n",
            "Loss training: 2.0527854\n",
            "Loss training: 1.9283533\n",
            "Loss training: 1.8276054\n",
            "Loss training: 1.7290221\n",
            "Loss training: 1.6559232\n",
            "Loss training: 1.5903807\n",
            "Loss training: 1.5135088\n",
            "\n",
            "***************************\n",
            "Trained on 1710 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 1.4430785\n",
            "Loss training: 1.3773311\n",
            "Loss training: 1.3187873\n",
            "Loss training: 1.2694242\n",
            "Loss training: 1.219976\n",
            "Loss training: 1.1749283\n",
            "Loss training: 1.1319054\n",
            "Loss training: 1.0937109\n",
            "Loss training: 1.058363\n",
            "Loss training: 1.020269\n",
            "\n",
            "***************************\n",
            "Trained on 1720 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.9883949\n",
            "Loss training: 0.95740217\n",
            "Loss training: 0.92735624\n",
            "Loss training: 0.90369064\n",
            "Loss training: 0.8761397\n",
            "Loss training: 0.85063374\n",
            "Loss training: 0.82642627\n",
            "Loss training: 0.8031908\n",
            "Loss training: 0.78312534\n",
            "Loss training: 0.7634027\n",
            "\n",
            "***************************\n",
            "Trained on 1730 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.74667746\n",
            "Loss training: 0.7283189\n",
            "Loss training: 0.7128878\n",
            "Loss training: 0.69675076\n",
            "Loss training: 0.6805144\n",
            "Loss training: 0.6680924\n",
            "Loss training: 0.653466\n",
            "Loss training: 0.6408727\n",
            "Loss training: 0.6305276\n",
            "Loss training: 0.619111\n",
            "\n",
            "***************************\n",
            "Trained on 1740 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.6099643\n",
            "Loss training: 0.5998503\n",
            "Loss training: 0.5877448\n",
            "Loss training: 0.57925993\n",
            "Loss training: 0.5701\n",
            "Loss training: 0.5633838\n",
            "Loss training: 0.5548561\n",
            "Loss training: 0.5453853\n",
            "Loss training: 0.53878146\n",
            "Loss training: 0.53141505\n",
            "\n",
            "***************************\n",
            "Trained on 1750 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.52475363\n",
            "Loss training: 0.5172619\n",
            "Loss training: 0.50987667\n",
            "Loss training: 0.5048078\n",
            "Loss training: 0.49812326\n",
            "Loss training: 0.49207297\n",
            "Loss training: 0.4875898\n",
            "Loss training: 0.48058683\n",
            "Loss training: 0.47520354\n",
            "Loss training: 0.46943888\n",
            "\n",
            "***************************\n",
            "Trained on 1760 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.46537745\n",
            "Loss training: 0.45854703\n",
            "Loss training: 0.45406342\n",
            "Loss training: 0.448379\n",
            "Loss training: 0.4410387\n",
            "Loss training: 0.43608803\n",
            "Loss training: 0.42892504\n",
            "Loss training: 0.42369482\n",
            "Loss training: 0.4175624\n",
            "Loss training: 0.409934\n",
            "\n",
            "***************************\n",
            "Trained on 1770 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.40728253\n",
            "Loss training: 0.40255097\n",
            "Loss training: 0.39800805\n",
            "Loss training: 0.39421925\n",
            "Loss training: 0.39011556\n",
            "Loss training: 0.3856616\n",
            "Loss training: 0.38255683\n",
            "Loss training: 0.37804833\n",
            "Loss training: 0.37529284\n",
            "Loss training: 0.37057886\n",
            "\n",
            "***************************\n",
            "Trained on 1780 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.36812413\n",
            "Loss training: 0.36291623\n",
            "Loss training: 0.36052737\n",
            "Loss training: 0.35614607\n",
            "Loss training: 0.35315964\n",
            "Loss training: 0.3498296\n",
            "Loss training: 0.34694415\n",
            "Loss training: 0.34278122\n",
            "Loss training: 0.34009486\n",
            "Loss training: 0.33744714\n",
            "\n",
            "***************************\n",
            "Trained on 1790 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.33444846\n",
            "Loss training: 0.33207312\n",
            "Loss training: 0.32810336\n",
            "Loss training: 0.32589316\n",
            "Loss training: 0.32219484\n",
            "Loss training: 0.31980726\n",
            "Loss training: 0.31932136\n",
            "Loss training: 0.31503636\n",
            "Loss training: 0.31303722\n",
            "Loss training: 0.31098863\n",
            "\n",
            "***************************\n",
            "Trained on 1800 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.30703804\n",
            "Loss training: 0.30583626\n",
            "Loss training: 0.30140018\n",
            "Loss training: 0.29923734\n",
            "Loss training: 0.2955927\n",
            "Loss training: 0.29321408\n",
            "Loss training: 0.29016635\n",
            "Loss training: 0.28993565\n",
            "Loss training: 0.28704575\n",
            "Loss training: 0.28250256\n",
            "\n",
            "***************************\n",
            "Trained on 1810 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.2801678\n",
            "Loss training: 0.28023794\n",
            "Loss training: 0.2781456\n",
            "Loss training: 0.27636495\n",
            "Loss training: 0.27015337\n",
            "Loss training: 0.27109528\n",
            "Loss training: 0.26982567\n",
            "Loss training: 0.2641975\n",
            "Loss training: 0.26447698\n",
            "Loss training: 0.25953457\n",
            "\n",
            "***************************\n",
            "Trained on 1820 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.26049083\n",
            "Loss training: 0.25975195\n",
            "Loss training: 0.2553424\n",
            "Loss training: 0.25576866\n",
            "Loss training: 0.25095028\n",
            "Loss training: 0.2482109\n",
            "Loss training: 0.24814534\n",
            "Loss training: 0.24693519\n",
            "Loss training: 0.24766651\n",
            "Loss training: 0.24253526\n",
            "\n",
            "***************************\n",
            "Trained on 1830 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.23968656\n",
            "Loss training: 0.23707896\n",
            "Loss training: 0.23483054\n",
            "Loss training: 0.23225707\n",
            "Loss training: 0.23000874\n",
            "Loss training: 0.22909014\n",
            "Loss training: 0.2282868\n",
            "Loss training: 0.22722653\n",
            "Loss training: 0.2267892\n",
            "Loss training: 0.2253111\n",
            "\n",
            "***************************\n",
            "Trained on 1840 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.2221032\n",
            "Loss training: 0.21983463\n",
            "Loss training: 0.21782634\n",
            "Loss training: 0.21605982\n",
            "Loss training: 0.2131238\n",
            "Loss training: 0.21078596\n",
            "Loss training: 0.21049656\n",
            "Loss training: 0.20869814\n",
            "Loss training: 0.20879056\n",
            "Loss training: 0.21032888\n",
            "\n",
            "***************************\n",
            "Trained on 1850 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.21392433\n",
            "Loss training: 0.21503158\n",
            "Loss training: 0.21834508\n",
            "Loss training: 0.20906022\n",
            "Loss training: 0.1995066\n",
            "Loss training: 0.19604377\n",
            "Loss training: 0.20012482\n",
            "Loss training: 0.2035583\n",
            "Loss training: 0.19891067\n",
            "Loss training: 0.1938793\n",
            "\n",
            "***************************\n",
            "Trained on 1860 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.18858671\n",
            "Loss training: 0.19011466\n",
            "Loss training: 0.19208838\n",
            "Loss training: 0.18741588\n",
            "Loss training: 0.18317826\n",
            "Loss training: 0.18239512\n",
            "Loss training: 0.18567912\n",
            "Loss training: 0.18605405\n",
            "Loss training: 0.18265438\n",
            "Loss training: 0.18096606\n",
            "\n",
            "***************************\n",
            "Trained on 1870 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.1773507\n",
            "Loss training: 0.17379077\n",
            "Loss training: 0.17212999\n",
            "Loss training: 0.17096514\n",
            "Loss training: 0.17001097\n",
            "Loss training: 0.16732395\n",
            "Loss training: 0.16677505\n",
            "Loss training: 0.16619848\n",
            "Loss training: 0.16653615\n",
            "Loss training: 0.1651531\n",
            "\n",
            "***************************\n",
            "Trained on 1880 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.16358738\n",
            "Loss training: 0.16017881\n",
            "Loss training: 0.15974815\n",
            "Loss training: 0.16113394\n",
            "Loss training: 0.1614649\n",
            "Loss training: 0.16025056\n",
            "Loss training: 0.16000459\n",
            "Loss training: 0.15690124\n",
            "Loss training: 0.1533736\n",
            "Loss training: 0.1531828\n",
            "\n",
            "***************************\n",
            "Trained on 1890 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.15172136\n",
            "Loss training: 0.15149663\n",
            "Loss training: 0.1497441\n",
            "Loss training: 0.14845446\n",
            "Loss training: 0.14662269\n",
            "Loss training: 0.14640257\n",
            "Loss training: 0.14526226\n",
            "Loss training: 0.1442057\n",
            "Loss training: 0.14308108\n",
            "Loss training: 0.14301151\n",
            "\n",
            "***************************\n",
            "Trained on 1900 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.14415461\n",
            "Loss training: 0.14535238\n",
            "Loss training: 0.14559798\n",
            "Loss training: 0.14279984\n",
            "Loss training: 0.13936035\n",
            "Loss training: 0.13756035\n",
            "Loss training: 0.13758177\n",
            "Loss training: 0.13830729\n",
            "Loss training: 0.13641101\n",
            "Loss training: 0.1345511\n",
            "\n",
            "***************************\n",
            "Trained on 1910 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.13471961\n",
            "Loss training: 0.13533379\n",
            "Loss training: 0.13327645\n",
            "Loss training: 0.13193093\n",
            "Loss training: 0.1317316\n",
            "Loss training: 0.13130172\n",
            "Loss training: 0.12965861\n",
            "Loss training: 0.12882708\n",
            "Loss training: 0.12935317\n",
            "Loss training: 0.12863882\n",
            "\n",
            "***************************\n",
            "Trained on 1920 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.12702326\n",
            "Loss training: 0.12684955\n",
            "Loss training: 0.12629665\n",
            "Loss training: 0.12579544\n",
            "Loss training: 0.12518081\n",
            "Loss training: 0.12436688\n",
            "Loss training: 0.12378088\n",
            "Loss training: 0.1230703\n",
            "Loss training: 0.12290725\n",
            "Loss training: 0.12229077\n",
            "\n",
            "***************************\n",
            "Trained on 1930 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.12266073\n",
            "Loss training: 0.121615425\n",
            "Loss training: 0.12082833\n",
            "Loss training: 0.12016739\n",
            "Loss training: 0.119966865\n",
            "Loss training: 0.11922186\n",
            "Loss training: 0.119014636\n",
            "Loss training: 0.11838833\n",
            "Loss training: 0.11783208\n",
            "Loss training: 0.11806003\n",
            "\n",
            "***************************\n",
            "Trained on 1940 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.11756102\n",
            "Loss training: 0.11716326\n",
            "Loss training: 0.116256006\n",
            "Loss training: 0.11621432\n",
            "Loss training: 0.11604525\n",
            "Loss training: 0.11577383\n",
            "Loss training: 0.11474156\n",
            "Loss training: 0.11463904\n",
            "Loss training: 0.11424717\n",
            "Loss training: 0.113778174\n",
            "\n",
            "***************************\n",
            "Trained on 1950 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.11342853\n",
            "Loss training: 0.11362248\n",
            "Loss training: 0.11270087\n",
            "Loss training: 0.1123733\n",
            "Loss training: 0.112186015\n",
            "Loss training: 0.1117725\n",
            "Loss training: 0.11148046\n",
            "Loss training: 0.11114493\n",
            "Loss training: 0.110843204\n",
            "Loss training: 0.11057324\n",
            "\n",
            "***************************\n",
            "Trained on 1960 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.11046198\n",
            "Loss training: 0.10989996\n",
            "Loss training: 0.10965392\n",
            "Loss training: 0.10932142\n",
            "Loss training: 0.109098576\n",
            "Loss training: 0.10891191\n",
            "Loss training: 0.10843637\n",
            "Loss training: 0.10828729\n",
            "Loss training: 0.108148865\n",
            "Loss training: 0.10773865\n",
            "\n",
            "***************************\n",
            "Trained on 1970 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.10737888\n",
            "Loss training: 0.10696081\n",
            "Loss training: 0.10702733\n",
            "Loss training: 0.1064002\n",
            "Loss training: 0.10641746\n",
            "Loss training: 0.10595523\n",
            "Loss training: 0.10600283\n",
            "Loss training: 0.10577673\n",
            "Loss training: 0.10537575\n",
            "Loss training: 0.10519056\n",
            "\n",
            "***************************\n",
            "Trained on 1980 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.105126314\n",
            "Loss training: 0.10465589\n",
            "Loss training: 0.104401276\n",
            "Loss training: 0.10421324\n",
            "Loss training: 0.104132846\n",
            "Loss training: 0.10375923\n",
            "Loss training: 0.10374266\n",
            "Loss training: 0.10337054\n",
            "Loss training: 0.10327849\n",
            "Loss training: 0.10303827\n",
            "\n",
            "***************************\n",
            "Trained on 1990 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.10291421\n",
            "Loss training: 0.10270744\n",
            "Loss training: 0.10271628\n",
            "Loss training: 0.10234534\n",
            "Loss training: 0.10238246\n",
            "Loss training: 0.101844124\n",
            "Loss training: 0.101904266\n",
            "Loss training: 0.1015653\n",
            "Loss training: 0.101304196\n",
            "Loss training: 0.10136041\n",
            "\n",
            "***************************\n",
            "Trained on 2000 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.101145394\n",
            "Loss training: 0.1009174\n",
            "Loss training: 0.100571334\n",
            "Loss training: 0.10055346\n",
            "Loss training: 0.10027277\n",
            "Loss training: 0.100276716\n",
            "Loss training: 0.10011355\n",
            "Loss training: 0.099983685\n",
            "Loss training: 0.099668905\n",
            "Loss training: 0.099583566\n",
            "\n",
            "***************************\n",
            "Trained on 2010 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.09947906\n",
            "Loss training: 0.099339\n",
            "Loss training: 0.099216335\n",
            "Loss training: 0.099108264\n",
            "Loss training: 0.09911727\n",
            "Loss training: 0.0988333\n",
            "Loss training: 0.098629534\n",
            "Loss training: 0.098647706\n",
            "Loss training: 0.0982334\n",
            "Loss training: 0.09822375\n",
            "\n",
            "***************************\n",
            "Trained on 2020 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.09807834\n",
            "Loss training: 0.09792981\n",
            "Loss training: 0.09784212\n",
            "Loss training: 0.097633034\n",
            "Loss training: 0.097697645\n",
            "Loss training: 0.097463936\n",
            "Loss training: 0.09730344\n",
            "Loss training: 0.09723459\n",
            "Loss training: 0.09706428\n",
            "Loss training: 0.09694893\n",
            "\n",
            "***************************\n",
            "Trained on 2030 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.09679307\n",
            "Loss training: 0.09678459\n",
            "Loss training: 0.096585855\n",
            "Loss training: 0.096439116\n",
            "Loss training: 0.09635159\n",
            "Loss training: 0.0962226\n",
            "Loss training: 0.09602578\n",
            "Loss training: 0.0960714\n",
            "Loss training: 0.09585292\n",
            "Loss training: 0.09571772\n",
            "\n",
            "***************************\n",
            "Trained on 2040 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.09560617\n",
            "Loss training: 0.095517956\n",
            "Loss training: 0.095427625\n",
            "Loss training: 0.095381685\n",
            "Loss training: 0.09509386\n",
            "Loss training: 0.09499743\n",
            "Loss training: 0.094968565\n",
            "Loss training: 0.09486796\n",
            "Loss training: 0.0948352\n",
            "Loss training: 0.09467538\n",
            "\n",
            "***************************\n",
            "Trained on 2050 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.09451525\n",
            "Loss training: 0.09444958\n",
            "Loss training: 0.09436875\n",
            "Loss training: 0.09426331\n",
            "Loss training: 0.094154745\n",
            "Loss training: 0.09406499\n",
            "Loss training: 0.09399167\n",
            "Loss training: 0.09393667\n",
            "Loss training: 0.0937517\n",
            "Loss training: 0.09377806\n",
            "\n",
            "***************************\n",
            "Trained on 2060 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.09362276\n",
            "Loss training: 0.09350502\n",
            "Loss training: 0.093387626\n",
            "Loss training: 0.09331922\n",
            "Loss training: 0.09324581\n",
            "Loss training: 0.09318829\n",
            "Loss training: 0.09318324\n",
            "Loss training: 0.093124345\n",
            "Loss training: 0.09285128\n",
            "Loss training: 0.09290454\n",
            "\n",
            "***************************\n",
            "Trained on 2070 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.09268725\n",
            "Loss training: 0.09277581\n",
            "Loss training: 0.092670515\n",
            "Loss training: 0.09249046\n",
            "Loss training: 0.09249461\n",
            "Loss training: 0.09238204\n",
            "Loss training: 0.092296734\n",
            "Loss training: 0.092345946\n",
            "Loss training: 0.09205226\n",
            "Loss training: 0.092471786\n",
            "\n",
            "***************************\n",
            "Trained on 2080 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.0926306\n",
            "Loss training: 0.0922337\n",
            "Loss training: 0.09198656\n",
            "Loss training: 0.09271821\n",
            "Loss training: 0.09265834\n",
            "Loss training: 0.09171447\n",
            "Loss training: 0.092350595\n",
            "Loss training: 0.09174813\n",
            "Loss training: 0.0916811\n",
            "Loss training: 0.09168608\n",
            "\n",
            "***************************\n",
            "Trained on 2090 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.09137746\n",
            "Loss training: 0.091975965\n",
            "Loss training: 0.09144704\n",
            "Loss training: 0.09137002\n",
            "Loss training: 0.09131832\n",
            "Loss training: 0.09107538\n",
            "Loss training: 0.09107697\n",
            "Loss training: 0.0911293\n",
            "Loss training: 0.09101611\n",
            "Loss training: 0.0908728\n",
            "\n",
            "***************************\n",
            "Trained on 2100 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.090804964\n",
            "Loss training: 0.09078523\n",
            "Loss training: 0.09059614\n",
            "Loss training: 0.09063654\n",
            "Loss training: 0.090548605\n",
            "Loss training: 0.09038263\n",
            "Loss training: 0.09033389\n",
            "Loss training: 0.09020371\n",
            "Loss training: 0.0903016\n",
            "Loss training: 0.09008022\n",
            "\n",
            "***************************\n",
            "Trained on 2110 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.09018253\n",
            "Loss training: 0.0900068\n",
            "Loss training: 0.090013966\n",
            "Loss training: 0.08997203\n",
            "Loss training: 0.08980333\n",
            "Loss training: 0.089816734\n",
            "Loss training: 0.08977547\n",
            "Loss training: 0.089804955\n",
            "Loss training: 0.08968226\n",
            "Loss training: 0.08994294\n",
            "\n",
            "***************************\n",
            "Trained on 2120 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.089547336\n",
            "Loss training: 0.089490876\n",
            "Loss training: 0.08948459\n",
            "Loss training: 0.08923451\n",
            "Loss training: 0.089554235\n",
            "Loss training: 0.08932681\n",
            "Loss training: 0.08908455\n",
            "Loss training: 0.08935421\n",
            "Loss training: 0.08934462\n",
            "Loss training: 0.089007564\n",
            "\n",
            "***************************\n",
            "Trained on 2130 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.088908985\n",
            "Loss training: 0.08887242\n",
            "Loss training: 0.0887665\n",
            "Loss training: 0.088773705\n",
            "Loss training: 0.08879041\n",
            "Loss training: 0.0886307\n",
            "Loss training: 0.08871911\n",
            "Loss training: 0.08886304\n",
            "Loss training: 0.088796854\n",
            "Loss training: 0.088566035\n",
            "\n",
            "***************************\n",
            "Trained on 2140 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08842736\n",
            "Loss training: 0.08855736\n",
            "Loss training: 0.08836232\n",
            "Loss training: 0.08827745\n",
            "Loss training: 0.088283814\n",
            "Loss training: 0.08818317\n",
            "Loss training: 0.08830837\n",
            "Loss training: 0.088128835\n",
            "Loss training: 0.08805053\n",
            "Loss training: 0.08804751\n",
            "\n",
            "***************************\n",
            "Trained on 2150 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.088048115\n",
            "Loss training: 0.0879033\n",
            "Loss training: 0.08793283\n",
            "Loss training: 0.08813789\n",
            "Loss training: 0.08806585\n",
            "Loss training: 0.08774178\n",
            "Loss training: 0.0877355\n",
            "Loss training: 0.0878436\n",
            "Loss training: 0.08788245\n",
            "Loss training: 0.08773628\n",
            "\n",
            "***************************\n",
            "Trained on 2160 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08763549\n",
            "Loss training: 0.087457575\n",
            "Loss training: 0.08768842\n",
            "Loss training: 0.087558486\n",
            "Loss training: 0.08741326\n",
            "Loss training: 0.08727717\n",
            "Loss training: 0.08726414\n",
            "Loss training: 0.087211795\n",
            "Loss training: 0.087343015\n",
            "Loss training: 0.087218106\n",
            "\n",
            "***************************\n",
            "Trained on 2170 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08716367\n",
            "Loss training: 0.08706953\n",
            "Loss training: 0.08728847\n",
            "Loss training: 0.08719003\n",
            "Loss training: 0.087044284\n",
            "Loss training: 0.08720954\n",
            "Loss training: 0.08706714\n",
            "Loss training: 0.08746053\n",
            "Loss training: 0.08711478\n",
            "Loss training: 0.08696519\n",
            "\n",
            "***************************\n",
            "Trained on 2180 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08687868\n",
            "Loss training: 0.08697222\n",
            "Loss training: 0.08670007\n",
            "Loss training: 0.08713943\n",
            "Loss training: 0.08696098\n",
            "Loss training: 0.08689319\n",
            "Loss training: 0.086638235\n",
            "Loss training: 0.086927354\n",
            "Loss training: 0.086567044\n",
            "Loss training: 0.086499564\n",
            "\n",
            "***************************\n",
            "Trained on 2190 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08636974\n",
            "Loss training: 0.08637794\n",
            "Loss training: 0.08627967\n",
            "Loss training: 0.08628571\n",
            "Loss training: 0.08625736\n",
            "Loss training: 0.08624681\n",
            "Loss training: 0.086177886\n",
            "Loss training: 0.08610335\n",
            "Loss training: 0.086097844\n",
            "Loss training: 0.08603296\n",
            "\n",
            "***************************\n",
            "Trained on 2200 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08598855\n",
            "Loss training: 0.08593868\n",
            "Loss training: 0.08595283\n",
            "Loss training: 0.08592462\n",
            "Loss training: 0.085930474\n",
            "Loss training: 0.0859681\n",
            "Loss training: 0.08579817\n",
            "Loss training: 0.085929245\n",
            "Loss training: 0.085853726\n",
            "Loss training: 0.0857025\n",
            "\n",
            "***************************\n",
            "Trained on 2210 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08565322\n",
            "Loss training: 0.08564004\n",
            "Loss training: 0.08564932\n",
            "Loss training: 0.08558807\n",
            "Loss training: 0.08556538\n",
            "Loss training: 0.085489094\n",
            "Loss training: 0.085481554\n",
            "Loss training: 0.08543775\n",
            "Loss training: 0.08553429\n",
            "Loss training: 0.085427515\n",
            "\n",
            "***************************\n",
            "Trained on 2220 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08532681\n",
            "Loss training: 0.085440256\n",
            "Loss training: 0.08577018\n",
            "Loss training: 0.08591027\n",
            "Loss training: 0.085404344\n",
            "Loss training: 0.08538185\n",
            "Loss training: 0.08568214\n",
            "Loss training: 0.085442126\n",
            "Loss training: 0.08515508\n",
            "Loss training: 0.08579039\n",
            "\n",
            "***************************\n",
            "Trained on 2230 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.085836805\n",
            "Loss training: 0.085079625\n",
            "Loss training: 0.085893996\n",
            "Loss training: 0.086270645\n",
            "Loss training: 0.08510829\n",
            "Loss training: 0.08583929\n",
            "Loss training: 0.0867088\n",
            "Loss training: 0.08489478\n",
            "Loss training: 0.08655401\n",
            "Loss training: 0.088175766\n",
            "\n",
            "***************************\n",
            "Trained on 2240 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.0856961\n",
            "Loss training: 0.08559462\n",
            "Loss training: 0.087744266\n",
            "Loss training: 0.08483854\n",
            "Loss training: 0.087517\n",
            "Loss training: 0.08795807\n",
            "Loss training: 0.085012086\n",
            "Loss training: 0.090576634\n",
            "Loss training: 0.08958946\n",
            "Loss training: 0.08654387\n",
            "\n",
            "***************************\n",
            "Trained on 2250 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.09321855\n",
            "Loss training: 0.08901746\n",
            "Loss training: 0.08800112\n",
            "Loss training: 0.09027452\n",
            "Loss training: 0.084786825\n",
            "Loss training: 0.08817445\n",
            "Loss training: 0.08501181\n",
            "Loss training: 0.08622314\n",
            "Loss training: 0.08614877\n",
            "Loss training: 0.0852026\n",
            "\n",
            "***************************\n",
            "Trained on 2260 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.086445555\n",
            "Loss training: 0.08453848\n",
            "Loss training: 0.08552351\n",
            "Loss training: 0.0847187\n",
            "Loss training: 0.08481138\n",
            "Loss training: 0.08481481\n",
            "Loss training: 0.08444697\n",
            "Loss training: 0.084805556\n",
            "Loss training: 0.084223405\n",
            "Loss training: 0.084744096\n",
            "\n",
            "***************************\n",
            "Trained on 2270 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.084208965\n",
            "Loss training: 0.084415585\n",
            "Loss training: 0.084359884\n",
            "Loss training: 0.08425774\n",
            "Loss training: 0.084586255\n",
            "Loss training: 0.08402945\n",
            "Loss training: 0.08426738\n",
            "Loss training: 0.083949134\n",
            "Loss training: 0.08428667\n",
            "Loss training: 0.08394332\n",
            "\n",
            "***************************\n",
            "Trained on 2280 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08394578\n",
            "Loss training: 0.08385799\n",
            "Loss training: 0.08378578\n",
            "Loss training: 0.083736844\n",
            "Loss training: 0.08380332\n",
            "Loss training: 0.08368116\n",
            "Loss training: 0.083779894\n",
            "Loss training: 0.0836391\n",
            "Loss training: 0.08374497\n",
            "Loss training: 0.08360244\n",
            "\n",
            "***************************\n",
            "Trained on 2290 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08368466\n",
            "Loss training: 0.08368733\n",
            "Loss training: 0.08364046\n",
            "Loss training: 0.08370282\n",
            "Loss training: 0.08353246\n",
            "Loss training: 0.08364595\n",
            "Loss training: 0.08342963\n",
            "Loss training: 0.083555825\n",
            "Loss training: 0.083383285\n",
            "Loss training: 0.08341203\n",
            "\n",
            "***************************\n",
            "Trained on 2300 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.0833927\n",
            "Loss training: 0.083332084\n",
            "Loss training: 0.08328411\n",
            "Loss training: 0.08328604\n",
            "Loss training: 0.08325504\n",
            "Loss training: 0.083238475\n",
            "Loss training: 0.083181135\n",
            "Loss training: 0.08312668\n",
            "Loss training: 0.08311715\n",
            "Loss training: 0.08309443\n",
            "\n",
            "***************************\n",
            "Trained on 2310 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08306974\n",
            "Loss training: 0.08303903\n",
            "Loss training: 0.08303298\n",
            "Loss training: 0.083008\n",
            "Loss training: 0.08298748\n",
            "Loss training: 0.082972646\n",
            "Loss training: 0.08296192\n",
            "Loss training: 0.08294679\n",
            "Loss training: 0.08291776\n",
            "Loss training: 0.08309371\n",
            "\n",
            "***************************\n",
            "Trained on 2320 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08294235\n",
            "Loss training: 0.08289196\n",
            "Loss training: 0.083028875\n",
            "Loss training: 0.0828051\n",
            "Loss training: 0.08298311\n",
            "Loss training: 0.08314165\n",
            "Loss training: 0.08275026\n",
            "Loss training: 0.08326796\n",
            "Loss training: 0.08332291\n",
            "Loss training: 0.08276638\n",
            "\n",
            "***************************\n",
            "Trained on 2330 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08307457\n",
            "Loss training: 0.083037436\n",
            "Loss training: 0.08274894\n",
            "Loss training: 0.08275257\n",
            "Loss training: 0.08276157\n",
            "Loss training: 0.08266783\n",
            "Loss training: 0.08266413\n",
            "Loss training: 0.082667165\n",
            "Loss training: 0.08265398\n",
            "Loss training: 0.08278679\n",
            "\n",
            "***************************\n",
            "Trained on 2340 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08262844\n",
            "Loss training: 0.08256942\n",
            "Loss training: 0.082994536\n",
            "Loss training: 0.082738765\n",
            "Loss training: 0.08244164\n",
            "Loss training: 0.08243974\n",
            "Loss training: 0.082508\n",
            "Loss training: 0.08244152\n",
            "Loss training: 0.0823696\n",
            "Loss training: 0.0823245\n",
            "\n",
            "***************************\n",
            "Trained on 2350 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.082285285\n",
            "Loss training: 0.0822676\n",
            "Loss training: 0.08226642\n",
            "Loss training: 0.08233064\n",
            "Loss training: 0.08240353\n",
            "Loss training: 0.082298525\n",
            "Loss training: 0.08218155\n",
            "Loss training: 0.082203634\n",
            "Loss training: 0.08219447\n",
            "Loss training: 0.082156636\n",
            "\n",
            "***************************\n",
            "Trained on 2360 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08218531\n",
            "Loss training: 0.08215451\n",
            "Loss training: 0.082100205\n",
            "Loss training: 0.08208168\n",
            "Loss training: 0.082182296\n",
            "Loss training: 0.08234048\n",
            "Loss training: 0.0820137\n",
            "Loss training: 0.08240497\n",
            "Loss training: 0.08200424\n",
            "Loss training: 0.08225568\n",
            "\n",
            "***************************\n",
            "Trained on 2370 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08233662\n",
            "Loss training: 0.082036905\n",
            "Loss training: 0.082259335\n",
            "Loss training: 0.08195184\n",
            "Loss training: 0.082264215\n",
            "Loss training: 0.082363345\n",
            "Loss training: 0.08192656\n",
            "Loss training: 0.08232783\n",
            "Loss training: 0.081970826\n",
            "Loss training: 0.08206806\n",
            "\n",
            "***************************\n",
            "Trained on 2380 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08215431\n",
            "Loss training: 0.082404695\n",
            "Loss training: 0.081954926\n",
            "Loss training: 0.08215974\n",
            "Loss training: 0.08181307\n",
            "Loss training: 0.08197464\n",
            "Loss training: 0.08204063\n",
            "Loss training: 0.0817253\n",
            "Loss training: 0.08201553\n",
            "Loss training: 0.08175778\n",
            "\n",
            "***************************\n",
            "Trained on 2390 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08167422\n",
            "Loss training: 0.08163945\n",
            "Loss training: 0.0816989\n",
            "Loss training: 0.08154191\n",
            "Loss training: 0.08168962\n",
            "Loss training: 0.08163325\n",
            "Loss training: 0.08162728\n",
            "Loss training: 0.081501305\n",
            "Loss training: 0.08160769\n",
            "Loss training: 0.08167237\n",
            "\n",
            "***************************\n",
            "Trained on 2400 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08150692\n",
            "Loss training: 0.08148946\n",
            "Loss training: 0.0814622\n",
            "Loss training: 0.08151644\n",
            "Loss training: 0.08141746\n",
            "Loss training: 0.081395686\n",
            "Loss training: 0.08139012\n",
            "Loss training: 0.081393085\n",
            "Loss training: 0.081395775\n",
            "Loss training: 0.081396915\n",
            "\n",
            "***************************\n",
            "Trained on 2410 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08147443\n",
            "Loss training: 0.081317455\n",
            "Loss training: 0.08130315\n",
            "Loss training: 0.081328996\n",
            "Loss training: 0.08127135\n",
            "Loss training: 0.081252575\n",
            "Loss training: 0.08131214\n",
            "Loss training: 0.08121517\n",
            "Loss training: 0.08123749\n",
            "Loss training: 0.08120304\n",
            "\n",
            "***************************\n",
            "Trained on 2420 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08118529\n",
            "Loss training: 0.081191\n",
            "Loss training: 0.08114136\n",
            "Loss training: 0.08120479\n",
            "Loss training: 0.0813696\n",
            "Loss training: 0.08152345\n",
            "Loss training: 0.08122174\n",
            "Loss training: 0.08113576\n",
            "Loss training: 0.081461795\n",
            "Loss training: 0.08119367\n",
            "\n",
            "***************************\n",
            "Trained on 2430 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.081152745\n",
            "Loss training: 0.081523284\n",
            "Loss training: 0.08147916\n",
            "Loss training: 0.08104187\n",
            "Loss training: 0.08114452\n",
            "Loss training: 0.08125297\n",
            "Loss training: 0.080972895\n",
            "Loss training: 0.08109056\n",
            "Loss training: 0.081013195\n",
            "Loss training: 0.081050396\n",
            "\n",
            "***************************\n",
            "Trained on 2440 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08114577\n",
            "Loss training: 0.08103488\n",
            "Loss training: 0.08095778\n",
            "Loss training: 0.08093355\n",
            "Loss training: 0.08097387\n",
            "Loss training: 0.08085239\n",
            "Loss training: 0.080953084\n",
            "Loss training: 0.08088307\n",
            "Loss training: 0.080945015\n",
            "Loss training: 0.08085737\n",
            "\n",
            "***************************\n",
            "Trained on 2450 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.080834426\n",
            "Loss training: 0.08093321\n",
            "Loss training: 0.08079316\n",
            "Loss training: 0.08091475\n",
            "Loss training: 0.08084483\n",
            "Loss training: 0.08083201\n",
            "Loss training: 0.08072892\n",
            "Loss training: 0.08073787\n",
            "Loss training: 0.080701545\n",
            "Loss training: 0.080717154\n",
            "\n",
            "***************************\n",
            "Trained on 2460 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.080692455\n",
            "Loss training: 0.080719486\n",
            "Loss training: 0.08066223\n",
            "Loss training: 0.080728635\n",
            "Loss training: 0.08076096\n",
            "Loss training: 0.080713496\n",
            "Loss training: 0.08081157\n",
            "Loss training: 0.0806125\n",
            "Loss training: 0.08082219\n",
            "Loss training: 0.08069583\n",
            "\n",
            "***************************\n",
            "Trained on 2470 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.080840744\n",
            "Loss training: 0.080654286\n",
            "Loss training: 0.08074027\n",
            "Loss training: 0.080604956\n",
            "Loss training: 0.080700964\n",
            "Loss training: 0.08071059\n",
            "Loss training: 0.080525704\n",
            "Loss training: 0.08070649\n",
            "Loss training: 0.080630794\n",
            "Loss training: 0.0806036\n",
            "\n",
            "***************************\n",
            "Trained on 2480 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08066369\n",
            "Loss training: 0.080862045\n",
            "Loss training: 0.08057599\n",
            "Loss training: 0.0804379\n",
            "Loss training: 0.080488645\n",
            "Loss training: 0.08040265\n",
            "Loss training: 0.080517985\n",
            "Loss training: 0.08051034\n",
            "Loss training: 0.08040324\n",
            "Loss training: 0.08053201\n",
            "\n",
            "***************************\n",
            "Trained on 2490 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08060693\n",
            "Loss training: 0.08038446\n",
            "Loss training: 0.08043467\n",
            "Loss training: 0.08054786\n",
            "Loss training: 0.08039625\n",
            "Loss training: 0.08033868\n",
            "Loss training: 0.080348805\n",
            "Loss training: 0.080293715\n",
            "Loss training: 0.08029991\n",
            "Loss training: 0.080296494\n",
            "\n",
            "***************************\n",
            "Trained on 2500 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08048715\n",
            "Loss training: 0.08039987\n",
            "Loss training: 0.08026119\n",
            "Loss training: 0.080275476\n",
            "Loss training: 0.08034788\n",
            "Loss training: 0.08024037\n",
            "Loss training: 0.080287635\n",
            "Loss training: 0.080240354\n",
            "Loss training: 0.08032565\n",
            "Loss training: 0.08033744\n",
            "\n",
            "***************************\n",
            "Trained on 2510 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.080223665\n",
            "Loss training: 0.080329135\n",
            "Loss training: 0.08011851\n",
            "Loss training: 0.08024759\n",
            "Loss training: 0.08021104\n",
            "Loss training: 0.08010528\n",
            "Loss training: 0.08022955\n",
            "Loss training: 0.08012225\n",
            "Loss training: 0.08009334\n",
            "Loss training: 0.08030343\n",
            "\n",
            "***************************\n",
            "Trained on 2520 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08008534\n",
            "Loss training: 0.08012491\n",
            "Loss training: 0.08038979\n",
            "Loss training: 0.08020337\n",
            "Loss training: 0.08033976\n",
            "Loss training: 0.08057123\n",
            "Loss training: 0.0807289\n",
            "Loss training: 0.08065442\n",
            "Loss training: 0.080156244\n",
            "Loss training: 0.08160815\n",
            "\n",
            "***************************\n",
            "Trained on 2530 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08077469\n",
            "Loss training: 0.08057408\n",
            "Loss training: 0.08235645\n",
            "Loss training: 0.08026825\n",
            "Loss training: 0.08094606\n",
            "Loss training: 0.08095249\n",
            "Loss training: 0.080046594\n",
            "Loss training: 0.0808088\n",
            "Loss training: 0.080123775\n",
            "Loss training: 0.080942474\n",
            "\n",
            "***************************\n",
            "Trained on 2540 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.080847934\n",
            "Loss training: 0.08083196\n",
            "Loss training: 0.08076154\n",
            "Loss training: 0.08005268\n",
            "Loss training: 0.08120522\n",
            "Loss training: 0.080490276\n",
            "Loss training: 0.08020049\n",
            "Loss training: 0.08062902\n",
            "Loss training: 0.08094949\n",
            "Loss training: 0.08008427\n",
            "\n",
            "***************************\n",
            "Trained on 2550 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.079967715\n",
            "Loss training: 0.080414966\n",
            "Loss training: 0.08016407\n",
            "Loss training: 0.07981382\n",
            "Loss training: 0.07987752\n",
            "Loss training: 0.08020835\n",
            "Loss training: 0.07999395\n",
            "Loss training: 0.079774894\n",
            "Loss training: 0.08032945\n",
            "Loss training: 0.08003462\n",
            "\n",
            "***************************\n",
            "Trained on 2560 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.07973778\n",
            "Loss training: 0.08042575\n",
            "Loss training: 0.07994957\n",
            "Loss training: 0.079814725\n",
            "Loss training: 0.0806061\n",
            "Loss training: 0.080091044\n",
            "Loss training: 0.07966935\n",
            "Loss training: 0.08035737\n",
            "Loss training: 0.08030812\n",
            "Loss training: 0.07965413\n",
            "\n",
            "***************************\n",
            "Trained on 2570 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08011734\n",
            "Loss training: 0.08031686\n",
            "Loss training: 0.07974474\n",
            "Loss training: 0.079835564\n",
            "Loss training: 0.08035635\n",
            "Loss training: 0.080044314\n",
            "Loss training: 0.07954669\n",
            "Loss training: 0.0800899\n",
            "Loss training: 0.0799619\n",
            "Loss training: 0.079535715\n",
            "\n",
            "***************************\n",
            "Trained on 2580 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08007446\n",
            "Loss training: 0.08020843\n",
            "Loss training: 0.07960466\n",
            "Loss training: 0.079683326\n",
            "Loss training: 0.08018712\n",
            "Loss training: 0.07969737\n",
            "Loss training: 0.07957525\n",
            "Loss training: 0.079822294\n",
            "Loss training: 0.079519235\n",
            "Loss training: 0.07964651\n",
            "\n",
            "***************************\n",
            "Trained on 2590 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08007711\n",
            "Loss training: 0.079492666\n",
            "Loss training: 0.07966011\n",
            "Loss training: 0.08006931\n",
            "Loss training: 0.07944677\n",
            "Loss training: 0.07974627\n",
            "Loss training: 0.07999506\n",
            "Loss training: 0.07951407\n",
            "Loss training: 0.079516664\n",
            "Loss training: 0.079861164\n",
            "\n",
            "***************************\n",
            "Trained on 2600 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.079514645\n",
            "Loss training: 0.07936056\n",
            "Loss training: 0.07958494\n",
            "Loss training: 0.07970221\n",
            "Loss training: 0.07939211\n",
            "Loss training: 0.07941918\n",
            "Loss training: 0.07962438\n",
            "Loss training: 0.07933442\n",
            "Loss training: 0.07938593\n",
            "Loss training: 0.07944167\n",
            "\n",
            "***************************\n",
            "Trained on 2610 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.079431936\n",
            "Loss training: 0.07972133\n",
            "Loss training: 0.0794486\n",
            "Loss training: 0.07927226\n",
            "Loss training: 0.07947085\n",
            "Loss training: 0.07949984\n",
            "Loss training: 0.07940043\n",
            "Loss training: 0.079244785\n",
            "Loss training: 0.07925963\n",
            "Loss training: 0.079574145\n",
            "\n",
            "***************************\n",
            "Trained on 2620 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.07965481\n",
            "Loss training: 0.07948736\n",
            "Loss training: 0.079205394\n",
            "Loss training: 0.07940892\n",
            "Loss training: 0.07989526\n",
            "Loss training: 0.07963911\n",
            "Loss training: 0.07929546\n",
            "Loss training: 0.08002183\n",
            "Loss training: 0.080183715\n",
            "Loss training: 0.07931799\n",
            "\n",
            "***************************\n",
            "Trained on 2630 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.07917263\n",
            "Loss training: 0.079341754\n",
            "Loss training: 0.07916268\n",
            "Loss training: 0.07909881\n",
            "Loss training: 0.0792001\n",
            "Loss training: 0.07925955\n",
            "Loss training: 0.079206765\n",
            "Loss training: 0.079087146\n",
            "Loss training: 0.07905125\n",
            "Loss training: 0.079204336\n",
            "\n",
            "***************************\n",
            "Trained on 2640 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.07930719\n",
            "Loss training: 0.07914823\n",
            "Loss training: 0.07913934\n",
            "Loss training: 0.079431355\n",
            "Loss training: 0.07937429\n",
            "Loss training: 0.079004064\n",
            "Loss training: 0.07922821\n",
            "Loss training: 0.0792\n",
            "Loss training: 0.07898714\n",
            "Loss training: 0.079200074\n",
            "\n",
            "***************************\n",
            "Trained on 2650 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.07917547\n",
            "Loss training: 0.07902829\n",
            "Loss training: 0.07919204\n",
            "Loss training: 0.07925955\n",
            "Loss training: 0.0790085\n",
            "Loss training: 0.07898971\n",
            "Loss training: 0.07913736\n",
            "Loss training: 0.078949615\n",
            "Loss training: 0.07897511\n",
            "Loss training: 0.07892119\n",
            "\n",
            "***************************\n",
            "Trained on 2660 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.07893088\n",
            "Loss training: 0.07891157\n",
            "Loss training: 0.07901954\n",
            "Loss training: 0.078890234\n",
            "Loss training: 0.07901641\n",
            "Loss training: 0.078904286\n",
            "Loss training: 0.07891599\n",
            "Loss training: 0.078885704\n",
            "Loss training: 0.07896629\n",
            "Loss training: 0.078971095\n",
            "\n",
            "***************************\n",
            "Trained on 2670 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.078853175\n",
            "Loss training: 0.079034746\n",
            "Loss training: 0.0788235\n",
            "Loss training: 0.07894857\n",
            "Loss training: 0.078837335\n",
            "Loss training: 0.07888603\n",
            "Loss training: 0.07891726\n",
            "Loss training: 0.07878012\n",
            "Loss training: 0.07890476\n",
            "Loss training: 0.07877586\n",
            "\n",
            "***************************\n",
            "Trained on 2680 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.078856535\n",
            "Loss training: 0.078828126\n",
            "Loss training: 0.07880564\n",
            "Loss training: 0.07886857\n",
            "Loss training: 0.07873421\n",
            "Loss training: 0.07896291\n",
            "Loss training: 0.0789219\n",
            "Loss training: 0.079062335\n",
            "Loss training: 0.078857265\n",
            "Loss training: 0.07886149\n",
            "\n",
            "***************************\n",
            "Trained on 2690 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.07918252\n",
            "Loss training: 0.07877184\n",
            "Loss training: 0.078812994\n",
            "Loss training: 0.07883729\n",
            "Loss training: 0.07870354\n",
            "Loss training: 0.078855984\n",
            "Loss training: 0.078713685\n",
            "Loss training: 0.07904438\n",
            "Loss training: 0.07876899\n",
            "Loss training: 0.07874962\n",
            "\n",
            "***************************\n",
            "Trained on 2700 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.078992516\n",
            "Loss training: 0.07870968\n",
            "Loss training: 0.0788471\n",
            "Loss training: 0.07872348\n",
            "Loss training: 0.07861288\n",
            "Loss training: 0.07878629\n",
            "Loss training: 0.079075135\n",
            "Loss training: 0.07901961\n",
            "Loss training: 0.07860848\n",
            "Loss training: 0.078766696\n",
            "\n",
            "***************************\n",
            "Trained on 2710 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.079019226\n",
            "Loss training: 0.07870533\n",
            "Loss training: 0.078599565\n",
            "Loss training: 0.07863282\n",
            "Loss training: 0.07875163\n",
            "Loss training: 0.078940995\n",
            "Loss training: 0.07860036\n",
            "Loss training: 0.078573935\n",
            "Loss training: 0.07864279\n",
            "Loss training: 0.07858124\n",
            "\n",
            "***************************\n",
            "Trained on 2720 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.0790454\n",
            "Loss training: 0.0808589\n",
            "Loss training: 0.08126225\n",
            "Loss training: 0.082693756\n",
            "Loss training: 0.08087017\n",
            "Loss training: 0.079019666\n",
            "Loss training: 0.08169072\n",
            "Loss training: 0.08116696\n",
            "Loss training: 0.07924315\n",
            "Loss training: 0.084074505\n",
            "\n",
            "***************************\n",
            "Trained on 2730 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.085446574\n",
            "Loss training: 0.08093428\n",
            "Loss training: 0.091092765\n",
            "Loss training: 0.08500074\n",
            "Loss training: 0.084928036\n",
            "Loss training: 0.08537128\n",
            "Loss training: 0.07942263\n",
            "Loss training: 0.08447083\n",
            "Loss training: 0.07945313\n",
            "Loss training: 0.08374287\n",
            "\n",
            "***************************\n",
            "Trained on 2740 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08046377\n",
            "Loss training: 0.08165698\n",
            "Loss training: 0.07973499\n",
            "Loss training: 0.081343636\n",
            "Loss training: 0.08047505\n",
            "Loss training: 0.080843054\n",
            "Loss training: 0.08017692\n",
            "Loss training: 0.07933373\n",
            "Loss training: 0.07946497\n",
            "Loss training: 0.07954904\n",
            "\n",
            "***************************\n",
            "Trained on 2750 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.07948362\n",
            "Loss training: 0.07966207\n",
            "Loss training: 0.07939418\n",
            "Loss training: 0.07898436\n",
            "Loss training: 0.07952995\n",
            "Loss training: 0.07880205\n",
            "Loss training: 0.07934762\n",
            "Loss training: 0.07856692\n",
            "Loss training: 0.07895608\n",
            "Loss training: 0.07856613\n",
            "\n",
            "***************************\n",
            "Trained on 2760 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.07927476\n",
            "Loss training: 0.078654595\n",
            "Loss training: 0.07888571\n",
            "Loss training: 0.0787609\n",
            "Loss training: 0.078680605\n",
            "Loss training: 0.07874542\n",
            "Loss training: 0.07849646\n",
            "Loss training: 0.07871859\n",
            "Loss training: 0.07856818\n",
            "Loss training: 0.078539625\n",
            "\n",
            "***************************\n",
            "Trained on 2770 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.07866093\n",
            "Loss training: 0.07842433\n",
            "Loss training: 0.078679755\n",
            "Loss training: 0.07841725\n",
            "Loss training: 0.078421816\n",
            "Loss training: 0.07837746\n",
            "Loss training: 0.0784152\n",
            "Loss training: 0.07844149\n",
            "Loss training: 0.078319296\n",
            "Loss training: 0.07836286\n",
            "\n",
            "***************************\n",
            "Trained on 2780 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.07831214\n",
            "Loss training: 0.07831304\n",
            "Loss training: 0.07830717\n",
            "Loss training: 0.07830239\n",
            "Loss training: 0.07827875\n",
            "Loss training: 0.078267954\n",
            "Loss training: 0.078247555\n",
            "Loss training: 0.078259975\n",
            "Loss training: 0.078238465\n",
            "Loss training: 0.07825762\n",
            "\n",
            "***************************\n",
            "Trained on 2790 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.07825056\n",
            "Loss training: 0.078226656\n",
            "Loss training: 0.078237064\n",
            "Loss training: 0.07819266\n",
            "Loss training: 0.078190096\n",
            "Loss training: 0.07820466\n",
            "Loss training: 0.07824562\n",
            "Loss training: 0.07821556\n",
            "Loss training: 0.07817613\n",
            "Loss training: 0.07816318\n",
            "\n",
            "***************************\n",
            "Trained on 2800 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.078150116\n",
            "Loss training: 0.07821108\n",
            "Loss training: 0.07818443\n",
            "Loss training: 0.07817171\n",
            "Loss training: 0.0782179\n",
            "Loss training: 0.07814497\n",
            "Loss training: 0.078211345\n",
            "Loss training: 0.078123555\n",
            "Loss training: 0.078277364\n",
            "Loss training: 0.07818982\n",
            "\n",
            "***************************\n",
            "Trained on 2810 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.07815938\n",
            "Loss training: 0.07816715\n",
            "Loss training: 0.07810025\n",
            "Loss training: 0.078113034\n",
            "Loss training: 0.07816534\n",
            "Loss training: 0.078098714\n",
            "Loss training: 0.07807155\n",
            "Loss training: 0.07810191\n",
            "Loss training: 0.0780484\n",
            "Loss training: 0.07821908\n",
            "\n",
            "***************************\n",
            "Trained on 2820 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.07821554\n",
            "Loss training: 0.07805854\n",
            "Loss training: 0.07845645\n",
            "Loss training: 0.078170724\n",
            "Loss training: 0.078187995\n",
            "Loss training: 0.07823218\n",
            "Loss training: 0.07800737\n",
            "Loss training: 0.07811941\n",
            "Loss training: 0.07813566\n",
            "Loss training: 0.07802031\n",
            "\n",
            "***************************\n",
            "Trained on 2830 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.07797765\n",
            "Loss training: 0.07799612\n",
            "Loss training: 0.07798243\n",
            "Loss training: 0.078006245\n",
            "Loss training: 0.07797852\n",
            "Loss training: 0.0779766\n",
            "Loss training: 0.07802203\n",
            "Loss training: 0.07794453\n",
            "Loss training: 0.07804987\n",
            "Loss training: 0.07801892\n",
            "\n",
            "***************************\n",
            "Trained on 2840 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.0779791\n",
            "Loss training: 0.077941544\n",
            "Loss training: 0.07807123\n",
            "Loss training: 0.0781822\n",
            "Loss training: 0.07794013\n",
            "Loss training: 0.07799442\n",
            "Loss training: 0.07812963\n",
            "Loss training: 0.07811005\n",
            "Loss training: 0.078015804\n",
            "Loss training: 0.07796072\n",
            "\n",
            "***************************\n",
            "Trained on 2850 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.078077815\n",
            "Loss training: 0.07792061\n",
            "Loss training: 0.07793926\n",
            "Loss training: 0.07801594\n",
            "Loss training: 0.077881075\n",
            "Loss training: 0.07806571\n",
            "Loss training: 0.07796824\n",
            "Loss training: 0.077920906\n",
            "Loss training: 0.077914305\n",
            "Loss training: 0.077892914\n",
            "\n",
            "***************************\n",
            "Trained on 2860 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.078057684\n",
            "Loss training: 0.07799403\n",
            "Loss training: 0.07823393\n",
            "Loss training: 0.077999726\n",
            "Loss training: 0.07791525\n",
            "Loss training: 0.078108676\n",
            "Loss training: 0.07784148\n",
            "Loss training: 0.07793255\n",
            "Loss training: 0.07795802\n",
            "Loss training: 0.07783387\n",
            "\n",
            "***************************\n",
            "Trained on 2870 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.07794432\n",
            "Loss training: 0.0779502\n",
            "Loss training: 0.07778888\n",
            "Loss training: 0.07786862\n",
            "Loss training: 0.07786507\n",
            "Loss training: 0.077773176\n",
            "Loss training: 0.07789408\n",
            "Loss training: 0.077857725\n",
            "Loss training: 0.07780188\n",
            "Loss training: 0.07784843\n",
            "\n",
            "***************************\n",
            "Trained on 2880 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.077941865\n",
            "Loss training: 0.078008294\n",
            "Loss training: 0.07782829\n",
            "Loss training: 0.0778547\n",
            "Loss training: 0.07790127\n",
            "Loss training: 0.077725045\n",
            "Loss training: 0.07785938\n",
            "Loss training: 0.07785556\n",
            "Loss training: 0.07773123\n",
            "Loss training: 0.07781581\n",
            "\n",
            "***************************\n",
            "Trained on 2890 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.077787116\n",
            "Loss training: 0.07772118\n",
            "Loss training: 0.077740245\n",
            "Loss training: 0.077801555\n",
            "Loss training: 0.07770319\n",
            "Loss training: 0.07792286\n",
            "Loss training: 0.07805017\n",
            "Loss training: 0.077697985\n",
            "Loss training: 0.07817788\n",
            "Loss training: 0.07819449\n",
            "\n",
            "***************************\n",
            "Trained on 2900 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.0777567\n",
            "Loss training: 0.07824208\n",
            "Loss training: 0.078552864\n",
            "Loss training: 0.07771943\n",
            "Loss training: 0.07810178\n",
            "Loss training: 0.078273796\n",
            "Loss training: 0.07768983\n",
            "Loss training: 0.07844096\n",
            "Loss training: 0.07863487\n",
            "Loss training: 0.07787695\n",
            "\n",
            "***************************\n",
            "Trained on 2910 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.07897198\n",
            "Loss training: 0.079678595\n",
            "Loss training: 0.077838734\n",
            "Loss training: 0.07960436\n",
            "Loss training: 0.08069128\n",
            "Loss training: 0.07797213\n",
            "Loss training: 0.07875749\n",
            "Loss training: 0.08001924\n",
            "Loss training: 0.077742286\n",
            "Loss training: 0.08041373\n",
            "\n",
            "***************************\n",
            "Trained on 2920 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.08013691\n",
            "Loss training: 0.07803172\n",
            "Loss training: 0.081127815\n",
            "Loss training: 0.07887621\n",
            "Loss training: 0.07879151\n",
            "Loss training: 0.08027844\n",
            "Loss training: 0.07791267\n",
            "Loss training: 0.07917969\n",
            "Loss training: 0.07827811\n",
            "Loss training: 0.07816243\n",
            "\n",
            "***************************\n",
            "Trained on 2930 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.07900792\n",
            "Loss training: 0.077670075\n",
            "Loss training: 0.07944951\n",
            "Loss training: 0.07829321\n",
            "Loss training: 0.0784727\n",
            "Loss training: 0.079022564\n",
            "Loss training: 0.07759293\n",
            "Loss training: 0.07839805\n",
            "Loss training: 0.077810824\n",
            "Loss training: 0.07809881\n",
            "\n",
            "***************************\n",
            "Trained on 2940 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.07834293\n",
            "Loss training: 0.077571444\n",
            "Loss training: 0.07838188\n",
            "Loss training: 0.07822025\n",
            "Loss training: 0.077869944\n",
            "Loss training: 0.07852776\n",
            "Loss training: 0.07809383\n",
            "Loss training: 0.07793735\n",
            "Loss training: 0.07807569\n",
            "Loss training: 0.077649966\n",
            "\n",
            "***************************\n",
            "Trained on 2950 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 0.077671535\n",
            "Loss training: 0.077565186\n",
            "Loss training: 0.07769505\n",
            "Loss training: 0.07762132\n",
            "Loss training: 0.077736676\n",
            "Loss training: 0.07805249\n",
            "Loss training: 0.07760681\n",
            "Loss training: 0.077615045\n",
            "Loss training: 0.07754303\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-00da44c0f790>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessed_graphs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'partition_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'graph'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessed_graphs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'partition_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m       \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessed_graphs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'partition_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m       )\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-061713ddbe24>\u001b[0m in \u001b[0;36moverfit_on_single_graph\u001b[0;34m(num_training_steps, learning_rate, graph, labels, mask)\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m       \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m       ) \n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m       \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltering_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36mcache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1977\u001b[0m         donate_tuple=donate_tuple)\n\u001b[1;32m   1978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1979\u001b[0;31m     \u001b[0mout_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_pmapped_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1980\u001b[0m     \u001b[0mout_pytree_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1981\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_pytree_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36mpmap_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1859\u001b[0m         \u001b[0min_axes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_axes_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_axes_thunk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_axes_thunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_fun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdonated_invars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdonated_invars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m         global_arg_shapes=p.global_arg_shapes_flat)\n\u001b[0m\u001b[1;32m   1862\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, fun, *args, **params)\u001b[0m\n\u001b[1;32m   1795\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1796\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'in_axes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1797\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mmap_bind\u001b[0;34m(primitive, fun, out_axes_thunk, *args, **params)\u001b[0m\n\u001b[1;32m   1826\u001b[0m       fun, primitive, top_trace and top_trace.level, tuple(params.items()))\n\u001b[1;32m   1827\u001b[0m   \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1828\u001b[0;31m   \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1829\u001b[0m   \u001b[0menv_trace_todo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtodo_and_xforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1830\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_todos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_trace_todo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, trace, fun, tracers, params)\u001b[0m\n\u001b[1;32m   1798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1800\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpost_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_call\u001b[0;34m(self, primitive, f, tracers, params)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m   \u001b[0mprocess_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/pxla.py\u001b[0m in \u001b[0;36mxla_pmap_impl\u001b[0;34m(fun, backend, axis_name, axis_size, global_axis_size, devices, name, in_axes, out_axes_thunk, donated_invars, global_arg_shapes, *args)\u001b[0m\n\u001b[1;32m    802\u001b[0m                           \u001b[0;34m(\u001b[0m\u001b[0;34m\"abstract args\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractify\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m                           (\"fingerprint\", fingerprint))\n\u001b[0;32m--> 804\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/profiler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTraceAnnotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecorator_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/pxla.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1563\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m     \u001b[0minput_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m     \u001b[0mout_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxla_executable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_sharded_on_local_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_check_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mbufs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout_bufs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if run_overfit_on_demo_graph:\n",
        "  print('*** Trying to overfit on a random demo graph ***')\n",
        "  overfit_on_demo_graph(\n",
        "      num_training_steps = 1000,\n",
        "      learning_rate = 0.001\n",
        "  )"
      ],
      "metadata": {
        "id": "ILwic3w-d5mI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}