{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final-version-2-sharded-networks.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bf319/Scaling_MPNNs/blob/main/final_version_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rLhMwiHHWbtK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3831eca-2991-49b4-aa16-d4fa035fe4d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-31 23:36:17--  https://raw.githubusercontent.com/deepmind/jraph/master/jraph/experimental/sharded_graphnet.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22444 (22K) [text/plain]\n",
            "Saving to: ‘sharded_graphnet.py.2’\n",
            "\n",
            "\rsharded_graphnet.py   0%[                    ]       0  --.-KB/s               \rsharded_graphnet.py 100%[===================>]  21.92K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2022-03-31 23:36:17 (13.3 MB/s) - ‘sharded_graphnet.py.2’ saved [22444/22444]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%pip install -q git+https://github.com/deepmind/dm-haiku\n",
        "%pip install -q jraph\n",
        "%pip install -q git+https://github.com/deepmind/jaxline\n",
        "%pip install -q ogb\n",
        "%pip install -q dgl\n",
        "%pip install -q optax\n",
        "%pip install -q metis\n",
        "\n",
        "!wget https://raw.githubusercontent.com/deepmind/jraph/master/jraph/experimental/sharded_graphnet.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.tools.colab_tpu\n",
        "jax.tools.colab_tpu.setup_tpu()\n",
        "\n",
        "num_devices = jax.local_device_count()\n",
        "jax.devices()"
      ],
      "metadata": {
        "id": "iT2wqf76kIRp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58a2b4f7-7650-41ad-9de3-38e4405bdbf7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ogb.nodeproppred import Evaluator\n",
        "from ogb.nodeproppred import DglNodePropPredDataset\n",
        "\n",
        "dataset = DglNodePropPredDataset(name = \"ogbn-proteins\")\n",
        "split_idx = dataset.get_idx_split()\n",
        "evaluator = Evaluator(name = 'ogbn-proteins')\n",
        "print(evaluator.expected_input_format)"
      ],
      "metadata": {
        "id": "xHClucOxWpAZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc22e629-c30c-4eb6-c453-b0eaedaea2a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using backend: pytorch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:02<00:00,  2.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into DGL objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  2.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n",
            "==== Expected input format of Evaluator for ogbn-proteins\n",
            "{'y_true': y_true, 'y_pred': y_pred}\n",
            "- y_true: numpy ndarray or torch tensor of shape (num_node, num_task)\n",
            "- y_pred: numpy ndarray or torch tensor of shape (num_node, num_task)\n",
            "where y_pred stores score values (for computing ROC-AUC),\n",
            "num_task is 112, and each row corresponds to one node.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "# There is only one graph in Node Property Prediction datasets\n",
        "ogbn_proteins_main_graph, ogbn_proteins_main_labels = dataset[0]\n",
        "\n",
        "'''\n",
        "  OGBN-Proteins\n",
        "    #Nodes = 132,534\n",
        "    #Edges = 39,561,252\n",
        "    #Diameter ~ 9 (https://cs.stanford.edu/people/jure/pubs/ogb-neurips20.pdf)\n",
        "    #Tasks = 112\n",
        "    #Split Type = Species\n",
        "    #Task Type = Binary classification\n",
        "    #Metric = ROC-AUC\n",
        "\n",
        "    Task:\n",
        "      The task is to predict the presence of protein functions in a multi-label binary classification setup,\n",
        "      where there are 112 kinds of labels to predict in total. \n",
        "      The performance is measured by the average of ROC-AUC scores across the 112 tasks.\n",
        "\n",
        "    #Others:\n",
        "      **undirected**\n",
        "      **weighted**\n",
        "      **typed (according to species)**\n",
        "\n",
        "  (1) Nodes represent proteins\n",
        "    (1.1) The proteins come from 8 species\n",
        "      len(set(graph.ndata['species'].reshape(-1).tolist())) == 8\n",
        "    (1.2) Each node has one feature associated with it (its species)\n",
        "      graph.ndata['species'].shape == (#nodes, 1)\n",
        "  \n",
        "  (2) Edges indicate different types of biologically meaningful associations between proteins\n",
        "    (2.1) All edges come with 8-dimensional features\n",
        "      graph.edata['feat'].shape == (2 * #edges, 8)\n",
        "\n",
        "'''\n",
        "# Get split labels\n",
        "train_label = dataset.labels[split_idx['train']]  # (86619, 112) -- binary values (presence of protein functions)\n",
        "valid_label = dataset.labels[split_idx['valid']]  # (21236, 112) -- binary values (presence of protein functions)\n",
        "test_label = dataset.labels[split_idx['test']]    # (24679, 112) -- binary values (presence of protein functions)\n",
        "\n",
        "# Create masks\n",
        "train_mask = jnp.zeros((ogbn_proteins_main_graph.num_nodes(), 1)).at[jnp.array(split_idx['train'])].set(1)\n",
        "valid_mask = jnp.zeros((ogbn_proteins_main_graph.num_nodes(), 1)).at[jnp.array(split_idx['valid'])].set(1)\n",
        "test_mask = jnp.zeros((ogbn_proteins_main_graph.num_nodes(), 1)).at[jnp.array(split_idx['test'])].set(1)"
      ],
      "metadata": {
        "id": "jCkzIEb4WsXU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jraph\n",
        "\n",
        "# From https://colab.research.google.com/github/deepmind/educational/blob/master/colabs/summer_schools/intro_to_graph_nets_tutorial_with_jraph.ipynb#scrollTo=7vEmAsr5bKN8\n",
        "def _nearest_multiple_of_8(x: int) -> int:\n",
        "  \"\"\"Computes the nearest power of two greater than x for padding.\"\"\"\n",
        "  if x % 8 == 0:\n",
        "    return x\n",
        "  else:\n",
        "    return (x // 8 + 1) * 8 \n",
        "\n",
        "def pad_graph_to_nearest_multiple_of_8(\n",
        "    graphs_tuple: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
        "  \"\"\"Pads a batched `GraphsTuple` to the nearest power of two.\n",
        "  For example, if a `GraphsTuple` has 7 nodes, 5 edges and 3 graphs, this method\n",
        "  would pad the `GraphsTuple` nodes and edges:\n",
        "    7 nodes --> 8 nodes (2^3)\n",
        "    5 edges --> 8 edges (2^3)\n",
        "  And since padding is accomplished using `jraph.pad_with_graphs`, an extra\n",
        "  graph and node is added:\n",
        "    8 nodes --> 9 nodes\n",
        "    3 graphs --> 4 graphs\n",
        "  Args:\n",
        "    graphs_tuple: a batched `GraphsTuple` (can be batch size 1).\n",
        "  Returns:\n",
        "    A graphs_tuple batched to the nearest power of two.\n",
        "  \"\"\"\n",
        "  # Add 1 since we need at least one padding node for pad_with_graphs.\n",
        "  pad_nodes_to = _nearest_multiple_of_8(jnp.sum(graphs_tuple.n_node)) + 1\n",
        "  pad_edges_to = _nearest_multiple_of_8(jnp.sum(graphs_tuple.n_edge))\n",
        "  # Add 1 since we need at least one padding graph for pad_with_graphs.\n",
        "  # We do not pad to nearest power of two because the batch size is fixed.\n",
        "  pad_graphs_to = graphs_tuple.n_node.shape[0] + 1\n",
        "  return jraph.pad_with_graphs(graphs_tuple, pad_nodes_to, pad_edges_to,\n",
        "                               pad_graphs_to)"
      ],
      "metadata": {
        "id": "FSbePOUh2NBB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "from random import randint\n",
        "import numpy as np\n",
        "import sharded_graphnet\n",
        "\n",
        "def get_demo_training_graph():\n",
        "  num_nodes = 16\n",
        "  num_edges = 8\n",
        "\n",
        "  rand_dgl_graph = dgl.rand_graph(num_nodes = num_nodes, num_edges = num_edges)\n",
        "\n",
        "  node_features = jnp.array([[randint(0, 7)] for i in range(num_nodes)])\n",
        "  edge_features = jnp.array([[0.1 * randint(0, 10) for _ in range(8)] for i in range(num_edges)])\n",
        "\n",
        "  senders = jnp.array(rand_dgl_graph.edges()[0])\n",
        "  receivers = jnp.array(rand_dgl_graph.edges()[1])\n",
        "\n",
        "  in_tuple = jraph.GraphsTuple(\n",
        "            nodes = node_features.astype(np.float32),\n",
        "            edges = edge_features.astype(np.float32),  \n",
        "            senders = senders.astype(np.int32), \n",
        "            receivers = receivers.astype(np.int32),\n",
        "            n_node = jnp.array([num_nodes]), \n",
        "            n_edge = jnp.array([num_edges]),\n",
        "            globals = None  # No global features\n",
        "          )\n",
        "  \n",
        "  labels = jnp.array([[randint(0, 1) for j in range(112)] for i in range(num_nodes)])\n",
        "  train_mask = jnp.ones((num_nodes, 1))\n",
        "  \n",
        "  in_tuple = in_tuple._replace(\n",
        "      nodes = {\n",
        "          'inputs': in_tuple.nodes, \n",
        "          'targets': labels, \n",
        "          'train_mask': train_mask, \n",
        "          }\n",
        "  )\n",
        "\n",
        "  # in_tuple = pad_graph_to_nearest_power_of_two(in_tuple)\n",
        "  \n",
        "  return sharded_graphnet.graphs_tuple_to_broadcasted_sharded_graphs_tuple(\n",
        "      in_tuple,\n",
        "      num_shards = num_devices\n",
        "      )\n",
        "  \n",
        "demo_graph = get_demo_training_graph()\n",
        "demo_labels = demo_graph.nodes['targets']\n",
        "demo_mask = demo_graph.nodes['train_mask']\n",
        "demo_graph = demo_graph._replace(nodes = demo_graph.nodes['inputs']) "
      ],
      "metadata": {
        "id": "SKoX4h1z9ItW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import jraph\n",
        "import sharded_graphnet\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "enc = OneHotEncoder()\n",
        "enc.fit(ogbn_proteins_main_graph.ndata['species'])\n",
        "\n",
        "def dgl_graph_to_jraph(node_ids, labels, train_mask, valid_mask, test_mask):\n",
        "  # First add back the node and edge features\n",
        "  dgl_graph_with_features = dgl.node_subgraph(ogbn_proteins_main_graph, node_ids)\n",
        "\n",
        "  node_features = jnp.array(enc.transform(dgl_graph_with_features.ndata['species']).toarray())\n",
        "  senders = jnp.array(dgl_graph_with_features.edges()[0])\n",
        "  receivers = jnp.array(dgl_graph_with_features.edges()[1])\n",
        "\n",
        "  # Edges -- here we should include the 8-dimensional edge features\n",
        "  edges = jnp.array(dgl_graph_with_features.edata['feat'])\n",
        "  # edges = jnp.concatenate([jnp.sin(jnp.pi * edges), jnp.cos(jnp.pi * edges)], axis=-1)\n",
        "  \n",
        "  n_node = jnp.array([dgl_graph_with_features.num_nodes()])\n",
        "  n_edge = jnp.array([dgl_graph_with_features.num_edges()])\n",
        "\n",
        "  in_tuple = jraph.GraphsTuple(\n",
        "            nodes = node_features.astype(np.float32),\n",
        "            senders = senders.astype(np.int32), \n",
        "            receivers = receivers.astype(np.int32),\n",
        "            edges = edges.astype(np.float32),  \n",
        "            n_node = n_node, \n",
        "            n_edge = n_edge,\n",
        "            globals = None  # No global features\n",
        "          )\n",
        "  \n",
        "  in_tuple = in_tuple._replace(\n",
        "      nodes = {\n",
        "          'inputs': in_tuple.nodes, \n",
        "          'targets': labels, \n",
        "          'train_mask': train_mask, \n",
        "          'valid_mask': valid_mask, \n",
        "          'test_mask': test_mask,\n",
        "          'padding_mask': jnp.ones((in_tuple.nodes.shape[0], 1)) \n",
        "                                                        # TODO: Check this above\n",
        "                                                        # Adding this mask so that we can remove the nodes added after padding \n",
        "                                                        # for the final ROC computations on the full train / valid / test splits\n",
        "                                                        # This is because I want to pass the predictions on the true nodes to the \n",
        "                                                        # ogbn-evaluator, so I would first need to remove the predictions that come from padding.\n",
        "          }\n",
        "  )\n",
        "\n",
        "  in_tuple = pad_graph_to_nearest_multiple_of_8(in_tuple)\n",
        "  \n",
        "  return sharded_graphnet.graphs_tuple_to_broadcasted_sharded_graphs_tuple(\n",
        "      in_tuple,\n",
        "      num_shards = num_devices\n",
        "      )\n",
        "  \n",
        "def get_labels_for_subgraph(node_ids):\n",
        "  return jnp.array(ogbn_proteins_main_labels.index_select(0, node_ids))"
      ],
      "metadata": {
        "id": "fvH_XRJVWuLw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dgl\n",
        "\n",
        "'''\n",
        "  Generate graph partition using metis, with balanced number of edges in each partition.\n",
        "  Note: \n",
        "    The subgraphs do not contain the node/edge data in the input graph (https://docs.dgl.ai/generated/dgl.metis_partition.html)\n",
        "'''\n",
        "num_partitions = 35  ## TODO: Find some way to decrease this to something reasonable (< 50)\n",
        "\n",
        "dgl_graph_metis_partition = dgl.metis_partition(ogbn_proteins_main_graph, num_partitions, balance_edges = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUI9s4-0mPz9",
        "outputId": "a14fc599-acd4-4fb8-9600-b5626c664511"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convert a graph into a bidirected graph: 2.022 seconds\n",
            "Construct multi-constraint weights: 0.016 seconds\n",
            "Metis partitioning: 26.307 seconds\n",
            "Split the graph: 0.666 seconds\n",
            "Construct subgraphs: 0.029 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_pos_embedding():\n",
        "  node_ids = dgl_graph_metis_partition[0].ndata['_ID']\n",
        "\n",
        "  labels = get_labels_for_subgraph(node_ids)\n",
        "  graph = dgl_graph_to_jraph(node_ids, \n",
        "                             labels, \n",
        "                             train_mask = train_mask.at[jnp.array(node_ids)].get(),\n",
        "                             valid_mask = valid_mask.at[jnp.array(node_ids)].get(),\n",
        "                             test_mask = test_mask.at[jnp.array(node_ids)].get()\n",
        "                             )\n",
        "  \n",
        "test_pos_embedding()"
      ],
      "metadata": {
        "id": "3_iBhg5vialB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert graphs to Jraph GraphsTuple\n",
        "processed_graphs = {}\n",
        "\n",
        "for idx in range(num_partitions):\n",
        "  node_ids = dgl_graph_metis_partition[idx].ndata['_ID']\n",
        "\n",
        "  labels = get_labels_for_subgraph(node_ids)\n",
        "  graph = dgl_graph_to_jraph(node_ids, \n",
        "                             labels, \n",
        "                             train_mask = train_mask.at[jnp.array(node_ids)].get(),\n",
        "                             valid_mask = valid_mask.at[jnp.array(node_ids)].get(),\n",
        "                             test_mask = test_mask.at[jnp.array(node_ids)].get()\n",
        "                             )\n",
        "\n",
        "  processed_graphs[f'partition_{idx}'] = {\n",
        "      'graph': graph._replace(nodes = graph.nodes['inputs']), \n",
        "      'labels': graph.nodes['targets'],\n",
        "      'train_mask': graph.nodes['train_mask'],\n",
        "      'valid_mask': graph.nodes['valid_mask'],\n",
        "      'test_mask': graph.nodes['test_mask'],\n",
        "      'padding_mask': graph.nodes['padding_mask']\n",
        "      }"
      ],
      "metadata": {
        "id": "s8-Ln58I_Fwp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import haiku as hk\n",
        "import jax\n",
        "import optax\n",
        "\n",
        "from typing import Sequence\n",
        "\n",
        "# See https://github.com/YuxuanXie/mcl/blob/5f7ee92e2a6bc89736263873a4ba9c14d1a676ff/glassy_dynamics/train_using_jax.py for alternative to using GraphMapFeatures\n",
        "# From https://github.com/YuxuanXie/mcl/blob/5f7ee92e2a6bc89736263873a4ba9c14d1a676ff/glassy_dynamics/train_using_jax.py\n",
        "\n",
        "hidden_dimension = 128\n",
        "num_message_passing_steps = 5 # Question: (256, 4) fails / (128, 6) works\n",
        "\n",
        "@jraph.concatenated_args\n",
        "def node_update_fn(feats: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Node update function for graph net.\"\"\"\n",
        "  net = hk.Sequential([hk.nets.MLP(output_sizes = [hidden_dimension, hidden_dimension], activation = jax.nn.relu, activate_final = False), hk.LayerNorm(axis = -1, create_scale = True, create_offset = True)])\n",
        "  return net(feats)\n",
        "\n",
        "@jraph.concatenated_args\n",
        "def edge_update_fn(feats: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Edge update function for graph net.\"\"\"\n",
        "  net = hk.Sequential([hk.nets.MLP(output_sizes = [hidden_dimension, hidden_dimension], activation = jax.nn.relu, activate_final = False), hk.LayerNorm(axis = -1, create_scale = True, create_offset = True)])\n",
        "  return net(feats)\n",
        "\n",
        "@hk.without_apply_rng\n",
        "@hk.transform\n",
        "def network_definition(graph):\n",
        "  \"\"\"Defines a graph neural network.\n",
        "  Args:\n",
        "    graph: Graphstuple the network processes.\n",
        "  Returns:\n",
        "    Decoded nodes.\n",
        "  \"\"\"\n",
        "  graph = graph._replace(\n",
        "      nodes = hk.Linear(hidden_dimension)(graph.nodes),\n",
        "      device_edges = hk.Linear(hidden_dimension)(graph.device_edges)\n",
        "  )\n",
        "  \n",
        "  sharded_gn = sharded_graphnet.ShardedEdgesGraphNetwork(\n",
        "      update_node_fn = node_update_fn,\n",
        "      update_edge_fn = edge_update_fn,\n",
        "      num_shards = num_devices\n",
        "      )\n",
        "\n",
        "  for _ in range(num_message_passing_steps):\n",
        "    residual_graph = sharded_gn(graph)\n",
        "    graph = graph._replace(\n",
        "        nodes = graph.nodes + residual_graph.nodes,\n",
        "        device_edges = graph.device_edges + residual_graph.device_edges\n",
        "    )\n",
        "\n",
        "  graph = graph._replace(\n",
        "      nodes = hk.Sequential([hk.Linear(hidden_dimension), jax.nn.relu, hk.Linear(112)])(graph.nodes)\n",
        "  )\n",
        "  return graph.nodes"
      ],
      "metadata": {
        "id": "gPg7ph7sWyOn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bcast_local_devices(value):\n",
        "    \"\"\"Broadcasts an object to all local devices.\"\"\"\n",
        "    devices = jax.local_devices()\n",
        "\n",
        "    def _replicate(x):\n",
        "      \"\"\"Replicate an object on each device.\"\"\"\n",
        "      x = jnp.array(x)\n",
        "      return jax.device_put_sharded(len(devices) * [x], devices)\n",
        "\n",
        "    return jax.tree_util.tree_map(_replicate, value)"
      ],
      "metadata": {
        "id": "z6Qh75qxQfii"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reshape_broadcasted_data(data):\n",
        "  '''\n",
        "    Node predictions / Labels / Masks are identical on all the devices so we only take\n",
        "    one of them in order to remove the leading axis.\n",
        "  '''\n",
        "  return np.array(data)[0]\n",
        "  \n",
        "def remove_mask_from_data(data, mask):\n",
        "  '''\n",
        "    data.shape = [num_nodes, 112]\n",
        "    mask.shape = [num_nodes, 1]\n",
        "\n",
        "    We want to only return the data where mask == True\n",
        "  '''\n",
        "  sliced_data = np.compress(np.array(mask).reshape(-1).astype(bool), data, axis = 0)\n",
        "  return np.array(sliced_data)"
      ],
      "metadata": {
        "id": "oJ5T_oplbg_t"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import functools\n",
        "import haiku as hk\n",
        "\n",
        "from random import randint\n",
        "from google.colab import files\n",
        "\n",
        "# Try to follow this tutorial https://github.com/YuxuanXie/mcl/blob/5f7ee92e2a6bc89736263873a4ba9c14d1a676ff/glassy_dynamics/train_using_jax.py\n",
        "def compute_loss(params, graph, label, mask):\n",
        "  predictions = network_definition.apply(params, graph)\n",
        "\n",
        "  # use optax here (https://github.com/deepmind/optax/blob/master/optax/_src/loss.py#L116#L139)\n",
        "  loss = optax.sigmoid_binary_cross_entropy(predictions, label)  # shape [num_nodes, num_classes]\n",
        "  loss = loss * mask\n",
        "  loss = jnp.sum(loss) / jnp.sum(mask) # loss = mean_with_mask(loss, mask)\n",
        "\n",
        "  return loss\n",
        "\n",
        "def train(num_training_steps, learning_rate, results_path):\n",
        "  replicated_params = jax.pmap(network_definition.init, axis_name = 'i')(bcast_local_devices(jax.random.PRNGKey(42)), processed_graphs['partition_0']['graph'])\n",
        "\n",
        "  # opt_init, opt_update = optax.lamb(learning_rate = learning_rate)  \n",
        "  opt_init, opt_update = optax.adam(learning_rate = learning_rate)  \n",
        "  replicated_opt_state = jax.pmap(opt_init, axis_name = 'i')(replicated_params)\n",
        "\n",
        "  @functools.partial(jax.pmap, axis_name='i')\n",
        "  def update(params, opt_state, graph, targets, mask):\n",
        "    # Compute the gradients on the given minibatch (individually on each device).\n",
        "    loss, grads = jax.value_and_grad(compute_loss)(params, graph, targets, mask)\n",
        "\n",
        "    # Combine the gradient across all devices (by taking their mean).\n",
        "    grads = jax.lax.pmean(grads, axis_name='i')\n",
        "\n",
        "    # Also combine the loss. Unnecessary for the update, but useful for logging.\n",
        "    loss = jax.lax.pmean(loss, axis_name='i')\n",
        "\n",
        "    # updates, opt_state = opt_update(updates = grads, state = opt_state, params = params)\n",
        "    updates, opt_state = opt_update(updates = grads, state = opt_state)\n",
        "\n",
        "    return optax.apply_updates(params, updates), opt_state, loss\n",
        "\n",
        "  # Train\n",
        "  for idx in range(num_training_steps):\n",
        "    random_partition_idx = randint(0, num_partitions - 1)\n",
        "    random_partition = processed_graphs[f'partition_{random_partition_idx}']\n",
        "\n",
        "    graph = random_partition['graph']\n",
        "    labels = random_partition['labels']   # Automatically broadcasted by the sharded graph net\n",
        "    mask = random_partition['train_mask'] # Automatically broadcasted by the sharded graph net\n",
        "\n",
        "    replicated_params, replicated_opt_state, loss = update(\n",
        "        replicated_params, \n",
        "        replicated_opt_state, \n",
        "        graph, \n",
        "        labels,\n",
        "        mask\n",
        "        )\n",
        "    \n",
        "    print('Loss training:', reshape_broadcasted_data(loss))\n",
        "\n",
        "    if (idx + 1) % 10 == 0:\n",
        "      print()\n",
        "      print(f'***************************')\n",
        "      print(f'Trained on {idx + 1} graphs')\n",
        "      print(f'***************************')\n",
        "      print()\n",
        "\n",
        "    if (idx + 1) % 2000 == 0:\n",
        "      # Save parameters every 1000 epochs\n",
        "      params_file = f'{results_path}/params_epochs_{idx + 1}.pickle'\n",
        "      opt_state_file = f'{results_path}/opt_state_epochs_{idx + 1}.pickle'\n",
        "\n",
        "      with open(params_file, 'wb') as f:\n",
        "        # Save parameters to file\n",
        "        pickle.dump(replicated_params, f)\n",
        "\n",
        "        # Download in case workspace gets restarted\n",
        "        files.download(params_file)\n",
        "\n",
        "      with open(opt_state_file, 'wb') as f:\n",
        "        # Save optimiser state to file\n",
        "        pickle.dump(replicated_opt_state, f)\n",
        "\n",
        "        # Download in case workspace gets restarted\n",
        "        files.download(opt_state_file)\n",
        "\n",
        "  return replicated_params\n",
        "\n",
        "def evaluate(params, num_graphs_eval):\n",
        "  # Evaluate\n",
        "  accumulated_loss = 0.0\n",
        "  accumulated_roc = 0\n",
        "  graphs_evaluated = 0\n",
        "\n",
        "  print('**** Note that this uses the train_mask ****')\n",
        "  print('**** Note that this uses the train_mask ****')\n",
        "  print('**** Note that this uses the train_mask ****')\n",
        "\n",
        "  for idx in range(num_graphs_eval):\n",
        "    random_partition_idx = idx\n",
        "    random_partition = processed_graphs[f'partition_{random_partition_idx}']\n",
        "\n",
        "    graph = random_partition['graph']\n",
        "    labels = random_partition['labels']     # Automatically broadcasted by the sharded graph net\n",
        "    mask = random_partition['test_mask']    # Automatically broadcasted by the sharded graph net\n",
        "\n",
        "    predictions, loss = predict_on_graph(params, graph, labels, mask)\n",
        "    loss = reshape_broadcasted_data(loss)\n",
        "    \n",
        "    collected_labels = reshape_broadcasted_data(labels)\n",
        "    collected_predictions = reshape_broadcasted_data(predictions)\n",
        "    collected_mask = reshape_broadcasted_data(mask)\n",
        "\n",
        "    try:\n",
        "      roc = evaluator.eval({\n",
        "          \"y_true\": remove_mask_from_data(collected_labels, collected_mask), \n",
        "          \"y_pred\": remove_mask_from_data(collected_predictions, collected_mask)\n",
        "          })['rocauc']\n",
        "\n",
        "      accumulated_loss += loss\n",
        "      accumulated_roc += roc\n",
        "      graphs_evaluated += 1\n",
        "\n",
        "      print(f'Test loss: {loss} | ROC: {roc}')\n",
        "    except Exception as err:\n",
        "      print(f'Error message: \\n{str(err)}')\n",
        "      print()\n",
        "\n",
        "    if (idx + 1) % 10 == 0:\n",
        "      print()\n",
        "      print(f'***************************')\n",
        "      print(f'Evaluated on {idx + 1} graphs')\n",
        "      print(f'***************************')\n",
        "      print()\n",
        "\n",
        "  print(f'Average test loss: {accumulated_loss / graphs_evaluated} | Average ROC: {accumulated_roc / graphs_evaluated}')\n",
        "\n",
        "@functools.partial(jax.pmap, axis_name='i')\n",
        "def predict_on_graph(params, graph, label, mask):\n",
        "  decoded_nodes = network_definition.apply(params, graph)\n",
        "\n",
        "  compute_loss_fn = functools.partial(compute_loss)\n",
        "  loss = compute_loss_fn(params, graph, label, mask)\n",
        "\n",
        "  return jax.nn.sigmoid(decoded_nodes), loss"
      ],
      "metadata": {
        "id": "xYVzddNITMSv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_on_full_sets(params):\n",
        "  final_predictions = {}\n",
        "\n",
        "  for i in range(num_partitions):\n",
        "    node_ids = dgl_graph_metis_partition[i].ndata['_ID']\n",
        "    partition = processed_graphs[f'partition_{i}']\n",
        "    \n",
        "    predictions, _ = predict_on_graph(params, \n",
        "                                      partition['graph'], \n",
        "                                      partition['labels'], \n",
        "                                      partition['test_mask']  # Only used in the loss computation, does not affect predictions\n",
        "                                      )\n",
        "\n",
        "    predictions_after_masked_nodes_are_removed = remove_mask_from_data(\n",
        "        reshape_broadcasted_data(predictions),\n",
        "        reshape_broadcasted_data(partition['padding_mask'])\n",
        "        )\n",
        "\n",
        "    for index, node_id in enumerate(node_ids):\n",
        "      final_predictions[node_id] = predictions_after_masked_nodes_are_removed[index]\n",
        "\n",
        "    if (i + 1) % 10 == 0:\n",
        "      print(f'Evaluated {i + 1} / {num_partitions} subgraphs...')\n",
        "\n",
        "  # Sort the final predictions based on the node ids\n",
        "  predictions_in_order = dict(sorted(final_predictions.items()))\n",
        "\n",
        "  # Convert the values to a list to be able to slice based on the ids of the \n",
        "  # nodes in the test set\n",
        "  predictions_in_order = list(predictions_in_order.values())\n",
        "\n",
        "  final_roc_train = evaluator.eval({\n",
        "      \"y_true\": np.array(train_label), \n",
        "      \"y_pred\": np.array([predictions_in_order[x] for x in split_idx['train']])\n",
        "      })['rocauc']\n",
        "\n",
        "  final_roc_valid = evaluator.eval({\n",
        "      \"y_true\": np.array(valid_label), \n",
        "      \"y_pred\": np.array([predictions_in_order[x] for x in split_idx['valid']])\n",
        "      })['rocauc']\n",
        "\n",
        "  final_roc_test = evaluator.eval({\n",
        "      \"y_true\": np.array(test_label),\n",
        "      \"y_pred\": np.array([predictions_in_order[x] for x in split_idx['test']])\n",
        "      })['rocauc']\n",
        "\n",
        "  print()\n",
        "  print(f'Final ROC on the train set {final_roc_train}')\n",
        "  print(f'Final ROC on the validation set {final_roc_valid}')\n",
        "  print(f'Final ROC on the test set {final_roc_test}')"
      ],
      "metadata": {
        "id": "Xt-6IXF8U5vs"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "current_time = datetime.today().strftime('%Y-%m-%d-%H:%M:%S')\n",
        "exp_path = f'/content/exp_{current_time}/'\n",
        "os.makedirs(exp_path, exist_ok = False)\n",
        "\n",
        "# Main training loop\n",
        "final_params = train(\n",
        "    num_training_steps = 10000, \n",
        "    learning_rate = 1e-4,\n",
        "    results_path = exp_path\n",
        "    )\n",
        "\n",
        "# with open('/content/exp_2022-03-30-09:51:17/params_epochs_9000.pickle', 'rb') as f:\n",
        "#     loaded_params = pickle.load(f)\n",
        "loaded_params = final_params\n",
        "evaluate_on_full_sets(loaded_params)\n",
        "\n",
        "'''\n",
        "  Previous runs (padding to power of 2)\n",
        "  (1) Configuration\n",
        "        learning_rate = 0.001\n",
        "        num_partitions = 50\n",
        "        hidden_dimension = 128\n",
        "        num_message_passing_steps = 5\n",
        "        num_training_steps = 1000\n",
        "    ROC on the train set 0.7348797273386144\n",
        "    ROC on the validation set 0.6025038939324504\n",
        "    ROC on the test set 0.5896861508337246\n",
        "\n",
        "  (2) Configuration\n",
        "        learning_rate = 0.001\n",
        "        num_partitions = 50\n",
        "        hidden_dimension = 128\n",
        "        num_message_passing_steps = 5\n",
        "        num_training_steps = 3000\n",
        "    ROC on the train set 0.8050085464161815\n",
        "    ROC on the validation set 0.6327603823722211\n",
        "    ROC on the test set 0.5078022533003436\n",
        "\n",
        "  (3) Configuration\n",
        "        learning_rate = 0.1 (Question: I think this might be too high -- based on the results in (5) with lower number of epochs)\n",
        "        num_partitions = 100\n",
        "        hidden_dimension = 128\n",
        "        num_message_passing_steps = 5\n",
        "        num_training_steps = 1000\n",
        "    ROC on the train set 0.5\n",
        "    ROC on the validation set 0.5 \n",
        "    ROC on the test set 0.5\n",
        "\n",
        "  (4) Configuration\n",
        "        learning_rate = 0.01\n",
        "        num_partitions = 100\n",
        "        hidden_dimension = 128\n",
        "        num_message_passing_steps = 5\n",
        "        num_training_steps = 100\n",
        "    ROC on the train set 0.6501172261188106\n",
        "    ROC on the validation set 0.5281974299591566\n",
        "    ROC on the test set 0.47652056321124514\n",
        "\n",
        "  (5) Configuration\n",
        "        learning_rate = 0.01\n",
        "        num_partitions = 100\n",
        "        hidden_dimension = 128\n",
        "        num_message_passing_steps = 5\n",
        "        num_training_steps = 500\n",
        "    ROC on the train set 0.6939371049645034\n",
        "    ROC on the validation set 0.559224577731843\n",
        "    ROC on the test set 0.5488968392833208\n",
        "\n",
        "  (6) Configuration\n",
        "        opt: LAMB\n",
        "        learning_rate: 1e-4\n",
        "        num-partitions = 100\n",
        "        hidden_dimension = 128\n",
        "        num_message_passing_steps = 5\n",
        "        num_training_steps = 500\n",
        "  ROC on the train set 0.6299712777663571\n",
        "  ROC on the validation set 0.5054189612195771\n",
        "  ROC on the test set 0.5083185060310427\n",
        "\n",
        "  ********************************************\n",
        "\n",
        "  Previous runs (padding to multiple of 8)\n",
        "  (1) Configuration\n",
        "        opt: LAMB\n",
        "        learning_rate: 1e-4\n",
        "        num-partitions = 35\n",
        "        hidden_dimension = 128\n",
        "        num_message_passing_steps = 5\n",
        "        num_training_steps = 500\n",
        "  ROC on the train set 0.6438794374674618\n",
        "  ROC on the validation set 0.5162833891590899\n",
        "  ROC on the test set 0.5175085147535061\n",
        "\n",
        "  (2) Configuration\n",
        "        opt: ADAM\n",
        "        learning_rate: 1e-4\n",
        "        num-partitions = 35\n",
        "        hidden_dimension = 128\n",
        "        num_message_passing_steps = 5\n",
        "        num_training_steps = 10000\n",
        "  ROC on the train set 0.8873957875287084\n",
        "  ROC on the validation set 0.604810879077169\n",
        "  ROC on the test set 0.57961973018596\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N4sdL1RSWurH",
        "outputId": "c32c7c49-e866-4d35-dad9-1ed544e420e2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss training: 97.748505\n",
            "Loss training: 89.37039\n",
            "Loss training: 86.41844\n",
            "Loss training: 81.27354\n",
            "Loss training: 82.12192\n",
            "Loss training: 73.32808\n",
            "Loss training: 71.559875\n",
            "Loss training: 69.96212\n",
            "Loss training: 68.24877\n",
            "Loss training: 71.94993\n",
            "\n",
            "***************************\n",
            "Trained on 10 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 67.45565\n",
            "Loss training: 68.50363\n",
            "Loss training: 63.83986\n",
            "Loss training: 71.83789\n",
            "Loss training: 67.89621\n",
            "Loss training: 65.847145\n",
            "Loss training: 69.11998\n",
            "Loss training: 64.44672\n",
            "Loss training: 73.62375\n",
            "Loss training: 72.1631\n",
            "\n",
            "***************************\n",
            "Trained on 20 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 66.68348\n",
            "Loss training: 63.622795\n",
            "Loss training: 68.34545\n",
            "Loss training: 63.477684\n",
            "Loss training: 65.88409\n",
            "Loss training: 67.19449\n",
            "Loss training: 67.37291\n",
            "Loss training: 59.442528\n",
            "Loss training: 62.419918\n",
            "Loss training: 59.53829\n",
            "\n",
            "***************************\n",
            "Trained on 30 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 62.16149\n",
            "Loss training: 57.878647\n",
            "Loss training: 64.68094\n",
            "Loss training: 61.188564\n",
            "Loss training: 69.9349\n",
            "Loss training: 56.132137\n",
            "Loss training: 68.29694\n",
            "Loss training: 56.86886\n",
            "Loss training: 60.547073\n",
            "Loss training: 56.98167\n",
            "\n",
            "***************************\n",
            "Trained on 40 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 65.58947\n",
            "Loss training: 59.746685\n",
            "Loss training: 53.38269\n",
            "Loss training: 51.854855\n",
            "Loss training: 60.970676\n",
            "Loss training: 54.51727\n",
            "Loss training: 51.5702\n",
            "Loss training: 52.24679\n",
            "Loss training: 68.260216\n",
            "Loss training: 47.710278\n",
            "\n",
            "***************************\n",
            "Trained on 50 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 48.26704\n",
            "Loss training: 47.584736\n",
            "Loss training: 53.193645\n",
            "Loss training: 40.116394\n",
            "Loss training: 43.44174\n",
            "Loss training: 55.252884\n",
            "Loss training: 47.498425\n",
            "Loss training: 53.7733\n",
            "Loss training: 50.583637\n",
            "Loss training: 39.884052\n",
            "\n",
            "***************************\n",
            "Trained on 60 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.503304\n",
            "Loss training: 49.61705\n",
            "Loss training: 56.052425\n",
            "Loss training: 62.261173\n",
            "Loss training: 43.72937\n",
            "Loss training: 57.479836\n",
            "Loss training: 43.06016\n",
            "Loss training: 50.33768\n",
            "Loss training: 41.100742\n",
            "Loss training: 56.388813\n",
            "\n",
            "***************************\n",
            "Trained on 70 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 55.07416\n",
            "Loss training: 62.380028\n",
            "Loss training: 38.298534\n",
            "Loss training: 46.04048\n",
            "Loss training: 53.180542\n",
            "Loss training: 52.82314\n",
            "Loss training: 37.119724\n",
            "Loss training: 50.62475\n",
            "Loss training: 50.44589\n",
            "Loss training: 47.166267\n",
            "\n",
            "***************************\n",
            "Trained on 80 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.329914\n",
            "Loss training: 57.51523\n",
            "Loss training: 46.297966\n",
            "Loss training: 43.846355\n",
            "Loss training: 49.270836\n",
            "Loss training: 45.606236\n",
            "Loss training: 59.441418\n",
            "Loss training: 56.359398\n",
            "Loss training: 59.683346\n",
            "Loss training: 40.60585\n",
            "\n",
            "***************************\n",
            "Trained on 90 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.99711\n",
            "Loss training: 39.176373\n",
            "Loss training: 35.246094\n",
            "Loss training: 70.54512\n",
            "Loss training: 33.836617\n",
            "Loss training: 62.78604\n",
            "Loss training: 31.344006\n",
            "Loss training: 43.07574\n",
            "Loss training: 44.265457\n",
            "Loss training: 39.44438\n",
            "\n",
            "***************************\n",
            "Trained on 100 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.79166\n",
            "Loss training: 38.591423\n",
            "Loss training: 23.96964\n",
            "Loss training: 44.023235\n",
            "Loss training: 44.046654\n",
            "Loss training: 31.781164\n",
            "Loss training: 59.593723\n",
            "Loss training: 35.389744\n",
            "Loss training: 37.32793\n",
            "Loss training: 40.340435\n",
            "\n",
            "***************************\n",
            "Trained on 110 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 49.99791\n",
            "Loss training: 37.34647\n",
            "Loss training: 47.144917\n",
            "Loss training: 45.537743\n",
            "Loss training: 45.36329\n",
            "Loss training: 49.5918\n",
            "Loss training: 37.453968\n",
            "Loss training: 43.28297\n",
            "Loss training: 44.700256\n",
            "Loss training: 49.340767\n",
            "\n",
            "***************************\n",
            "Trained on 120 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 50.878307\n",
            "Loss training: 27.241596\n",
            "Loss training: 21.00731\n",
            "Loss training: 20.816336\n",
            "Loss training: 42.787548\n",
            "Loss training: 50.08742\n",
            "Loss training: 39.92979\n",
            "Loss training: 31.653639\n",
            "Loss training: 19.549505\n",
            "Loss training: 49.596416\n",
            "\n",
            "***************************\n",
            "Trained on 130 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.80114\n",
            "Loss training: 29.34575\n",
            "Loss training: 38.557938\n",
            "Loss training: 31.085773\n",
            "Loss training: 42.520733\n",
            "Loss training: 59.168335\n",
            "Loss training: 61.84309\n",
            "Loss training: 30.623384\n",
            "Loss training: 44.260494\n",
            "Loss training: 35.680725\n",
            "\n",
            "***************************\n",
            "Trained on 140 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 56.734776\n",
            "Loss training: 57.60664\n",
            "Loss training: 54.31213\n",
            "Loss training: 53.59558\n",
            "Loss training: 58.02895\n",
            "Loss training: 37.22081\n",
            "Loss training: 29.499083\n",
            "Loss training: 43.74872\n",
            "Loss training: 33.786243\n",
            "Loss training: 33.782265\n",
            "\n",
            "***************************\n",
            "Trained on 150 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.159386\n",
            "Loss training: 41.83317\n",
            "Loss training: 47.74354\n",
            "Loss training: 37.234753\n",
            "Loss training: 55.691338\n",
            "Loss training: 41.404324\n",
            "Loss training: 40.50819\n",
            "Loss training: 19.004366\n",
            "Loss training: 36.727444\n",
            "Loss training: 56.82511\n",
            "\n",
            "***************************\n",
            "Trained on 160 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.55521\n",
            "Loss training: 77.30779\n",
            "Loss training: 46.80365\n",
            "Loss training: 46.7935\n",
            "Loss training: 41.463764\n",
            "Loss training: 56.37728\n",
            "Loss training: 50.216564\n",
            "Loss training: 56.70987\n",
            "Loss training: 36.852848\n",
            "Loss training: 42.390175\n",
            "\n",
            "***************************\n",
            "Trained on 170 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.918442\n",
            "Loss training: 57.890648\n",
            "Loss training: 53.66317\n",
            "Loss training: 30.16378\n",
            "Loss training: 42.368057\n",
            "Loss training: 30.058842\n",
            "Loss training: 29.850069\n",
            "Loss training: 49.7535\n",
            "Loss training: 36.208233\n",
            "Loss training: 49.455578\n",
            "\n",
            "***************************\n",
            "Trained on 180 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 57.39809\n",
            "Loss training: 25.176846\n",
            "Loss training: 36.19469\n",
            "Loss training: 41.473602\n",
            "Loss training: 33.683178\n",
            "Loss training: 41.392696\n",
            "Loss training: 48.25837\n",
            "Loss training: 39.103165\n",
            "Loss training: 34.918583\n",
            "Loss training: 35.166256\n",
            "\n",
            "***************************\n",
            "Trained on 190 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.675827\n",
            "Loss training: 43.59652\n",
            "Loss training: 44.285767\n",
            "Loss training: 29.350681\n",
            "Loss training: 55.71772\n",
            "Loss training: 30.788782\n",
            "Loss training: 50.314156\n",
            "Loss training: 42.76909\n",
            "Loss training: 33.63482\n",
            "Loss training: 34.88936\n",
            "\n",
            "***************************\n",
            "Trained on 200 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 57.93704\n",
            "Loss training: 49.653492\n",
            "Loss training: 54.986885\n",
            "Loss training: 41.72566\n",
            "Loss training: 54.769447\n",
            "Loss training: 47.290855\n",
            "Loss training: 36.854145\n",
            "Loss training: 36.91584\n",
            "Loss training: 34.084686\n",
            "Loss training: 31.323933\n",
            "\n",
            "***************************\n",
            "Trained on 210 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.78681\n",
            "Loss training: 72.07453\n",
            "Loss training: 29.160257\n",
            "Loss training: 46.373215\n",
            "Loss training: 24.85699\n",
            "Loss training: 55.29028\n",
            "Loss training: 35.950756\n",
            "Loss training: 49.15559\n",
            "Loss training: 71.157486\n",
            "Loss training: 54.44359\n",
            "\n",
            "***************************\n",
            "Trained on 220 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.708717\n",
            "Loss training: 33.953136\n",
            "Loss training: 41.71174\n",
            "Loss training: 52.71508\n",
            "Loss training: 53.04809\n",
            "Loss training: 48.51013\n",
            "Loss training: 36.584812\n",
            "Loss training: 54.748028\n",
            "Loss training: 49.79662\n",
            "Loss training: 46.370316\n",
            "\n",
            "***************************\n",
            "Trained on 230 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 46.64462\n",
            "Loss training: 46.26993\n",
            "Loss training: 36.63721\n",
            "Loss training: 46.757767\n",
            "Loss training: 52.056675\n",
            "Loss training: 35.771957\n",
            "Loss training: 19.093796\n",
            "Loss training: 52.222256\n",
            "Loss training: 45.515255\n",
            "Loss training: 36.160645\n",
            "\n",
            "***************************\n",
            "Trained on 240 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 18.488182\n",
            "Loss training: 53.527683\n",
            "Loss training: 44.338943\n",
            "Loss training: 46.71156\n",
            "Loss training: 53.43788\n",
            "Loss training: 29.91428\n",
            "Loss training: 70.67994\n",
            "Loss training: 35.15143\n",
            "Loss training: 33.008186\n",
            "Loss training: 42.74908\n",
            "\n",
            "***************************\n",
            "Trained on 250 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.561024\n",
            "Loss training: 40.77704\n",
            "Loss training: 55.379826\n",
            "Loss training: 42.352425\n",
            "Loss training: 42.39937\n",
            "Loss training: 35.236397\n",
            "Loss training: 41.011597\n",
            "Loss training: 54.48836\n",
            "Loss training: 35.209854\n",
            "Loss training: 31.389107\n",
            "\n",
            "***************************\n",
            "Trained on 260 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 47.496532\n",
            "Loss training: 30.65248\n",
            "Loss training: 34.706932\n",
            "Loss training: 53.603348\n",
            "Loss training: 45.672504\n",
            "Loss training: 42.204365\n",
            "Loss training: 33.563793\n",
            "Loss training: 48.427917\n",
            "Loss training: 43.96499\n",
            "Loss training: 45.45058\n",
            "\n",
            "***************************\n",
            "Trained on 270 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.297207\n",
            "Loss training: 34.808254\n",
            "Loss training: 34.64071\n",
            "Loss training: 44.800446\n",
            "Loss training: 30.82937\n",
            "Loss training: 42.552177\n",
            "Loss training: 30.354387\n",
            "Loss training: 40.90806\n",
            "Loss training: 27.97791\n",
            "Loss training: 43.0432\n",
            "\n",
            "***************************\n",
            "Trained on 280 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.348194\n",
            "Loss training: 34.4271\n",
            "Loss training: 43.97762\n",
            "Loss training: 32.08854\n",
            "Loss training: 26.699554\n",
            "Loss training: 33.875385\n",
            "Loss training: 43.409595\n",
            "Loss training: 56.351448\n",
            "Loss training: 41.19146\n",
            "Loss training: 16.696842\n",
            "\n",
            "***************************\n",
            "Trained on 290 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.65751\n",
            "Loss training: 44.699745\n",
            "Loss training: 76.20801\n",
            "Loss training: 48.768738\n",
            "Loss training: 16.92632\n",
            "Loss training: 44.764435\n",
            "Loss training: 55.36065\n",
            "Loss training: 38.94075\n",
            "Loss training: 44.582123\n",
            "Loss training: 44.533913\n",
            "\n",
            "***************************\n",
            "Trained on 300 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 53.385868\n",
            "Loss training: 44.054356\n",
            "Loss training: 47.827496\n",
            "Loss training: 50.27059\n",
            "Loss training: 33.191673\n",
            "Loss training: 26.684301\n",
            "Loss training: 30.476048\n",
            "Loss training: 25.011337\n",
            "Loss training: 34.574898\n",
            "Loss training: 34.819393\n",
            "\n",
            "***************************\n",
            "Trained on 310 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.036749\n",
            "Loss training: 41.839947\n",
            "Loss training: 28.051308\n",
            "Loss training: 28.384743\n",
            "Loss training: 41.806034\n",
            "Loss training: 43.380238\n",
            "Loss training: 24.111053\n",
            "Loss training: 36.908916\n",
            "Loss training: 49.408955\n",
            "Loss training: 40.692947\n",
            "\n",
            "***************************\n",
            "Trained on 320 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.851387\n",
            "Loss training: 16.126976\n",
            "Loss training: 49.27118\n",
            "Loss training: 43.82652\n",
            "Loss training: 30.302351\n",
            "Loss training: 33.426872\n",
            "Loss training: 48.228893\n",
            "Loss training: 33.199192\n",
            "Loss training: 39.143036\n",
            "Loss training: 42.857616\n",
            "\n",
            "***************************\n",
            "Trained on 330 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 58.02838\n",
            "Loss training: 32.27462\n",
            "Loss training: 54.98587\n",
            "Loss training: 32.294147\n",
            "Loss training: 53.562153\n",
            "Loss training: 54.105537\n",
            "Loss training: 48.84351\n",
            "Loss training: 35.86758\n",
            "Loss training: 50.35988\n",
            "Loss training: 52.597515\n",
            "\n",
            "***************************\n",
            "Trained on 340 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.160507\n",
            "Loss training: 36.89017\n",
            "Loss training: 36.21377\n",
            "Loss training: 46.026463\n",
            "Loss training: 51.919895\n",
            "Loss training: 30.614246\n",
            "Loss training: 39.657574\n",
            "Loss training: 48.75142\n",
            "Loss training: 34.99521\n",
            "Loss training: 47.462185\n",
            "\n",
            "***************************\n",
            "Trained on 350 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.679136\n",
            "Loss training: 49.773216\n",
            "Loss training: 47.909466\n",
            "Loss training: 30.422712\n",
            "Loss training: 23.523535\n",
            "Loss training: 16.539608\n",
            "Loss training: 30.235926\n",
            "Loss training: 53.159805\n",
            "Loss training: 32.057285\n",
            "Loss training: 41.240826\n",
            "\n",
            "***************************\n",
            "Trained on 360 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.285303\n",
            "Loss training: 33.206398\n",
            "Loss training: 29.683834\n",
            "Loss training: 29.452131\n",
            "Loss training: 16.221176\n",
            "Loss training: 23.053083\n",
            "Loss training: 47.623127\n",
            "Loss training: 23.01015\n",
            "Loss training: 30.15066\n",
            "Loss training: 26.89886\n",
            "\n",
            "***************************\n",
            "Trained on 370 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 53.56898\n",
            "Loss training: 50.73416\n",
            "Loss training: 28.550356\n",
            "Loss training: 55.53544\n",
            "Loss training: 40.228413\n",
            "Loss training: 53.507626\n",
            "Loss training: 39.354053\n",
            "Loss training: 44.425453\n",
            "Loss training: 51.664696\n",
            "Loss training: 36.9303\n",
            "\n",
            "***************************\n",
            "Trained on 380 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.670265\n",
            "Loss training: 41.646034\n",
            "Loss training: 35.6562\n",
            "Loss training: 72.55831\n",
            "Loss training: 51.787243\n",
            "Loss training: 48.713337\n",
            "Loss training: 30.278591\n",
            "Loss training: 44.097576\n",
            "Loss training: 34.87145\n",
            "Loss training: 48.95404\n",
            "\n",
            "***************************\n",
            "Trained on 390 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.358997\n",
            "Loss training: 34.891552\n",
            "Loss training: 47.595146\n",
            "Loss training: 37.270306\n",
            "Loss training: 33.05998\n",
            "Loss training: 28.90096\n",
            "Loss training: 51.84945\n",
            "Loss training: 34.338432\n",
            "Loss training: 16.104359\n",
            "Loss training: 43.863792\n",
            "\n",
            "***************************\n",
            "Trained on 400 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 54.08276\n",
            "Loss training: 53.333385\n",
            "Loss training: 25.98889\n",
            "Loss training: 34.21464\n",
            "Loss training: 38.689552\n",
            "Loss training: 30.926464\n",
            "Loss training: 37.39661\n",
            "Loss training: 46.340332\n",
            "Loss training: 46.22174\n",
            "Loss training: 26.186958\n",
            "\n",
            "***************************\n",
            "Trained on 410 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 75.833\n",
            "Loss training: 37.834377\n",
            "Loss training: 23.924591\n",
            "Loss training: 48.6434\n",
            "Loss training: 41.69503\n",
            "Loss training: 45.184193\n",
            "Loss training: 23.816002\n",
            "Loss training: 30.340178\n",
            "Loss training: 48.22648\n",
            "Loss training: 49.112175\n",
            "\n",
            "***************************\n",
            "Trained on 420 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 50.974834\n",
            "Loss training: 46.73176\n",
            "Loss training: 34.76799\n",
            "Loss training: 52.9864\n",
            "Loss training: 31.669601\n",
            "Loss training: 37.977905\n",
            "Loss training: 46.955982\n",
            "Loss training: 39.75266\n",
            "Loss training: 35.99137\n",
            "Loss training: 39.572983\n",
            "\n",
            "***************************\n",
            "Trained on 430 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 47.85791\n",
            "Loss training: 34.76599\n",
            "Loss training: 23.3024\n",
            "Loss training: 34.23076\n",
            "Loss training: 47.033535\n",
            "Loss training: 34.646526\n",
            "Loss training: 49.249367\n",
            "Loss training: 33.8341\n",
            "Loss training: 26.833937\n",
            "Loss training: 43.565144\n",
            "\n",
            "***************************\n",
            "Trained on 440 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.039646\n",
            "Loss training: 30.218222\n",
            "Loss training: 73.52884\n",
            "Loss training: 23.187025\n",
            "Loss training: 39.5227\n",
            "Loss training: 50.505825\n",
            "Loss training: 35.95946\n",
            "Loss training: 32.775368\n",
            "Loss training: 34.526424\n",
            "Loss training: 35.138676\n",
            "\n",
            "***************************\n",
            "Trained on 450 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.25731\n",
            "Loss training: 48.455383\n",
            "Loss training: 17.588408\n",
            "Loss training: 52.963634\n",
            "Loss training: 23.206347\n",
            "Loss training: 41.490173\n",
            "Loss training: 34.206055\n",
            "Loss training: 46.3223\n",
            "Loss training: 16.140047\n",
            "Loss training: 30.02994\n",
            "\n",
            "***************************\n",
            "Trained on 460 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 52.49803\n",
            "Loss training: 33.070328\n",
            "Loss training: 32.862778\n",
            "Loss training: 50.80913\n",
            "Loss training: 34.420444\n",
            "Loss training: 47.997295\n",
            "Loss training: 41.60475\n",
            "Loss training: 44.922806\n",
            "Loss training: 30.466585\n",
            "Loss training: 34.930397\n",
            "\n",
            "***************************\n",
            "Trained on 470 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 51.39483\n",
            "Loss training: 40.20238\n",
            "Loss training: 48.58617\n",
            "Loss training: 42.0849\n",
            "Loss training: 48.365658\n",
            "Loss training: 45.296272\n",
            "Loss training: 48.010143\n",
            "Loss training: 44.032764\n",
            "Loss training: 72.93108\n",
            "Loss training: 39.35398\n",
            "\n",
            "***************************\n",
            "Trained on 480 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.676964\n",
            "Loss training: 32.718254\n",
            "Loss training: 47.99426\n",
            "Loss training: 51.26533\n",
            "Loss training: 45.719097\n",
            "Loss training: 49.764\n",
            "Loss training: 34.528366\n",
            "Loss training: 33.65112\n",
            "Loss training: 51.390858\n",
            "Loss training: 36.76333\n",
            "\n",
            "***************************\n",
            "Trained on 490 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.086708\n",
            "Loss training: 43.074017\n",
            "Loss training: 41.398457\n",
            "Loss training: 44.06896\n",
            "Loss training: 34.856434\n",
            "Loss training: 34.332962\n",
            "Loss training: 51.78601\n",
            "Loss training: 51.496025\n",
            "Loss training: 51.858253\n",
            "Loss training: 43.00256\n",
            "\n",
            "***************************\n",
            "Trained on 500 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.639515\n",
            "Loss training: 48.44637\n",
            "Loss training: 39.872593\n",
            "Loss training: 30.893332\n",
            "Loss training: 39.599663\n",
            "Loss training: 47.9491\n",
            "Loss training: 68.86269\n",
            "Loss training: 49.23749\n",
            "Loss training: 49.13384\n",
            "Loss training: 40.060097\n",
            "\n",
            "***************************\n",
            "Trained on 510 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.90134\n",
            "Loss training: 38.365856\n",
            "Loss training: 50.139034\n",
            "Loss training: 24.376799\n",
            "Loss training: 45.572807\n",
            "Loss training: 41.29306\n",
            "Loss training: 35.2905\n",
            "Loss training: 33.813923\n",
            "Loss training: 33.36587\n",
            "Loss training: 48.857845\n",
            "\n",
            "***************************\n",
            "Trained on 520 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.590202\n",
            "Loss training: 16.094017\n",
            "Loss training: 38.893036\n",
            "Loss training: 15.707084\n",
            "Loss training: 37.680424\n",
            "Loss training: 26.770864\n",
            "Loss training: 40.14178\n",
            "Loss training: 32.596985\n",
            "Loss training: 49.122795\n",
            "Loss training: 32.2191\n",
            "\n",
            "***************************\n",
            "Trained on 530 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.810673\n",
            "Loss training: 32.304134\n",
            "Loss training: 51.508\n",
            "Loss training: 43.93827\n",
            "Loss training: 33.632816\n",
            "Loss training: 15.275609\n",
            "Loss training: 28.881824\n",
            "Loss training: 40.76238\n",
            "Loss training: 61.62494\n",
            "Loss training: 31.88335\n",
            "\n",
            "***************************\n",
            "Trained on 540 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 48.29017\n",
            "Loss training: 52.312717\n",
            "Loss training: 45.90588\n",
            "Loss training: 36.192837\n",
            "Loss training: 29.987625\n",
            "Loss training: 44.847954\n",
            "Loss training: 32.623573\n",
            "Loss training: 39.198395\n",
            "Loss training: 34.904907\n",
            "Loss training: 39.29065\n",
            "\n",
            "***************************\n",
            "Trained on 550 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.601234\n",
            "Loss training: 48.841663\n",
            "Loss training: 15.977064\n",
            "Loss training: 44.56245\n",
            "Loss training: 69.60865\n",
            "Loss training: 15.776988\n",
            "Loss training: 44.271435\n",
            "Loss training: 36.429764\n",
            "Loss training: 27.705513\n",
            "Loss training: 38.852688\n",
            "\n",
            "***************************\n",
            "Trained on 560 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.598324\n",
            "Loss training: 48.75783\n",
            "Loss training: 44.778404\n",
            "Loss training: 48.67738\n",
            "Loss training: 34.218613\n",
            "Loss training: 34.326584\n",
            "Loss training: 39.3611\n",
            "Loss training: 43.33766\n",
            "Loss training: 40.971436\n",
            "Loss training: 47.102875\n",
            "\n",
            "***************************\n",
            "Trained on 570 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.02294\n",
            "Loss training: 28.500404\n",
            "Loss training: 47.301613\n",
            "Loss training: 39.824345\n",
            "Loss training: 52.024117\n",
            "Loss training: 42.257305\n",
            "Loss training: 29.444475\n",
            "Loss training: 39.389977\n",
            "Loss training: 40.00447\n",
            "Loss training: 34.209087\n",
            "\n",
            "***************************\n",
            "Trained on 580 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.377825\n",
            "Loss training: 26.47549\n",
            "Loss training: 40.73492\n",
            "Loss training: 46.047226\n",
            "Loss training: 38.857105\n",
            "Loss training: 38.84777\n",
            "Loss training: 45.268703\n",
            "Loss training: 33.248863\n",
            "Loss training: 28.364878\n",
            "Loss training: 35.620068\n",
            "\n",
            "***************************\n",
            "Trained on 590 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.83609\n",
            "Loss training: 48.631283\n",
            "Loss training: 43.549576\n",
            "Loss training: 32.810345\n",
            "Loss training: 53.907894\n",
            "Loss training: 30.327768\n",
            "Loss training: 49.104107\n",
            "Loss training: 28.320162\n",
            "Loss training: 33.85302\n",
            "Loss training: 47.939804\n",
            "\n",
            "***************************\n",
            "Trained on 600 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.31673\n",
            "Loss training: 48.21945\n",
            "Loss training: 40.517746\n",
            "Loss training: 34.818836\n",
            "Loss training: 43.70968\n",
            "Loss training: 40.136448\n",
            "Loss training: 43.309998\n",
            "Loss training: 33.069744\n",
            "Loss training: 31.937384\n",
            "Loss training: 51.626217\n",
            "\n",
            "***************************\n",
            "Trained on 610 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.610092\n",
            "Loss training: 42.138363\n",
            "Loss training: 27.229042\n",
            "Loss training: 33.60263\n",
            "Loss training: 48.701736\n",
            "Loss training: 54.66297\n",
            "Loss training: 38.83969\n",
            "Loss training: 51.049355\n",
            "Loss training: 32.91235\n",
            "Loss training: 54.084393\n",
            "\n",
            "***************************\n",
            "Trained on 620 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.527916\n",
            "Loss training: 45.100727\n",
            "Loss training: 42.8655\n",
            "Loss training: 42.50039\n",
            "Loss training: 41.505337\n",
            "Loss training: 28.828941\n",
            "Loss training: 49.64025\n",
            "Loss training: 47.95266\n",
            "Loss training: 26.168566\n",
            "Loss training: 47.41475\n",
            "\n",
            "***************************\n",
            "Trained on 630 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.34501\n",
            "Loss training: 43.09831\n",
            "Loss training: 30.988817\n",
            "Loss training: 32.35014\n",
            "Loss training: 39.91083\n",
            "Loss training: 31.507288\n",
            "Loss training: 34.672916\n",
            "Loss training: 23.454905\n",
            "Loss training: 30.266289\n",
            "Loss training: 33.39039\n",
            "\n",
            "***************************\n",
            "Trained on 640 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.716698\n",
            "Loss training: 41.146866\n",
            "Loss training: 39.808678\n",
            "Loss training: 41.1648\n",
            "Loss training: 36.9952\n",
            "Loss training: 41.014435\n",
            "Loss training: 31.813183\n",
            "Loss training: 36.462048\n",
            "Loss training: 45.457645\n",
            "Loss training: 37.4565\n",
            "\n",
            "***************************\n",
            "Trained on 650 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.207188\n",
            "Loss training: 52.73073\n",
            "Loss training: 55.07041\n",
            "Loss training: 47.295517\n",
            "Loss training: 33.800125\n",
            "Loss training: 32.37037\n",
            "Loss training: 46.85844\n",
            "Loss training: 41.65315\n",
            "Loss training: 43.585857\n",
            "Loss training: 16.411955\n",
            "\n",
            "***************************\n",
            "Trained on 660 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 47.403404\n",
            "Loss training: 45.26107\n",
            "Loss training: 47.469067\n",
            "Loss training: 45.04164\n",
            "Loss training: 23.088882\n",
            "Loss training: 49.679012\n",
            "Loss training: 33.130478\n",
            "Loss training: 34.226818\n",
            "Loss training: 36.12518\n",
            "Loss training: 35.604385\n",
            "\n",
            "***************************\n",
            "Trained on 670 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 15.68968\n",
            "Loss training: 32.01046\n",
            "Loss training: 29.873451\n",
            "Loss training: 38.27892\n",
            "Loss training: 44.01549\n",
            "Loss training: 51.435482\n",
            "Loss training: 32.038006\n",
            "Loss training: 43.551163\n",
            "Loss training: 33.863514\n",
            "Loss training: 38.353146\n",
            "\n",
            "***************************\n",
            "Trained on 680 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.90991\n",
            "Loss training: 43.431244\n",
            "Loss training: 30.277796\n",
            "Loss training: 46.667366\n",
            "Loss training: 49.752472\n",
            "Loss training: 30.463732\n",
            "Loss training: 32.155674\n",
            "Loss training: 53.1941\n",
            "Loss training: 40.478943\n",
            "Loss training: 32.80944\n",
            "\n",
            "***************************\n",
            "Trained on 690 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.31095\n",
            "Loss training: 33.540695\n",
            "Loss training: 34.233124\n",
            "Loss training: 41.862877\n",
            "Loss training: 29.834187\n",
            "Loss training: 52.426212\n",
            "Loss training: 47.440544\n",
            "Loss training: 32.165577\n",
            "Loss training: 34.057755\n",
            "Loss training: 33.490852\n",
            "\n",
            "***************************\n",
            "Trained on 700 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 51.026928\n",
            "Loss training: 46.080196\n",
            "Loss training: 40.351273\n",
            "Loss training: 31.405792\n",
            "Loss training: 41.902824\n",
            "Loss training: 32.246166\n",
            "Loss training: 25.98621\n",
            "Loss training: 39.07881\n",
            "Loss training: 34.880394\n",
            "Loss training: 28.690277\n",
            "\n",
            "***************************\n",
            "Trained on 710 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 72.69037\n",
            "Loss training: 41.825275\n",
            "Loss training: 38.91971\n",
            "Loss training: 29.873343\n",
            "Loss training: 31.011993\n",
            "Loss training: 46.860996\n",
            "Loss training: 35.413948\n",
            "Loss training: 24.817778\n",
            "Loss training: 53.414017\n",
            "Loss training: 42.780605\n",
            "\n",
            "***************************\n",
            "Trained on 720 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.13855\n",
            "Loss training: 24.538006\n",
            "Loss training: 33.408733\n",
            "Loss training: 38.727493\n",
            "Loss training: 31.917706\n",
            "Loss training: 31.966871\n",
            "Loss training: 45.458748\n",
            "Loss training: 33.942257\n",
            "Loss training: 46.33673\n",
            "Loss training: 33.845013\n",
            "\n",
            "***************************\n",
            "Trained on 730 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 75.44501\n",
            "Loss training: 42.7016\n",
            "Loss training: 30.921824\n",
            "Loss training: 45.82176\n",
            "Loss training: 43.082527\n",
            "Loss training: 30.824427\n",
            "Loss training: 51.747063\n",
            "Loss training: 38.664097\n",
            "Loss training: 34.351562\n",
            "Loss training: 50.894096\n",
            "\n",
            "***************************\n",
            "Trained on 740 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 48.349033\n",
            "Loss training: 32.64664\n",
            "Loss training: 45.151756\n",
            "Loss training: 33.486584\n",
            "Loss training: 41.59693\n",
            "Loss training: 30.039764\n",
            "Loss training: 44.425106\n",
            "Loss training: 42.8994\n",
            "Loss training: 30.356049\n",
            "Loss training: 32.19872\n",
            "\n",
            "***************************\n",
            "Trained on 750 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.7328\n",
            "Loss training: 69.60579\n",
            "Loss training: 23.068615\n",
            "Loss training: 43.464226\n",
            "Loss training: 33.56578\n",
            "Loss training: 29.313505\n",
            "Loss training: 24.811632\n",
            "Loss training: 33.169025\n",
            "Loss training: 39.655098\n",
            "Loss training: 31.55831\n",
            "\n",
            "***************************\n",
            "Trained on 760 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.555614\n",
            "Loss training: 32.01178\n",
            "Loss training: 27.060345\n",
            "Loss training: 15.381576\n",
            "Loss training: 34.221508\n",
            "Loss training: 31.961372\n",
            "Loss training: 31.905073\n",
            "Loss training: 33.310547\n",
            "Loss training: 30.026714\n",
            "Loss training: 32.016182\n",
            "\n",
            "***************************\n",
            "Trained on 770 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.342976\n",
            "Loss training: 29.5852\n",
            "Loss training: 30.778736\n",
            "Loss training: 31.908516\n",
            "Loss training: 33.759014\n",
            "Loss training: 34.04948\n",
            "Loss training: 42.786884\n",
            "Loss training: 14.849445\n",
            "Loss training: 71.546486\n",
            "Loss training: 33.735214\n",
            "\n",
            "***************************\n",
            "Trained on 780 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.74535\n",
            "Loss training: 38.430702\n",
            "Loss training: 44.5674\n",
            "Loss training: 40.62979\n",
            "Loss training: 38.57049\n",
            "Loss training: 32.563602\n",
            "Loss training: 40.32363\n",
            "Loss training: 44.48009\n",
            "Loss training: 30.947376\n",
            "Loss training: 45.61228\n",
            "\n",
            "***************************\n",
            "Trained on 790 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 48.03689\n",
            "Loss training: 31.806355\n",
            "Loss training: 40.05327\n",
            "Loss training: 39.826244\n",
            "Loss training: 31.489956\n",
            "Loss training: 28.835917\n",
            "Loss training: 23.177898\n",
            "Loss training: 40.341125\n",
            "Loss training: 31.405685\n",
            "Loss training: 40.315704\n",
            "\n",
            "***************************\n",
            "Trained on 800 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 52.512074\n",
            "Loss training: 70.45322\n",
            "Loss training: 24.38538\n",
            "Loss training: 42.528408\n",
            "Loss training: 40.031815\n",
            "Loss training: 42.620834\n",
            "Loss training: 43.415707\n",
            "Loss training: 43.16757\n",
            "Loss training: 40.08446\n",
            "Loss training: 47.22834\n",
            "\n",
            "***************************\n",
            "Trained on 810 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.792675\n",
            "Loss training: 40.036064\n",
            "Loss training: 40.669983\n",
            "Loss training: 42.768867\n",
            "Loss training: 37.10108\n",
            "Loss training: 31.730171\n",
            "Loss training: 46.111282\n",
            "Loss training: 29.226156\n",
            "Loss training: 50.353218\n",
            "Loss training: 29.601913\n",
            "\n",
            "***************************\n",
            "Trained on 820 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 49.450676\n",
            "Loss training: 33.187042\n",
            "Loss training: 45.16433\n",
            "Loss training: 45.031105\n",
            "Loss training: 39.795624\n",
            "Loss training: 33.207253\n",
            "Loss training: 32.52981\n",
            "Loss training: 38.02445\n",
            "Loss training: 48.69515\n",
            "Loss training: 42.399715\n",
            "\n",
            "***************************\n",
            "Trained on 830 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.61024\n",
            "Loss training: 44.157223\n",
            "Loss training: 16.000227\n",
            "Loss training: 40.190666\n",
            "Loss training: 30.979128\n",
            "Loss training: 22.829075\n",
            "Loss training: 36.749245\n",
            "Loss training: 30.921463\n",
            "Loss training: 34.130146\n",
            "Loss training: 33.14296\n",
            "\n",
            "***************************\n",
            "Trained on 840 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.12926\n",
            "Loss training: 36.32374\n",
            "Loss training: 49.572895\n",
            "Loss training: 32.29218\n",
            "Loss training: 44.045807\n",
            "Loss training: 31.95342\n",
            "Loss training: 31.753727\n",
            "Loss training: 31.384768\n",
            "Loss training: 50.466198\n",
            "Loss training: 39.83845\n",
            "\n",
            "***************************\n",
            "Trained on 850 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.387905\n",
            "Loss training: 45.531494\n",
            "Loss training: 43.438\n",
            "Loss training: 51.165085\n",
            "Loss training: 30.733864\n",
            "Loss training: 31.496758\n",
            "Loss training: 41.232536\n",
            "Loss training: 37.45656\n",
            "Loss training: 26.442001\n",
            "Loss training: 32.27446\n",
            "\n",
            "***************************\n",
            "Trained on 860 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.302574\n",
            "Loss training: 32.847565\n",
            "Loss training: 31.997847\n",
            "Loss training: 29.463238\n",
            "Loss training: 36.1819\n",
            "Loss training: 36.113434\n",
            "Loss training: 28.567995\n",
            "Loss training: 42.485855\n",
            "Loss training: 41.014145\n",
            "Loss training: 41.878826\n",
            "\n",
            "***************************\n",
            "Trained on 870 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.495207\n",
            "Loss training: 40.948204\n",
            "Loss training: 48.614452\n",
            "Loss training: 40.757492\n",
            "Loss training: 14.4788885\n",
            "Loss training: 33.038998\n",
            "Loss training: 25.203873\n",
            "Loss training: 33.855988\n",
            "Loss training: 50.244354\n",
            "Loss training: 46.59693\n",
            "\n",
            "***************************\n",
            "Trained on 880 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.600937\n",
            "Loss training: 46.47381\n",
            "Loss training: 44.59717\n",
            "Loss training: 40.49541\n",
            "Loss training: 31.933481\n",
            "Loss training: 46.263832\n",
            "Loss training: 47.275074\n",
            "Loss training: 47.05495\n",
            "Loss training: 30.797949\n",
            "Loss training: 44.508087\n",
            "\n",
            "***************************\n",
            "Trained on 890 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 16.012228\n",
            "Loss training: 22.811237\n",
            "Loss training: 43.013046\n",
            "Loss training: 30.92878\n",
            "Loss training: 46.52842\n",
            "Loss training: 34.719105\n",
            "Loss training: 32.630535\n",
            "Loss training: 41.027077\n",
            "Loss training: 40.84374\n",
            "Loss training: 44.64313\n",
            "\n",
            "***************************\n",
            "Trained on 900 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.426132\n",
            "Loss training: 42.63803\n",
            "Loss training: 31.514668\n",
            "Loss training: 28.979536\n",
            "Loss training: 42.458035\n",
            "Loss training: 71.16739\n",
            "Loss training: 25.907555\n",
            "Loss training: 44.179012\n",
            "Loss training: 24.821018\n",
            "Loss training: 24.497292\n",
            "\n",
            "***************************\n",
            "Trained on 910 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.450153\n",
            "Loss training: 37.180443\n",
            "Loss training: 41.668076\n",
            "Loss training: 34.00239\n",
            "Loss training: 29.715313\n",
            "Loss training: 46.942486\n",
            "Loss training: 29.15066\n",
            "Loss training: 48.774017\n",
            "Loss training: 26.065647\n",
            "Loss training: 35.335835\n",
            "\n",
            "***************************\n",
            "Trained on 920 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.451294\n",
            "Loss training: 28.105322\n",
            "Loss training: 51.96803\n",
            "Loss training: 32.07299\n",
            "Loss training: 26.941795\n",
            "Loss training: 16.31845\n",
            "Loss training: 15.555954\n",
            "Loss training: 33.256363\n",
            "Loss training: 42.3037\n",
            "Loss training: 31.532713\n",
            "\n",
            "***************************\n",
            "Trained on 930 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.30964\n",
            "Loss training: 33.02209\n",
            "Loss training: 36.895805\n",
            "Loss training: 53.429806\n",
            "Loss training: 23.984158\n",
            "Loss training: 33.49787\n",
            "Loss training: 35.61433\n",
            "Loss training: 28.914068\n",
            "Loss training: 32.95157\n",
            "Loss training: 49.89862\n",
            "\n",
            "***************************\n",
            "Trained on 940 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.126762\n",
            "Loss training: 34.453053\n",
            "Loss training: 40.0872\n",
            "Loss training: 41.726063\n",
            "Loss training: 33.03704\n",
            "Loss training: 46.708\n",
            "Loss training: 44.767384\n",
            "Loss training: 31.905071\n",
            "Loss training: 48.2928\n",
            "Loss training: 38.274925\n",
            "\n",
            "***************************\n",
            "Trained on 950 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.56634\n",
            "Loss training: 42.1049\n",
            "Loss training: 33.60614\n",
            "Loss training: 33.198517\n",
            "Loss training: 27.36727\n",
            "Loss training: 30.15391\n",
            "Loss training: 33.008278\n",
            "Loss training: 47.941616\n",
            "Loss training: 44.715134\n",
            "Loss training: 22.46978\n",
            "\n",
            "***************************\n",
            "Trained on 960 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.610893\n",
            "Loss training: 44.492313\n",
            "Loss training: 46.116596\n",
            "Loss training: 30.759031\n",
            "Loss training: 47.421856\n",
            "Loss training: 42.150703\n",
            "Loss training: 29.157324\n",
            "Loss training: 32.89379\n",
            "Loss training: 41.041294\n",
            "Loss training: 35.246014\n",
            "\n",
            "***************************\n",
            "Trained on 970 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.387995\n",
            "Loss training: 45.168854\n",
            "Loss training: 49.70909\n",
            "Loss training: 52.009884\n",
            "Loss training: 45.440308\n",
            "Loss training: 45.21682\n",
            "Loss training: 33.208847\n",
            "Loss training: 31.426826\n",
            "Loss training: 29.485153\n",
            "Loss training: 32.10024\n",
            "\n",
            "***************************\n",
            "Trained on 980 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.221878\n",
            "Loss training: 44.846897\n",
            "Loss training: 34.443474\n",
            "Loss training: 38.62287\n",
            "Loss training: 31.428064\n",
            "Loss training: 15.545817\n",
            "Loss training: 26.5318\n",
            "Loss training: 30.565407\n",
            "Loss training: 29.86667\n",
            "Loss training: 25.709759\n",
            "\n",
            "***************************\n",
            "Trained on 990 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.96718\n",
            "Loss training: 33.745018\n",
            "Loss training: 25.617048\n",
            "Loss training: 30.026466\n",
            "Loss training: 79.02944\n",
            "Loss training: 25.936104\n",
            "Loss training: 41.38613\n",
            "Loss training: 43.43066\n",
            "Loss training: 40.671795\n",
            "Loss training: 29.882843\n",
            "\n",
            "***************************\n",
            "Trained on 1000 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.6457\n",
            "Loss training: 29.38718\n",
            "Loss training: 40.000313\n",
            "Loss training: 28.945314\n",
            "Loss training: 16.811888\n",
            "Loss training: 42.88797\n",
            "Loss training: 43.98985\n",
            "Loss training: 30.153963\n",
            "Loss training: 45.47141\n",
            "Loss training: 28.50756\n",
            "\n",
            "***************************\n",
            "Trained on 1010 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.439137\n",
            "Loss training: 44.336967\n",
            "Loss training: 25.487982\n",
            "Loss training: 29.687748\n",
            "Loss training: 40.787296\n",
            "Loss training: 25.278833\n",
            "Loss training: 37.15896\n",
            "Loss training: 40.10751\n",
            "Loss training: 45.884434\n",
            "Loss training: 44.07014\n",
            "\n",
            "***************************\n",
            "Trained on 1020 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.70393\n",
            "Loss training: 42.236443\n",
            "Loss training: 49.3466\n",
            "Loss training: 37.158566\n",
            "Loss training: 44.62826\n",
            "Loss training: 37.30518\n",
            "Loss training: 30.324802\n",
            "Loss training: 44.74381\n",
            "Loss training: 28.408636\n",
            "Loss training: 44.09567\n",
            "\n",
            "***************************\n",
            "Trained on 1030 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 14.755538\n",
            "Loss training: 52.668095\n",
            "Loss training: 43.867542\n",
            "Loss training: 40.40831\n",
            "Loss training: 49.769794\n",
            "Loss training: 43.403904\n",
            "Loss training: 51.09942\n",
            "Loss training: 24.848835\n",
            "Loss training: 45.77195\n",
            "Loss training: 31.302998\n",
            "\n",
            "***************************\n",
            "Trained on 1040 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.111835\n",
            "Loss training: 48.15997\n",
            "Loss training: 34.61178\n",
            "Loss training: 30.49069\n",
            "Loss training: 41.66924\n",
            "Loss training: 47.287266\n",
            "Loss training: 33.71761\n",
            "Loss training: 49.576805\n",
            "Loss training: 41.161808\n",
            "Loss training: 44.147793\n",
            "\n",
            "***************************\n",
            "Trained on 1050 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.945694\n",
            "Loss training: 31.7326\n",
            "Loss training: 24.843128\n",
            "Loss training: 34.2802\n",
            "Loss training: 51.434223\n",
            "Loss training: 34.05719\n",
            "Loss training: 41.905224\n",
            "Loss training: 40.24844\n",
            "Loss training: 14.214428\n",
            "Loss training: 43.486347\n",
            "\n",
            "***************************\n",
            "Trained on 1060 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.016634\n",
            "Loss training: 32.756252\n",
            "Loss training: 32.495564\n",
            "Loss training: 41.66809\n",
            "Loss training: 81.42917\n",
            "Loss training: 14.307389\n",
            "Loss training: 40.779045\n",
            "Loss training: 44.30746\n",
            "Loss training: 28.453972\n",
            "Loss training: 46.915813\n",
            "\n",
            "***************************\n",
            "Trained on 1070 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.009333\n",
            "Loss training: 50.580383\n",
            "Loss training: 15.089947\n",
            "Loss training: 15.135662\n",
            "Loss training: 47.664925\n",
            "Loss training: 48.71557\n",
            "Loss training: 45.040154\n",
            "Loss training: 34.098476\n",
            "Loss training: 31.815273\n",
            "Loss training: 34.354763\n",
            "\n",
            "***************************\n",
            "Trained on 1080 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 43.17847\n",
            "Loss training: 30.396551\n",
            "Loss training: 31.109911\n",
            "Loss training: 24.6018\n",
            "Loss training: 44.872166\n",
            "Loss training: 39.606777\n",
            "Loss training: 29.049238\n",
            "Loss training: 29.731146\n",
            "Loss training: 42.365376\n",
            "Loss training: 41.839848\n",
            "\n",
            "***************************\n",
            "Trained on 1090 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.08166\n",
            "Loss training: 45.904858\n",
            "Loss training: 22.644657\n",
            "Loss training: 31.370293\n",
            "Loss training: 45.775883\n",
            "Loss training: 46.93777\n",
            "Loss training: 52.724453\n",
            "Loss training: 72.95856\n",
            "Loss training: 29.105198\n",
            "Loss training: 34.518867\n",
            "\n",
            "***************************\n",
            "Trained on 1100 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.74534\n",
            "Loss training: 38.99113\n",
            "Loss training: 43.0796\n",
            "Loss training: 42.549213\n",
            "Loss training: 47.528625\n",
            "Loss training: 38.037876\n",
            "Loss training: 33.905262\n",
            "Loss training: 33.77379\n",
            "Loss training: 37.60709\n",
            "Loss training: 45.447525\n",
            "\n",
            "***************************\n",
            "Trained on 1110 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.333416\n",
            "Loss training: 44.304977\n",
            "Loss training: 44.90745\n",
            "Loss training: 46.531223\n",
            "Loss training: 31.611895\n",
            "Loss training: 43.980835\n",
            "Loss training: 33.751217\n",
            "Loss training: 26.089176\n",
            "Loss training: 29.638859\n",
            "Loss training: 30.775198\n",
            "\n",
            "***************************\n",
            "Trained on 1120 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.389843\n",
            "Loss training: 45.836563\n",
            "Loss training: 32.980755\n",
            "Loss training: 25.59088\n",
            "Loss training: 39.34696\n",
            "Loss training: 45.040974\n",
            "Loss training: 24.683882\n",
            "Loss training: 41.000927\n",
            "Loss training: 22.627827\n",
            "Loss training: 39.971878\n",
            "\n",
            "***************************\n",
            "Trained on 1130 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 70.93129\n",
            "Loss training: 38.492184\n",
            "Loss training: 29.648987\n",
            "Loss training: 29.084745\n",
            "Loss training: 31.578438\n",
            "Loss training: 43.652256\n",
            "Loss training: 41.796112\n",
            "Loss training: 29.029655\n",
            "Loss training: 49.59959\n",
            "Loss training: 47.39302\n",
            "\n",
            "***************************\n",
            "Trained on 1140 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 47.003902\n",
            "Loss training: 33.968105\n",
            "Loss training: 33.5507\n",
            "Loss training: 36.702137\n",
            "Loss training: 40.184566\n",
            "Loss training: 32.991547\n",
            "Loss training: 32.843502\n",
            "Loss training: 31.319551\n",
            "Loss training: 39.19769\n",
            "Loss training: 28.249537\n",
            "\n",
            "***************************\n",
            "Trained on 1150 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.311571\n",
            "Loss training: 43.469017\n",
            "Loss training: 43.517452\n",
            "Loss training: 30.580809\n",
            "Loss training: 31.312962\n",
            "Loss training: 43.454437\n",
            "Loss training: 69.24266\n",
            "Loss training: 33.14683\n",
            "Loss training: 29.987326\n",
            "Loss training: 29.341938\n",
            "\n",
            "***************************\n",
            "Trained on 1160 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.995268\n",
            "Loss training: 42.178764\n",
            "Loss training: 51.771786\n",
            "Loss training: 40.928833\n",
            "Loss training: 33.36795\n",
            "Loss training: 28.87597\n",
            "Loss training: 39.990463\n",
            "Loss training: 61.554306\n",
            "Loss training: 60.60909\n",
            "Loss training: 41.886734\n",
            "\n",
            "***************************\n",
            "Trained on 1170 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.253443\n",
            "Loss training: 31.996758\n",
            "Loss training: 40.247196\n",
            "Loss training: 40.214603\n",
            "Loss training: 42.988575\n",
            "Loss training: 36.091904\n",
            "Loss training: 32.59518\n",
            "Loss training: 32.814205\n",
            "Loss training: 45.15719\n",
            "Loss training: 31.363752\n",
            "\n",
            "***************************\n",
            "Trained on 1180 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.809614\n",
            "Loss training: 39.920044\n",
            "Loss training: 31.158253\n",
            "Loss training: 50.444473\n",
            "Loss training: 34.182545\n",
            "Loss training: 43.666775\n",
            "Loss training: 26.53139\n",
            "Loss training: 26.430653\n",
            "Loss training: 43.809708\n",
            "Loss training: 34.167267\n",
            "\n",
            "***************************\n",
            "Trained on 1190 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 47.024933\n",
            "Loss training: 29.523767\n",
            "Loss training: 31.92174\n",
            "Loss training: 28.043299\n",
            "Loss training: 33.049118\n",
            "Loss training: 31.426136\n",
            "Loss training: 38.74931\n",
            "Loss training: 42.752743\n",
            "Loss training: 42.436142\n",
            "Loss training: 45.038292\n",
            "\n",
            "***************************\n",
            "Trained on 1200 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 47.270897\n",
            "Loss training: 33.46736\n",
            "Loss training: 41.43173\n",
            "Loss training: 29.365772\n",
            "Loss training: 42.817932\n",
            "Loss training: 29.065239\n",
            "Loss training: 25.971386\n",
            "Loss training: 15.2921\n",
            "Loss training: 44.110523\n",
            "Loss training: 29.53713\n",
            "\n",
            "***************************\n",
            "Trained on 1210 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 43.96359\n",
            "Loss training: 43.722458\n",
            "Loss training: 29.53844\n",
            "Loss training: 29.340294\n",
            "Loss training: 22.763403\n",
            "Loss training: 41.522003\n",
            "Loss training: 28.480099\n",
            "Loss training: 42.872284\n",
            "Loss training: 38.194706\n",
            "Loss training: 45.716198\n",
            "\n",
            "***************************\n",
            "Trained on 1220 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.942642\n",
            "Loss training: 40.360172\n",
            "Loss training: 51.192196\n",
            "Loss training: 42.695465\n",
            "Loss training: 39.12097\n",
            "Loss training: 25.06607\n",
            "Loss training: 49.566704\n",
            "Loss training: 50.982117\n",
            "Loss training: 35.856907\n",
            "Loss training: 42.447384\n",
            "\n",
            "***************************\n",
            "Trained on 1230 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.22713\n",
            "Loss training: 23.35654\n",
            "Loss training: 46.039097\n",
            "Loss training: 47.332127\n",
            "Loss training: 44.13845\n",
            "Loss training: 40.330612\n",
            "Loss training: 46.914993\n",
            "Loss training: 47.746876\n",
            "Loss training: 39.252476\n",
            "Loss training: 42.805046\n",
            "\n",
            "***************************\n",
            "Trained on 1240 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.299625\n",
            "Loss training: 69.131805\n",
            "Loss training: 39.291214\n",
            "Loss training: 31.575144\n",
            "Loss training: 26.376099\n",
            "Loss training: 40.635265\n",
            "Loss training: 39.39513\n",
            "Loss training: 43.776413\n",
            "Loss training: 46.279858\n",
            "Loss training: 25.830921\n",
            "\n",
            "***************************\n",
            "Trained on 1250 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.94205\n",
            "Loss training: 38.418922\n",
            "Loss training: 45.87776\n",
            "Loss training: 28.403223\n",
            "Loss training: 41.702038\n",
            "Loss training: 43.027924\n",
            "Loss training: 37.248882\n",
            "Loss training: 45.21036\n",
            "Loss training: 47.41041\n",
            "Loss training: 29.391083\n",
            "\n",
            "***************************\n",
            "Trained on 1260 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 51.704998\n",
            "Loss training: 31.890566\n",
            "Loss training: 39.54292\n",
            "Loss training: 43.31988\n",
            "Loss training: 41.980602\n",
            "Loss training: 28.695139\n",
            "Loss training: 32.57068\n",
            "Loss training: 39.577976\n",
            "Loss training: 46.73128\n",
            "Loss training: 28.735336\n",
            "\n",
            "***************************\n",
            "Trained on 1270 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 46.66489\n",
            "Loss training: 31.253633\n",
            "Loss training: 32.310772\n",
            "Loss training: 38.177532\n",
            "Loss training: 50.04943\n",
            "Loss training: 30.9546\n",
            "Loss training: 40.616608\n",
            "Loss training: 25.279655\n",
            "Loss training: 41.706\n",
            "Loss training: 44.26589\n",
            "\n",
            "***************************\n",
            "Trained on 1280 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.18988\n",
            "Loss training: 14.472715\n",
            "Loss training: 33.708076\n",
            "Loss training: 30.677778\n",
            "Loss training: 69.952354\n",
            "Loss training: 42.79207\n",
            "Loss training: 42.187126\n",
            "Loss training: 24.153353\n",
            "Loss training: 30.592842\n",
            "Loss training: 41.462578\n",
            "\n",
            "***************************\n",
            "Trained on 1290 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 60.205006\n",
            "Loss training: 15.679672\n",
            "Loss training: 57.504536\n",
            "Loss training: 47.284058\n",
            "Loss training: 24.319149\n",
            "Loss training: 42.37037\n",
            "Loss training: 33.940395\n",
            "Loss training: 33.475845\n",
            "Loss training: 41.745792\n",
            "Loss training: 39.68479\n",
            "\n",
            "***************************\n",
            "Trained on 1300 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.80628\n",
            "Loss training: 30.042631\n",
            "Loss training: 39.965263\n",
            "Loss training: 42.49962\n",
            "Loss training: 41.86721\n",
            "Loss training: 46.52223\n",
            "Loss training: 27.84698\n",
            "Loss training: 15.096187\n",
            "Loss training: 40.049297\n",
            "Loss training: 42.438763\n",
            "\n",
            "***************************\n",
            "Trained on 1310 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.31382\n",
            "Loss training: 43.144882\n",
            "Loss training: 43.489166\n",
            "Loss training: 63.497055\n",
            "Loss training: 39.665653\n",
            "Loss training: 47.704\n",
            "Loss training: 39.94649\n",
            "Loss training: 30.27371\n",
            "Loss training: 33.69914\n",
            "Loss training: 14.898681\n",
            "\n",
            "***************************\n",
            "Trained on 1320 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.098705\n",
            "Loss training: 40.281044\n",
            "Loss training: 38.07696\n",
            "Loss training: 32.247574\n",
            "Loss training: 51.69301\n",
            "Loss training: 40.516846\n",
            "Loss training: 51.268745\n",
            "Loss training: 42.53733\n",
            "Loss training: 42.297672\n",
            "Loss training: 14.973475\n",
            "\n",
            "***************************\n",
            "Trained on 1330 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.940882\n",
            "Loss training: 33.32056\n",
            "Loss training: 25.978569\n",
            "Loss training: 48.76076\n",
            "Loss training: 48.436638\n",
            "Loss training: 32.691246\n",
            "Loss training: 41.374428\n",
            "Loss training: 41.292713\n",
            "Loss training: 38.753548\n",
            "Loss training: 40.8366\n",
            "\n",
            "***************************\n",
            "Trained on 1340 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.805588\n",
            "Loss training: 40.700165\n",
            "Loss training: 41.25978\n",
            "Loss training: 40.62077\n",
            "Loss training: 40.71794\n",
            "Loss training: 47.56948\n",
            "Loss training: 40.148235\n",
            "Loss training: 35.301308\n",
            "Loss training: 31.18766\n",
            "Loss training: 40.80863\n",
            "\n",
            "***************************\n",
            "Trained on 1350 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.197342\n",
            "Loss training: 40.974087\n",
            "Loss training: 25.285088\n",
            "Loss training: 40.356445\n",
            "Loss training: 39.095715\n",
            "Loss training: 31.948267\n",
            "Loss training: 38.790054\n",
            "Loss training: 39.874004\n",
            "Loss training: 33.650124\n",
            "Loss training: 14.216111\n",
            "\n",
            "***************************\n",
            "Trained on 1360 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 48.764133\n",
            "Loss training: 32.016567\n",
            "Loss training: 47.912323\n",
            "Loss training: 30.427557\n",
            "Loss training: 31.615892\n",
            "Loss training: 45.601074\n",
            "Loss training: 24.071455\n",
            "Loss training: 32.873585\n",
            "Loss training: 29.52906\n",
            "Loss training: 40.013977\n",
            "\n",
            "***************************\n",
            "Trained on 1370 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.367998\n",
            "Loss training: 23.937088\n",
            "Loss training: 23.935009\n",
            "Loss training: 40.49119\n",
            "Loss training: 51.604\n",
            "Loss training: 38.605633\n",
            "Loss training: 38.418377\n",
            "Loss training: 31.607286\n",
            "Loss training: 31.327208\n",
            "Loss training: 28.870169\n",
            "\n",
            "***************************\n",
            "Trained on 1380 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.343403\n",
            "Loss training: 39.912006\n",
            "Loss training: 43.59758\n",
            "Loss training: 30.96777\n",
            "Loss training: 33.44174\n",
            "Loss training: 30.625103\n",
            "Loss training: 48.95283\n",
            "Loss training: 32.02962\n",
            "Loss training: 39.26578\n",
            "Loss training: 30.702312\n",
            "\n",
            "***************************\n",
            "Trained on 1390 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.405193\n",
            "Loss training: 23.085987\n",
            "Loss training: 39.468025\n",
            "Loss training: 14.327119\n",
            "Loss training: 44.509003\n",
            "Loss training: 33.83725\n",
            "Loss training: 36.89163\n",
            "Loss training: 24.823229\n",
            "Loss training: 42.98543\n",
            "Loss training: 29.569326\n",
            "\n",
            "***************************\n",
            "Trained on 1400 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.881817\n",
            "Loss training: 30.121227\n",
            "Loss training: 39.78672\n",
            "Loss training: 43.38925\n",
            "Loss training: 29.319445\n",
            "Loss training: 25.029198\n",
            "Loss training: 22.047308\n",
            "Loss training: 33.87009\n",
            "Loss training: 21.902596\n",
            "Loss training: 47.28833\n",
            "\n",
            "***************************\n",
            "Trained on 1410 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 46.85972\n",
            "Loss training: 46.081207\n",
            "Loss training: 28.668404\n",
            "Loss training: 33.108826\n",
            "Loss training: 45.333717\n",
            "Loss training: 39.555313\n",
            "Loss training: 31.507896\n",
            "Loss training: 65.15276\n",
            "Loss training: 42.82925\n",
            "Loss training: 29.131388\n",
            "\n",
            "***************************\n",
            "Trained on 1420 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.127449\n",
            "Loss training: 38.788456\n",
            "Loss training: 29.298285\n",
            "Loss training: 40.66953\n",
            "Loss training: 47.807304\n",
            "Loss training: 47.479595\n",
            "Loss training: 28.351254\n",
            "Loss training: 31.10625\n",
            "Loss training: 39.063995\n",
            "Loss training: 38.80002\n",
            "\n",
            "***************************\n",
            "Trained on 1430 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.740223\n",
            "Loss training: 42.79441\n",
            "Loss training: 42.808357\n",
            "Loss training: 26.12023\n",
            "Loss training: 42.190994\n",
            "Loss training: 15.368363\n",
            "Loss training: 63.502533\n",
            "Loss training: 41.810566\n",
            "Loss training: 40.74109\n",
            "Loss training: 33.185955\n",
            "\n",
            "***************************\n",
            "Trained on 1440 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.40891\n",
            "Loss training: 32.274734\n",
            "Loss training: 58.933838\n",
            "Loss training: 32.52311\n",
            "Loss training: 30.894104\n",
            "Loss training: 41.830387\n",
            "Loss training: 41.626957\n",
            "Loss training: 31.993105\n",
            "Loss training: 32.075356\n",
            "Loss training: 41.22568\n",
            "\n",
            "***************************\n",
            "Trained on 1450 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.777885\n",
            "Loss training: 29.844202\n",
            "Loss training: 31.19799\n",
            "Loss training: 28.967718\n",
            "Loss training: 30.92453\n",
            "Loss training: 22.477367\n",
            "Loss training: 38.946186\n",
            "Loss training: 50.77714\n",
            "Loss training: 48.011486\n",
            "Loss training: 50.318233\n",
            "\n",
            "***************************\n",
            "Trained on 1460 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.588863\n",
            "Loss training: 29.25015\n",
            "Loss training: 31.47859\n",
            "Loss training: 25.387342\n",
            "Loss training: 40.48021\n",
            "Loss training: 40.27253\n",
            "Loss training: 25.165724\n",
            "Loss training: 21.913643\n",
            "Loss training: 38.490925\n",
            "Loss training: 32.708538\n",
            "\n",
            "***************************\n",
            "Trained on 1470 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.2321\n",
            "Loss training: 29.014196\n",
            "Loss training: 41.24522\n",
            "Loss training: 31.341148\n",
            "Loss training: 38.187637\n",
            "Loss training: 32.42434\n",
            "Loss training: 39.936592\n",
            "Loss training: 37.015575\n",
            "Loss training: 40.513203\n",
            "Loss training: 24.71433\n",
            "\n",
            "***************************\n",
            "Trained on 1480 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.049156\n",
            "Loss training: 44.743904\n",
            "Loss training: 34.110462\n",
            "Loss training: 22.514221\n",
            "Loss training: 38.4669\n",
            "Loss training: 35.860184\n",
            "Loss training: 38.563408\n",
            "Loss training: 39.739117\n",
            "Loss training: 32.610443\n",
            "Loss training: 42.230686\n",
            "\n",
            "***************************\n",
            "Trained on 1490 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.070568\n",
            "Loss training: 39.62479\n",
            "Loss training: 43.083828\n",
            "Loss training: 28.177027\n",
            "Loss training: 21.631802\n",
            "Loss training: 37.819\n",
            "Loss training: 41.18118\n",
            "Loss training: 39.717487\n",
            "Loss training: 47.63088\n",
            "Loss training: 31.578856\n",
            "\n",
            "***************************\n",
            "Trained on 1500 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.549892\n",
            "Loss training: 40.75163\n",
            "Loss training: 38.61956\n",
            "Loss training: 34.8812\n",
            "Loss training: 47.540375\n",
            "Loss training: 38.344135\n",
            "Loss training: 28.694675\n",
            "Loss training: 38.158203\n",
            "Loss training: 31.90904\n",
            "Loss training: 34.175793\n",
            "\n",
            "***************************\n",
            "Trained on 1510 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.663977\n",
            "Loss training: 29.660732\n",
            "Loss training: 36.27304\n",
            "Loss training: 32.929825\n",
            "Loss training: 74.71631\n",
            "Loss training: 36.1199\n",
            "Loss training: 38.122128\n",
            "Loss training: 40.22125\n",
            "Loss training: 29.340132\n",
            "Loss training: 37.937115\n",
            "\n",
            "***************************\n",
            "Trained on 1520 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.04524\n",
            "Loss training: 36.906784\n",
            "Loss training: 32.594128\n",
            "Loss training: 23.706377\n",
            "Loss training: 28.916483\n",
            "Loss training: 32.214973\n",
            "Loss training: 47.07448\n",
            "Loss training: 30.984167\n",
            "Loss training: 28.955662\n",
            "Loss training: 49.38881\n",
            "\n",
            "***************************\n",
            "Trained on 1530 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.87325\n",
            "Loss training: 27.763416\n",
            "Loss training: 39.195187\n",
            "Loss training: 29.955568\n",
            "Loss training: 38.001633\n",
            "Loss training: 14.964764\n",
            "Loss training: 33.88337\n",
            "Loss training: 37.503754\n",
            "Loss training: 39.031494\n",
            "Loss training: 29.023962\n",
            "\n",
            "***************************\n",
            "Trained on 1540 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.399704\n",
            "Loss training: 27.961676\n",
            "Loss training: 29.396164\n",
            "Loss training: 29.226076\n",
            "Loss training: 24.204311\n",
            "Loss training: 31.828997\n",
            "Loss training: 31.72452\n",
            "Loss training: 27.570889\n",
            "Loss training: 28.956337\n",
            "Loss training: 29.64011\n",
            "\n",
            "***************************\n",
            "Trained on 1550 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.154346\n",
            "Loss training: 31.004414\n",
            "Loss training: 50.66985\n",
            "Loss training: 42.40744\n",
            "Loss training: 31.872208\n",
            "Loss training: 45.03021\n",
            "Loss training: 45.655865\n",
            "Loss training: 51.044876\n",
            "Loss training: 37.753605\n",
            "Loss training: 32.482777\n",
            "\n",
            "***************************\n",
            "Trained on 1560 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 15.855451\n",
            "Loss training: 45.52884\n",
            "Loss training: 28.929394\n",
            "Loss training: 44.76843\n",
            "Loss training: 31.39064\n",
            "Loss training: 34.60634\n",
            "Loss training: 44.713703\n",
            "Loss training: 33.898777\n",
            "Loss training: 30.311443\n",
            "Loss training: 29.4568\n",
            "\n",
            "***************************\n",
            "Trained on 1570 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 71.02644\n",
            "Loss training: 43.84231\n",
            "Loss training: 14.18079\n",
            "Loss training: 30.463167\n",
            "Loss training: 32.650696\n",
            "Loss training: 31.254229\n",
            "Loss training: 33.6156\n",
            "Loss training: 42.564743\n",
            "Loss training: 30.305387\n",
            "Loss training: 29.025867\n",
            "\n",
            "***************************\n",
            "Trained on 1580 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.34981\n",
            "Loss training: 23.975216\n",
            "Loss training: 43.65095\n",
            "Loss training: 23.635344\n",
            "Loss training: 62.223835\n",
            "Loss training: 23.27917\n",
            "Loss training: 22.711876\n",
            "Loss training: 28.97943\n",
            "Loss training: 31.884705\n",
            "Loss training: 43.452454\n",
            "\n",
            "***************************\n",
            "Trained on 1590 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.902157\n",
            "Loss training: 36.073235\n",
            "Loss training: 28.735615\n",
            "Loss training: 35.15521\n",
            "Loss training: 27.55698\n",
            "Loss training: 31.57356\n",
            "Loss training: 39.001656\n",
            "Loss training: 41.923214\n",
            "Loss training: 40.206867\n",
            "Loss training: 28.499937\n",
            "\n",
            "***************************\n",
            "Trained on 1600 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.836784\n",
            "Loss training: 42.62498\n",
            "Loss training: 39.093277\n",
            "Loss training: 25.822348\n",
            "Loss training: 39.39811\n",
            "Loss training: 37.092144\n",
            "Loss training: 38.47046\n",
            "Loss training: 36.472683\n",
            "Loss training: 68.184654\n",
            "Loss training: 14.625246\n",
            "\n",
            "***************************\n",
            "Trained on 1610 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.946987\n",
            "Loss training: 40.73762\n",
            "Loss training: 51.724247\n",
            "Loss training: 63.395237\n",
            "Loss training: 60.912357\n",
            "Loss training: 31.494001\n",
            "Loss training: 40.2288\n",
            "Loss training: 32.210133\n",
            "Loss training: 27.636648\n",
            "Loss training: 41.312874\n",
            "\n",
            "***************************\n",
            "Trained on 1620 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 49.23104\n",
            "Loss training: 43.40518\n",
            "Loss training: 16.7088\n",
            "Loss training: 27.486156\n",
            "Loss training: 41.09762\n",
            "Loss training: 27.390608\n",
            "Loss training: 15.40485\n",
            "Loss training: 40.0839\n",
            "Loss training: 37.657005\n",
            "Loss training: 29.620459\n",
            "\n",
            "***************************\n",
            "Trained on 1630 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.643793\n",
            "Loss training: 27.518583\n",
            "Loss training: 14.478652\n",
            "Loss training: 40.58668\n",
            "Loss training: 40.140903\n",
            "Loss training: 32.29611\n",
            "Loss training: 50.51318\n",
            "Loss training: 68.09561\n",
            "Loss training: 31.809357\n",
            "Loss training: 22.151194\n",
            "\n",
            "***************************\n",
            "Trained on 1640 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 14.782929\n",
            "Loss training: 32.226917\n",
            "Loss training: 58.18679\n",
            "Loss training: 46.47209\n",
            "Loss training: 36.711296\n",
            "Loss training: 32.60811\n",
            "Loss training: 46.271347\n",
            "Loss training: 36.767666\n",
            "Loss training: 32.69462\n",
            "Loss training: 28.580828\n",
            "\n",
            "***************************\n",
            "Trained on 1650 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.51699\n",
            "Loss training: 40.22966\n",
            "Loss training: 29.518145\n",
            "Loss training: 30.084347\n",
            "Loss training: 39.30336\n",
            "Loss training: 30.12331\n",
            "Loss training: 28.520382\n",
            "Loss training: 44.387245\n",
            "Loss training: 28.002058\n",
            "Loss training: 23.64874\n",
            "\n",
            "***************************\n",
            "Trained on 1660 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.57118\n",
            "Loss training: 38.10702\n",
            "Loss training: 45.730106\n",
            "Loss training: 48.831802\n",
            "Loss training: 31.589354\n",
            "Loss training: 44.79712\n",
            "Loss training: 52.65548\n",
            "Loss training: 32.699486\n",
            "Loss training: 38.90264\n",
            "Loss training: 45.28473\n",
            "\n",
            "***************************\n",
            "Trained on 1670 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 43.079605\n",
            "Loss training: 39.125607\n",
            "Loss training: 30.587732\n",
            "Loss training: 35.39272\n",
            "Loss training: 60.15293\n",
            "Loss training: 38.614388\n",
            "Loss training: 43.322\n",
            "Loss training: 43.66464\n",
            "Loss training: 28.327507\n",
            "Loss training: 30.900663\n",
            "\n",
            "***************************\n",
            "Trained on 1680 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.442726\n",
            "Loss training: 27.258425\n",
            "Loss training: 40.303\n",
            "Loss training: 34.54709\n",
            "Loss training: 45.963623\n",
            "Loss training: 46.818035\n",
            "Loss training: 43.428738\n",
            "Loss training: 38.44485\n",
            "Loss training: 28.61683\n",
            "Loss training: 38.874603\n",
            "\n",
            "***************************\n",
            "Trained on 1690 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.34189\n",
            "Loss training: 39.56506\n",
            "Loss training: 60.893936\n",
            "Loss training: 29.64111\n",
            "Loss training: 50.005936\n",
            "Loss training: 34.347588\n",
            "Loss training: 38.96798\n",
            "Loss training: 30.823788\n",
            "Loss training: 14.460801\n",
            "Loss training: 41.81135\n",
            "\n",
            "***************************\n",
            "Trained on 1700 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.593483\n",
            "Loss training: 37.461735\n",
            "Loss training: 37.546345\n",
            "Loss training: 27.55219\n",
            "Loss training: 33.635975\n",
            "Loss training: 22.400238\n",
            "Loss training: 39.46698\n",
            "Loss training: 32.322918\n",
            "Loss training: 36.212067\n",
            "Loss training: 32.46101\n",
            "\n",
            "***************************\n",
            "Trained on 1710 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.894123\n",
            "Loss training: 38.247772\n",
            "Loss training: 39.972054\n",
            "Loss training: 13.908011\n",
            "Loss training: 32.56903\n",
            "Loss training: 31.2009\n",
            "Loss training: 28.619827\n",
            "Loss training: 64.3008\n",
            "Loss training: 43.411247\n",
            "Loss training: 40.35309\n",
            "\n",
            "***************************\n",
            "Trained on 1720 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 14.784534\n",
            "Loss training: 27.739908\n",
            "Loss training: 44.515957\n",
            "Loss training: 22.27756\n",
            "Loss training: 49.322395\n",
            "Loss training: 29.67637\n",
            "Loss training: 31.794178\n",
            "Loss training: 28.798985\n",
            "Loss training: 41.30532\n",
            "Loss training: 40.51994\n",
            "\n",
            "***************************\n",
            "Trained on 1730 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.707504\n",
            "Loss training: 25.93091\n",
            "Loss training: 28.401424\n",
            "Loss training: 23.654097\n",
            "Loss training: 27.29853\n",
            "Loss training: 30.964216\n",
            "Loss training: 48.00275\n",
            "Loss training: 42.548283\n",
            "Loss training: 34.121025\n",
            "Loss training: 38.794952\n",
            "\n",
            "***************************\n",
            "Trained on 1740 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.38461\n",
            "Loss training: 38.547245\n",
            "Loss training: 38.2848\n",
            "Loss training: 27.42565\n",
            "Loss training: 29.287445\n",
            "Loss training: 29.191467\n",
            "Loss training: 42.25916\n",
            "Loss training: 31.074198\n",
            "Loss training: 33.749493\n",
            "Loss training: 27.193514\n",
            "\n",
            "***************************\n",
            "Trained on 1750 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 48.004837\n",
            "Loss training: 42.714268\n",
            "Loss training: 39.672\n",
            "Loss training: 32.61756\n",
            "Loss training: 30.7024\n",
            "Loss training: 27.97729\n",
            "Loss training: 39.70091\n",
            "Loss training: 27.072004\n",
            "Loss training: 38.1461\n",
            "Loss training: 38.970856\n",
            "\n",
            "***************************\n",
            "Trained on 1760 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.520554\n",
            "Loss training: 29.11304\n",
            "Loss training: 27.963606\n",
            "Loss training: 62.134205\n",
            "Loss training: 28.03169\n",
            "Loss training: 21.984802\n",
            "Loss training: 28.854544\n",
            "Loss training: 38.54563\n",
            "Loss training: 30.042711\n",
            "Loss training: 34.325718\n",
            "\n",
            "***************************\n",
            "Trained on 1770 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.99143\n",
            "Loss training: 35.59896\n",
            "Loss training: 21.43738\n",
            "Loss training: 30.381472\n",
            "Loss training: 47.452427\n",
            "Loss training: 58.79067\n",
            "Loss training: 46.156666\n",
            "Loss training: 29.594397\n",
            "Loss training: 38.33129\n",
            "Loss training: 38.96518\n",
            "\n",
            "***************************\n",
            "Trained on 1780 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 43.53784\n",
            "Loss training: 20.940983\n",
            "Loss training: 29.451899\n",
            "Loss training: 43.79269\n",
            "Loss training: 16.124563\n",
            "Loss training: 48.09939\n",
            "Loss training: 27.656435\n",
            "Loss training: 43.230263\n",
            "Loss training: 30.60811\n",
            "Loss training: 32.25082\n",
            "\n",
            "***************************\n",
            "Trained on 1790 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 52.831387\n",
            "Loss training: 41.360863\n",
            "Loss training: 42.25538\n",
            "Loss training: 23.24213\n",
            "Loss training: 41.695587\n",
            "Loss training: 27.987434\n",
            "Loss training: 27.921778\n",
            "Loss training: 38.297466\n",
            "Loss training: 39.024666\n",
            "Loss training: 49.578213\n",
            "\n",
            "***************************\n",
            "Trained on 1800 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.012093\n",
            "Loss training: 22.37458\n",
            "Loss training: 29.864777\n",
            "Loss training: 35.656708\n",
            "Loss training: 40.605377\n",
            "Loss training: 53.34266\n",
            "Loss training: 47.929478\n",
            "Loss training: 47.530315\n",
            "Loss training: 27.27167\n",
            "Loss training: 28.911299\n",
            "\n",
            "***************************\n",
            "Trained on 1810 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.98419\n",
            "Loss training: 23.025755\n",
            "Loss training: 49.166416\n",
            "Loss training: 45.74221\n",
            "Loss training: 38.09506\n",
            "Loss training: 47.20001\n",
            "Loss training: 34.498817\n",
            "Loss training: 38.54573\n",
            "Loss training: 41.48889\n",
            "Loss training: 41.500404\n",
            "\n",
            "***************************\n",
            "Trained on 1820 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.65819\n",
            "Loss training: 31.351168\n",
            "Loss training: 43.908173\n",
            "Loss training: 27.136707\n",
            "Loss training: 40.65114\n",
            "Loss training: 34.024563\n",
            "Loss training: 36.955338\n",
            "Loss training: 30.399418\n",
            "Loss training: 27.075071\n",
            "Loss training: 47.697346\n",
            "\n",
            "***************************\n",
            "Trained on 1830 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.86208\n",
            "Loss training: 28.250454\n",
            "Loss training: 33.363075\n",
            "Loss training: 39.386383\n",
            "Loss training: 38.750267\n",
            "Loss training: 33.91319\n",
            "Loss training: 36.72869\n",
            "Loss training: 27.238878\n",
            "Loss training: 45.11798\n",
            "Loss training: 42.581573\n",
            "\n",
            "***************************\n",
            "Trained on 1840 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.61554\n",
            "Loss training: 42.184902\n",
            "Loss training: 39.992447\n",
            "Loss training: 39.900177\n",
            "Loss training: 44.764137\n",
            "Loss training: 29.801495\n",
            "Loss training: 14.343616\n",
            "Loss training: 14.260782\n",
            "Loss training: 27.618141\n",
            "Loss training: 38.347164\n",
            "\n",
            "***************************\n",
            "Trained on 1850 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.495995\n",
            "Loss training: 28.972652\n",
            "Loss training: 47.749348\n",
            "Loss training: 41.877075\n",
            "Loss training: 36.794212\n",
            "Loss training: 30.798006\n",
            "Loss training: 46.36055\n",
            "Loss training: 36.82198\n",
            "Loss training: 27.897692\n",
            "Loss training: 44.407104\n",
            "\n",
            "***************************\n",
            "Trained on 1860 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.785313\n",
            "Loss training: 43.628708\n",
            "Loss training: 37.03782\n",
            "Loss training: 32.50428\n",
            "Loss training: 46.573257\n",
            "Loss training: 30.629314\n",
            "Loss training: 69.05956\n",
            "Loss training: 26.674992\n",
            "Loss training: 40.471706\n",
            "Loss training: 44.737576\n",
            "\n",
            "***************************\n",
            "Trained on 1870 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.580154\n",
            "Loss training: 31.202946\n",
            "Loss training: 39.96396\n",
            "Loss training: 39.701977\n",
            "Loss training: 39.78501\n",
            "Loss training: 46.549583\n",
            "Loss training: 43.865063\n",
            "Loss training: 45.909924\n",
            "Loss training: 46.259544\n",
            "Loss training: 41.394604\n",
            "\n",
            "***************************\n",
            "Trained on 1880 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.89198\n",
            "Loss training: 35.2475\n",
            "Loss training: 31.233738\n",
            "Loss training: 27.476868\n",
            "Loss training: 38.26373\n",
            "Loss training: 38.723614\n",
            "Loss training: 21.976395\n",
            "Loss training: 44.398205\n",
            "Loss training: 44.287968\n",
            "Loss training: 31.64019\n",
            "\n",
            "***************************\n",
            "Trained on 1890 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.608936\n",
            "Loss training: 60.914803\n",
            "Loss training: 40.145092\n",
            "Loss training: 41.93388\n",
            "Loss training: 41.967606\n",
            "Loss training: 41.136784\n",
            "Loss training: 21.836515\n",
            "Loss training: 39.788765\n",
            "Loss training: 37.44866\n",
            "Loss training: 40.10084\n",
            "\n",
            "***************************\n",
            "Trained on 1900 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.788227\n",
            "Loss training: 39.23132\n",
            "Loss training: 39.984917\n",
            "Loss training: 37.455124\n",
            "Loss training: 28.84254\n",
            "Loss training: 46.792915\n",
            "Loss training: 25.195415\n",
            "Loss training: 28.120834\n",
            "Loss training: 30.634445\n",
            "Loss training: 41.14524\n",
            "\n",
            "***************************\n",
            "Trained on 1910 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.689247\n",
            "Loss training: 22.99956\n",
            "Loss training: 33.469982\n",
            "Loss training: 31.439138\n",
            "Loss training: 32.618984\n",
            "Loss training: 42.148743\n",
            "Loss training: 35.4944\n",
            "Loss training: 41.58801\n",
            "Loss training: 46.83886\n",
            "Loss training: 37.32557\n",
            "\n",
            "***************************\n",
            "Trained on 1920 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.098366\n",
            "Loss training: 38.968292\n",
            "Loss training: 43.670887\n",
            "Loss training: 36.485924\n",
            "Loss training: 48.450386\n",
            "Loss training: 32.687004\n",
            "Loss training: 46.04675\n",
            "Loss training: 49.45212\n",
            "Loss training: 64.065315\n",
            "Loss training: 32.531094\n",
            "\n",
            "***************************\n",
            "Trained on 1930 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 15.97789\n",
            "Loss training: 31.93494\n",
            "Loss training: 29.85998\n",
            "Loss training: 32.529804\n",
            "Loss training: 40.743275\n",
            "Loss training: 42.853928\n",
            "Loss training: 40.233574\n",
            "Loss training: 27.486486\n",
            "Loss training: 39.083946\n",
            "Loss training: 27.139257\n",
            "\n",
            "***************************\n",
            "Trained on 1940 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.332806\n",
            "Loss training: 25.570734\n",
            "Loss training: 56.59647\n",
            "Loss training: 47.06582\n",
            "Loss training: 40.32184\n",
            "Loss training: 42.09829\n",
            "Loss training: 31.502314\n",
            "Loss training: 31.672075\n",
            "Loss training: 31.480576\n",
            "Loss training: 41.936123\n",
            "\n",
            "***************************\n",
            "Trained on 1950 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.80054\n",
            "Loss training: 32.617996\n",
            "Loss training: 35.6559\n",
            "Loss training: 39.412178\n",
            "Loss training: 30.278374\n",
            "Loss training: 39.689163\n",
            "Loss training: 34.417694\n",
            "Loss training: 38.844925\n",
            "Loss training: 32.153976\n",
            "Loss training: 39.62741\n",
            "\n",
            "***************************\n",
            "Trained on 1960 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 47.374157\n",
            "Loss training: 24.075142\n",
            "Loss training: 45.84075\n",
            "Loss training: 42.091022\n",
            "Loss training: 31.129498\n",
            "Loss training: 37.807007\n",
            "Loss training: 49.768074\n",
            "Loss training: 38.43852\n",
            "Loss training: 43.030495\n",
            "Loss training: 30.434452\n",
            "\n",
            "***************************\n",
            "Trained on 1970 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.751997\n",
            "Loss training: 27.803331\n",
            "Loss training: 38.647594\n",
            "Loss training: 29.96584\n",
            "Loss training: 51.90923\n",
            "Loss training: 30.227945\n",
            "Loss training: 32.283195\n",
            "Loss training: 33.7446\n",
            "Loss training: 29.246195\n",
            "Loss training: 40.020832\n",
            "\n",
            "***************************\n",
            "Trained on 1980 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 48.881374\n",
            "Loss training: 47.122578\n",
            "Loss training: 31.263887\n",
            "Loss training: 27.025974\n",
            "Loss training: 40.486553\n",
            "Loss training: 23.296564\n",
            "Loss training: 55.81156\n",
            "Loss training: 45.32222\n",
            "Loss training: 38.311817\n",
            "Loss training: 50.490833\n",
            "\n",
            "***************************\n",
            "Trained on 1990 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 47.06999\n",
            "Loss training: 39.862312\n",
            "Loss training: 46.549286\n",
            "Loss training: 40.94213\n",
            "Loss training: 28.475582\n",
            "Loss training: 37.757\n",
            "Loss training: 26.656427\n",
            "Loss training: 28.08468\n",
            "Loss training: 40.41722\n",
            "Loss training: 31.675695\n",
            "\n",
            "***************************\n",
            "Trained on 2000 graphs\n",
            "***************************\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_794ebff9-1855-4107-87ea-b71b4a02afa7\", \"params_epochs_2000.pickle\", 22204504)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c52a909c-8b2a-41e2-97b9-0a8ff6dd4f87\", \"opt_state_epochs_2000.pickle\", 44409100)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss training: 29.990026\n",
            "Loss training: 30.777138\n",
            "Loss training: 45.579338\n",
            "Loss training: 35.86273\n",
            "Loss training: 43.78818\n",
            "Loss training: 23.37101\n",
            "Loss training: 40.10294\n",
            "Loss training: 21.665678\n",
            "Loss training: 27.509613\n",
            "Loss training: 29.826664\n",
            "\n",
            "***************************\n",
            "Trained on 2010 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.505955\n",
            "Loss training: 50.53469\n",
            "Loss training: 36.389763\n",
            "Loss training: 20.920153\n",
            "Loss training: 39.654144\n",
            "Loss training: 39.207237\n",
            "Loss training: 50.51307\n",
            "Loss training: 37.72933\n",
            "Loss training: 41.66285\n",
            "Loss training: 20.81236\n",
            "\n",
            "***************************\n",
            "Trained on 2020 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.982761\n",
            "Loss training: 33.7972\n",
            "Loss training: 32.142944\n",
            "Loss training: 38.965294\n",
            "Loss training: 29.9451\n",
            "Loss training: 41.171986\n",
            "Loss training: 39.25287\n",
            "Loss training: 33.473244\n",
            "Loss training: 36.82729\n",
            "Loss training: 40.448612\n",
            "\n",
            "***************************\n",
            "Trained on 2030 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.1385\n",
            "Loss training: 24.642736\n",
            "Loss training: 36.710197\n",
            "Loss training: 33.333862\n",
            "Loss training: 26.456318\n",
            "Loss training: 26.775328\n",
            "Loss training: 40.008595\n",
            "Loss training: 37.369373\n",
            "Loss training: 38.87466\n",
            "Loss training: 35.569687\n",
            "\n",
            "***************************\n",
            "Trained on 2040 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.209236\n",
            "Loss training: 33.630505\n",
            "Loss training: 30.011726\n",
            "Loss training: 33.664555\n",
            "Loss training: 43.866013\n",
            "Loss training: 40.544445\n",
            "Loss training: 42.983837\n",
            "Loss training: 27.308289\n",
            "Loss training: 27.809366\n",
            "Loss training: 36.934727\n",
            "\n",
            "***************************\n",
            "Trained on 2050 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.480148\n",
            "Loss training: 20.363852\n",
            "Loss training: 39.47868\n",
            "Loss training: 27.141142\n",
            "Loss training: 30.45079\n",
            "Loss training: 27.68947\n",
            "Loss training: 46.83231\n",
            "Loss training: 28.593605\n",
            "Loss training: 23.1237\n",
            "Loss training: 27.347235\n",
            "\n",
            "***************************\n",
            "Trained on 2060 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.482143\n",
            "Loss training: 39.0615\n",
            "Loss training: 29.55545\n",
            "Loss training: 40.318043\n",
            "Loss training: 33.27786\n",
            "Loss training: 26.498104\n",
            "Loss training: 29.048622\n",
            "Loss training: 42.33817\n",
            "Loss training: 36.983776\n",
            "Loss training: 47.238796\n",
            "\n",
            "***************************\n",
            "Trained on 2070 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.080357\n",
            "Loss training: 42.22547\n",
            "Loss training: 32.959373\n",
            "Loss training: 29.385164\n",
            "Loss training: 35.664696\n",
            "Loss training: 36.999718\n",
            "Loss training: 31.296171\n",
            "Loss training: 61.423363\n",
            "Loss training: 29.872566\n",
            "Loss training: 23.139763\n",
            "\n",
            "***************************\n",
            "Trained on 2080 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.228127\n",
            "Loss training: 25.340431\n",
            "Loss training: 35.69001\n",
            "Loss training: 37.768635\n",
            "Loss training: 20.57188\n",
            "Loss training: 46.82023\n",
            "Loss training: 53.69417\n",
            "Loss training: 37.386555\n",
            "Loss training: 28.84168\n",
            "Loss training: 47.73744\n",
            "\n",
            "***************************\n",
            "Trained on 2090 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.478064\n",
            "Loss training: 38.825344\n",
            "Loss training: 38.008923\n",
            "Loss training: 39.804825\n",
            "Loss training: 39.74516\n",
            "Loss training: 39.119976\n",
            "Loss training: 36.207867\n",
            "Loss training: 43.796528\n",
            "Loss training: 43.24676\n",
            "Loss training: 39.588768\n",
            "\n",
            "***************************\n",
            "Trained on 2100 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.460636\n",
            "Loss training: 36.522404\n",
            "Loss training: 29.51857\n",
            "Loss training: 14.797755\n",
            "Loss training: 27.469973\n",
            "Loss training: 22.371994\n",
            "Loss training: 31.311684\n",
            "Loss training: 29.033037\n",
            "Loss training: 41.15102\n",
            "Loss training: 31.588058\n",
            "\n",
            "***************************\n",
            "Trained on 2110 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 64.09837\n",
            "Loss training: 49.171497\n",
            "Loss training: 38.86511\n",
            "Loss training: 53.999626\n",
            "Loss training: 34.011044\n",
            "Loss training: 29.45036\n",
            "Loss training: 46.66873\n",
            "Loss training: 16.949442\n",
            "Loss training: 41.929127\n",
            "Loss training: 31.741398\n",
            "\n",
            "***************************\n",
            "Trained on 2120 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.446533\n",
            "Loss training: 30.248463\n",
            "Loss training: 43.7032\n",
            "Loss training: 14.996562\n",
            "Loss training: 40.692005\n",
            "Loss training: 49.998077\n",
            "Loss training: 39.291557\n",
            "Loss training: 40.142807\n",
            "Loss training: 31.650028\n",
            "Loss training: 41.617424\n",
            "\n",
            "***************************\n",
            "Trained on 2130 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.322514\n",
            "Loss training: 27.57684\n",
            "Loss training: 27.531178\n",
            "Loss training: 44.6713\n",
            "Loss training: 31.069134\n",
            "Loss training: 56.603542\n",
            "Loss training: 41.07834\n",
            "Loss training: 40.235516\n",
            "Loss training: 13.950508\n",
            "Loss training: 27.477324\n",
            "\n",
            "***************************\n",
            "Trained on 2140 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.908812\n",
            "Loss training: 45.540447\n",
            "Loss training: 46.444336\n",
            "Loss training: 27.180431\n",
            "Loss training: 27.09655\n",
            "Loss training: 38.297863\n",
            "Loss training: 45.857246\n",
            "Loss training: 36.98712\n",
            "Loss training: 21.30178\n",
            "Loss training: 26.636982\n",
            "\n",
            "***************************\n",
            "Trained on 2150 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 13.857646\n",
            "Loss training: 39.01185\n",
            "Loss training: 30.925913\n",
            "Loss training: 27.335787\n",
            "Loss training: 29.779224\n",
            "Loss training: 22.935652\n",
            "Loss training: 34.087196\n",
            "Loss training: 45.00229\n",
            "Loss training: 37.817993\n",
            "Loss training: 27.074062\n",
            "\n",
            "***************************\n",
            "Trained on 2160 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.514313\n",
            "Loss training: 45.120865\n",
            "Loss training: 35.824127\n",
            "Loss training: 28.639036\n",
            "Loss training: 32.847496\n",
            "Loss training: 38.06352\n",
            "Loss training: 27.463165\n",
            "Loss training: 24.029161\n",
            "Loss training: 35.572094\n",
            "Loss training: 35.45023\n",
            "\n",
            "***************************\n",
            "Trained on 2170 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.845016\n",
            "Loss training: 25.64416\n",
            "Loss training: 29.961063\n",
            "Loss training: 31.534748\n",
            "Loss training: 22.783611\n",
            "Loss training: 36.463478\n",
            "Loss training: 36.0586\n",
            "Loss training: 41.226467\n",
            "Loss training: 43.66669\n",
            "Loss training: 29.295206\n",
            "\n",
            "***************************\n",
            "Trained on 2180 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.993164\n",
            "Loss training: 28.672655\n",
            "Loss training: 30.157274\n",
            "Loss training: 24.034208\n",
            "Loss training: 23.9646\n",
            "Loss training: 43.494606\n",
            "Loss training: 23.772549\n",
            "Loss training: 29.84616\n",
            "Loss training: 42.686726\n",
            "Loss training: 29.896418\n",
            "\n",
            "***************************\n",
            "Trained on 2190 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.452927\n",
            "Loss training: 23.592733\n",
            "Loss training: 27.11366\n",
            "Loss training: 31.212154\n",
            "Loss training: 41.621063\n",
            "Loss training: 39.4653\n",
            "Loss training: 37.245872\n",
            "Loss training: 40.195095\n",
            "Loss training: 27.348948\n",
            "Loss training: 41.303524\n",
            "\n",
            "***************************\n",
            "Trained on 2200 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.73498\n",
            "Loss training: 25.628515\n",
            "Loss training: 44.43974\n",
            "Loss training: 44.416893\n",
            "Loss training: 27.618525\n",
            "Loss training: 29.928848\n",
            "Loss training: 45.823483\n",
            "Loss training: 36.86328\n",
            "Loss training: 42.93267\n",
            "Loss training: 37.534935\n",
            "\n",
            "***************************\n",
            "Trained on 2210 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.06572\n",
            "Loss training: 13.594217\n",
            "Loss training: 43.458668\n",
            "Loss training: 29.474564\n",
            "Loss training: 43.043217\n",
            "Loss training: 41.873325\n",
            "Loss training: 37.171364\n",
            "Loss training: 27.258999\n",
            "Loss training: 44.70837\n",
            "Loss training: 28.757315\n",
            "\n",
            "***************************\n",
            "Trained on 2220 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 21.172792\n",
            "Loss training: 30.174696\n",
            "Loss training: 22.674198\n",
            "Loss training: 45.268864\n",
            "Loss training: 38.140453\n",
            "Loss training: 28.37367\n",
            "Loss training: 36.4003\n",
            "Loss training: 46.213463\n",
            "Loss training: 27.114336\n",
            "Loss training: 35.415485\n",
            "\n",
            "***************************\n",
            "Trained on 2230 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.78302\n",
            "Loss training: 36.649315\n",
            "Loss training: 13.792949\n",
            "Loss training: 40.6288\n",
            "Loss training: 22.219149\n",
            "Loss training: 25.352417\n",
            "Loss training: 42.25498\n",
            "Loss training: 20.14015\n",
            "Loss training: 43.755177\n",
            "Loss training: 30.555233\n",
            "\n",
            "***************************\n",
            "Trained on 2240 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.175688\n",
            "Loss training: 36.564102\n",
            "Loss training: 36.77606\n",
            "Loss training: 37.539696\n",
            "Loss training: 31.544607\n",
            "Loss training: 36.788334\n",
            "Loss training: 45.611378\n",
            "Loss training: 13.710873\n",
            "Loss training: 33.756035\n",
            "Loss training: 29.563879\n",
            "\n",
            "***************************\n",
            "Trained on 2250 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.522686\n",
            "Loss training: 35.331333\n",
            "Loss training: 44.11456\n",
            "Loss training: 28.955702\n",
            "Loss training: 19.767668\n",
            "Loss training: 33.722736\n",
            "Loss training: 28.802334\n",
            "Loss training: 24.814682\n",
            "Loss training: 19.23181\n",
            "Loss training: 44.153084\n",
            "\n",
            "***************************\n",
            "Trained on 2260 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.027378\n",
            "Loss training: 13.795814\n",
            "Loss training: 43.581955\n",
            "Loss training: 47.03479\n",
            "Loss training: 42.565674\n",
            "Loss training: 41.941765\n",
            "Loss training: 36.526325\n",
            "Loss training: 54.76048\n",
            "Loss training: 42.57301\n",
            "Loss training: 41.000732\n",
            "\n",
            "***************************\n",
            "Trained on 2270 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 47.302868\n",
            "Loss training: 29.414228\n",
            "Loss training: 13.844176\n",
            "Loss training: 26.024496\n",
            "Loss training: 37.327896\n",
            "Loss training: 38.3636\n",
            "Loss training: 39.141254\n",
            "Loss training: 31.403347\n",
            "Loss training: 29.060925\n",
            "Loss training: 22.760065\n",
            "\n",
            "***************************\n",
            "Trained on 2280 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.327503\n",
            "Loss training: 35.673386\n",
            "Loss training: 29.65509\n",
            "Loss training: 30.72298\n",
            "Loss training: 29.402805\n",
            "Loss training: 13.369718\n",
            "Loss training: 38.323826\n",
            "Loss training: 28.520182\n",
            "Loss training: 29.971052\n",
            "Loss training: 40.186775\n",
            "\n",
            "***************************\n",
            "Trained on 2290 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.822422\n",
            "Loss training: 33.69491\n",
            "Loss training: 29.209574\n",
            "Loss training: 20.009563\n",
            "Loss training: 46.727226\n",
            "Loss training: 22.409767\n",
            "Loss training: 22.291386\n",
            "Loss training: 38.02889\n",
            "Loss training: 37.44081\n",
            "Loss training: 30.097502\n",
            "\n",
            "***************************\n",
            "Trained on 2300 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.242184\n",
            "Loss training: 36.675713\n",
            "Loss training: 24.227356\n",
            "Loss training: 29.996141\n",
            "Loss training: 42.25943\n",
            "Loss training: 37.053802\n",
            "Loss training: 40.006287\n",
            "Loss training: 46.92321\n",
            "Loss training: 30.728083\n",
            "Loss training: 27.985853\n",
            "\n",
            "***************************\n",
            "Trained on 2310 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.166151\n",
            "Loss training: 13.696335\n",
            "Loss training: 41.939663\n",
            "Loss training: 13.47989\n",
            "Loss training: 31.020132\n",
            "Loss training: 30.977665\n",
            "Loss training: 37.890236\n",
            "Loss training: 37.347755\n",
            "Loss training: 30.961685\n",
            "Loss training: 31.493113\n",
            "\n",
            "***************************\n",
            "Trained on 2320 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 43.150066\n",
            "Loss training: 27.492474\n",
            "Loss training: 39.255066\n",
            "Loss training: 28.662823\n",
            "Loss training: 23.261505\n",
            "Loss training: 39.384304\n",
            "Loss training: 37.655563\n",
            "Loss training: 23.56516\n",
            "Loss training: 30.261915\n",
            "Loss training: 51.903603\n",
            "\n",
            "***************************\n",
            "Trained on 2330 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.118866\n",
            "Loss training: 29.807428\n",
            "Loss training: 37.148136\n",
            "Loss training: 47.44227\n",
            "Loss training: 30.34939\n",
            "Loss training: 30.586569\n",
            "Loss training: 29.969929\n",
            "Loss training: 30.974777\n",
            "Loss training: 37.471626\n",
            "Loss training: 36.20319\n",
            "\n",
            "***************************\n",
            "Trained on 2340 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.46384\n",
            "Loss training: 37.65563\n",
            "Loss training: 41.998222\n",
            "Loss training: 27.715902\n",
            "Loss training: 40.96276\n",
            "Loss training: 36.982334\n",
            "Loss training: 28.886625\n",
            "Loss training: 38.374355\n",
            "Loss training: 43.027065\n",
            "Loss training: 33.038067\n",
            "\n",
            "***************************\n",
            "Trained on 2350 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.586279\n",
            "Loss training: 21.789978\n",
            "Loss training: 41.442535\n",
            "Loss training: 29.255304\n",
            "Loss training: 40.263916\n",
            "Loss training: 41.81756\n",
            "Loss training: 27.066992\n",
            "Loss training: 40.44907\n",
            "Loss training: 37.00824\n",
            "Loss training: 26.988573\n",
            "\n",
            "***************************\n",
            "Trained on 2360 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 54.31742\n",
            "Loss training: 37.611656\n",
            "Loss training: 43.543076\n",
            "Loss training: 22.055943\n",
            "Loss training: 14.033103\n",
            "Loss training: 38.07041\n",
            "Loss training: 32.10894\n",
            "Loss training: 38.301414\n",
            "Loss training: 37.41251\n",
            "Loss training: 30.000893\n",
            "\n",
            "***************************\n",
            "Trained on 2370 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.735016\n",
            "Loss training: 31.123991\n",
            "Loss training: 38.236435\n",
            "Loss training: 24.136065\n",
            "Loss training: 41.323772\n",
            "Loss training: 30.488596\n",
            "Loss training: 29.292206\n",
            "Loss training: 46.094875\n",
            "Loss training: 30.23134\n",
            "Loss training: 38.640743\n",
            "\n",
            "***************************\n",
            "Trained on 2380 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.438744\n",
            "Loss training: 46.795097\n",
            "Loss training: 29.59446\n",
            "Loss training: 46.19447\n",
            "Loss training: 27.89452\n",
            "Loss training: 42.069073\n",
            "Loss training: 29.373785\n",
            "Loss training: 27.008839\n",
            "Loss training: 42.97272\n",
            "Loss training: 27.126509\n",
            "\n",
            "***************************\n",
            "Trained on 2390 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 14.325956\n",
            "Loss training: 37.051723\n",
            "Loss training: 42.36748\n",
            "Loss training: 19.783743\n",
            "Loss training: 13.775865\n",
            "Loss training: 41.30453\n",
            "Loss training: 39.13518\n",
            "Loss training: 36.401157\n",
            "Loss training: 26.869894\n",
            "Loss training: 26.87566\n",
            "\n",
            "***************************\n",
            "Trained on 2400 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.816345\n",
            "Loss training: 27.260763\n",
            "Loss training: 30.02868\n",
            "Loss training: 37.327488\n",
            "Loss training: 27.646421\n",
            "Loss training: 33.341877\n",
            "Loss training: 28.965055\n",
            "Loss training: 25.966675\n",
            "Loss training: 46.542694\n",
            "Loss training: 29.060686\n",
            "\n",
            "***************************\n",
            "Trained on 2410 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.98587\n",
            "Loss training: 13.459086\n",
            "Loss training: 29.82761\n",
            "Loss training: 22.869308\n",
            "Loss training: 25.207655\n",
            "Loss training: 27.20938\n",
            "Loss training: 31.368813\n",
            "Loss training: 39.064503\n",
            "Loss training: 40.968758\n",
            "Loss training: 45.33544\n",
            "\n",
            "***************************\n",
            "Trained on 2420 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.57016\n",
            "Loss training: 27.401184\n",
            "Loss training: 27.753124\n",
            "Loss training: 29.449776\n",
            "Loss training: 38.29179\n",
            "Loss training: 31.244188\n",
            "Loss training: 35.803677\n",
            "Loss training: 30.400772\n",
            "Loss training: 34.594654\n",
            "Loss training: 28.15258\n",
            "\n",
            "***************************\n",
            "Trained on 2430 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 55.208954\n",
            "Loss training: 22.962215\n",
            "Loss training: 40.501976\n",
            "Loss training: 47.193874\n",
            "Loss training: 36.292015\n",
            "Loss training: 13.710844\n",
            "Loss training: 29.15249\n",
            "Loss training: 14.008781\n",
            "Loss training: 36.441273\n",
            "Loss training: 30.579521\n",
            "\n",
            "***************************\n",
            "Trained on 2440 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.165476\n",
            "Loss training: 25.430769\n",
            "Loss training: 36.784927\n",
            "Loss training: 38.97331\n",
            "Loss training: 43.611618\n",
            "Loss training: 35.44566\n",
            "Loss training: 39.853317\n",
            "Loss training: 30.017023\n",
            "Loss training: 45.88117\n",
            "Loss training: 39.74853\n",
            "\n",
            "***************************\n",
            "Trained on 2450 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.325485\n",
            "Loss training: 37.81485\n",
            "Loss training: 13.189019\n",
            "Loss training: 39.453846\n",
            "Loss training: 41.86848\n",
            "Loss training: 40.048866\n",
            "Loss training: 26.916388\n",
            "Loss training: 41.59943\n",
            "Loss training: 30.411018\n",
            "Loss training: 28.655897\n",
            "\n",
            "***************************\n",
            "Trained on 2460 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.99036\n",
            "Loss training: 43.997852\n",
            "Loss training: 30.185074\n",
            "Loss training: 41.619915\n",
            "Loss training: 30.966448\n",
            "Loss training: 30.108858\n",
            "Loss training: 41.95502\n",
            "Loss training: 26.915352\n",
            "Loss training: 28.714146\n",
            "Loss training: 42.000626\n",
            "\n",
            "***************************\n",
            "Trained on 2470 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.079807\n",
            "Loss training: 20.615828\n",
            "Loss training: 13.026451\n",
            "Loss training: 23.512264\n",
            "Loss training: 41.230385\n",
            "Loss training: 37.238705\n",
            "Loss training: 41.04911\n",
            "Loss training: 27.129\n",
            "Loss training: 37.746834\n",
            "Loss training: 27.305172\n",
            "\n",
            "***************************\n",
            "Trained on 2480 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.87993\n",
            "Loss training: 23.591791\n",
            "Loss training: 41.250225\n",
            "Loss training: 37.253704\n",
            "Loss training: 30.208853\n",
            "Loss training: 35.28916\n",
            "Loss training: 35.421356\n",
            "Loss training: 35.155296\n",
            "Loss training: 31.113302\n",
            "Loss training: 40.04286\n",
            "\n",
            "***************************\n",
            "Trained on 2490 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.98815\n",
            "Loss training: 39.981323\n",
            "Loss training: 40.805275\n",
            "Loss training: 40.56504\n",
            "Loss training: 26.578026\n",
            "Loss training: 31.891262\n",
            "Loss training: 63.395676\n",
            "Loss training: 22.343088\n",
            "Loss training: 38.230404\n",
            "Loss training: 36.40531\n",
            "\n",
            "***************************\n",
            "Trained on 2500 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 45.02551\n",
            "Loss training: 38.688824\n",
            "Loss training: 25.3723\n",
            "Loss training: 45.90058\n",
            "Loss training: 29.973011\n",
            "Loss training: 37.684074\n",
            "Loss training: 43.770267\n",
            "Loss training: 36.125553\n",
            "Loss training: 16.905056\n",
            "Loss training: 38.81952\n",
            "\n",
            "***************************\n",
            "Trained on 2510 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.942085\n",
            "Loss training: 30.366682\n",
            "Loss training: 43.9426\n",
            "Loss training: 27.113567\n",
            "Loss training: 49.936993\n",
            "Loss training: 29.694635\n",
            "Loss training: 22.303913\n",
            "Loss training: 38.004833\n",
            "Loss training: 47.319622\n",
            "Loss training: 39.73736\n",
            "\n",
            "***************************\n",
            "Trained on 2520 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.740444\n",
            "Loss training: 36.98117\n",
            "Loss training: 45.295765\n",
            "Loss training: 33.783604\n",
            "Loss training: 63.028797\n",
            "Loss training: 19.970375\n",
            "Loss training: 43.285763\n",
            "Loss training: 56.57512\n",
            "Loss training: 36.786037\n",
            "Loss training: 27.250551\n",
            "\n",
            "***************************\n",
            "Trained on 2530 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.280674\n",
            "Loss training: 36.088\n",
            "Loss training: 30.951126\n",
            "Loss training: 19.816956\n",
            "Loss training: 35.296024\n",
            "Loss training: 25.612972\n",
            "Loss training: 19.235048\n",
            "Loss training: 30.557829\n",
            "Loss training: 37.763824\n",
            "Loss training: 29.255241\n",
            "\n",
            "***************************\n",
            "Trained on 2540 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.1864\n",
            "Loss training: 35.715588\n",
            "Loss training: 28.873524\n",
            "Loss training: 36.62967\n",
            "Loss training: 25.171085\n",
            "Loss training: 31.664532\n",
            "Loss training: 38.10992\n",
            "Loss training: 35.939236\n",
            "Loss training: 44.75665\n",
            "Loss training: 26.85263\n",
            "\n",
            "***************************\n",
            "Trained on 2550 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.667625\n",
            "Loss training: 30.567934\n",
            "Loss training: 28.750893\n",
            "Loss training: 22.729698\n",
            "Loss training: 38.957382\n",
            "Loss training: 28.7346\n",
            "Loss training: 34.859623\n",
            "Loss training: 32.14902\n",
            "Loss training: 40.79784\n",
            "Loss training: 37.156597\n",
            "\n",
            "***************************\n",
            "Trained on 2560 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.046316\n",
            "Loss training: 28.908396\n",
            "Loss training: 28.385376\n",
            "Loss training: 36.854813\n",
            "Loss training: 36.977295\n",
            "Loss training: 37.28661\n",
            "Loss training: 40.53503\n",
            "Loss training: 37.278393\n",
            "Loss training: 35.918358\n",
            "Loss training: 42.3845\n",
            "\n",
            "***************************\n",
            "Trained on 2570 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.902676\n",
            "Loss training: 26.888987\n",
            "Loss training: 34.465744\n",
            "Loss training: 33.383156\n",
            "Loss training: 37.57609\n",
            "Loss training: 30.820814\n",
            "Loss training: 30.76853\n",
            "Loss training: 27.166992\n",
            "Loss training: 23.836628\n",
            "Loss training: 24.331852\n",
            "\n",
            "***************************\n",
            "Trained on 2580 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.85357\n",
            "Loss training: 26.901142\n",
            "Loss training: 28.92937\n",
            "Loss training: 22.022074\n",
            "Loss training: 23.800356\n",
            "Loss training: 30.49265\n",
            "Loss training: 39.58891\n",
            "Loss training: 34.62697\n",
            "Loss training: 23.355572\n",
            "Loss training: 36.730297\n",
            "\n",
            "***************************\n",
            "Trained on 2590 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.979916\n",
            "Loss training: 36.52716\n",
            "Loss training: 30.426811\n",
            "Loss training: 13.471106\n",
            "Loss training: 26.77567\n",
            "Loss training: 40.688915\n",
            "Loss training: 43.43937\n",
            "Loss training: 37.72425\n",
            "Loss training: 34.84255\n",
            "Loss training: 30.487598\n",
            "\n",
            "***************************\n",
            "Trained on 2600 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.63983\n",
            "Loss training: 39.455353\n",
            "Loss training: 36.604935\n",
            "Loss training: 41.25683\n",
            "Loss training: 32.84118\n",
            "Loss training: 36.290203\n",
            "Loss training: 35.360214\n",
            "Loss training: 28.268696\n",
            "Loss training: 19.619774\n",
            "Loss training: 35.96046\n",
            "\n",
            "***************************\n",
            "Trained on 2610 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.854395\n",
            "Loss training: 41.563564\n",
            "Loss training: 37.12294\n",
            "Loss training: 48.95284\n",
            "Loss training: 18.560719\n",
            "Loss training: 28.990744\n",
            "Loss training: 35.33878\n",
            "Loss training: 39.336964\n",
            "Loss training: 35.16311\n",
            "Loss training: 33.1987\n",
            "\n",
            "***************************\n",
            "Trained on 2620 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.952473\n",
            "Loss training: 28.625425\n",
            "Loss training: 14.459049\n",
            "Loss training: 30.503122\n",
            "Loss training: 29.405254\n",
            "Loss training: 26.165482\n",
            "Loss training: 39.042233\n",
            "Loss training: 33.090553\n",
            "Loss training: 29.109745\n",
            "Loss training: 35.849476\n",
            "\n",
            "***************************\n",
            "Trained on 2630 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.97023\n",
            "Loss training: 38.916298\n",
            "Loss training: 28.334183\n",
            "Loss training: 26.926092\n",
            "Loss training: 37.87776\n",
            "Loss training: 40.067207\n",
            "Loss training: 41.479324\n",
            "Loss training: 34.828945\n",
            "Loss training: 41.626892\n",
            "Loss training: 37.425972\n",
            "\n",
            "***************************\n",
            "Trained on 2640 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.102196\n",
            "Loss training: 23.817383\n",
            "Loss training: 24.336908\n",
            "Loss training: 38.191235\n",
            "Loss training: 37.792923\n",
            "Loss training: 36.050663\n",
            "Loss training: 43.041096\n",
            "Loss training: 41.545456\n",
            "Loss training: 53.39124\n",
            "Loss training: 38.80752\n",
            "\n",
            "***************************\n",
            "Trained on 2650 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.873886\n",
            "Loss training: 39.44522\n",
            "Loss training: 36.35073\n",
            "Loss training: 36.893013\n",
            "Loss training: 34.80464\n",
            "Loss training: 20.384367\n",
            "Loss training: 38.78635\n",
            "Loss training: 29.693527\n",
            "Loss training: 36.895676\n",
            "Loss training: 29.64414\n",
            "\n",
            "***************************\n",
            "Trained on 2660 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.22966\n",
            "Loss training: 36.244526\n",
            "Loss training: 30.24337\n",
            "Loss training: 23.092289\n",
            "Loss training: 36.805744\n",
            "Loss training: 50.26133\n",
            "Loss training: 37.963852\n",
            "Loss training: 28.88483\n",
            "Loss training: 27.250593\n",
            "Loss training: 28.6246\n",
            "\n",
            "***************************\n",
            "Trained on 2670 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.92594\n",
            "Loss training: 29.873428\n",
            "Loss training: 14.554813\n",
            "Loss training: 35.878956\n",
            "Loss training: 28.027227\n",
            "Loss training: 38.936333\n",
            "Loss training: 45.32923\n",
            "Loss training: 19.391619\n",
            "Loss training: 28.786915\n",
            "Loss training: 27.89465\n",
            "\n",
            "***************************\n",
            "Trained on 2680 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.869747\n",
            "Loss training: 36.41042\n",
            "Loss training: 42.67994\n",
            "Loss training: 38.391735\n",
            "Loss training: 34.853565\n",
            "Loss training: 36.295277\n",
            "Loss training: 37.79102\n",
            "Loss training: 24.363974\n",
            "Loss training: 40.243237\n",
            "Loss training: 27.765188\n",
            "\n",
            "***************************\n",
            "Trained on 2690 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.660122\n",
            "Loss training: 38.39825\n",
            "Loss training: 30.893044\n",
            "Loss training: 40.247257\n",
            "Loss training: 47.82459\n",
            "Loss training: 28.65253\n",
            "Loss training: 43.208214\n",
            "Loss training: 36.308487\n",
            "Loss training: 32.976555\n",
            "Loss training: 28.08849\n",
            "\n",
            "***************************\n",
            "Trained on 2700 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.418455\n",
            "Loss training: 36.828384\n",
            "Loss training: 37.26853\n",
            "Loss training: 26.341137\n",
            "Loss training: 19.1418\n",
            "Loss training: 45.76327\n",
            "Loss training: 23.55363\n",
            "Loss training: 38.894108\n",
            "Loss training: 38.803852\n",
            "Loss training: 51.32489\n",
            "\n",
            "***************************\n",
            "Trained on 2710 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.85021\n",
            "Loss training: 41.860264\n",
            "Loss training: 31.157087\n",
            "Loss training: 42.197197\n",
            "Loss training: 39.63637\n",
            "Loss training: 46.58705\n",
            "Loss training: 46.068687\n",
            "Loss training: 38.431572\n",
            "Loss training: 29.846575\n",
            "Loss training: 28.61361\n",
            "\n",
            "***************************\n",
            "Trained on 2720 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 46.201992\n",
            "Loss training: 36.152687\n",
            "Loss training: 46.18972\n",
            "Loss training: 58.82597\n",
            "Loss training: 33.97087\n",
            "Loss training: 36.65506\n",
            "Loss training: 40.092438\n",
            "Loss training: 45.623947\n",
            "Loss training: 23.54049\n",
            "Loss training: 46.46834\n",
            "\n",
            "***************************\n",
            "Trained on 2730 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.235113\n",
            "Loss training: 42.20085\n",
            "Loss training: 35.04563\n",
            "Loss training: 43.10273\n",
            "Loss training: 37.626934\n",
            "Loss training: 22.481447\n",
            "Loss training: 42.749138\n",
            "Loss training: 32.67511\n",
            "Loss training: 29.134361\n",
            "Loss training: 45.835884\n",
            "\n",
            "***************************\n",
            "Trained on 2740 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.370274\n",
            "Loss training: 46.033638\n",
            "Loss training: 30.138123\n",
            "Loss training: 27.149204\n",
            "Loss training: 45.314247\n",
            "Loss training: 22.743052\n",
            "Loss training: 33.91233\n",
            "Loss training: 45.238342\n",
            "Loss training: 25.153915\n",
            "Loss training: 45.039272\n",
            "\n",
            "***************************\n",
            "Trained on 2750 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.85305\n",
            "Loss training: 40.94154\n",
            "Loss training: 41.816727\n",
            "Loss training: 38.71806\n",
            "Loss training: 28.975088\n",
            "Loss training: 30.785376\n",
            "Loss training: 30.504217\n",
            "Loss training: 44.94667\n",
            "Loss training: 39.015614\n",
            "Loss training: 29.797295\n",
            "\n",
            "***************************\n",
            "Trained on 2760 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.34187\n",
            "Loss training: 35.606228\n",
            "Loss training: 38.196793\n",
            "Loss training: 27.34126\n",
            "Loss training: 38.980583\n",
            "Loss training: 39.700344\n",
            "Loss training: 26.665718\n",
            "Loss training: 28.446291\n",
            "Loss training: 34.73084\n",
            "Loss training: 23.173128\n",
            "\n",
            "***************************\n",
            "Trained on 2770 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.025923\n",
            "Loss training: 13.7047205\n",
            "Loss training: 28.948336\n",
            "Loss training: 38.38416\n",
            "Loss training: 41.061905\n",
            "Loss training: 34.43979\n",
            "Loss training: 13.727877\n",
            "Loss training: 38.52818\n",
            "Loss training: 27.162981\n",
            "Loss training: 34.536304\n",
            "\n",
            "***************************\n",
            "Trained on 2780 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.334118\n",
            "Loss training: 26.950542\n",
            "Loss training: 39.6066\n",
            "Loss training: 31.296085\n",
            "Loss training: 31.126957\n",
            "Loss training: 43.346134\n",
            "Loss training: 41.18052\n",
            "Loss training: 36.794968\n",
            "Loss training: 27.890469\n",
            "Loss training: 28.617271\n",
            "\n",
            "***************************\n",
            "Trained on 2790 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.125614\n",
            "Loss training: 28.377426\n",
            "Loss training: 26.871368\n",
            "Loss training: 28.100548\n",
            "Loss training: 35.911015\n",
            "Loss training: 26.968388\n",
            "Loss training: 38.246723\n",
            "Loss training: 13.464774\n",
            "Loss training: 27.88176\n",
            "Loss training: 49.607136\n",
            "\n",
            "***************************\n",
            "Trained on 2800 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.272438\n",
            "Loss training: 43.13012\n",
            "Loss training: 43.795822\n",
            "Loss training: 36.130997\n",
            "Loss training: 27.338236\n",
            "Loss training: 42.18684\n",
            "Loss training: 30.2609\n",
            "Loss training: 30.269514\n",
            "Loss training: 27.100363\n",
            "Loss training: 27.238533\n",
            "\n",
            "***************************\n",
            "Trained on 2810 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.196022\n",
            "Loss training: 57.612175\n",
            "Loss training: 43.035923\n",
            "Loss training: 28.698975\n",
            "Loss training: 46.617676\n",
            "Loss training: 39.275757\n",
            "Loss training: 38.991127\n",
            "Loss training: 49.083004\n",
            "Loss training: 27.447397\n",
            "Loss training: 29.022415\n",
            "\n",
            "***************************\n",
            "Trained on 2820 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.141275\n",
            "Loss training: 26.334448\n",
            "Loss training: 22.56599\n",
            "Loss training: 27.43666\n",
            "Loss training: 45.75647\n",
            "Loss training: 29.503443\n",
            "Loss training: 56.623158\n",
            "Loss training: 40.26983\n",
            "Loss training: 44.848713\n",
            "Loss training: 33.304466\n",
            "\n",
            "***************************\n",
            "Trained on 2830 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.632263\n",
            "Loss training: 44.0907\n",
            "Loss training: 29.829514\n",
            "Loss training: 42.565235\n",
            "Loss training: 22.318552\n",
            "Loss training: 22.139036\n",
            "Loss training: 36.191048\n",
            "Loss training: 32.196285\n",
            "Loss training: 42.167862\n",
            "Loss training: 38.235092\n",
            "\n",
            "***************************\n",
            "Trained on 2840 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.88017\n",
            "Loss training: 21.992933\n",
            "Loss training: 30.143034\n",
            "Loss training: 42.895256\n",
            "Loss training: 43.569973\n",
            "Loss training: 42.436226\n",
            "Loss training: 37.189827\n",
            "Loss training: 31.53207\n",
            "Loss training: 29.338531\n",
            "Loss training: 24.64311\n",
            "\n",
            "***************************\n",
            "Trained on 2850 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.203583\n",
            "Loss training: 41.05966\n",
            "Loss training: 35.854385\n",
            "Loss training: 42.05061\n",
            "Loss training: 41.964466\n",
            "Loss training: 36.734333\n",
            "Loss training: 29.864634\n",
            "Loss training: 58.61432\n",
            "Loss training: 41.85385\n",
            "Loss training: 28.322737\n",
            "\n",
            "***************************\n",
            "Trained on 2860 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.35236\n",
            "Loss training: 28.062689\n",
            "Loss training: 29.03422\n",
            "Loss training: 27.736883\n",
            "Loss training: 45.676582\n",
            "Loss training: 36.567444\n",
            "Loss training: 44.906025\n",
            "Loss training: 38.312244\n",
            "Loss training: 16.39619\n",
            "Loss training: 31.293795\n",
            "\n",
            "***************************\n",
            "Trained on 2870 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.771667\n",
            "Loss training: 34.83874\n",
            "Loss training: 41.808617\n",
            "Loss training: 41.470055\n",
            "Loss training: 47.547123\n",
            "Loss training: 27.179379\n",
            "Loss training: 25.385399\n",
            "Loss training: 39.798046\n",
            "Loss training: 47.982765\n",
            "Loss training: 35.46584\n",
            "\n",
            "***************************\n",
            "Trained on 2880 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.304943\n",
            "Loss training: 31.375364\n",
            "Loss training: 29.711529\n",
            "Loss training: 36.93621\n",
            "Loss training: 40.580322\n",
            "Loss training: 30.325418\n",
            "Loss training: 29.235998\n",
            "Loss training: 44.76663\n",
            "Loss training: 43.044685\n",
            "Loss training: 41.458736\n",
            "\n",
            "***************************\n",
            "Trained on 2890 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.834875\n",
            "Loss training: 42.928802\n",
            "Loss training: 40.13527\n",
            "Loss training: 21.891413\n",
            "Loss training: 29.68571\n",
            "Loss training: 28.305641\n",
            "Loss training: 41.139687\n",
            "Loss training: 37.553642\n",
            "Loss training: 33.440765\n",
            "Loss training: 33.340324\n",
            "\n",
            "***************************\n",
            "Trained on 2900 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.686588\n",
            "Loss training: 41.37495\n",
            "Loss training: 36.28462\n",
            "Loss training: 35.153355\n",
            "Loss training: 36.430347\n",
            "Loss training: 35.673607\n",
            "Loss training: 29.46753\n",
            "Loss training: 34.456474\n",
            "Loss training: 28.692799\n",
            "Loss training: 28.97107\n",
            "\n",
            "***************************\n",
            "Trained on 2910 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.532236\n",
            "Loss training: 28.013193\n",
            "Loss training: 31.345476\n",
            "Loss training: 40.318825\n",
            "Loss training: 13.20362\n",
            "Loss training: 30.802752\n",
            "Loss training: 34.294037\n",
            "Loss training: 29.471046\n",
            "Loss training: 40.52416\n",
            "Loss training: 33.283253\n",
            "\n",
            "***************************\n",
            "Trained on 2920 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.203411\n",
            "Loss training: 45.363926\n",
            "Loss training: 13.218786\n",
            "Loss training: 37.179745\n",
            "Loss training: 37.509495\n",
            "Loss training: 34.727337\n",
            "Loss training: 34.883106\n",
            "Loss training: 34.825638\n",
            "Loss training: 28.802893\n",
            "Loss training: 36.526142\n",
            "\n",
            "***************************\n",
            "Trained on 2930 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.608034\n",
            "Loss training: 58.5541\n",
            "Loss training: 30.585596\n",
            "Loss training: 35.00107\n",
            "Loss training: 33.191303\n",
            "Loss training: 30.215017\n",
            "Loss training: 45.3602\n",
            "Loss training: 28.382668\n",
            "Loss training: 41.40261\n",
            "Loss training: 27.985117\n",
            "\n",
            "***************************\n",
            "Trained on 2940 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.414501\n",
            "Loss training: 36.462673\n",
            "Loss training: 37.493835\n",
            "Loss training: 36.36949\n",
            "Loss training: 30.306059\n",
            "Loss training: 28.8912\n",
            "Loss training: 36.775063\n",
            "Loss training: 42.197464\n",
            "Loss training: 26.952757\n",
            "Loss training: 29.350094\n",
            "\n",
            "***************************\n",
            "Trained on 2950 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.75499\n",
            "Loss training: 23.494793\n",
            "Loss training: 34.435863\n",
            "Loss training: 41.463764\n",
            "Loss training: 41.049305\n",
            "Loss training: 50.750664\n",
            "Loss training: 40.833782\n",
            "Loss training: 38.615814\n",
            "Loss training: 28.13242\n",
            "Loss training: 35.16035\n",
            "\n",
            "***************************\n",
            "Trained on 2960 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.281216\n",
            "Loss training: 39.180447\n",
            "Loss training: 23.554981\n",
            "Loss training: 30.013388\n",
            "Loss training: 33.805992\n",
            "Loss training: 35.091614\n",
            "Loss training: 36.02808\n",
            "Loss training: 34.8203\n",
            "Loss training: 41.89773\n",
            "Loss training: 36.16254\n",
            "\n",
            "***************************\n",
            "Trained on 2970 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.812662\n",
            "Loss training: 41.023506\n",
            "Loss training: 38.578403\n",
            "Loss training: 36.071884\n",
            "Loss training: 28.50137\n",
            "Loss training: 40.50516\n",
            "Loss training: 44.21816\n",
            "Loss training: 33.325443\n",
            "Loss training: 36.95923\n",
            "Loss training: 27.472065\n",
            "\n",
            "***************************\n",
            "Trained on 2980 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.896126\n",
            "Loss training: 32.589695\n",
            "Loss training: 29.555271\n",
            "Loss training: 34.87992\n",
            "Loss training: 26.808468\n",
            "Loss training: 30.740257\n",
            "Loss training: 40.273537\n",
            "Loss training: 35.067875\n",
            "Loss training: 26.71397\n",
            "Loss training: 27.96489\n",
            "\n",
            "***************************\n",
            "Trained on 2990 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.823147\n",
            "Loss training: 35.713608\n",
            "Loss training: 30.427948\n",
            "Loss training: 13.225648\n",
            "Loss training: 13.143318\n",
            "Loss training: 34.32981\n",
            "Loss training: 28.386547\n",
            "Loss training: 35.53154\n",
            "Loss training: 29.822609\n",
            "Loss training: 39.038475\n",
            "\n",
            "***************************\n",
            "Trained on 3000 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.099205\n",
            "Loss training: 35.63108\n",
            "Loss training: 38.476887\n",
            "Loss training: 30.059174\n",
            "Loss training: 34.567734\n",
            "Loss training: 20.454796\n",
            "Loss training: 42.409294\n",
            "Loss training: 34.25668\n",
            "Loss training: 34.484497\n",
            "Loss training: 36.939938\n",
            "\n",
            "***************************\n",
            "Trained on 3010 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 18.990843\n",
            "Loss training: 23.213614\n",
            "Loss training: 36.4902\n",
            "Loss training: 37.479126\n",
            "Loss training: 40.449482\n",
            "Loss training: 27.010382\n",
            "Loss training: 38.931866\n",
            "Loss training: 22.006569\n",
            "Loss training: 34.580067\n",
            "Loss training: 28.525974\n",
            "\n",
            "***************************\n",
            "Trained on 3020 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.724\n",
            "Loss training: 34.08834\n",
            "Loss training: 34.004185\n",
            "Loss training: 33.775608\n",
            "Loss training: 31.057732\n",
            "Loss training: 28.445606\n",
            "Loss training: 34.28341\n",
            "Loss training: 27.267435\n",
            "Loss training: 53.6746\n",
            "Loss training: 30.371363\n",
            "\n",
            "***************************\n",
            "Trained on 3030 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.916668\n",
            "Loss training: 31.054647\n",
            "Loss training: 29.278671\n",
            "Loss training: 18.34067\n",
            "Loss training: 18.187878\n",
            "Loss training: 39.25584\n",
            "Loss training: 40.44207\n",
            "Loss training: 28.616125\n",
            "Loss training: 37.33615\n",
            "Loss training: 24.98799\n",
            "\n",
            "***************************\n",
            "Trained on 3040 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.266136\n",
            "Loss training: 34.66338\n",
            "Loss training: 26.865683\n",
            "Loss training: 39.451836\n",
            "Loss training: 39.310173\n",
            "Loss training: 33.6974\n",
            "Loss training: 23.51654\n",
            "Loss training: 39.14661\n",
            "Loss training: 38.36158\n",
            "Loss training: 26.106195\n",
            "\n",
            "***************************\n",
            "Trained on 3050 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.04308\n",
            "Loss training: 23.304834\n",
            "Loss training: 38.84368\n",
            "Loss training: 45.719807\n",
            "Loss training: 36.606342\n",
            "Loss training: 47.3315\n",
            "Loss training: 31.334196\n",
            "Loss training: 29.568293\n",
            "Loss training: 22.145643\n",
            "Loss training: 42.135094\n",
            "\n",
            "***************************\n",
            "Trained on 3060 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 47.247684\n",
            "Loss training: 33.57353\n",
            "Loss training: 34.710987\n",
            "Loss training: 43.929043\n",
            "Loss training: 37.03884\n",
            "Loss training: 41.68474\n",
            "Loss training: 21.432745\n",
            "Loss training: 21.38141\n",
            "Loss training: 38.322655\n",
            "Loss training: 23.066305\n",
            "\n",
            "***************************\n",
            "Trained on 3070 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.419617\n",
            "Loss training: 13.429114\n",
            "Loss training: 29.111313\n",
            "Loss training: 26.751543\n",
            "Loss training: 37.46023\n",
            "Loss training: 33.62845\n",
            "Loss training: 13.060709\n",
            "Loss training: 39.92028\n",
            "Loss training: 40.51177\n",
            "Loss training: 36.41418\n",
            "\n",
            "***************************\n",
            "Trained on 3080 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.399948\n",
            "Loss training: 22.329788\n",
            "Loss training: 38.52265\n",
            "Loss training: 39.570747\n",
            "Loss training: 13.178672\n",
            "Loss training: 44.741917\n",
            "Loss training: 43.30545\n",
            "Loss training: 53.146088\n",
            "Loss training: 34.584732\n",
            "Loss training: 27.777023\n",
            "\n",
            "***************************\n",
            "Trained on 3090 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.568886\n",
            "Loss training: 41.68241\n",
            "Loss training: 28.842453\n",
            "Loss training: 28.148285\n",
            "Loss training: 30.439518\n",
            "Loss training: 30.706898\n",
            "Loss training: 19.642038\n",
            "Loss training: 42.88469\n",
            "Loss training: 18.98565\n",
            "Loss training: 43.026188\n",
            "\n",
            "***************************\n",
            "Trained on 3100 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.771587\n",
            "Loss training: 29.25102\n",
            "Loss training: 35.27412\n",
            "Loss training: 35.156425\n",
            "Loss training: 33.650288\n",
            "Loss training: 38.398956\n",
            "Loss training: 22.36389\n",
            "Loss training: 43.44183\n",
            "Loss training: 26.35013\n",
            "Loss training: 33.259254\n",
            "\n",
            "***************************\n",
            "Trained on 3110 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.51736\n",
            "Loss training: 35.600857\n",
            "Loss training: 31.890612\n",
            "Loss training: 41.308178\n",
            "Loss training: 21.925508\n",
            "Loss training: 24.09939\n",
            "Loss training: 37.69555\n",
            "Loss training: 28.003574\n",
            "Loss training: 29.736143\n",
            "Loss training: 27.90183\n",
            "\n",
            "***************************\n",
            "Trained on 3120 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.228569\n",
            "Loss training: 42.081448\n",
            "Loss training: 28.553848\n",
            "Loss training: 34.953724\n",
            "Loss training: 35.81058\n",
            "Loss training: 34.569073\n",
            "Loss training: 38.511993\n",
            "Loss training: 28.242748\n",
            "Loss training: 27.61547\n",
            "Loss training: 21.891123\n",
            "\n",
            "***************************\n",
            "Trained on 3130 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.06307\n",
            "Loss training: 46.333412\n",
            "Loss training: 26.172888\n",
            "Loss training: 28.452648\n",
            "Loss training: 45.383015\n",
            "Loss training: 36.53397\n",
            "Loss training: 35.70521\n",
            "Loss training: 40.32926\n",
            "Loss training: 42.788055\n",
            "Loss training: 30.38202\n",
            "\n",
            "***************************\n",
            "Trained on 3140 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.081633\n",
            "Loss training: 50.47611\n",
            "Loss training: 27.249908\n",
            "Loss training: 34.948925\n",
            "Loss training: 22.045221\n",
            "Loss training: 42.65523\n",
            "Loss training: 39.538807\n",
            "Loss training: 40.70199\n",
            "Loss training: 28.277525\n",
            "Loss training: 27.792091\n",
            "\n",
            "***************************\n",
            "Trained on 3150 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.949665\n",
            "Loss training: 26.975006\n",
            "Loss training: 37.82146\n",
            "Loss training: 27.482979\n",
            "Loss training: 29.944407\n",
            "Loss training: 27.607098\n",
            "Loss training: 38.61602\n",
            "Loss training: 26.84184\n",
            "Loss training: 34.78987\n",
            "Loss training: 30.27717\n",
            "\n",
            "***************************\n",
            "Trained on 3160 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.893219\n",
            "Loss training: 27.25602\n",
            "Loss training: 28.716288\n",
            "Loss training: 39.963547\n",
            "Loss training: 36.986668\n",
            "Loss training: 36.94327\n",
            "Loss training: 26.722155\n",
            "Loss training: 14.095705\n",
            "Loss training: 33.138306\n",
            "Loss training: 38.70318\n",
            "\n",
            "***************************\n",
            "Trained on 3170 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.748657\n",
            "Loss training: 36.57672\n",
            "Loss training: 27.054222\n",
            "Loss training: 29.18253\n",
            "Loss training: 35.96517\n",
            "Loss training: 45.830246\n",
            "Loss training: 34.758877\n",
            "Loss training: 26.701296\n",
            "Loss training: 37.094044\n",
            "Loss training: 44.954887\n",
            "\n",
            "***************************\n",
            "Trained on 3180 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.31934\n",
            "Loss training: 35.786175\n",
            "Loss training: 27.361034\n",
            "Loss training: 21.913351\n",
            "Loss training: 23.793304\n",
            "Loss training: 43.907513\n",
            "Loss training: 45.160553\n",
            "Loss training: 43.800407\n",
            "Loss training: 21.743889\n",
            "Loss training: 23.756006\n",
            "\n",
            "***************************\n",
            "Trained on 3190 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.15767\n",
            "Loss training: 30.635145\n",
            "Loss training: 36.922848\n",
            "Loss training: 38.22519\n",
            "Loss training: 30.496569\n",
            "Loss training: 42.552784\n",
            "Loss training: 21.689602\n",
            "Loss training: 35.50412\n",
            "Loss training: 41.160686\n",
            "Loss training: 40.070675\n",
            "\n",
            "***************************\n",
            "Trained on 3200 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.598995\n",
            "Loss training: 20.820347\n",
            "Loss training: 28.166561\n",
            "Loss training: 34.21959\n",
            "Loss training: 34.994133\n",
            "Loss training: 27.948872\n",
            "Loss training: 42.06433\n",
            "Loss training: 22.12597\n",
            "Loss training: 37.944492\n",
            "Loss training: 35.630455\n",
            "\n",
            "***************************\n",
            "Trained on 3210 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.74725\n",
            "Loss training: 41.26531\n",
            "Loss training: 26.860785\n",
            "Loss training: 44.417084\n",
            "Loss training: 34.98014\n",
            "Loss training: 36.10801\n",
            "Loss training: 30.425808\n",
            "Loss training: 27.737335\n",
            "Loss training: 33.247883\n",
            "Loss training: 35.277374\n",
            "\n",
            "***************************\n",
            "Trained on 3220 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.85612\n",
            "Loss training: 21.81821\n",
            "Loss training: 33.01522\n",
            "Loss training: 39.205242\n",
            "Loss training: 26.734983\n",
            "Loss training: 41.65951\n",
            "Loss training: 21.468866\n",
            "Loss training: 26.17047\n",
            "Loss training: 21.432196\n",
            "Loss training: 26.730011\n",
            "\n",
            "***************************\n",
            "Trained on 3230 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.405317\n",
            "Loss training: 28.3701\n",
            "Loss training: 43.69846\n",
            "Loss training: 40.275764\n",
            "Loss training: 23.283722\n",
            "Loss training: 42.585472\n",
            "Loss training: 40.38662\n",
            "Loss training: 30.20101\n",
            "Loss training: 37.26038\n",
            "Loss training: 26.51192\n",
            "\n",
            "***************************\n",
            "Trained on 3240 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.481537\n",
            "Loss training: 40.976738\n",
            "Loss training: 37.34912\n",
            "Loss training: 33.189537\n",
            "Loss training: 35.22738\n",
            "Loss training: 27.356287\n",
            "Loss training: 37.28397\n",
            "Loss training: 36.762424\n",
            "Loss training: 41.51155\n",
            "Loss training: 36.75457\n",
            "\n",
            "***************************\n",
            "Trained on 3250 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 43.910984\n",
            "Loss training: 39.937782\n",
            "Loss training: 39.479725\n",
            "Loss training: 34.55914\n",
            "Loss training: 26.935726\n",
            "Loss training: 23.603367\n",
            "Loss training: 40.155434\n",
            "Loss training: 26.851995\n",
            "Loss training: 39.950325\n",
            "Loss training: 38.63562\n",
            "\n",
            "***************************\n",
            "Trained on 3260 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.15323\n",
            "Loss training: 18.972061\n",
            "Loss training: 22.76438\n",
            "Loss training: 40.42085\n",
            "Loss training: 35.802216\n",
            "Loss training: 36.51768\n",
            "Loss training: 26.512539\n",
            "Loss training: 28.928642\n",
            "Loss training: 26.097778\n",
            "Loss training: 40.49921\n",
            "\n",
            "***************************\n",
            "Trained on 3270 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.841265\n",
            "Loss training: 37.35001\n",
            "Loss training: 36.081333\n",
            "Loss training: 21.662771\n",
            "Loss training: 35.685257\n",
            "Loss training: 35.603893\n",
            "Loss training: 32.944923\n",
            "Loss training: 34.589886\n",
            "Loss training: 41.617775\n",
            "Loss training: 23.679771\n",
            "\n",
            "***************************\n",
            "Trained on 3280 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.307785\n",
            "Loss training: 39.813225\n",
            "Loss training: 39.34252\n",
            "Loss training: 39.310905\n",
            "Loss training: 30.612465\n",
            "Loss training: 36.562973\n",
            "Loss training: 32.14392\n",
            "Loss training: 26.767708\n",
            "Loss training: 47.01555\n",
            "Loss training: 36.04376\n",
            "\n",
            "***************************\n",
            "Trained on 3290 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.643524\n",
            "Loss training: 28.24738\n",
            "Loss training: 26.835165\n",
            "Loss training: 36.1226\n",
            "Loss training: 40.974777\n",
            "Loss training: 39.06281\n",
            "Loss training: 27.788717\n",
            "Loss training: 38.903347\n",
            "Loss training: 35.209846\n",
            "Loss training: 39.07898\n",
            "\n",
            "***************************\n",
            "Trained on 3300 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 13.305528\n",
            "Loss training: 39.043865\n",
            "Loss training: 29.135345\n",
            "Loss training: 32.009254\n",
            "Loss training: 29.443783\n",
            "Loss training: 38.03258\n",
            "Loss training: 35.287434\n",
            "Loss training: 26.824778\n",
            "Loss training: 22.864729\n",
            "Loss training: 40.52546\n",
            "\n",
            "***************************\n",
            "Trained on 3310 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.769669\n",
            "Loss training: 28.546946\n",
            "Loss training: 40.13863\n",
            "Loss training: 48.668907\n",
            "Loss training: 28.18595\n",
            "Loss training: 28.793997\n",
            "Loss training: 38.599094\n",
            "Loss training: 30.608374\n",
            "Loss training: 46.042557\n",
            "Loss training: 41.058105\n",
            "\n",
            "***************************\n",
            "Trained on 3320 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.95753\n",
            "Loss training: 30.77951\n",
            "Loss training: 21.44047\n",
            "Loss training: 27.808262\n",
            "Loss training: 44.249016\n",
            "Loss training: 41.439075\n",
            "Loss training: 21.762531\n",
            "Loss training: 38.522728\n",
            "Loss training: 38.466873\n",
            "Loss training: 43.84422\n",
            "\n",
            "***************************\n",
            "Trained on 3330 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 17.920897\n",
            "Loss training: 26.824678\n",
            "Loss training: 17.559752\n",
            "Loss training: 43.3633\n",
            "Loss training: 27.31217\n",
            "Loss training: 35.093025\n",
            "Loss training: 35.189888\n",
            "Loss training: 37.14758\n",
            "Loss training: 30.340233\n",
            "Loss training: 26.528212\n",
            "\n",
            "***************************\n",
            "Trained on 3340 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.646423\n",
            "Loss training: 33.30346\n",
            "Loss training: 37.1849\n",
            "Loss training: 35.20897\n",
            "Loss training: 32.66052\n",
            "Loss training: 36.67534\n",
            "Loss training: 27.013155\n",
            "Loss training: 34.88678\n",
            "Loss training: 26.209866\n",
            "Loss training: 37.06421\n",
            "\n",
            "***************************\n",
            "Trained on 3350 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 42.073647\n",
            "Loss training: 26.756943\n",
            "Loss training: 33.889957\n",
            "Loss training: 45.861767\n",
            "Loss training: 22.064003\n",
            "Loss training: 43.966248\n",
            "Loss training: 34.71448\n",
            "Loss training: 27.160461\n",
            "Loss training: 33.927197\n",
            "Loss training: 28.015322\n",
            "\n",
            "***************************\n",
            "Trained on 3360 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.203667\n",
            "Loss training: 34.70501\n",
            "Loss training: 30.936813\n",
            "Loss training: 33.182354\n",
            "Loss training: 25.765284\n",
            "Loss training: 41.514217\n",
            "Loss training: 29.13298\n",
            "Loss training: 34.739506\n",
            "Loss training: 29.601086\n",
            "Loss training: 38.46257\n",
            "\n",
            "***************************\n",
            "Trained on 3370 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.326725\n",
            "Loss training: 37.786003\n",
            "Loss training: 39.742012\n",
            "Loss training: 22.982973\n",
            "Loss training: 40.221256\n",
            "Loss training: 31.984386\n",
            "Loss training: 34.999348\n",
            "Loss training: 40.919807\n",
            "Loss training: 27.3374\n",
            "Loss training: 28.963394\n",
            "\n",
            "***************************\n",
            "Trained on 3380 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.738506\n",
            "Loss training: 41.775047\n",
            "Loss training: 22.763666\n",
            "Loss training: 27.195013\n",
            "Loss training: 13.464267\n",
            "Loss training: 39.03939\n",
            "Loss training: 39.835484\n",
            "Loss training: 35.531372\n",
            "Loss training: 17.784023\n",
            "Loss training: 34.339314\n",
            "\n",
            "***************************\n",
            "Trained on 3390 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.128767\n",
            "Loss training: 32.55581\n",
            "Loss training: 38.53914\n",
            "Loss training: 32.435833\n",
            "Loss training: 28.012445\n",
            "Loss training: 32.23774\n",
            "Loss training: 35.49658\n",
            "Loss training: 41.85418\n",
            "Loss training: 31.870369\n",
            "Loss training: 26.872194\n",
            "\n",
            "***************************\n",
            "Trained on 3400 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.762955\n",
            "Loss training: 27.487598\n",
            "Loss training: 27.220623\n",
            "Loss training: 26.791574\n",
            "Loss training: 34.011562\n",
            "Loss training: 26.570568\n",
            "Loss training: 30.366463\n",
            "Loss training: 40.75492\n",
            "Loss training: 33.383327\n",
            "Loss training: 18.80097\n",
            "\n",
            "***************************\n",
            "Trained on 3410 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.657982\n",
            "Loss training: 31.22718\n",
            "Loss training: 34.685894\n",
            "Loss training: 35.214348\n",
            "Loss training: 29.098007\n",
            "Loss training: 26.871754\n",
            "Loss training: 27.292095\n",
            "Loss training: 22.826624\n",
            "Loss training: 39.4451\n",
            "Loss training: 39.94323\n",
            "\n",
            "***************************\n",
            "Trained on 3420 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.702084\n",
            "Loss training: 38.98091\n",
            "Loss training: 22.803251\n",
            "Loss training: 36.491642\n",
            "Loss training: 32.356476\n",
            "Loss training: 29.985603\n",
            "Loss training: 36.46352\n",
            "Loss training: 36.199547\n",
            "Loss training: 34.653175\n",
            "Loss training: 41.945473\n",
            "\n",
            "***************************\n",
            "Trained on 3430 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.527683\n",
            "Loss training: 28.167372\n",
            "Loss training: 35.25321\n",
            "Loss training: 44.446335\n",
            "Loss training: 19.997412\n",
            "Loss training: 21.237244\n",
            "Loss training: 33.734116\n",
            "Loss training: 28.985054\n",
            "Loss training: 21.05479\n",
            "Loss training: 33.37081\n",
            "\n",
            "***************************\n",
            "Trained on 3440 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.720078\n",
            "Loss training: 41.253437\n",
            "Loss training: 27.730898\n",
            "Loss training: 20.83594\n",
            "Loss training: 39.299236\n",
            "Loss training: 30.35725\n",
            "Loss training: 45.595978\n",
            "Loss training: 36.118122\n",
            "Loss training: 34.950256\n",
            "Loss training: 26.964647\n",
            "\n",
            "***************************\n",
            "Trained on 3450 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.41888\n",
            "Loss training: 28.146555\n",
            "Loss training: 30.044714\n",
            "Loss training: 27.9896\n",
            "Loss training: 13.239965\n",
            "Loss training: 27.755316\n",
            "Loss training: 43.88774\n",
            "Loss training: 38.72382\n",
            "Loss training: 29.705593\n",
            "Loss training: 17.930414\n",
            "\n",
            "***************************\n",
            "Trained on 3460 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.817173\n",
            "Loss training: 31.462011\n",
            "Loss training: 30.86981\n",
            "Loss training: 34.18852\n",
            "Loss training: 13.00657\n",
            "Loss training: 22.68749\n",
            "Loss training: 33.87178\n",
            "Loss training: 29.232328\n",
            "Loss training: 35.082527\n",
            "Loss training: 39.029835\n",
            "\n",
            "***************************\n",
            "Trained on 3470 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.991516\n",
            "Loss training: 22.002071\n",
            "Loss training: 29.979393\n",
            "Loss training: 30.045904\n",
            "Loss training: 26.882782\n",
            "Loss training: 13.031808\n",
            "Loss training: 32.72118\n",
            "Loss training: 39.395054\n",
            "Loss training: 29.294807\n",
            "Loss training: 21.481236\n",
            "\n",
            "***************************\n",
            "Trained on 3480 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.530516\n",
            "Loss training: 35.3561\n",
            "Loss training: 35.027626\n",
            "Loss training: 28.422432\n",
            "Loss training: 38.133087\n",
            "Loss training: 35.434803\n",
            "Loss training: 30.041063\n",
            "Loss training: 26.905542\n",
            "Loss training: 34.16986\n",
            "Loss training: 40.015163\n",
            "\n",
            "***************************\n",
            "Trained on 3490 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.835241\n",
            "Loss training: 29.638767\n",
            "Loss training: 20.51372\n",
            "Loss training: 20.446075\n",
            "Loss training: 22.96124\n",
            "Loss training: 37.59239\n",
            "Loss training: 20.17591\n",
            "Loss training: 33.01895\n",
            "Loss training: 34.48056\n",
            "Loss training: 21.123146\n",
            "\n",
            "***************************\n",
            "Trained on 3500 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.433643\n",
            "Loss training: 36.29445\n",
            "Loss training: 29.182394\n",
            "Loss training: 37.08687\n",
            "Loss training: 18.188978\n",
            "Loss training: 34.29219\n",
            "Loss training: 26.692455\n",
            "Loss training: 32.644615\n",
            "Loss training: 29.020256\n",
            "Loss training: 33.707203\n",
            "\n",
            "***************************\n",
            "Trained on 3510 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.285282\n",
            "Loss training: 28.9701\n",
            "Loss training: 28.881605\n",
            "Loss training: 26.944817\n",
            "Loss training: 33.116398\n",
            "Loss training: 30.436512\n",
            "Loss training: 40.1043\n",
            "Loss training: 13.862979\n",
            "Loss training: 39.686882\n",
            "Loss training: 37.708813\n",
            "\n",
            "***************************\n",
            "Trained on 3520 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.129112\n",
            "Loss training: 45.065\n",
            "Loss training: 20.155712\n",
            "Loss training: 39.53863\n",
            "Loss training: 28.73385\n",
            "Loss training: 35.96874\n",
            "Loss training: 26.304836\n",
            "Loss training: 28.03287\n",
            "Loss training: 26.6618\n",
            "Loss training: 25.934038\n",
            "\n",
            "***************************\n",
            "Trained on 3530 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.048513\n",
            "Loss training: 37.047123\n",
            "Loss training: 45.00979\n",
            "Loss training: 44.6612\n",
            "Loss training: 27.58512\n",
            "Loss training: 21.589924\n",
            "Loss training: 40.04982\n",
            "Loss training: 27.049398\n",
            "Loss training: 39.027813\n",
            "Loss training: 35.207108\n",
            "\n",
            "***************************\n",
            "Trained on 3540 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.48037\n",
            "Loss training: 39.40615\n",
            "Loss training: 33.002747\n",
            "Loss training: 37.644333\n",
            "Loss training: 33.224415\n",
            "Loss training: 39.137978\n",
            "Loss training: 27.488022\n",
            "Loss training: 24.89296\n",
            "Loss training: 40.37716\n",
            "Loss training: 40.811485\n",
            "\n",
            "***************************\n",
            "Trained on 3550 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.371395\n",
            "Loss training: 40.39952\n",
            "Loss training: 39.23862\n",
            "Loss training: 35.339516\n",
            "Loss training: 37.34795\n",
            "Loss training: 36.564198\n",
            "Loss training: 28.13733\n",
            "Loss training: 34.452732\n",
            "Loss training: 28.63314\n",
            "Loss training: 28.827707\n",
            "\n",
            "***************************\n",
            "Trained on 3560 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.8562\n",
            "Loss training: 33.42523\n",
            "Loss training: 21.58916\n",
            "Loss training: 41.26565\n",
            "Loss training: 33.925686\n",
            "Loss training: 38.29525\n",
            "Loss training: 37.352573\n",
            "Loss training: 25.300964\n",
            "Loss training: 28.05601\n",
            "Loss training: 36.899025\n",
            "\n",
            "***************************\n",
            "Trained on 3570 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.858036\n",
            "Loss training: 26.754812\n",
            "Loss training: 29.41728\n",
            "Loss training: 35.458416\n",
            "Loss training: 23.332201\n",
            "Loss training: 40.23918\n",
            "Loss training: 23.090036\n",
            "Loss training: 40.067013\n",
            "Loss training: 35.607002\n",
            "Loss training: 28.232708\n",
            "\n",
            "***************************\n",
            "Trained on 3580 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.54363\n",
            "Loss training: 31.18024\n",
            "Loss training: 40.497227\n",
            "Loss training: 26.825274\n",
            "Loss training: 47.12695\n",
            "Loss training: 35.368492\n",
            "Loss training: 33.3221\n",
            "Loss training: 26.697224\n",
            "Loss training: 27.594128\n",
            "Loss training: 26.222769\n",
            "\n",
            "***************************\n",
            "Trained on 3590 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.92858\n",
            "Loss training: 32.549923\n",
            "Loss training: 38.109947\n",
            "Loss training: 30.328522\n",
            "Loss training: 36.32127\n",
            "Loss training: 35.043278\n",
            "Loss training: 38.853436\n",
            "Loss training: 25.022415\n",
            "Loss training: 34.91685\n",
            "Loss training: 12.914881\n",
            "\n",
            "***************************\n",
            "Trained on 3600 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.453045\n",
            "Loss training: 18.019077\n",
            "Loss training: 36.497025\n",
            "Loss training: 29.309994\n",
            "Loss training: 40.719055\n",
            "Loss training: 26.933249\n",
            "Loss training: 35.645683\n",
            "Loss training: 34.354256\n",
            "Loss training: 26.382938\n",
            "Loss training: 28.1308\n",
            "\n",
            "***************************\n",
            "Trained on 3610 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.811518\n",
            "Loss training: 38.009693\n",
            "Loss training: 37.342667\n",
            "Loss training: 39.267513\n",
            "Loss training: 29.158325\n",
            "Loss training: 28.254202\n",
            "Loss training: 29.43243\n",
            "Loss training: 34.489033\n",
            "Loss training: 34.65689\n",
            "Loss training: 26.226662\n",
            "\n",
            "***************************\n",
            "Trained on 3620 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.25705\n",
            "Loss training: 45.214085\n",
            "Loss training: 39.728874\n",
            "Loss training: 32.9933\n",
            "Loss training: 28.091515\n",
            "Loss training: 38.905754\n",
            "Loss training: 39.36288\n",
            "Loss training: 30.901764\n",
            "Loss training: 41.73675\n",
            "Loss training: 39.32611\n",
            "\n",
            "***************************\n",
            "Trained on 3630 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.709652\n",
            "Loss training: 35.163486\n",
            "Loss training: 37.063725\n",
            "Loss training: 38.64839\n",
            "Loss training: 27.115454\n",
            "Loss training: 20.682484\n",
            "Loss training: 26.5139\n",
            "Loss training: 40.30968\n",
            "Loss training: 31.789036\n",
            "Loss training: 35.895554\n",
            "\n",
            "***************************\n",
            "Trained on 3640 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.112156\n",
            "Loss training: 28.040794\n",
            "Loss training: 23.406376\n",
            "Loss training: 35.294323\n",
            "Loss training: 29.514605\n",
            "Loss training: 36.584793\n",
            "Loss training: 38.97086\n",
            "Loss training: 27.342894\n",
            "Loss training: 38.598938\n",
            "Loss training: 38.66194\n",
            "\n",
            "***************************\n",
            "Trained on 3650 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.91775\n",
            "Loss training: 22.797424\n",
            "Loss training: 27.89016\n",
            "Loss training: 40.619434\n",
            "Loss training: 33.140247\n",
            "Loss training: 33.020706\n",
            "Loss training: 25.532536\n",
            "Loss training: 26.895624\n",
            "Loss training: 13.242393\n",
            "Loss training: 32.376736\n",
            "\n",
            "***************************\n",
            "Trained on 3660 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.499634\n",
            "Loss training: 36.489235\n",
            "Loss training: 32.31619\n",
            "Loss training: 22.558739\n",
            "Loss training: 34.01517\n",
            "Loss training: 34.04486\n",
            "Loss training: 29.519339\n",
            "Loss training: 21.868567\n",
            "Loss training: 26.506628\n",
            "Loss training: 26.429138\n",
            "\n",
            "***************************\n",
            "Trained on 3670 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 21.419605\n",
            "Loss training: 29.138584\n",
            "Loss training: 26.107586\n",
            "Loss training: 12.998692\n",
            "Loss training: 33.32335\n",
            "Loss training: 27.859625\n",
            "Loss training: 36.62007\n",
            "Loss training: 35.632626\n",
            "Loss training: 36.619755\n",
            "Loss training: 39.07811\n",
            "\n",
            "***************************\n",
            "Trained on 3680 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.272957\n",
            "Loss training: 38.03789\n",
            "Loss training: 27.461958\n",
            "Loss training: 25.421759\n",
            "Loss training: 25.328758\n",
            "Loss training: 36.573906\n",
            "Loss training: 30.513077\n",
            "Loss training: 26.956799\n",
            "Loss training: 35.93806\n",
            "Loss training: 19.245756\n",
            "\n",
            "***************************\n",
            "Trained on 3690 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.174778\n",
            "Loss training: 26.590734\n",
            "Loss training: 12.681239\n",
            "Loss training: 36.15457\n",
            "Loss training: 26.526793\n",
            "Loss training: 12.977143\n",
            "Loss training: 36.883327\n",
            "Loss training: 38.8733\n",
            "Loss training: 28.142405\n",
            "Loss training: 36.003773\n",
            "\n",
            "***************************\n",
            "Trained on 3700 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.82002\n",
            "Loss training: 36.015774\n",
            "Loss training: 35.86556\n",
            "Loss training: 29.449785\n",
            "Loss training: 39.483913\n",
            "Loss training: 30.878618\n",
            "Loss training: 33.956352\n",
            "Loss training: 27.549839\n",
            "Loss training: 30.536604\n",
            "Loss training: 21.405123\n",
            "\n",
            "***************************\n",
            "Trained on 3710 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.249052\n",
            "Loss training: 22.900951\n",
            "Loss training: 28.736023\n",
            "Loss training: 26.553959\n",
            "Loss training: 40.96727\n",
            "Loss training: 36.372005\n",
            "Loss training: 29.376862\n",
            "Loss training: 45.022446\n",
            "Loss training: 29.98505\n",
            "Loss training: 33.98933\n",
            "\n",
            "***************************\n",
            "Trained on 3720 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.356808\n",
            "Loss training: 34.021095\n",
            "Loss training: 33.641026\n",
            "Loss training: 34.04998\n",
            "Loss training: 26.96796\n",
            "Loss training: 45.133644\n",
            "Loss training: 34.37355\n",
            "Loss training: 27.66497\n",
            "Loss training: 34.249424\n",
            "Loss training: 13.017404\n",
            "\n",
            "***************************\n",
            "Trained on 3730 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.88362\n",
            "Loss training: 39.94144\n",
            "Loss training: 39.733246\n",
            "Loss training: 33.6176\n",
            "Loss training: 35.568424\n",
            "Loss training: 27.13221\n",
            "Loss training: 33.153244\n",
            "Loss training: 21.776218\n",
            "Loss training: 20.363707\n",
            "Loss training: 28.961458\n",
            "\n",
            "***************************\n",
            "Trained on 3740 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.755514\n",
            "Loss training: 45.013947\n",
            "Loss training: 40.211006\n",
            "Loss training: 20.374374\n",
            "Loss training: 44.208405\n",
            "Loss training: 30.899382\n",
            "Loss training: 20.5589\n",
            "Loss training: 35.642544\n",
            "Loss training: 41.71708\n",
            "Loss training: 37.356712\n",
            "\n",
            "***************************\n",
            "Trained on 3750 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.380264\n",
            "Loss training: 27.144167\n",
            "Loss training: 26.608023\n",
            "Loss training: 28.049353\n",
            "Loss training: 39.333523\n",
            "Loss training: 26.50065\n",
            "Loss training: 38.10008\n",
            "Loss training: 32.584892\n",
            "Loss training: 30.770498\n",
            "Loss training: 23.753897\n",
            "\n",
            "***************************\n",
            "Trained on 3760 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.379894\n",
            "Loss training: 35.311096\n",
            "Loss training: 27.839384\n",
            "Loss training: 27.538134\n",
            "Loss training: 30.147661\n",
            "Loss training: 28.540125\n",
            "Loss training: 36.45325\n",
            "Loss training: 36.81344\n",
            "Loss training: 39.985046\n",
            "Loss training: 34.263996\n",
            "\n",
            "***************************\n",
            "Trained on 3770 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.075695\n",
            "Loss training: 27.459513\n",
            "Loss training: 44.841774\n",
            "Loss training: 32.142883\n",
            "Loss training: 35.028275\n",
            "Loss training: 43.44253\n",
            "Loss training: 36.090706\n",
            "Loss training: 28.999651\n",
            "Loss training: 22.816282\n",
            "Loss training: 27.957851\n",
            "\n",
            "***************************\n",
            "Trained on 3780 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.43704\n",
            "Loss training: 33.991055\n",
            "Loss training: 33.23712\n",
            "Loss training: 29.84469\n",
            "Loss training: 39.322697\n",
            "Loss training: 40.895138\n",
            "Loss training: 33.563828\n",
            "Loss training: 13.040575\n",
            "Loss training: 40.344463\n",
            "Loss training: 40.137726\n",
            "\n",
            "***************************\n",
            "Trained on 3790 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.55361\n",
            "Loss training: 39.266968\n",
            "Loss training: 38.67881\n",
            "Loss training: 26.935423\n",
            "Loss training: 27.706799\n",
            "Loss training: 20.187906\n",
            "Loss training: 29.330376\n",
            "Loss training: 36.225723\n",
            "Loss training: 39.57927\n",
            "Loss training: 34.574375\n",
            "\n",
            "***************************\n",
            "Trained on 3800 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.54039\n",
            "Loss training: 18.223305\n",
            "Loss training: 26.391937\n",
            "Loss training: 33.391014\n",
            "Loss training: 32.160618\n",
            "Loss training: 34.678886\n",
            "Loss training: 39.524597\n",
            "Loss training: 33.860962\n",
            "Loss training: 34.543793\n",
            "Loss training: 36.31499\n",
            "\n",
            "***************************\n",
            "Trained on 3810 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.268536\n",
            "Loss training: 33.157997\n",
            "Loss training: 31.11362\n",
            "Loss training: 13.174542\n",
            "Loss training: 38.303757\n",
            "Loss training: 12.902274\n",
            "Loss training: 36.058495\n",
            "Loss training: 22.454235\n",
            "Loss training: 30.258663\n",
            "Loss training: 25.16495\n",
            "\n",
            "***************************\n",
            "Trained on 3820 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.996464\n",
            "Loss training: 29.558231\n",
            "Loss training: 19.78018\n",
            "Loss training: 24.744318\n",
            "Loss training: 26.552965\n",
            "Loss training: 12.902438\n",
            "Loss training: 28.984486\n",
            "Loss training: 37.838108\n",
            "Loss training: 33.47633\n",
            "Loss training: 29.740475\n",
            "\n",
            "***************************\n",
            "Trained on 3830 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.545326\n",
            "Loss training: 34.1441\n",
            "Loss training: 12.450428\n",
            "Loss training: 37.708492\n",
            "Loss training: 33.221523\n",
            "Loss training: 32.874527\n",
            "Loss training: 41.34222\n",
            "Loss training: 32.432465\n",
            "Loss training: 37.467373\n",
            "Loss training: 38.88998\n",
            "\n",
            "***************************\n",
            "Trained on 3840 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 19.736439\n",
            "Loss training: 26.074137\n",
            "Loss training: 34.585938\n",
            "Loss training: 35.60284\n",
            "Loss training: 40.15553\n",
            "Loss training: 34.617485\n",
            "Loss training: 28.289362\n",
            "Loss training: 32.137463\n",
            "Loss training: 33.72585\n",
            "Loss training: 37.53722\n",
            "\n",
            "***************************\n",
            "Trained on 3850 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.981525\n",
            "Loss training: 40.907547\n",
            "Loss training: 37.44532\n",
            "Loss training: 12.568654\n",
            "Loss training: 18.929111\n",
            "Loss training: 38.42731\n",
            "Loss training: 29.326355\n",
            "Loss training: 36.20439\n",
            "Loss training: 26.320938\n",
            "Loss training: 28.929966\n",
            "\n",
            "***************************\n",
            "Trained on 3860 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.889454\n",
            "Loss training: 33.770256\n",
            "Loss training: 21.789454\n",
            "Loss training: 26.035748\n",
            "Loss training: 26.859707\n",
            "Loss training: 21.488317\n",
            "Loss training: 28.351303\n",
            "Loss training: 36.92631\n",
            "Loss training: 28.068289\n",
            "Loss training: 34.968376\n",
            "\n",
            "***************************\n",
            "Trained on 3870 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.512043\n",
            "Loss training: 35.27886\n",
            "Loss training: 32.939747\n",
            "Loss training: 34.785217\n",
            "Loss training: 33.545025\n",
            "Loss training: 28.350395\n",
            "Loss training: 28.92341\n",
            "Loss training: 36.65717\n",
            "Loss training: 30.055326\n",
            "Loss training: 29.852654\n",
            "\n",
            "***************************\n",
            "Trained on 3880 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 43.916916\n",
            "Loss training: 39.821087\n",
            "Loss training: 34.20614\n",
            "Loss training: 26.062967\n",
            "Loss training: 28.246632\n",
            "Loss training: 32.358982\n",
            "Loss training: 36.118248\n",
            "Loss training: 39.810577\n",
            "Loss training: 25.092754\n",
            "Loss training: 13.554362\n",
            "\n",
            "***************************\n",
            "Trained on 3890 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.20198\n",
            "Loss training: 33.401134\n",
            "Loss training: 36.6268\n",
            "Loss training: 33.89443\n",
            "Loss training: 33.34991\n",
            "Loss training: 54.291447\n",
            "Loss training: 22.49353\n",
            "Loss training: 22.66237\n",
            "Loss training: 38.126286\n",
            "Loss training: 37.833683\n",
            "\n",
            "***************************\n",
            "Trained on 3900 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.28809\n",
            "Loss training: 34.595875\n",
            "Loss training: 38.46488\n",
            "Loss training: 19.072605\n",
            "Loss training: 38.97545\n",
            "Loss training: 29.09451\n",
            "Loss training: 27.679668\n",
            "Loss training: 13.311805\n",
            "Loss training: 27.941195\n",
            "Loss training: 34.30427\n",
            "\n",
            "***************************\n",
            "Trained on 3910 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 13.235553\n",
            "Loss training: 34.855846\n",
            "Loss training: 29.13177\n",
            "Loss training: 35.10749\n",
            "Loss training: 12.840215\n",
            "Loss training: 35.181168\n",
            "Loss training: 36.61509\n",
            "Loss training: 26.11104\n",
            "Loss training: 49.836147\n",
            "Loss training: 27.8864\n",
            "\n",
            "***************************\n",
            "Trained on 3920 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.518871\n",
            "Loss training: 35.41594\n",
            "Loss training: 26.764082\n",
            "Loss training: 36.214462\n",
            "Loss training: 35.29549\n",
            "Loss training: 38.89599\n",
            "Loss training: 28.13167\n",
            "Loss training: 19.985502\n",
            "Loss training: 22.8329\n",
            "Loss training: 35.70605\n",
            "\n",
            "***************************\n",
            "Trained on 3930 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.265688\n",
            "Loss training: 46.765884\n",
            "Loss training: 26.807688\n",
            "Loss training: 18.914703\n",
            "Loss training: 21.61706\n",
            "Loss training: 17.804333\n",
            "Loss training: 26.696898\n",
            "Loss training: 41.523796\n",
            "Loss training: 38.160187\n",
            "Loss training: 44.963627\n",
            "\n",
            "***************************\n",
            "Trained on 3940 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.833202\n",
            "Loss training: 13.980156\n",
            "Loss training: 35.516056\n",
            "Loss training: 40.583275\n",
            "Loss training: 37.657993\n",
            "Loss training: 37.5482\n",
            "Loss training: 17.559002\n",
            "Loss training: 34.698223\n",
            "Loss training: 34.2288\n",
            "Loss training: 27.749483\n",
            "\n",
            "***************************\n",
            "Trained on 3950 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.92821\n",
            "Loss training: 33.018513\n",
            "Loss training: 34.000374\n",
            "Loss training: 35.545868\n",
            "Loss training: 29.428816\n",
            "Loss training: 34.693768\n",
            "Loss training: 27.702051\n",
            "Loss training: 27.988075\n",
            "Loss training: 39.961044\n",
            "Loss training: 40.065178\n",
            "\n",
            "***************************\n",
            "Trained on 3960 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.129982\n",
            "Loss training: 35.81937\n",
            "Loss training: 39.852875\n",
            "Loss training: 19.682468\n",
            "Loss training: 19.49632\n",
            "Loss training: 33.464664\n",
            "Loss training: 26.802097\n",
            "Loss training: 26.30873\n",
            "Loss training: 26.204815\n",
            "Loss training: 39.475475\n",
            "\n",
            "***************************\n",
            "Trained on 3970 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.889935\n",
            "Loss training: 39.04674\n",
            "Loss training: 21.572844\n",
            "Loss training: 35.716076\n",
            "Loss training: 34.65278\n",
            "Loss training: 18.268272\n",
            "Loss training: 33.572765\n",
            "Loss training: 27.019817\n",
            "Loss training: 35.704422\n",
            "Loss training: 29.547836\n",
            "\n",
            "***************************\n",
            "Trained on 3980 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.553272\n",
            "Loss training: 40.040012\n",
            "Loss training: 24.45154\n",
            "Loss training: 32.502586\n",
            "Loss training: 30.634203\n",
            "Loss training: 16.83779\n",
            "Loss training: 26.003525\n",
            "Loss training: 33.973904\n",
            "Loss training: 16.550278\n",
            "Loss training: 38.255238\n",
            "\n",
            "***************************\n",
            "Trained on 3990 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 16.368828\n",
            "Loss training: 12.833313\n",
            "Loss training: 37.778065\n",
            "Loss training: 40.250923\n",
            "Loss training: 37.997967\n",
            "Loss training: 29.34021\n",
            "Loss training: 26.581491\n",
            "Loss training: 32.554405\n",
            "Loss training: 38.53655\n",
            "Loss training: 34.21061\n",
            "\n",
            "***************************\n",
            "Trained on 4000 graphs\n",
            "***************************\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_53650e7a-686c-43f0-9805-612baa69acd7\", \"params_epochs_4000.pickle\", 22204504)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8b4295e3-f638-47bf-b2ed-dda9e4792545\", \"opt_state_epochs_4000.pickle\", 44409100)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss training: 28.242743\n",
            "Loss training: 34.401028\n",
            "Loss training: 39.258286\n",
            "Loss training: 25.902586\n",
            "Loss training: 27.061543\n",
            "Loss training: 26.561186\n",
            "Loss training: 33.688698\n",
            "Loss training: 45.239983\n",
            "Loss training: 36.219593\n",
            "Loss training: 33.665867\n",
            "\n",
            "***************************\n",
            "Trained on 4010 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.509752\n",
            "Loss training: 33.646347\n",
            "Loss training: 43.88592\n",
            "Loss training: 17.155106\n",
            "Loss training: 24.487831\n",
            "Loss training: 30.131502\n",
            "Loss training: 35.706135\n",
            "Loss training: 37.324192\n",
            "Loss training: 22.77784\n",
            "Loss training: 26.90574\n",
            "\n",
            "***************************\n",
            "Trained on 4020 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.000996\n",
            "Loss training: 16.498962\n",
            "Loss training: 36.302853\n",
            "Loss training: 22.54553\n",
            "Loss training: 26.736486\n",
            "Loss training: 33.825172\n",
            "Loss training: 42.6931\n",
            "Loss training: 35.459534\n",
            "Loss training: 26.198061\n",
            "Loss training: 22.375727\n",
            "\n",
            "***************************\n",
            "Trained on 4030 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.678185\n",
            "Loss training: 45.39398\n",
            "Loss training: 38.933285\n",
            "Loss training: 29.466482\n",
            "Loss training: 27.165428\n",
            "Loss training: 34.25281\n",
            "Loss training: 29.302458\n",
            "Loss training: 40.012592\n",
            "Loss training: 25.349264\n",
            "Loss training: 35.64551\n",
            "\n",
            "***************************\n",
            "Trained on 4040 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.238575\n",
            "Loss training: 29.857128\n",
            "Loss training: 29.754011\n",
            "Loss training: 44.016907\n",
            "Loss training: 32.28465\n",
            "Loss training: 13.016619\n",
            "Loss training: 41.35688\n",
            "Loss training: 33.51963\n",
            "Loss training: 29.601858\n",
            "Loss training: 29.568644\n",
            "\n",
            "***************************\n",
            "Trained on 4050 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.535583\n",
            "Loss training: 34.8856\n",
            "Loss training: 26.83171\n",
            "Loss training: 35.134624\n",
            "Loss training: 34.827126\n",
            "Loss training: 32.78356\n",
            "Loss training: 42.061714\n",
            "Loss training: 35.190914\n",
            "Loss training: 37.27005\n",
            "Loss training: 34.078\n",
            "\n",
            "***************************\n",
            "Trained on 4060 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.151676\n",
            "Loss training: 39.394478\n",
            "Loss training: 39.43654\n",
            "Loss training: 22.471523\n",
            "Loss training: 40.202465\n",
            "Loss training: 39.00767\n",
            "Loss training: 39.53013\n",
            "Loss training: 16.753199\n",
            "Loss training: 35.26764\n",
            "Loss training: 32.304287\n",
            "\n",
            "***************************\n",
            "Trained on 4070 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 43.39154\n",
            "Loss training: 37.789413\n",
            "Loss training: 26.63366\n",
            "Loss training: 31.836243\n",
            "Loss training: 30.20625\n",
            "Loss training: 38.72115\n",
            "Loss training: 26.173899\n",
            "Loss training: 29.548864\n",
            "Loss training: 22.747972\n",
            "Loss training: 35.03253\n",
            "\n",
            "***************************\n",
            "Trained on 4080 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 21.543459\n",
            "Loss training: 34.85112\n",
            "Loss training: 22.405962\n",
            "Loss training: 16.28685\n",
            "Loss training: 34.412354\n",
            "Loss training: 24.55278\n",
            "Loss training: 30.331852\n",
            "Loss training: 28.563953\n",
            "Loss training: 34.273037\n",
            "Loss training: 37.223408\n",
            "\n",
            "***************************\n",
            "Trained on 4090 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.818123\n",
            "Loss training: 35.928577\n",
            "Loss training: 36.54321\n",
            "Loss training: 33.960045\n",
            "Loss training: 16.343626\n",
            "Loss training: 36.177967\n",
            "Loss training: 39.562492\n",
            "Loss training: 26.528282\n",
            "Loss training: 28.619242\n",
            "Loss training: 33.054848\n",
            "\n",
            "***************************\n",
            "Trained on 4100 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 20.823812\n",
            "Loss training: 31.041872\n",
            "Loss training: 28.202808\n",
            "Loss training: 26.691202\n",
            "Loss training: 27.757946\n",
            "Loss training: 26.281988\n",
            "Loss training: 35.64851\n",
            "Loss training: 39.481373\n",
            "Loss training: 21.080956\n",
            "Loss training: 39.537277\n",
            "\n",
            "***************************\n",
            "Trained on 4110 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.111942\n",
            "Loss training: 28.204725\n",
            "Loss training: 29.306519\n",
            "Loss training: 27.735365\n",
            "Loss training: 26.114601\n",
            "Loss training: 22.323763\n",
            "Loss training: 37.083664\n",
            "Loss training: 32.070385\n",
            "Loss training: 35.504322\n",
            "Loss training: 34.961643\n",
            "\n",
            "***************************\n",
            "Trained on 4120 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.560268\n",
            "Loss training: 26.523928\n",
            "Loss training: 35.0146\n",
            "Loss training: 28.621117\n",
            "Loss training: 26.971107\n",
            "Loss training: 38.093735\n",
            "Loss training: 27.595245\n",
            "Loss training: 16.262377\n",
            "Loss training: 30.588957\n",
            "Loss training: 39.307884\n",
            "\n",
            "***************************\n",
            "Trained on 4130 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.225883\n",
            "Loss training: 29.215883\n",
            "Loss training: 33.557278\n",
            "Loss training: 28.673866\n",
            "Loss training: 32.967026\n",
            "Loss training: 33.63398\n",
            "Loss training: 26.815573\n",
            "Loss training: 41.5017\n",
            "Loss training: 32.89642\n",
            "Loss training: 35.71727\n",
            "\n",
            "***************************\n",
            "Trained on 4140 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 16.442627\n",
            "Loss training: 28.26835\n",
            "Loss training: 34.488865\n",
            "Loss training: 12.879031\n",
            "Loss training: 16.276302\n",
            "Loss training: 26.62784\n",
            "Loss training: 31.16313\n",
            "Loss training: 41.040424\n",
            "Loss training: 39.84479\n",
            "Loss training: 36.08659\n",
            "\n",
            "***************************\n",
            "Trained on 4150 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.434315\n",
            "Loss training: 38.063663\n",
            "Loss training: 19.829256\n",
            "Loss training: 28.746542\n",
            "Loss training: 29.774027\n",
            "Loss training: 12.685954\n",
            "Loss training: 34.00052\n",
            "Loss training: 40.030724\n",
            "Loss training: 27.61075\n",
            "Loss training: 12.704639\n",
            "\n",
            "***************************\n",
            "Trained on 4160 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.983883\n",
            "Loss training: 46.631325\n",
            "Loss training: 28.108212\n",
            "Loss training: 35.362186\n",
            "Loss training: 33.161217\n",
            "Loss training: 16.578012\n",
            "Loss training: 41.151764\n",
            "Loss training: 34.488754\n",
            "Loss training: 36.046547\n",
            "Loss training: 38.455273\n",
            "\n",
            "***************************\n",
            "Trained on 4170 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.95699\n",
            "Loss training: 26.625755\n",
            "Loss training: 34.79656\n",
            "Loss training: 28.958689\n",
            "Loss training: 28.838282\n",
            "Loss training: 16.198557\n",
            "Loss training: 38.36273\n",
            "Loss training: 28.448456\n",
            "Loss training: 35.646046\n",
            "Loss training: 29.323616\n",
            "\n",
            "***************************\n",
            "Trained on 4180 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.771145\n",
            "Loss training: 33.836143\n",
            "Loss training: 28.233967\n",
            "Loss training: 27.776087\n",
            "Loss training: 38.263546\n",
            "Loss training: 35.644844\n",
            "Loss training: 27.451046\n",
            "Loss training: 32.274548\n",
            "Loss training: 32.005993\n",
            "Loss training: 27.735828\n",
            "\n",
            "***************************\n",
            "Trained on 4190 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.955616\n",
            "Loss training: 39.88823\n",
            "Loss training: 27.595964\n",
            "Loss training: 33.917538\n",
            "Loss training: 29.687662\n",
            "Loss training: 26.550573\n",
            "Loss training: 16.382874\n",
            "Loss training: 13.175709\n",
            "Loss training: 44.33806\n",
            "Loss training: 35.918503\n",
            "\n",
            "***************************\n",
            "Trained on 4200 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.316326\n",
            "Loss training: 39.018604\n",
            "Loss training: 38.01136\n",
            "Loss training: 34.995697\n",
            "Loss training: 27.865402\n",
            "Loss training: 31.546387\n",
            "Loss training: 37.530388\n",
            "Loss training: 36.962074\n",
            "Loss training: 33.700283\n",
            "Loss training: 37.73271\n",
            "\n",
            "***************************\n",
            "Trained on 4210 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.889297\n",
            "Loss training: 31.197819\n",
            "Loss training: 35.089317\n",
            "Loss training: 27.575325\n",
            "Loss training: 12.590644\n",
            "Loss training: 21.60042\n",
            "Loss training: 29.293447\n",
            "Loss training: 17.990015\n",
            "Loss training: 33.547497\n",
            "Loss training: 19.490437\n",
            "\n",
            "***************************\n",
            "Trained on 4220 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.936455\n",
            "Loss training: 26.610304\n",
            "Loss training: 44.002903\n",
            "Loss training: 32.277546\n",
            "Loss training: 16.047049\n",
            "Loss training: 34.57916\n",
            "Loss training: 38.50807\n",
            "Loss training: 37.464184\n",
            "Loss training: 37.04208\n",
            "Loss training: 33.805553\n",
            "\n",
            "***************************\n",
            "Trained on 4230 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.215515\n",
            "Loss training: 34.032288\n",
            "Loss training: 42.829876\n",
            "Loss training: 33.39823\n",
            "Loss training: 19.31032\n",
            "Loss training: 31.213898\n",
            "Loss training: 26.8292\n",
            "Loss training: 38.41899\n",
            "Loss training: 30.846119\n",
            "Loss training: 26.497662\n",
            "\n",
            "***************************\n",
            "Trained on 4240 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.027922\n",
            "Loss training: 39.229847\n",
            "Loss training: 32.969803\n",
            "Loss training: 28.508537\n",
            "Loss training: 34.46967\n",
            "Loss training: 39.00074\n",
            "Loss training: 26.333218\n",
            "Loss training: 35.29523\n",
            "Loss training: 19.009068\n",
            "Loss training: 35.888412\n",
            "\n",
            "***************************\n",
            "Trained on 4250 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.432484\n",
            "Loss training: 33.977997\n",
            "Loss training: 27.501732\n",
            "Loss training: 23.174109\n",
            "Loss training: 34.369354\n",
            "Loss training: 24.032606\n",
            "Loss training: 27.320736\n",
            "Loss training: 35.60305\n",
            "Loss training: 34.833294\n",
            "Loss training: 19.426569\n",
            "\n",
            "***************************\n",
            "Trained on 4260 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.28792\n",
            "Loss training: 26.467754\n",
            "Loss training: 12.935558\n",
            "Loss training: 36.95744\n",
            "Loss training: 27.261343\n",
            "Loss training: 36.420486\n",
            "Loss training: 37.775715\n",
            "Loss training: 26.085417\n",
            "Loss training: 28.28924\n",
            "Loss training: 37.358654\n",
            "\n",
            "***************************\n",
            "Trained on 4270 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.381817\n",
            "Loss training: 12.450592\n",
            "Loss training: 12.414857\n",
            "Loss training: 25.921276\n",
            "Loss training: 26.096014\n",
            "Loss training: 29.355606\n",
            "Loss training: 29.161867\n",
            "Loss training: 27.305157\n",
            "Loss training: 25.503113\n",
            "Loss training: 27.282303\n",
            "\n",
            "***************************\n",
            "Trained on 4280 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.59493\n",
            "Loss training: 33.629116\n",
            "Loss training: 34.470585\n",
            "Loss training: 37.74698\n",
            "Loss training: 27.134348\n",
            "Loss training: 38.817715\n",
            "Loss training: 28.847271\n",
            "Loss training: 34.723816\n",
            "Loss training: 30.600122\n",
            "Loss training: 37.96878\n",
            "\n",
            "***************************\n",
            "Trained on 4290 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.559753\n",
            "Loss training: 37.87634\n",
            "Loss training: 37.843246\n",
            "Loss training: 25.750898\n",
            "Loss training: 25.547293\n",
            "Loss training: 35.556248\n",
            "Loss training: 29.671677\n",
            "Loss training: 25.412596\n",
            "Loss training: 34.352634\n",
            "Loss training: 33.147556\n",
            "\n",
            "***************************\n",
            "Trained on 4300 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.864788\n",
            "Loss training: 36.57876\n",
            "Loss training: 33.997173\n",
            "Loss training: 27.183945\n",
            "Loss training: 35.88466\n",
            "Loss training: 25.671253\n",
            "Loss training: 25.921967\n",
            "Loss training: 33.945915\n",
            "Loss training: 37.573093\n",
            "Loss training: 27.038662\n",
            "\n",
            "***************************\n",
            "Trained on 4310 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.66343\n",
            "Loss training: 38.858852\n",
            "Loss training: 24.389355\n",
            "Loss training: 37.92775\n",
            "Loss training: 28.549221\n",
            "Loss training: 35.173996\n",
            "Loss training: 30.693333\n",
            "Loss training: 45.00227\n",
            "Loss training: 37.442703\n",
            "Loss training: 35.591393\n",
            "\n",
            "***************************\n",
            "Trained on 4320 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.05041\n",
            "Loss training: 38.826912\n",
            "Loss training: 12.505955\n",
            "Loss training: 32.415485\n",
            "Loss training: 32.975937\n",
            "Loss training: 37.038635\n",
            "Loss training: 34.779423\n",
            "Loss training: 23.66345\n",
            "Loss training: 29.742197\n",
            "Loss training: 38.23289\n",
            "\n",
            "***************************\n",
            "Trained on 4330 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.468437\n",
            "Loss training: 28.593739\n",
            "Loss training: 22.743633\n",
            "Loss training: 34.572224\n",
            "Loss training: 30.16594\n",
            "Loss training: 36.7221\n",
            "Loss training: 38.99352\n",
            "Loss training: 33.265846\n",
            "Loss training: 21.68682\n",
            "Loss training: 22.275478\n",
            "\n",
            "***************************\n",
            "Trained on 4340 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.485622\n",
            "Loss training: 36.427265\n",
            "Loss training: 33.63568\n",
            "Loss training: 33.311554\n",
            "Loss training: 27.27569\n",
            "Loss training: 35.57096\n",
            "Loss training: 37.970387\n",
            "Loss training: 33.8651\n",
            "Loss training: 26.223345\n",
            "Loss training: 34.989365\n",
            "\n",
            "***************************\n",
            "Trained on 4350 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.302856\n",
            "Loss training: 25.781193\n",
            "Loss training: 37.702946\n",
            "Loss training: 32.82618\n",
            "Loss training: 28.787552\n",
            "Loss training: 26.656666\n",
            "Loss training: 25.798086\n",
            "Loss training: 29.148138\n",
            "Loss training: 21.93768\n",
            "Loss training: 36.750126\n",
            "\n",
            "***************************\n",
            "Trained on 4360 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.950283\n",
            "Loss training: 18.966259\n",
            "Loss training: 39.17809\n",
            "Loss training: 35.175087\n",
            "Loss training: 26.455181\n",
            "Loss training: 35.267193\n",
            "Loss training: 29.113716\n",
            "Loss training: 21.151524\n",
            "Loss training: 26.452803\n",
            "Loss training: 40.680187\n",
            "\n",
            "***************************\n",
            "Trained on 4370 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.730057\n",
            "Loss training: 30.320562\n",
            "Loss training: 35.035866\n",
            "Loss training: 36.274914\n",
            "Loss training: 21.045286\n",
            "Loss training: 19.399067\n",
            "Loss training: 30.889126\n",
            "Loss training: 32.780777\n",
            "Loss training: 38.303272\n",
            "Loss training: 39.03482\n",
            "\n",
            "***************************\n",
            "Trained on 4380 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.871622\n",
            "Loss training: 19.129868\n",
            "Loss training: 33.783768\n",
            "Loss training: 27.236345\n",
            "Loss training: 27.349876\n",
            "Loss training: 28.1594\n",
            "Loss training: 26.566801\n",
            "Loss training: 26.026379\n",
            "Loss training: 66.53737\n",
            "Loss training: 38.767868\n",
            "\n",
            "***************************\n",
            "Trained on 4390 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 53.288544\n",
            "Loss training: 27.141905\n",
            "Loss training: 15.903622\n",
            "Loss training: 27.987293\n",
            "Loss training: 32.85965\n",
            "Loss training: 28.419165\n",
            "Loss training: 27.177708\n",
            "Loss training: 26.592941\n",
            "Loss training: 22.6407\n",
            "Loss training: 47.48327\n",
            "\n",
            "***************************\n",
            "Trained on 4400 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.478832\n",
            "Loss training: 29.36506\n",
            "Loss training: 38.00084\n",
            "Loss training: 35.968025\n",
            "Loss training: 35.767296\n",
            "Loss training: 40.227154\n",
            "Loss training: 55.08697\n",
            "Loss training: 48.436665\n",
            "Loss training: 26.93853\n",
            "Loss training: 30.667164\n",
            "\n",
            "***************************\n",
            "Trained on 4410 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.350613\n",
            "Loss training: 53.601395\n",
            "Loss training: 26.4038\n",
            "Loss training: 41.91933\n",
            "Loss training: 29.693861\n",
            "Loss training: 27.361923\n",
            "Loss training: 41.090454\n",
            "Loss training: 28.233782\n",
            "Loss training: 30.8918\n",
            "Loss training: 39.789753\n",
            "\n",
            "***************************\n",
            "Trained on 4420 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 51.568687\n",
            "Loss training: 39.356777\n",
            "Loss training: 51.376064\n",
            "Loss training: 30.726034\n",
            "Loss training: 28.068317\n",
            "Loss training: 21.048317\n",
            "Loss training: 35.49402\n",
            "Loss training: 28.976196\n",
            "Loss training: 31.392633\n",
            "Loss training: 42.06279\n",
            "\n",
            "***************************\n",
            "Trained on 4430 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.42377\n",
            "Loss training: 28.032963\n",
            "Loss training: 35.829323\n",
            "Loss training: 37.512905\n",
            "Loss training: 34.13852\n",
            "Loss training: 26.647938\n",
            "Loss training: 45.31798\n",
            "Loss training: 22.764011\n",
            "Loss training: 19.682467\n",
            "Loss training: 29.211315\n",
            "\n",
            "***************************\n",
            "Trained on 4440 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.014603\n",
            "Loss training: 32.08378\n",
            "Loss training: 30.726099\n",
            "Loss training: 36.14012\n",
            "Loss training: 34.689632\n",
            "Loss training: 26.348295\n",
            "Loss training: 37.989124\n",
            "Loss training: 29.44643\n",
            "Loss training: 37.10522\n",
            "Loss training: 21.830645\n",
            "\n",
            "***************************\n",
            "Trained on 4450 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.523533\n",
            "Loss training: 33.07706\n",
            "Loss training: 34.86724\n",
            "Loss training: 26.302862\n",
            "Loss training: 36.90285\n",
            "Loss training: 22.28668\n",
            "Loss training: 33.6935\n",
            "Loss training: 42.213467\n",
            "Loss training: 36.01519\n",
            "Loss training: 34.846172\n",
            "\n",
            "***************************\n",
            "Trained on 4460 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.745161\n",
            "Loss training: 38.007095\n",
            "Loss training: 39.5892\n",
            "Loss training: 17.584913\n",
            "Loss training: 29.798042\n",
            "Loss training: 29.923683\n",
            "Loss training: 38.760025\n",
            "Loss training: 29.115658\n",
            "Loss training: 28.049208\n",
            "Loss training: 35.681774\n",
            "\n",
            "***************************\n",
            "Trained on 4470 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.300926\n",
            "Loss training: 35.43544\n",
            "Loss training: 35.01207\n",
            "Loss training: 37.549454\n",
            "Loss training: 37.446423\n",
            "Loss training: 21.154732\n",
            "Loss training: 26.591072\n",
            "Loss training: 26.603539\n",
            "Loss training: 28.565378\n",
            "Loss training: 36.19944\n",
            "\n",
            "***************************\n",
            "Trained on 4480 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.267277\n",
            "Loss training: 33.26371\n",
            "Loss training: 36.05707\n",
            "Loss training: 32.882195\n",
            "Loss training: 40.054047\n",
            "Loss training: 39.56208\n",
            "Loss training: 35.424534\n",
            "Loss training: 27.757889\n",
            "Loss training: 34.106926\n",
            "Loss training: 35.05117\n",
            "\n",
            "***************************\n",
            "Trained on 4490 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.94211\n",
            "Loss training: 35.781532\n",
            "Loss training: 33.449543\n",
            "Loss training: 31.3459\n",
            "Loss training: 38.437542\n",
            "Loss training: 19.046139\n",
            "Loss training: 22.356607\n",
            "Loss training: 22.309793\n",
            "Loss training: 28.763435\n",
            "Loss training: 44.99602\n",
            "\n",
            "***************************\n",
            "Trained on 4500 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.810528\n",
            "Loss training: 28.78859\n",
            "Loss training: 26.090479\n",
            "Loss training: 35.072742\n",
            "Loss training: 33.64708\n",
            "Loss training: 33.35553\n",
            "Loss training: 37.90323\n",
            "Loss training: 34.349064\n",
            "Loss training: 34.467243\n",
            "Loss training: 39.133728\n",
            "\n",
            "***************************\n",
            "Trained on 4510 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.277702\n",
            "Loss training: 37.34828\n",
            "Loss training: 29.50365\n",
            "Loss training: 35.00137\n",
            "Loss training: 12.921142\n",
            "Loss training: 26.008287\n",
            "Loss training: 20.982662\n",
            "Loss training: 33.49153\n",
            "Loss training: 21.96731\n",
            "Loss training: 34.491535\n",
            "\n",
            "***************************\n",
            "Trained on 4520 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.485943\n",
            "Loss training: 18.656567\n",
            "Loss training: 20.918787\n",
            "Loss training: 27.502195\n",
            "Loss training: 26.637154\n",
            "Loss training: 35.654694\n",
            "Loss training: 35.38804\n",
            "Loss training: 34.46784\n",
            "Loss training: 28.74876\n",
            "Loss training: 38.24005\n",
            "\n",
            "***************************\n",
            "Trained on 4530 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.454382\n",
            "Loss training: 24.990128\n",
            "Loss training: 37.203728\n",
            "Loss training: 39.044395\n",
            "Loss training: 33.36636\n",
            "Loss training: 26.042227\n",
            "Loss training: 33.896168\n",
            "Loss training: 33.23438\n",
            "Loss training: 16.302929\n",
            "Loss training: 33.054718\n",
            "\n",
            "***************************\n",
            "Trained on 4540 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 43.500244\n",
            "Loss training: 38.178165\n",
            "Loss training: 26.646685\n",
            "Loss training: 34.237362\n",
            "Loss training: 34.770187\n",
            "Loss training: 32.35998\n",
            "Loss training: 25.729671\n",
            "Loss training: 27.136545\n",
            "Loss training: 23.802135\n",
            "Loss training: 33.887367\n",
            "\n",
            "***************************\n",
            "Trained on 4550 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.462267\n",
            "Loss training: 30.981905\n",
            "Loss training: 23.673597\n",
            "Loss training: 37.785793\n",
            "Loss training: 28.12016\n",
            "Loss training: 37.29779\n",
            "Loss training: 32.720867\n",
            "Loss training: 30.315193\n",
            "Loss training: 33.625656\n",
            "Loss training: 12.949074\n",
            "\n",
            "***************************\n",
            "Trained on 4560 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.787067\n",
            "Loss training: 27.810217\n",
            "Loss training: 38.226982\n",
            "Loss training: 37.12601\n",
            "Loss training: 33.975117\n",
            "Loss training: 16.111889\n",
            "Loss training: 30.016714\n",
            "Loss training: 30.545961\n",
            "Loss training: 42.73022\n",
            "Loss training: 37.47725\n",
            "\n",
            "***************************\n",
            "Trained on 4570 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.856453\n",
            "Loss training: 29.61383\n",
            "Loss training: 38.71688\n",
            "Loss training: 23.590258\n",
            "Loss training: 21.162848\n",
            "Loss training: 32.63251\n",
            "Loss training: 34.171608\n",
            "Loss training: 33.358555\n",
            "Loss training: 37.10662\n",
            "Loss training: 36.78296\n",
            "\n",
            "***************************\n",
            "Trained on 4580 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 18.417683\n",
            "Loss training: 32.740715\n",
            "Loss training: 32.70556\n",
            "Loss training: 29.59071\n",
            "Loss training: 28.64524\n",
            "Loss training: 29.412462\n",
            "Loss training: 28.528477\n",
            "Loss training: 25.941048\n",
            "Loss training: 32.95055\n",
            "Loss training: 25.49558\n",
            "\n",
            "***************************\n",
            "Trained on 4590 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.909557\n",
            "Loss training: 34.26298\n",
            "Loss training: 37.812496\n",
            "Loss training: 33.355103\n",
            "Loss training: 36.920853\n",
            "Loss training: 21.059227\n",
            "Loss training: 34.02363\n",
            "Loss training: 36.281155\n",
            "Loss training: 20.935478\n",
            "Loss training: 28.31949\n",
            "\n",
            "***************************\n",
            "Trained on 4600 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.071379\n",
            "Loss training: 36.6941\n",
            "Loss training: 35.48245\n",
            "Loss training: 29.279211\n",
            "Loss training: 37.61888\n",
            "Loss training: 43.832253\n",
            "Loss training: 12.685137\n",
            "Loss training: 22.486044\n",
            "Loss training: 42.754704\n",
            "Loss training: 32.07871\n",
            "\n",
            "***************************\n",
            "Trained on 4610 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.587511\n",
            "Loss training: 28.63605\n",
            "Loss training: 29.569887\n",
            "Loss training: 22.167957\n",
            "Loss training: 32.716682\n",
            "Loss training: 26.618004\n",
            "Loss training: 39.299393\n",
            "Loss training: 32.115894\n",
            "Loss training: 37.914513\n",
            "Loss training: 37.726578\n",
            "\n",
            "***************************\n",
            "Trained on 4620 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.632881\n",
            "Loss training: 28.338045\n",
            "Loss training: 33.61488\n",
            "Loss training: 33.08209\n",
            "Loss training: 29.503986\n",
            "Loss training: 37.259525\n",
            "Loss training: 27.706417\n",
            "Loss training: 21.970146\n",
            "Loss training: 31.446146\n",
            "Loss training: 23.862684\n",
            "\n",
            "***************************\n",
            "Trained on 4630 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 20.822111\n",
            "Loss training: 29.431427\n",
            "Loss training: 21.687963\n",
            "Loss training: 32.851246\n",
            "Loss training: 21.596003\n",
            "Loss training: 37.80738\n",
            "Loss training: 35.23941\n",
            "Loss training: 28.315838\n",
            "Loss training: 33.764816\n",
            "Loss training: 16.348738\n",
            "\n",
            "***************************\n",
            "Trained on 4640 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.276442\n",
            "Loss training: 16.022572\n",
            "Loss training: 36.995716\n",
            "Loss training: 43.6737\n",
            "Loss training: 27.737713\n",
            "Loss training: 36.975426\n",
            "Loss training: 33.56087\n",
            "Loss training: 18.900955\n",
            "Loss training: 34.04919\n",
            "Loss training: 30.03639\n",
            "\n",
            "***************************\n",
            "Trained on 4650 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.876205\n",
            "Loss training: 31.772337\n",
            "Loss training: 27.316645\n",
            "Loss training: 42.69412\n",
            "Loss training: 13.270992\n",
            "Loss training: 37.06911\n",
            "Loss training: 26.655275\n",
            "Loss training: 25.1427\n",
            "Loss training: 28.34707\n",
            "Loss training: 38.016357\n",
            "\n",
            "***************************\n",
            "Trained on 4660 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.129772\n",
            "Loss training: 36.87528\n",
            "Loss training: 37.15797\n",
            "Loss training: 37.761887\n",
            "Loss training: 20.962564\n",
            "Loss training: 21.768505\n",
            "Loss training: 21.749392\n",
            "Loss training: 33.138878\n",
            "Loss training: 43.393032\n",
            "Loss training: 16.587204\n",
            "\n",
            "***************************\n",
            "Trained on 4670 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.892002\n",
            "Loss training: 34.12548\n",
            "Loss training: 28.296518\n",
            "Loss training: 33.022285\n",
            "Loss training: 21.519245\n",
            "Loss training: 31.630728\n",
            "Loss training: 21.434736\n",
            "Loss training: 28.350039\n",
            "Loss training: 42.06762\n",
            "Loss training: 35.948925\n",
            "\n",
            "***************************\n",
            "Trained on 4680 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.801735\n",
            "Loss training: 34.83339\n",
            "Loss training: 29.439074\n",
            "Loss training: 28.284632\n",
            "Loss training: 29.782183\n",
            "Loss training: 28.109676\n",
            "Loss training: 29.27002\n",
            "Loss training: 16.451996\n",
            "Loss training: 33.243275\n",
            "Loss training: 38.45905\n",
            "\n",
            "***************************\n",
            "Trained on 4690 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.723766\n",
            "Loss training: 43.767426\n",
            "Loss training: 26.954718\n",
            "Loss training: 25.871056\n",
            "Loss training: 28.37622\n",
            "Loss training: 32.74857\n",
            "Loss training: 29.864899\n",
            "Loss training: 21.509983\n",
            "Loss training: 28.078653\n",
            "Loss training: 21.434124\n",
            "\n",
            "***************************\n",
            "Trained on 4700 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.84406\n",
            "Loss training: 31.4173\n",
            "Loss training: 38.7988\n",
            "Loss training: 26.342802\n",
            "Loss training: 21.559591\n",
            "Loss training: 33.685795\n",
            "Loss training: 20.957567\n",
            "Loss training: 28.565996\n",
            "Loss training: 36.414314\n",
            "Loss training: 26.383074\n",
            "\n",
            "***************************\n",
            "Trained on 4710 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.74334\n",
            "Loss training: 21.295345\n",
            "Loss training: 25.810755\n",
            "Loss training: 33.830166\n",
            "Loss training: 31.030502\n",
            "Loss training: 34.10104\n",
            "Loss training: 35.65686\n",
            "Loss training: 27.535385\n",
            "Loss training: 32.627304\n",
            "Loss training: 31.359873\n",
            "\n",
            "***************************\n",
            "Trained on 4720 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.314957\n",
            "Loss training: 35.11626\n",
            "Loss training: 30.816278\n",
            "Loss training: 34.924953\n",
            "Loss training: 34.320095\n",
            "Loss training: 30.748447\n",
            "Loss training: 34.64644\n",
            "Loss training: 28.54846\n",
            "Loss training: 18.881985\n",
            "Loss training: 33.97464\n",
            "\n",
            "***************************\n",
            "Trained on 4730 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.0912\n",
            "Loss training: 31.018032\n",
            "Loss training: 27.219248\n",
            "Loss training: 34.976837\n",
            "Loss training: 27.515688\n",
            "Loss training: 39.632072\n",
            "Loss training: 37.366592\n",
            "Loss training: 34.17859\n",
            "Loss training: 21.120607\n",
            "Loss training: 37.10689\n",
            "\n",
            "***************************\n",
            "Trained on 4740 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.07964\n",
            "Loss training: 30.782312\n",
            "Loss training: 28.366034\n",
            "Loss training: 25.785631\n",
            "Loss training: 29.712147\n",
            "Loss training: 32.082516\n",
            "Loss training: 33.584793\n",
            "Loss training: 33.975296\n",
            "Loss training: 31.899755\n",
            "Loss training: 28.079405\n",
            "\n",
            "***************************\n",
            "Trained on 4750 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.05233\n",
            "Loss training: 26.89019\n",
            "Loss training: 16.139345\n",
            "Loss training: 35.09708\n",
            "Loss training: 36.499947\n",
            "Loss training: 35.82834\n",
            "Loss training: 33.576183\n",
            "Loss training: 36.816174\n",
            "Loss training: 34.868793\n",
            "Loss training: 15.9042015\n",
            "\n",
            "***************************\n",
            "Trained on 4760 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.169281\n",
            "Loss training: 29.822824\n",
            "Loss training: 30.922983\n",
            "Loss training: 18.422304\n",
            "Loss training: 28.060083\n",
            "Loss training: 35.146473\n",
            "Loss training: 20.659977\n",
            "Loss training: 43.70135\n",
            "Loss training: 38.271267\n",
            "Loss training: 30.695017\n",
            "\n",
            "***************************\n",
            "Trained on 4770 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.389044\n",
            "Loss training: 33.414394\n",
            "Loss training: 27.480995\n",
            "Loss training: 36.33467\n",
            "Loss training: 34.173798\n",
            "Loss training: 30.278114\n",
            "Loss training: 34.316914\n",
            "Loss training: 29.963562\n",
            "Loss training: 32.074104\n",
            "Loss training: 38.18445\n",
            "\n",
            "***************************\n",
            "Trained on 4780 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.010496\n",
            "Loss training: 32.81721\n",
            "Loss training: 27.445395\n",
            "Loss training: 15.904811\n",
            "Loss training: 32.678955\n",
            "Loss training: 32.562126\n",
            "Loss training: 35.492138\n",
            "Loss training: 43.1087\n",
            "Loss training: 28.350286\n",
            "Loss training: 29.548504\n",
            "\n",
            "***************************\n",
            "Trained on 4790 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.415808\n",
            "Loss training: 33.346767\n",
            "Loss training: 42.519135\n",
            "Loss training: 32.173866\n",
            "Loss training: 32.006367\n",
            "Loss training: 25.584356\n",
            "Loss training: 29.868504\n",
            "Loss training: 35.767593\n",
            "Loss training: 38.692894\n",
            "Loss training: 41.87506\n",
            "\n",
            "***************************\n",
            "Trained on 4800 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.425344\n",
            "Loss training: 22.263624\n",
            "Loss training: 37.89388\n",
            "Loss training: 30.087101\n",
            "Loss training: 34.176987\n",
            "Loss training: 34.663136\n",
            "Loss training: 25.988985\n",
            "Loss training: 23.617033\n",
            "Loss training: 26.790396\n",
            "Loss training: 33.31247\n",
            "\n",
            "***************************\n",
            "Trained on 4810 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.589018\n",
            "Loss training: 34.25696\n",
            "Loss training: 34.060936\n",
            "Loss training: 32.844807\n",
            "Loss training: 37.29843\n",
            "Loss training: 25.705011\n",
            "Loss training: 28.716366\n",
            "Loss training: 28.82943\n",
            "Loss training: 16.038439\n",
            "Loss training: 35.587177\n",
            "\n",
            "***************************\n",
            "Trained on 4820 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 12.797395\n",
            "Loss training: 20.893513\n",
            "Loss training: 27.132584\n",
            "Loss training: 37.411736\n",
            "Loss training: 29.06839\n",
            "Loss training: 21.60617\n",
            "Loss training: 38.179874\n",
            "Loss training: 37.408012\n",
            "Loss training: 21.470434\n",
            "Loss training: 12.576179\n",
            "\n",
            "***************************\n",
            "Trained on 4830 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 20.373764\n",
            "Loss training: 24.999964\n",
            "Loss training: 37.23779\n",
            "Loss training: 28.991665\n",
            "Loss training: 27.9437\n",
            "Loss training: 32.644096\n",
            "Loss training: 21.217747\n",
            "Loss training: 34.73473\n",
            "Loss training: 33.581158\n",
            "Loss training: 33.68107\n",
            "\n",
            "***************************\n",
            "Trained on 4840 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.843029\n",
            "Loss training: 23.193514\n",
            "Loss training: 36.211914\n",
            "Loss training: 43.78459\n",
            "Loss training: 33.246616\n",
            "Loss training: 37.947296\n",
            "Loss training: 31.882528\n",
            "Loss training: 34.939182\n",
            "Loss training: 26.647171\n",
            "Loss training: 36.05353\n",
            "\n",
            "***************************\n",
            "Trained on 4850 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 20.229572\n",
            "Loss training: 37.74109\n",
            "Loss training: 41.999115\n",
            "Loss training: 26.506674\n",
            "Loss training: 26.473116\n",
            "Loss training: 27.735817\n",
            "Loss training: 26.63418\n",
            "Loss training: 32.918262\n",
            "Loss training: 33.263195\n",
            "Loss training: 37.16217\n",
            "\n",
            "***************************\n",
            "Trained on 4860 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.80182\n",
            "Loss training: 20.266136\n",
            "Loss training: 28.669195\n",
            "Loss training: 26.005098\n",
            "Loss training: 37.1997\n",
            "Loss training: 29.217192\n",
            "Loss training: 18.693245\n",
            "Loss training: 33.366688\n",
            "Loss training: 25.700462\n",
            "Loss training: 33.146954\n",
            "\n",
            "***************************\n",
            "Trained on 4870 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 12.460128\n",
            "Loss training: 26.69589\n",
            "Loss training: 15.850874\n",
            "Loss training: 22.850504\n",
            "Loss training: 15.662886\n",
            "Loss training: 20.994944\n",
            "Loss training: 32.00184\n",
            "Loss training: 27.69952\n",
            "Loss training: 26.605465\n",
            "Loss training: 35.71321\n",
            "\n",
            "***************************\n",
            "Trained on 4880 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.356033\n",
            "Loss training: 15.487082\n",
            "Loss training: 26.36927\n",
            "Loss training: 37.83429\n",
            "Loss training: 15.421746\n",
            "Loss training: 28.21613\n",
            "Loss training: 37.074604\n",
            "Loss training: 27.45943\n",
            "Loss training: 33.292767\n",
            "Loss training: 36.951366\n",
            "\n",
            "***************************\n",
            "Trained on 4890 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.14461\n",
            "Loss training: 34.825523\n",
            "Loss training: 36.481255\n",
            "Loss training: 23.124962\n",
            "Loss training: 36.38152\n",
            "Loss training: 31.758514\n",
            "Loss training: 37.577896\n",
            "Loss training: 26.03058\n",
            "Loss training: 31.37036\n",
            "Loss training: 20.626081\n",
            "\n",
            "***************************\n",
            "Trained on 4900 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.253456\n",
            "Loss training: 33.18864\n",
            "Loss training: 38.139088\n",
            "Loss training: 17.890032\n",
            "Loss training: 29.46035\n",
            "Loss training: 43.145336\n",
            "Loss training: 27.393438\n",
            "Loss training: 22.726564\n",
            "Loss training: 33.11045\n",
            "Loss training: 37.485703\n",
            "\n",
            "***************************\n",
            "Trained on 4910 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.55595\n",
            "Loss training: 33.25898\n",
            "Loss training: 29.933876\n",
            "Loss training: 20.982534\n",
            "Loss training: 15.703431\n",
            "Loss training: 22.526121\n",
            "Loss training: 31.52299\n",
            "Loss training: 34.981003\n",
            "Loss training: 32.7265\n",
            "Loss training: 29.409176\n",
            "\n",
            "***************************\n",
            "Trained on 4920 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.772617\n",
            "Loss training: 35.04758\n",
            "Loss training: 33.92834\n",
            "Loss training: 22.589184\n",
            "Loss training: 26.505056\n",
            "Loss training: 27.546997\n",
            "Loss training: 16.370735\n",
            "Loss training: 31.521656\n",
            "Loss training: 26.939215\n",
            "Loss training: 39.42581\n",
            "\n",
            "***************************\n",
            "Trained on 4930 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 12.929601\n",
            "Loss training: 27.62721\n",
            "Loss training: 12.721696\n",
            "Loss training: 30.00538\n",
            "Loss training: 15.569318\n",
            "Loss training: 32.808723\n",
            "Loss training: 34.20091\n",
            "Loss training: 28.484188\n",
            "Loss training: 37.127872\n",
            "Loss training: 28.484722\n",
            "\n",
            "***************************\n",
            "Trained on 4940 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.929092\n",
            "Loss training: 32.364902\n",
            "Loss training: 26.74764\n",
            "Loss training: 36.65654\n",
            "Loss training: 36.468967\n",
            "Loss training: 29.326813\n",
            "Loss training: 39.507435\n",
            "Loss training: 29.023764\n",
            "Loss training: 28.877295\n",
            "Loss training: 21.142277\n",
            "\n",
            "***************************\n",
            "Trained on 4950 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.115005\n",
            "Loss training: 34.341602\n",
            "Loss training: 29.449347\n",
            "Loss training: 33.1613\n",
            "Loss training: 28.765635\n",
            "Loss training: 32.34778\n",
            "Loss training: 31.280645\n",
            "Loss training: 27.73829\n",
            "Loss training: 22.759264\n",
            "Loss training: 36.75219\n",
            "\n",
            "***************************\n",
            "Trained on 4960 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.672848\n",
            "Loss training: 36.256454\n",
            "Loss training: 36.27072\n",
            "Loss training: 37.47123\n",
            "Loss training: 27.216408\n",
            "Loss training: 27.281857\n",
            "Loss training: 36.83974\n",
            "Loss training: 33.850117\n",
            "Loss training: 32.377712\n",
            "Loss training: 25.964169\n",
            "\n",
            "***************************\n",
            "Trained on 4970 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.884129\n",
            "Loss training: 29.023611\n",
            "Loss training: 12.320906\n",
            "Loss training: 32.97533\n",
            "Loss training: 34.600494\n",
            "Loss training: 36.081856\n",
            "Loss training: 36.15757\n",
            "Loss training: 32.54268\n",
            "Loss training: 22.616186\n",
            "Loss training: 35.439682\n",
            "\n",
            "***************************\n",
            "Trained on 4980 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.771053\n",
            "Loss training: 25.887743\n",
            "Loss training: 35.398186\n",
            "Loss training: 25.6963\n",
            "Loss training: 15.737514\n",
            "Loss training: 35.76576\n",
            "Loss training: 21.622673\n",
            "Loss training: 32.81059\n",
            "Loss training: 43.263157\n",
            "Loss training: 31.995821\n",
            "\n",
            "***************************\n",
            "Trained on 4990 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.459435\n",
            "Loss training: 27.152378\n",
            "Loss training: 31.638742\n",
            "Loss training: 34.599316\n",
            "Loss training: 35.505833\n",
            "Loss training: 28.16537\n",
            "Loss training: 37.915928\n",
            "Loss training: 36.938255\n",
            "Loss training: 32.675404\n",
            "Loss training: 22.399761\n",
            "\n",
            "***************************\n",
            "Trained on 5000 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.504955\n",
            "Loss training: 25.85006\n",
            "Loss training: 29.156301\n",
            "Loss training: 20.736734\n",
            "Loss training: 20.617245\n",
            "Loss training: 33.0221\n",
            "Loss training: 22.373943\n",
            "Loss training: 33.032352\n",
            "Loss training: 12.481813\n",
            "Loss training: 32.130093\n",
            "\n",
            "***************************\n",
            "Trained on 5010 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 21.535265\n",
            "Loss training: 25.470314\n",
            "Loss training: 27.603735\n",
            "Loss training: 32.901684\n",
            "Loss training: 20.69021\n",
            "Loss training: 28.319263\n",
            "Loss training: 36.938107\n",
            "Loss training: 37.313812\n",
            "Loss training: 30.949457\n",
            "Loss training: 27.950602\n",
            "\n",
            "***************************\n",
            "Trained on 5020 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.81103\n",
            "Loss training: 33.606678\n",
            "Loss training: 32.256935\n",
            "Loss training: 27.7742\n",
            "Loss training: 29.726358\n",
            "Loss training: 31.393562\n",
            "Loss training: 12.781613\n",
            "Loss training: 25.536358\n",
            "Loss training: 27.331692\n",
            "Loss training: 12.170882\n",
            "\n",
            "***************************\n",
            "Trained on 5030 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.329952\n",
            "Loss training: 15.996432\n",
            "Loss training: 26.638779\n",
            "Loss training: 26.787056\n",
            "Loss training: 25.792574\n",
            "Loss training: 26.114918\n",
            "Loss training: 34.524696\n",
            "Loss training: 27.183384\n",
            "Loss training: 34.85135\n",
            "Loss training: 29.373844\n",
            "\n",
            "***************************\n",
            "Trained on 5040 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.603325\n",
            "Loss training: 26.592882\n",
            "Loss training: 26.51768\n",
            "Loss training: 27.321056\n",
            "Loss training: 43.26298\n",
            "Loss training: 34.3769\n",
            "Loss training: 36.57203\n",
            "Loss training: 37.624474\n",
            "Loss training: 36.281586\n",
            "Loss training: 33.499462\n",
            "\n",
            "***************************\n",
            "Trained on 5050 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 21.944517\n",
            "Loss training: 26.404549\n",
            "Loss training: 27.465351\n",
            "Loss training: 27.267126\n",
            "Loss training: 26.532131\n",
            "Loss training: 35.256485\n",
            "Loss training: 33.907784\n",
            "Loss training: 23.830078\n",
            "Loss training: 21.604376\n",
            "Loss training: 25.387691\n",
            "\n",
            "***************************\n",
            "Trained on 5060 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.04376\n",
            "Loss training: 21.331835\n",
            "Loss training: 35.175323\n",
            "Loss training: 28.863884\n",
            "Loss training: 39.135403\n",
            "Loss training: 35.210224\n",
            "Loss training: 21.198425\n",
            "Loss training: 31.008633\n",
            "Loss training: 37.336384\n",
            "Loss training: 36.950516\n",
            "\n",
            "***************************\n",
            "Trained on 5070 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.849258\n",
            "Loss training: 35.527027\n",
            "Loss training: 32.549484\n",
            "Loss training: 18.19183\n",
            "Loss training: 32.36812\n",
            "Loss training: 35.769535\n",
            "Loss training: 37.736507\n",
            "Loss training: 35.78553\n",
            "Loss training: 33.93951\n",
            "Loss training: 28.174524\n",
            "\n",
            "***************************\n",
            "Trained on 5080 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.317965\n",
            "Loss training: 34.42381\n",
            "Loss training: 27.952847\n",
            "Loss training: 33.762974\n",
            "Loss training: 35.994305\n",
            "Loss training: 22.923256\n",
            "Loss training: 36.038975\n",
            "Loss training: 26.486553\n",
            "Loss training: 23.437654\n",
            "Loss training: 43.730354\n",
            "\n",
            "***************************\n",
            "Trained on 5090 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.762367\n",
            "Loss training: 18.914928\n",
            "Loss training: 27.090725\n",
            "Loss training: 15.879367\n",
            "Loss training: 33.115646\n",
            "Loss training: 32.942932\n",
            "Loss training: 20.442768\n",
            "Loss training: 32.814205\n",
            "Loss training: 33.083218\n",
            "Loss training: 32.133026\n",
            "\n",
            "***************************\n",
            "Trained on 5100 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.08084\n",
            "Loss training: 32.885437\n",
            "Loss training: 37.363052\n",
            "Loss training: 35.553005\n",
            "Loss training: 25.993002\n",
            "Loss training: 33.300007\n",
            "Loss training: 31.9383\n",
            "Loss training: 32.610348\n",
            "Loss training: 34.73913\n",
            "Loss training: 12.571164\n",
            "\n",
            "***************************\n",
            "Trained on 5110 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.69731\n",
            "Loss training: 32.6149\n",
            "Loss training: 38.034927\n",
            "Loss training: 35.560394\n",
            "Loss training: 28.822626\n",
            "Loss training: 33.175568\n",
            "Loss training: 25.922314\n",
            "Loss training: 28.33433\n",
            "Loss training: 37.592808\n",
            "Loss training: 20.633457\n",
            "\n",
            "***************************\n",
            "Trained on 5120 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 19.324705\n",
            "Loss training: 27.237825\n",
            "Loss training: 26.946886\n",
            "Loss training: 15.949147\n",
            "Loss training: 34.79309\n",
            "Loss training: 35.782516\n",
            "Loss training: 26.91268\n",
            "Loss training: 39.349937\n",
            "Loss training: 37.526596\n",
            "Loss training: 35.49755\n",
            "\n",
            "***************************\n",
            "Trained on 5130 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.093185\n",
            "Loss training: 38.027622\n",
            "Loss training: 26.916094\n",
            "Loss training: 28.561321\n",
            "Loss training: 32.738625\n",
            "Loss training: 28.711552\n",
            "Loss training: 34.258465\n",
            "Loss training: 37.51847\n",
            "Loss training: 22.497816\n",
            "Loss training: 41.005535\n",
            "\n",
            "***************************\n",
            "Trained on 5140 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 43.398552\n",
            "Loss training: 37.59296\n",
            "Loss training: 34.741585\n",
            "Loss training: 29.365604\n",
            "Loss training: 35.229145\n",
            "Loss training: 33.83168\n",
            "Loss training: 32.774258\n",
            "Loss training: 26.77604\n",
            "Loss training: 26.419285\n",
            "Loss training: 33.06566\n",
            "\n",
            "***************************\n",
            "Trained on 5150 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.75493\n",
            "Loss training: 17.870396\n",
            "Loss training: 12.418047\n",
            "Loss training: 33.89931\n",
            "Loss training: 34.028446\n",
            "Loss training: 29.491304\n",
            "Loss training: 30.796638\n",
            "Loss training: 17.42509\n",
            "Loss training: 26.469381\n",
            "Loss training: 34.66226\n",
            "\n",
            "***************************\n",
            "Trained on 5160 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 16.193508\n",
            "Loss training: 17.226795\n",
            "Loss training: 26.36852\n",
            "Loss training: 31.999498\n",
            "Loss training: 35.872826\n",
            "Loss training: 24.6865\n",
            "Loss training: 37.984722\n",
            "Loss training: 21.96165\n",
            "Loss training: 36.44184\n",
            "Loss training: 38.92751\n",
            "\n",
            "***************************\n",
            "Trained on 5170 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.573042\n",
            "Loss training: 32.452667\n",
            "Loss training: 30.05983\n",
            "Loss training: 26.077326\n",
            "Loss training: 16.55264\n",
            "Loss training: 12.684007\n",
            "Loss training: 33.512928\n",
            "Loss training: 32.539345\n",
            "Loss training: 25.275839\n",
            "Loss training: 37.578564\n",
            "\n",
            "***************************\n",
            "Trained on 5180 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 21.641176\n",
            "Loss training: 28.300268\n",
            "Loss training: 34.017048\n",
            "Loss training: 36.965263\n",
            "Loss training: 28.53503\n",
            "Loss training: 31.870882\n",
            "Loss training: 36.74033\n",
            "Loss training: 21.733706\n",
            "Loss training: 31.615442\n",
            "Loss training: 36.277504\n",
            "\n",
            "***************************\n",
            "Trained on 5190 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 17.615427\n",
            "Loss training: 36.03497\n",
            "Loss training: 31.84557\n",
            "Loss training: 21.214052\n",
            "Loss training: 31.287863\n",
            "Loss training: 29.65611\n",
            "Loss training: 15.556318\n",
            "Loss training: 33.86268\n",
            "Loss training: 36.741505\n",
            "Loss training: 28.934347\n",
            "\n",
            "***************************\n",
            "Trained on 5200 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.92257\n",
            "Loss training: 42.92412\n",
            "Loss training: 28.72103\n",
            "Loss training: 28.91307\n",
            "Loss training: 33.920918\n",
            "Loss training: 25.456985\n",
            "Loss training: 32.04174\n",
            "Loss training: 25.816057\n",
            "Loss training: 32.230305\n",
            "Loss training: 33.116085\n",
            "\n",
            "***************************\n",
            "Trained on 5210 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.792002\n",
            "Loss training: 16.942997\n",
            "Loss training: 32.81496\n",
            "Loss training: 36.054077\n",
            "Loss training: 37.5917\n",
            "Loss training: 30.793211\n",
            "Loss training: 34.94039\n",
            "Loss training: 28.990429\n",
            "Loss training: 34.90708\n",
            "Loss training: 25.642843\n",
            "\n",
            "***************************\n",
            "Trained on 5220 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 13.490786\n",
            "Loss training: 35.841206\n",
            "Loss training: 38.594234\n",
            "Loss training: 32.672855\n",
            "Loss training: 25.672707\n",
            "Loss training: 38.60086\n",
            "Loss training: 25.1034\n",
            "Loss training: 31.162354\n",
            "Loss training: 33.94693\n",
            "Loss training: 31.426147\n",
            "\n",
            "***************************\n",
            "Trained on 5230 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.273457\n",
            "Loss training: 21.213165\n",
            "Loss training: 21.347258\n",
            "Loss training: 32.22536\n",
            "Loss training: 25.49779\n",
            "Loss training: 28.21576\n",
            "Loss training: 20.94498\n",
            "Loss training: 17.18574\n",
            "Loss training: 33.071568\n",
            "Loss training: 37.368668\n",
            "\n",
            "***************************\n",
            "Trained on 5240 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.298172\n",
            "Loss training: 28.764942\n",
            "Loss training: 16.90844\n",
            "Loss training: 21.069601\n",
            "Loss training: 37.797558\n",
            "Loss training: 27.269684\n",
            "Loss training: 16.712498\n",
            "Loss training: 32.00362\n",
            "Loss training: 32.70591\n",
            "Loss training: 23.965384\n",
            "\n",
            "***************************\n",
            "Trained on 5250 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.77785\n",
            "Loss training: 31.808672\n",
            "Loss training: 31.371193\n",
            "Loss training: 29.351437\n",
            "Loss training: 31.550106\n",
            "Loss training: 28.762617\n",
            "Loss training: 37.20378\n",
            "Loss training: 31.240553\n",
            "Loss training: 12.485367\n",
            "Loss training: 20.781971\n",
            "\n",
            "***************************\n",
            "Trained on 5260 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 16.909763\n",
            "Loss training: 28.735504\n",
            "Loss training: 42.829712\n",
            "Loss training: 34.933487\n",
            "Loss training: 32.62019\n",
            "Loss training: 31.639008\n",
            "Loss training: 34.90806\n",
            "Loss training: 41.605858\n",
            "Loss training: 34.714664\n",
            "Loss training: 12.277942\n",
            "\n",
            "***************************\n",
            "Trained on 5270 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.76976\n",
            "Loss training: 22.809835\n",
            "Loss training: 27.345821\n",
            "Loss training: 20.546883\n",
            "Loss training: 26.039936\n",
            "Loss training: 26.73381\n",
            "Loss training: 34.016785\n",
            "Loss training: 32.520264\n",
            "Loss training: 29.001497\n",
            "Loss training: 18.41674\n",
            "\n",
            "***************************\n",
            "Trained on 5280 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.820168\n",
            "Loss training: 28.975283\n",
            "Loss training: 42.324295\n",
            "Loss training: 23.610435\n",
            "Loss training: 12.378398\n",
            "Loss training: 37.431335\n",
            "Loss training: 31.106293\n",
            "Loss training: 27.452454\n",
            "Loss training: 29.453333\n",
            "Loss training: 36.579067\n",
            "\n",
            "***************************\n",
            "Trained on 5290 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 20.944391\n",
            "Loss training: 33.70802\n",
            "Loss training: 32.00088\n",
            "Loss training: 21.123013\n",
            "Loss training: 22.581598\n",
            "Loss training: 26.398758\n",
            "Loss training: 29.245636\n",
            "Loss training: 27.679384\n",
            "Loss training: 27.098303\n",
            "Loss training: 36.165688\n",
            "\n",
            "***************************\n",
            "Trained on 5300 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.511402\n",
            "Loss training: 31.108915\n",
            "Loss training: 32.506973\n",
            "Loss training: 33.222385\n",
            "Loss training: 37.058605\n",
            "Loss training: 35.377995\n",
            "Loss training: 32.339256\n",
            "Loss training: 28.15165\n",
            "Loss training: 36.09468\n",
            "Loss training: 29.961903\n",
            "\n",
            "***************************\n",
            "Trained on 5310 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.724398\n",
            "Loss training: 35.305637\n",
            "Loss training: 20.49115\n",
            "Loss training: 26.682878\n",
            "Loss training: 33.939056\n",
            "Loss training: 35.011806\n",
            "Loss training: 35.76493\n",
            "Loss training: 25.76283\n",
            "Loss training: 20.324202\n",
            "Loss training: 27.3471\n",
            "\n",
            "***************************\n",
            "Trained on 5320 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.289528\n",
            "Loss training: 38.18243\n",
            "Loss training: 29.464186\n",
            "Loss training: 32.786682\n",
            "Loss training: 37.08809\n",
            "Loss training: 17.63061\n",
            "Loss training: 37.354523\n",
            "Loss training: 27.267612\n",
            "Loss training: 28.820951\n",
            "Loss training: 25.980822\n",
            "\n",
            "***************************\n",
            "Trained on 5330 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.874435\n",
            "Loss training: 37.235\n",
            "Loss training: 35.80592\n",
            "Loss training: 31.578127\n",
            "Loss training: 31.172037\n",
            "Loss training: 20.668623\n",
            "Loss training: 21.887774\n",
            "Loss training: 17.019518\n",
            "Loss training: 37.35302\n",
            "Loss training: 34.62441\n",
            "\n",
            "***************************\n",
            "Trained on 5340 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.894184\n",
            "Loss training: 37.733242\n",
            "Loss training: 33.255825\n",
            "Loss training: 37.422073\n",
            "Loss training: 16.191294\n",
            "Loss training: 37.077564\n",
            "Loss training: 31.801668\n",
            "Loss training: 15.63639\n",
            "Loss training: 28.486475\n",
            "Loss training: 42.807007\n",
            "\n",
            "***************************\n",
            "Trained on 5350 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.870125\n",
            "Loss training: 12.5514965\n",
            "Loss training: 15.49005\n",
            "Loss training: 27.144268\n",
            "Loss training: 21.372898\n",
            "Loss training: 35.061462\n",
            "Loss training: 41.79679\n",
            "Loss training: 32.62395\n",
            "Loss training: 31.894869\n",
            "Loss training: 33.91201\n",
            "\n",
            "***************************\n",
            "Trained on 5360 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.163002\n",
            "Loss training: 26.90373\n",
            "Loss training: 23.244375\n",
            "Loss training: 32.10203\n",
            "Loss training: 12.591174\n",
            "Loss training: 33.39087\n",
            "Loss training: 32.899734\n",
            "Loss training: 32.07238\n",
            "Loss training: 34.477585\n",
            "Loss training: 20.897356\n",
            "\n",
            "***************************\n",
            "Trained on 5370 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.414692\n",
            "Loss training: 26.26403\n",
            "Loss training: 25.79165\n",
            "Loss training: 36.31415\n",
            "Loss training: 28.411863\n",
            "Loss training: 29.533195\n",
            "Loss training: 32.040077\n",
            "Loss training: 27.529242\n",
            "Loss training: 26.765127\n",
            "Loss training: 25.349798\n",
            "\n",
            "***************************\n",
            "Trained on 5380 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.791964\n",
            "Loss training: 31.597324\n",
            "Loss training: 37.710308\n",
            "Loss training: 37.36481\n",
            "Loss training: 20.403873\n",
            "Loss training: 20.695011\n",
            "Loss training: 28.836348\n",
            "Loss training: 33.455708\n",
            "Loss training: 33.59692\n",
            "Loss training: 27.583925\n",
            "\n",
            "***************************\n",
            "Trained on 5390 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.827019\n",
            "Loss training: 31.321905\n",
            "Loss training: 43.555946\n",
            "Loss training: 31.10263\n",
            "Loss training: 26.898663\n",
            "Loss training: 25.434036\n",
            "Loss training: 27.762451\n",
            "Loss training: 25.189693\n",
            "Loss training: 24.948729\n",
            "Loss training: 32.723255\n",
            "\n",
            "***************************\n",
            "Trained on 5400 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.0242\n",
            "Loss training: 36.248833\n",
            "Loss training: 33.273453\n",
            "Loss training: 35.891243\n",
            "Loss training: 35.665493\n",
            "Loss training: 26.54883\n",
            "Loss training: 27.133003\n",
            "Loss training: 35.452366\n",
            "Loss training: 30.292156\n",
            "Loss training: 36.81578\n",
            "\n",
            "***************************\n",
            "Trained on 5410 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.430498\n",
            "Loss training: 35.15982\n",
            "Loss training: 26.50736\n",
            "Loss training: 17.563837\n",
            "Loss training: 37.273174\n",
            "Loss training: 34.646965\n",
            "Loss training: 16.76448\n",
            "Loss training: 34.29958\n",
            "Loss training: 32.509247\n",
            "Loss training: 38.158882\n",
            "\n",
            "***************************\n",
            "Trained on 5420 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.44514\n",
            "Loss training: 26.608202\n",
            "Loss training: 32.92125\n",
            "Loss training: 34.261883\n",
            "Loss training: 43.40435\n",
            "Loss training: 23.043581\n",
            "Loss training: 39.03425\n",
            "Loss training: 13.554412\n",
            "Loss training: 31.974606\n",
            "Loss training: 38.681625\n",
            "\n",
            "***************************\n",
            "Trained on 5430 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.287193\n",
            "Loss training: 15.998157\n",
            "Loss training: 41.548393\n",
            "Loss training: 15.972539\n",
            "Loss training: 15.713572\n",
            "Loss training: 34.23989\n",
            "Loss training: 18.427164\n",
            "Loss training: 23.992804\n",
            "Loss training: 32.127666\n",
            "Loss training: 40.333233\n",
            "\n",
            "***************************\n",
            "Trained on 5440 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 44.256912\n",
            "Loss training: 42.79809\n",
            "Loss training: 25.914968\n",
            "Loss training: 26.58009\n",
            "Loss training: 38.64378\n",
            "Loss training: 27.827948\n",
            "Loss training: 31.081045\n",
            "Loss training: 30.770962\n",
            "Loss training: 26.021967\n",
            "Loss training: 43.46456\n",
            "\n",
            "***************************\n",
            "Trained on 5450 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 21.297869\n",
            "Loss training: 23.43142\n",
            "Loss training: 22.102879\n",
            "Loss training: 21.085876\n",
            "Loss training: 34.351463\n",
            "Loss training: 36.506123\n",
            "Loss training: 33.562336\n",
            "Loss training: 34.896114\n",
            "Loss training: 32.23451\n",
            "Loss training: 27.452835\n",
            "\n",
            "***************************\n",
            "Trained on 5460 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 20.898302\n",
            "Loss training: 34.00742\n",
            "Loss training: 35.95554\n",
            "Loss training: 25.853298\n",
            "Loss training: 17.41257\n",
            "Loss training: 17.105068\n",
            "Loss training: 33.328197\n",
            "Loss training: 34.760845\n",
            "Loss training: 33.901733\n",
            "Loss training: 32.45231\n",
            "\n",
            "***************************\n",
            "Trained on 5470 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 43.902542\n",
            "Loss training: 33.52667\n",
            "Loss training: 35.45111\n",
            "Loss training: 33.607857\n",
            "Loss training: 36.948833\n",
            "Loss training: 37.71175\n",
            "Loss training: 26.056845\n",
            "Loss training: 27.101187\n",
            "Loss training: 34.591743\n",
            "Loss training: 26.720139\n",
            "\n",
            "***************************\n",
            "Trained on 5480 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.03275\n",
            "Loss training: 36.91185\n",
            "Loss training: 27.933067\n",
            "Loss training: 32.463318\n",
            "Loss training: 28.587288\n",
            "Loss training: 32.942005\n",
            "Loss training: 17.358603\n",
            "Loss training: 12.60749\n",
            "Loss training: 25.27173\n",
            "Loss training: 31.82517\n",
            "\n",
            "***************************\n",
            "Trained on 5490 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.266542\n",
            "Loss training: 26.076223\n",
            "Loss training: 32.067905\n",
            "Loss training: 33.061665\n",
            "Loss training: 27.980772\n",
            "Loss training: 22.889791\n",
            "Loss training: 43.28252\n",
            "Loss training: 29.618332\n",
            "Loss training: 32.30827\n",
            "Loss training: 31.29549\n",
            "\n",
            "***************************\n",
            "Trained on 5500 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.666733\n",
            "Loss training: 34.476948\n",
            "Loss training: 34.061382\n",
            "Loss training: 28.234758\n",
            "Loss training: 31.564743\n",
            "Loss training: 27.046656\n",
            "Loss training: 22.360163\n",
            "Loss training: 42.11717\n",
            "Loss training: 36.083965\n",
            "Loss training: 35.53797\n",
            "\n",
            "***************************\n",
            "Trained on 5510 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.719719\n",
            "Loss training: 17.479534\n",
            "Loss training: 28.825216\n",
            "Loss training: 25.420328\n",
            "Loss training: 16.344011\n",
            "Loss training: 16.166527\n",
            "Loss training: 28.268679\n",
            "Loss training: 28.170986\n",
            "Loss training: 36.23433\n",
            "Loss training: 25.527164\n",
            "\n",
            "***************************\n",
            "Trained on 5520 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.80063\n",
            "Loss training: 32.848022\n",
            "Loss training: 28.189314\n",
            "Loss training: 32.552948\n",
            "Loss training: 21.046703\n",
            "Loss training: 27.5615\n",
            "Loss training: 32.438137\n",
            "Loss training: 31.446795\n",
            "Loss training: 25.227562\n",
            "Loss training: 18.020845\n",
            "\n",
            "***************************\n",
            "Trained on 5530 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.157177\n",
            "Loss training: 27.030333\n",
            "Loss training: 16.060062\n",
            "Loss training: 33.146225\n",
            "Loss training: 25.621672\n",
            "Loss training: 28.939089\n",
            "Loss training: 33.370197\n",
            "Loss training: 15.71595\n",
            "Loss training: 20.44255\n",
            "Loss training: 15.469636\n",
            "\n",
            "***************************\n",
            "Trained on 5540 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.682632\n",
            "Loss training: 31.27438\n",
            "Loss training: 35.106026\n",
            "Loss training: 42.203213\n",
            "Loss training: 27.956835\n",
            "Loss training: 28.487017\n",
            "Loss training: 41.169235\n",
            "Loss training: 25.50005\n",
            "Loss training: 35.727768\n",
            "Loss training: 28.45124\n",
            "\n",
            "***************************\n",
            "Trained on 5550 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.900866\n",
            "Loss training: 28.714428\n",
            "Loss training: 26.69545\n",
            "Loss training: 26.610573\n",
            "Loss training: 28.837818\n",
            "Loss training: 26.895262\n",
            "Loss training: 35.767075\n",
            "Loss training: 28.509073\n",
            "Loss training: 37.892284\n",
            "Loss training: 37.642754\n",
            "\n",
            "***************************\n",
            "Trained on 5560 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 21.038542\n",
            "Loss training: 25.443405\n",
            "Loss training: 35.917755\n",
            "Loss training: 32.503323\n",
            "Loss training: 20.936289\n",
            "Loss training: 20.863708\n",
            "Loss training: 27.946293\n",
            "Loss training: 27.979126\n",
            "Loss training: 32.30615\n",
            "Loss training: 32.063812\n",
            "\n",
            "***************************\n",
            "Trained on 5570 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.123966\n",
            "Loss training: 31.086739\n",
            "Loss training: 20.368462\n",
            "Loss training: 34.38114\n",
            "Loss training: 26.4233\n",
            "Loss training: 35.19975\n",
            "Loss training: 28.050808\n",
            "Loss training: 31.16793\n",
            "Loss training: 25.789568\n",
            "Loss training: 31.086283\n",
            "\n",
            "***************************\n",
            "Trained on 5580 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.285126\n",
            "Loss training: 35.02826\n",
            "Loss training: 31.371832\n",
            "Loss training: 16.929346\n",
            "Loss training: 30.370592\n",
            "Loss training: 12.211983\n",
            "Loss training: 12.179981\n",
            "Loss training: 30.494083\n",
            "Loss training: 32.917686\n",
            "Loss training: 25.360743\n",
            "\n",
            "***************************\n",
            "Trained on 5590 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.98037\n",
            "Loss training: 31.318186\n",
            "Loss training: 25.759857\n",
            "Loss training: 35.488026\n",
            "Loss training: 30.965298\n",
            "Loss training: 31.810213\n",
            "Loss training: 33.11794\n",
            "Loss training: 37.435017\n",
            "Loss training: 20.5903\n",
            "Loss training: 28.080492\n",
            "\n",
            "***************************\n",
            "Trained on 5600 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.103346\n",
            "Loss training: 36.176468\n",
            "Loss training: 22.762726\n",
            "Loss training: 34.580498\n",
            "Loss training: 20.645975\n",
            "Loss training: 20.449556\n",
            "Loss training: 26.929125\n",
            "Loss training: 29.730858\n",
            "Loss training: 32.267647\n",
            "Loss training: 20.247114\n",
            "\n",
            "***************************\n",
            "Trained on 5610 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.366896\n",
            "Loss training: 31.052006\n",
            "Loss training: 32.616787\n",
            "Loss training: 34.187103\n",
            "Loss training: 28.329075\n",
            "Loss training: 35.497433\n",
            "Loss training: 26.910868\n",
            "Loss training: 20.459904\n",
            "Loss training: 29.256233\n",
            "Loss training: 12.419179\n",
            "\n",
            "***************************\n",
            "Trained on 5620 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.37272\n",
            "Loss training: 22.28185\n",
            "Loss training: 26.836935\n",
            "Loss training: 28.2824\n",
            "Loss training: 32.763184\n",
            "Loss training: 27.83593\n",
            "Loss training: 28.5623\n",
            "Loss training: 35.33951\n",
            "Loss training: 30.952776\n",
            "Loss training: 12.140462\n",
            "\n",
            "***************************\n",
            "Trained on 5630 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.96955\n",
            "Loss training: 25.387438\n",
            "Loss training: 28.347082\n",
            "Loss training: 28.294806\n",
            "Loss training: 28.094713\n",
            "Loss training: 30.770824\n",
            "Loss training: 30.639994\n",
            "Loss training: 33.301933\n",
            "Loss training: 35.34749\n",
            "Loss training: 24.826794\n",
            "\n",
            "***************************\n",
            "Trained on 5640 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.000708\n",
            "Loss training: 42.700207\n",
            "Loss training: 42.23763\n",
            "Loss training: 30.359474\n",
            "Loss training: 32.681953\n",
            "Loss training: 30.927788\n",
            "Loss training: 32.65936\n",
            "Loss training: 35.45091\n",
            "Loss training: 25.764149\n",
            "Loss training: 38.37182\n",
            "\n",
            "***************************\n",
            "Trained on 5650 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.160915\n",
            "Loss training: 37.7067\n",
            "Loss training: 34.989597\n",
            "Loss training: 31.574934\n",
            "Loss training: 28.676691\n",
            "Loss training: 32.42307\n",
            "Loss training: 32.1586\n",
            "Loss training: 37.863087\n",
            "Loss training: 20.983768\n",
            "Loss training: 28.918308\n",
            "\n",
            "***************************\n",
            "Trained on 5660 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.15904\n",
            "Loss training: 34.87495\n",
            "Loss training: 15.418175\n",
            "Loss training: 27.31534\n",
            "Loss training: 31.145792\n",
            "Loss training: 25.242607\n",
            "Loss training: 26.878094\n",
            "Loss training: 27.948051\n",
            "Loss training: 24.952635\n",
            "Loss training: 38.29143\n",
            "\n",
            "***************************\n",
            "Trained on 5670 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 20.433636\n",
            "Loss training: 32.156178\n",
            "Loss training: 36.9108\n",
            "Loss training: 12.193072\n",
            "Loss training: 25.396358\n",
            "Loss training: 32.822334\n",
            "Loss training: 31.445631\n",
            "Loss training: 24.854406\n",
            "Loss training: 25.185638\n",
            "Loss training: 27.053545\n",
            "\n",
            "***************************\n",
            "Trained on 5680 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.90323\n",
            "Loss training: 27.785835\n",
            "Loss training: 30.672121\n",
            "Loss training: 27.11228\n",
            "Loss training: 35.021423\n",
            "Loss training: 26.603035\n",
            "Loss training: 15.377324\n",
            "Loss training: 34.697445\n",
            "Loss training: 12.024965\n",
            "Loss training: 36.747658\n",
            "\n",
            "***************************\n",
            "Trained on 5690 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.60284\n",
            "Loss training: 26.500347\n",
            "Loss training: 31.15756\n",
            "Loss training: 26.592358\n",
            "Loss training: 27.354918\n",
            "Loss training: 29.835688\n",
            "Loss training: 12.067843\n",
            "Loss training: 32.870384\n",
            "Loss training: 25.65564\n",
            "Loss training: 27.29745\n",
            "\n",
            "***************************\n",
            "Trained on 5700 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.011112\n",
            "Loss training: 16.280434\n",
            "Loss training: 32.683903\n",
            "Loss training: 36.06279\n",
            "Loss training: 32.705936\n",
            "Loss training: 29.753841\n",
            "Loss training: 35.809364\n",
            "Loss training: 34.643566\n",
            "Loss training: 15.801671\n",
            "Loss training: 29.813648\n",
            "\n",
            "***************************\n",
            "Trained on 5710 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.814112\n",
            "Loss training: 31.214487\n",
            "Loss training: 36.813427\n",
            "Loss training: 32.622078\n",
            "Loss training: 27.33821\n",
            "Loss training: 35.200134\n",
            "Loss training: 27.94836\n",
            "Loss training: 26.523874\n",
            "Loss training: 34.74022\n",
            "Loss training: 31.827969\n",
            "\n",
            "***************************\n",
            "Trained on 5720 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.91592\n",
            "Loss training: 27.917192\n",
            "Loss training: 31.462553\n",
            "Loss training: 33.05309\n",
            "Loss training: 15.474121\n",
            "Loss training: 35.4614\n",
            "Loss training: 37.18448\n",
            "Loss training: 28.15534\n",
            "Loss training: 20.330006\n",
            "Loss training: 28.860765\n",
            "\n",
            "***************************\n",
            "Trained on 5730 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.131664\n",
            "Loss training: 41.57367\n",
            "Loss training: 32.483772\n",
            "Loss training: 26.033062\n",
            "Loss training: 37.376373\n",
            "Loss training: 15.2395115\n",
            "Loss training: 26.415785\n",
            "Loss training: 28.507803\n",
            "Loss training: 29.055277\n",
            "Loss training: 26.981762\n",
            "\n",
            "***************************\n",
            "Trained on 5740 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.58502\n",
            "Loss training: 28.906801\n",
            "Loss training: 15.550748\n",
            "Loss training: 20.435053\n",
            "Loss training: 37.096912\n",
            "Loss training: 27.77152\n",
            "Loss training: 20.111763\n",
            "Loss training: 36.504997\n",
            "Loss training: 30.06555\n",
            "Loss training: 32.04096\n",
            "\n",
            "***************************\n",
            "Trained on 5750 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.466602\n",
            "Loss training: 36.78476\n",
            "Loss training: 27.747828\n",
            "Loss training: 20.302328\n",
            "Loss training: 34.9357\n",
            "Loss training: 20.890379\n",
            "Loss training: 36.45033\n",
            "Loss training: 36.798046\n",
            "Loss training: 26.978613\n",
            "Loss training: 15.159923\n",
            "\n",
            "***************************\n",
            "Trained on 5760 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.05569\n",
            "Loss training: 25.014818\n",
            "Loss training: 32.823647\n",
            "Loss training: 20.352833\n",
            "Loss training: 31.360521\n",
            "Loss training: 25.790993\n",
            "Loss training: 35.82996\n",
            "Loss training: 36.980927\n",
            "Loss training: 32.210976\n",
            "Loss training: 35.23463\n",
            "\n",
            "***************************\n",
            "Trained on 5770 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 12.438569\n",
            "Loss training: 26.814785\n",
            "Loss training: 25.613466\n",
            "Loss training: 32.777294\n",
            "Loss training: 42.16392\n",
            "Loss training: 35.453384\n",
            "Loss training: 30.903393\n",
            "Loss training: 28.633846\n",
            "Loss training: 31.941227\n",
            "Loss training: 32.611027\n",
            "\n",
            "***************************\n",
            "Trained on 5780 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.012718\n",
            "Loss training: 15.13313\n",
            "Loss training: 31.656803\n",
            "Loss training: 35.300545\n",
            "Loss training: 20.575878\n",
            "Loss training: 20.435814\n",
            "Loss training: 26.43987\n",
            "Loss training: 31.228354\n",
            "Loss training: 31.787292\n",
            "Loss training: 30.94906\n",
            "\n",
            "***************************\n",
            "Trained on 5790 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.222208\n",
            "Loss training: 27.051748\n",
            "Loss training: 41.81256\n",
            "Loss training: 20.433342\n",
            "Loss training: 31.242027\n",
            "Loss training: 27.823463\n",
            "Loss training: 31.477167\n",
            "Loss training: 31.052668\n",
            "Loss training: 33.218067\n",
            "Loss training: 30.375965\n",
            "\n",
            "***************************\n",
            "Trained on 5800 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.87834\n",
            "Loss training: 15.013777\n",
            "Loss training: 32.202568\n",
            "Loss training: 33.002228\n",
            "Loss training: 16.20301\n",
            "Loss training: 30.09309\n",
            "Loss training: 26.84114\n",
            "Loss training: 34.727776\n",
            "Loss training: 28.832651\n",
            "Loss training: 34.58568\n",
            "\n",
            "***************************\n",
            "Trained on 5810 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.758211\n",
            "Loss training: 32.288486\n",
            "Loss training: 26.96427\n",
            "Loss training: 26.578558\n",
            "Loss training: 35.01652\n",
            "Loss training: 34.263363\n",
            "Loss training: 31.920195\n",
            "Loss training: 29.89242\n",
            "Loss training: 25.479235\n",
            "Loss training: 34.07725\n",
            "\n",
            "***************************\n",
            "Trained on 5820 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.442877\n",
            "Loss training: 30.608139\n",
            "Loss training: 41.333176\n",
            "Loss training: 34.373955\n",
            "Loss training: 22.523172\n",
            "Loss training: 25.38363\n",
            "Loss training: 15.885975\n",
            "Loss training: 12.42135\n",
            "Loss training: 26.541702\n",
            "Loss training: 35.555763\n",
            "\n",
            "***************************\n",
            "Trained on 5830 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.764757\n",
            "Loss training: 27.708992\n",
            "Loss training: 40.88514\n",
            "Loss training: 25.296316\n",
            "Loss training: 15.505566\n",
            "Loss training: 27.515202\n",
            "Loss training: 15.898897\n",
            "Loss training: 36.961205\n",
            "Loss training: 34.24841\n",
            "Loss training: 26.386337\n",
            "\n",
            "***************************\n",
            "Trained on 5840 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.773006\n",
            "Loss training: 20.789452\n",
            "Loss training: 12.22303\n",
            "Loss training: 32.19675\n",
            "Loss training: 20.514385\n",
            "Loss training: 25.431751\n",
            "Loss training: 20.343493\n",
            "Loss training: 28.112242\n",
            "Loss training: 31.962315\n",
            "Loss training: 32.39971\n",
            "\n",
            "***************************\n",
            "Trained on 5850 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.6646\n",
            "Loss training: 31.710781\n",
            "Loss training: 27.713322\n",
            "Loss training: 35.160072\n",
            "Loss training: 27.154282\n",
            "Loss training: 26.553951\n",
            "Loss training: 32.506058\n",
            "Loss training: 20.374304\n",
            "Loss training: 27.559994\n",
            "Loss training: 27.02067\n",
            "\n",
            "***************************\n",
            "Trained on 5860 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.797974\n",
            "Loss training: 26.554192\n",
            "Loss training: 25.563667\n",
            "Loss training: 30.163355\n",
            "Loss training: 31.363865\n",
            "Loss training: 24.952665\n",
            "Loss training: 22.443844\n",
            "Loss training: 27.323725\n",
            "Loss training: 27.301128\n",
            "Loss training: 31.516861\n",
            "\n",
            "***************************\n",
            "Trained on 5870 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.379566\n",
            "Loss training: 15.737696\n",
            "Loss training: 26.624773\n",
            "Loss training: 28.63381\n",
            "Loss training: 35.335243\n",
            "Loss training: 15.806427\n",
            "Loss training: 34.040016\n",
            "Loss training: 31.847696\n",
            "Loss training: 31.029787\n",
            "Loss training: 30.716343\n",
            "\n",
            "***************************\n",
            "Trained on 5880 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.920937\n",
            "Loss training: 35.617786\n",
            "Loss training: 31.429815\n",
            "Loss training: 27.638828\n",
            "Loss training: 30.415157\n",
            "Loss training: 34.16335\n",
            "Loss training: 28.399298\n",
            "Loss training: 16.263542\n",
            "Loss training: 32.282356\n",
            "Loss training: 31.919046\n",
            "\n",
            "***************************\n",
            "Trained on 5890 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.686016\n",
            "Loss training: 34.911102\n",
            "Loss training: 32.31199\n",
            "Loss training: 34.827057\n",
            "Loss training: 13.325316\n",
            "Loss training: 30.807018\n",
            "Loss training: 34.076355\n",
            "Loss training: 26.738337\n",
            "Loss training: 36.596733\n",
            "Loss training: 27.80599\n",
            "\n",
            "***************************\n",
            "Trained on 5900 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.958908\n",
            "Loss training: 33.787457\n",
            "Loss training: 37.38337\n",
            "Loss training: 22.514843\n",
            "Loss training: 32.80242\n",
            "Loss training: 33.77027\n",
            "Loss training: 32.3017\n",
            "Loss training: 31.488535\n",
            "Loss training: 31.74805\n",
            "Loss training: 27.965858\n",
            "\n",
            "***************************\n",
            "Trained on 5910 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.31788\n",
            "Loss training: 12.605933\n",
            "Loss training: 30.584946\n",
            "Loss training: 35.158657\n",
            "Loss training: 36.390453\n",
            "Loss training: 32.134357\n",
            "Loss training: 15.402645\n",
            "Loss training: 15.202387\n",
            "Loss training: 16.02235\n",
            "Loss training: 35.043373\n",
            "\n",
            "***************************\n",
            "Trained on 5920 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.381638\n",
            "Loss training: 15.366248\n",
            "Loss training: 28.70846\n",
            "Loss training: 20.619537\n",
            "Loss training: 35.143276\n",
            "Loss training: 36.58788\n",
            "Loss training: 34.61539\n",
            "Loss training: 31.96774\n",
            "Loss training: 35.95889\n",
            "Loss training: 28.017805\n",
            "\n",
            "***************************\n",
            "Trained on 5930 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.382269\n",
            "Loss training: 34.329777\n",
            "Loss training: 27.455627\n",
            "Loss training: 28.68678\n",
            "Loss training: 27.844183\n",
            "Loss training: 30.723993\n",
            "Loss training: 27.508606\n",
            "Loss training: 27.292994\n",
            "Loss training: 16.33069\n",
            "Loss training: 22.316408\n",
            "\n",
            "***************************\n",
            "Trained on 5940 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.233139\n",
            "Loss training: 35.00123\n",
            "Loss training: 16.337944\n",
            "Loss training: 32.045216\n",
            "Loss training: 31.63431\n",
            "Loss training: 28.23436\n",
            "Loss training: 26.636518\n",
            "Loss training: 12.436785\n",
            "Loss training: 32.407696\n",
            "Loss training: 31.175903\n",
            "\n",
            "***************************\n",
            "Trained on 5950 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.680021\n",
            "Loss training: 32.894215\n",
            "Loss training: 12.319539\n",
            "Loss training: 12.258711\n",
            "Loss training: 25.236599\n",
            "Loss training: 26.534613\n",
            "Loss training: 34.99934\n",
            "Loss training: 12.137067\n",
            "Loss training: 35.205124\n",
            "Loss training: 25.048214\n",
            "\n",
            "***************************\n",
            "Trained on 5960 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 17.459682\n",
            "Loss training: 31.408482\n",
            "Loss training: 30.49913\n",
            "Loss training: 30.184042\n",
            "Loss training: 34.608387\n",
            "Loss training: 36.7483\n",
            "Loss training: 20.96374\n",
            "Loss training: 20.757132\n",
            "Loss training: 26.744331\n",
            "Loss training: 30.774118\n",
            "\n",
            "***************************\n",
            "Trained on 5970 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.145905\n",
            "Loss training: 24.972273\n",
            "Loss training: 34.3657\n",
            "Loss training: 16.369009\n",
            "Loss training: 15.810529\n",
            "Loss training: 32.411453\n",
            "Loss training: 26.581537\n",
            "Loss training: 26.55845\n",
            "Loss training: 30.37082\n",
            "Loss training: 27.597376\n",
            "\n",
            "***************************\n",
            "Trained on 5980 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.21451\n",
            "Loss training: 21.284136\n",
            "Loss training: 35.085327\n",
            "Loss training: 29.669342\n",
            "Loss training: 31.303482\n",
            "Loss training: 37.43383\n",
            "Loss training: 32.93244\n",
            "Loss training: 25.883911\n",
            "Loss training: 32.65785\n",
            "Loss training: 13.045663\n",
            "\n",
            "***************************\n",
            "Trained on 5990 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.93495\n",
            "Loss training: 36.881298\n",
            "Loss training: 22.338963\n",
            "Loss training: 27.54071\n",
            "Loss training: 31.35693\n",
            "Loss training: 31.757269\n",
            "Loss training: 34.145634\n",
            "Loss training: 42.696453\n",
            "Loss training: 32.19528\n",
            "Loss training: 30.829998\n",
            "\n",
            "***************************\n",
            "Trained on 6000 graphs\n",
            "***************************\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_80720161-088f-47d5-a758-664a40568e84\", \"params_epochs_6000.pickle\", 22204504)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d65f8dcc-a519-4076-ac92-97a5bbb3f186\", \"opt_state_epochs_6000.pickle\", 44409100)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss training: 37.70204\n",
            "Loss training: 25.079899\n",
            "Loss training: 16.957212\n",
            "Loss training: 30.44358\n",
            "Loss training: 31.322287\n",
            "Loss training: 37.200638\n",
            "Loss training: 32.699955\n",
            "Loss training: 27.281794\n",
            "Loss training: 32.93762\n",
            "Loss training: 36.832287\n",
            "\n",
            "***************************\n",
            "Trained on 6010 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.26106\n",
            "Loss training: 33.078236\n",
            "Loss training: 31.022182\n",
            "Loss training: 25.290705\n",
            "Loss training: 36.808933\n",
            "Loss training: 32.658524\n",
            "Loss training: 27.939949\n",
            "Loss training: 36.485924\n",
            "Loss training: 12.215613\n",
            "Loss training: 36.339504\n",
            "\n",
            "***************************\n",
            "Trained on 6020 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.117077\n",
            "Loss training: 33.223103\n",
            "Loss training: 28.645042\n",
            "Loss training: 15.907441\n",
            "Loss training: 37.45313\n",
            "Loss training: 24.546764\n",
            "Loss training: 27.355972\n",
            "Loss training: 28.588652\n",
            "Loss training: 39.46273\n",
            "Loss training: 32.13061\n",
            "\n",
            "***************************\n",
            "Trained on 6030 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.031845\n",
            "Loss training: 38.590725\n",
            "Loss training: 28.402657\n",
            "Loss training: 31.691422\n",
            "Loss training: 32.904213\n",
            "Loss training: 21.695988\n",
            "Loss training: 28.855156\n",
            "Loss training: 36.92832\n",
            "Loss training: 42.80549\n",
            "Loss training: 32.00124\n",
            "\n",
            "***************************\n",
            "Trained on 6040 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.41289\n",
            "Loss training: 12.717068\n",
            "Loss training: 15.217047\n",
            "Loss training: 32.249943\n",
            "Loss training: 31.976868\n",
            "Loss training: 35.72008\n",
            "Loss training: 33.846245\n",
            "Loss training: 25.74765\n",
            "Loss training: 12.420012\n",
            "Loss training: 44.87847\n",
            "\n",
            "***************************\n",
            "Trained on 6050 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.325207\n",
            "Loss training: 32.88774\n",
            "Loss training: 22.978909\n",
            "Loss training: 28.202808\n",
            "Loss training: 27.454262\n",
            "Loss training: 20.834871\n",
            "Loss training: 32.170258\n",
            "Loss training: 31.808992\n",
            "Loss training: 32.82404\n",
            "Loss training: 26.28523\n",
            "\n",
            "***************************\n",
            "Trained on 6060 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 21.336473\n",
            "Loss training: 26.895739\n",
            "Loss training: 32.923737\n",
            "Loss training: 32.602222\n",
            "Loss training: 26.812021\n",
            "Loss training: 36.22459\n",
            "Loss training: 15.809739\n",
            "Loss training: 27.963604\n",
            "Loss training: 15.858946\n",
            "Loss training: 27.2325\n",
            "\n",
            "***************************\n",
            "Trained on 6070 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.925657\n",
            "Loss training: 35.418922\n",
            "Loss training: 26.753716\n",
            "Loss training: 20.265461\n",
            "Loss training: 32.01426\n",
            "Loss training: 26.642485\n",
            "Loss training: 31.892584\n",
            "Loss training: 15.413441\n",
            "Loss training: 32.38214\n",
            "Loss training: 31.092222\n",
            "\n",
            "***************************\n",
            "Trained on 6080 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.989017\n",
            "Loss training: 25.425825\n",
            "Loss training: 41.79021\n",
            "Loss training: 28.081505\n",
            "Loss training: 31.653763\n",
            "Loss training: 34.44484\n",
            "Loss training: 40.871605\n",
            "Loss training: 34.154186\n",
            "Loss training: 37.49476\n",
            "Loss training: 14.882377\n",
            "\n",
            "***************************\n",
            "Trained on 6090 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 14.732042\n",
            "Loss training: 27.227457\n",
            "Loss training: 31.654924\n",
            "Loss training: 31.353804\n",
            "Loss training: 31.198149\n",
            "Loss training: 36.86293\n",
            "Loss training: 15.743376\n",
            "Loss training: 15.39612\n",
            "Loss training: 27.65454\n",
            "Loss training: 25.49848\n",
            "\n",
            "***************************\n",
            "Trained on 6100 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.3868\n",
            "Loss training: 29.977674\n",
            "Loss training: 29.689217\n",
            "Loss training: 31.436453\n",
            "Loss training: 32.336292\n",
            "Loss training: 36.57095\n",
            "Loss training: 26.843573\n",
            "Loss training: 27.108227\n",
            "Loss training: 34.581284\n",
            "Loss training: 26.213486\n",
            "\n",
            "***************************\n",
            "Trained on 6110 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.36057\n",
            "Loss training: 34.377594\n",
            "Loss training: 25.182901\n",
            "Loss training: 20.679998\n",
            "Loss training: 37.368317\n",
            "Loss training: 33.839127\n",
            "Loss training: 41.46344\n",
            "Loss training: 28.588844\n",
            "Loss training: 37.106373\n",
            "Loss training: 31.025112\n",
            "\n",
            "***************************\n",
            "Trained on 6120 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.619778\n",
            "Loss training: 31.01673\n",
            "Loss training: 34.62021\n",
            "Loss training: 24.720451\n",
            "Loss training: 15.63128\n",
            "Loss training: 27.10966\n",
            "Loss training: 27.993782\n",
            "Loss training: 12.315799\n",
            "Loss training: 25.58827\n",
            "Loss training: 28.899437\n",
            "\n",
            "***************************\n",
            "Trained on 6130 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.075788\n",
            "Loss training: 34.048466\n",
            "Loss training: 36.90758\n",
            "Loss training: 36.976543\n",
            "Loss training: 16.39869\n",
            "Loss training: 27.471457\n",
            "Loss training: 22.010147\n",
            "Loss training: 33.817135\n",
            "Loss training: 36.347034\n",
            "Loss training: 35.52969\n",
            "\n",
            "***************************\n",
            "Trained on 6140 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.399096\n",
            "Loss training: 27.163519\n",
            "Loss training: 34.32943\n",
            "Loss training: 31.375261\n",
            "Loss training: 35.41204\n",
            "Loss training: 24.943626\n",
            "Loss training: 30.822504\n",
            "Loss training: 15.840819\n",
            "Loss training: 21.539852\n",
            "Loss training: 41.333866\n",
            "\n",
            "***************************\n",
            "Trained on 6150 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.054012\n",
            "Loss training: 40.74597\n",
            "Loss training: 32.108574\n",
            "Loss training: 31.699497\n",
            "Loss training: 24.951588\n",
            "Loss training: 33.13681\n",
            "Loss training: 31.157497\n",
            "Loss training: 32.105186\n",
            "Loss training: 31.996971\n",
            "Loss training: 20.80449\n",
            "\n",
            "***************************\n",
            "Trained on 6160 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.426018\n",
            "Loss training: 26.5518\n",
            "Loss training: 12.595197\n",
            "Loss training: 28.244331\n",
            "Loss training: 27.716843\n",
            "Loss training: 20.835615\n",
            "Loss training: 30.882364\n",
            "Loss training: 30.560827\n",
            "Loss training: 26.927471\n",
            "Loss training: 34.975864\n",
            "\n",
            "***************************\n",
            "Trained on 6170 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 16.240242\n",
            "Loss training: 31.22556\n",
            "Loss training: 16.406445\n",
            "Loss training: 31.752522\n",
            "Loss training: 31.529781\n",
            "Loss training: 29.371698\n",
            "Loss training: 28.955267\n",
            "Loss training: 35.507477\n",
            "Loss training: 37.2859\n",
            "Loss training: 30.763706\n",
            "\n",
            "***************************\n",
            "Trained on 6180 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.732397\n",
            "Loss training: 12.240513\n",
            "Loss training: 36.247772\n",
            "Loss training: 30.995733\n",
            "Loss training: 32.837414\n",
            "Loss training: 30.09999\n",
            "Loss training: 12.039904\n",
            "Loss training: 29.756731\n",
            "Loss training: 15.138235\n",
            "Loss training: 34.794662\n",
            "\n",
            "***************************\n",
            "Trained on 6190 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.44456\n",
            "Loss training: 15.692077\n",
            "Loss training: 29.577545\n",
            "Loss training: 31.117628\n",
            "Loss training: 14.631685\n",
            "Loss training: 28.63534\n",
            "Loss training: 42.01234\n",
            "Loss training: 14.495298\n",
            "Loss training: 33.018265\n",
            "Loss training: 33.339287\n",
            "\n",
            "***************************\n",
            "Trained on 6200 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 12.140815\n",
            "Loss training: 33.39015\n",
            "Loss training: 22.583696\n",
            "Loss training: 30.206738\n",
            "Loss training: 34.65518\n",
            "Loss training: 31.454472\n",
            "Loss training: 30.653559\n",
            "Loss training: 30.280212\n",
            "Loss training: 30.227833\n",
            "Loss training: 32.065903\n",
            "\n",
            "***************************\n",
            "Trained on 6210 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 15.324963\n",
            "Loss training: 30.57926\n",
            "Loss training: 30.504263\n",
            "Loss training: 30.996498\n",
            "Loss training: 21.06411\n",
            "Loss training: 30.703566\n",
            "Loss training: 30.40341\n",
            "Loss training: 30.114103\n",
            "Loss training: 24.910755\n",
            "Loss training: 21.795473\n",
            "\n",
            "***************************\n",
            "Trained on 6220 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 14.900441\n",
            "Loss training: 25.053032\n",
            "Loss training: 30.93628\n",
            "Loss training: 34.32086\n",
            "Loss training: 26.549755\n",
            "Loss training: 30.389368\n",
            "Loss training: 36.677143\n",
            "Loss training: 33.914093\n",
            "Loss training: 31.976763\n",
            "Loss training: 30.518257\n",
            "\n",
            "***************************\n",
            "Trained on 6230 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.296522\n",
            "Loss training: 15.007373\n",
            "Loss training: 34.80897\n",
            "Loss training: 33.44144\n",
            "Loss training: 30.040287\n",
            "Loss training: 26.49003\n",
            "Loss training: 29.654377\n",
            "Loss training: 29.99244\n",
            "Loss training: 29.38123\n",
            "Loss training: 26.185963\n",
            "\n",
            "***************************\n",
            "Trained on 6240 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 12.333341\n",
            "Loss training: 35.191147\n",
            "Loss training: 33.30183\n",
            "Loss training: 33.862526\n",
            "Loss training: 20.619307\n",
            "Loss training: 25.813274\n",
            "Loss training: 33.452045\n",
            "Loss training: 32.276623\n",
            "Loss training: 31.805817\n",
            "Loss training: 20.109364\n",
            "\n",
            "***************************\n",
            "Trained on 6250 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.890371\n",
            "Loss training: 30.250278\n",
            "Loss training: 27.840544\n",
            "Loss training: 26.667074\n",
            "Loss training: 36.035286\n",
            "Loss training: 26.61338\n",
            "Loss training: 29.97312\n",
            "Loss training: 29.95619\n",
            "Loss training: 34.54856\n",
            "Loss training: 29.384722\n",
            "\n",
            "***************************\n",
            "Trained on 6260 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.5706\n",
            "Loss training: 26.445301\n",
            "Loss training: 29.273039\n",
            "Loss training: 33.62694\n",
            "Loss training: 12.193406\n",
            "Loss training: 14.54762\n",
            "Loss training: 35.339817\n",
            "Loss training: 40.922245\n",
            "Loss training: 31.824984\n",
            "Loss training: 26.782982\n",
            "\n",
            "***************************\n",
            "Trained on 6270 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.00896\n",
            "Loss training: 11.939623\n",
            "Loss training: 14.640276\n",
            "Loss training: 35.280415\n",
            "Loss training: 36.655727\n",
            "Loss training: 36.4927\n",
            "Loss training: 33.12203\n",
            "Loss training: 24.949999\n",
            "Loss training: 26.372747\n",
            "Loss training: 31.175829\n",
            "\n",
            "***************************\n",
            "Trained on 6280 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.26948\n",
            "Loss training: 30.568047\n",
            "Loss training: 14.34832\n",
            "Loss training: 26.279552\n",
            "Loss training: 29.608624\n",
            "Loss training: 30.868206\n",
            "Loss training: 11.913261\n",
            "Loss training: 32.144417\n",
            "Loss training: 33.566227\n",
            "Loss training: 29.589966\n",
            "\n",
            "***************************\n",
            "Trained on 6290 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.02702\n",
            "Loss training: 14.314215\n",
            "Loss training: 34.56988\n",
            "Loss training: 29.146793\n",
            "Loss training: 20.57123\n",
            "Loss training: 30.874823\n",
            "Loss training: 29.811064\n",
            "Loss training: 40.89723\n",
            "Loss training: 24.940063\n",
            "Loss training: 29.09048\n",
            "\n",
            "***************************\n",
            "Trained on 6300 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.059612\n",
            "Loss training: 27.157673\n",
            "Loss training: 32.062374\n",
            "Loss training: 22.577564\n",
            "Loss training: 27.732828\n",
            "Loss training: 34.93293\n",
            "Loss training: 27.269455\n",
            "Loss training: 26.211222\n",
            "Loss training: 30.123928\n",
            "Loss training: 29.87218\n",
            "\n",
            "***************************\n",
            "Trained on 6310 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.74313\n",
            "Loss training: 25.280659\n",
            "Loss training: 36.399837\n",
            "Loss training: 32.076572\n",
            "Loss training: 27.952347\n",
            "Loss training: 26.227085\n",
            "Loss training: 35.64607\n",
            "Loss training: 26.697996\n",
            "Loss training: 31.513344\n",
            "Loss training: 33.46838\n",
            "\n",
            "***************************\n",
            "Trained on 6320 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.183651\n",
            "Loss training: 26.506456\n",
            "Loss training: 31.786398\n",
            "Loss training: 33.805397\n",
            "Loss training: 26.727354\n",
            "Loss training: 30.416336\n",
            "Loss training: 30.871914\n",
            "Loss training: 29.416763\n",
            "Loss training: 33.744938\n",
            "Loss training: 28.954445\n",
            "\n",
            "***************************\n",
            "Trained on 6330 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.256958\n",
            "Loss training: 20.557505\n",
            "Loss training: 30.212887\n",
            "Loss training: 25.127691\n",
            "Loss training: 24.936602\n",
            "Loss training: 21.997538\n",
            "Loss training: 20.580969\n",
            "Loss training: 31.999905\n",
            "Loss training: 15.090881\n",
            "Loss training: 34.67053\n",
            "\n",
            "***************************\n",
            "Trained on 6340 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.878872\n",
            "Loss training: 26.8579\n",
            "Loss training: 31.776108\n",
            "Loss training: 27.44988\n",
            "Loss training: 26.735586\n",
            "Loss training: 31.748535\n",
            "Loss training: 30.632233\n",
            "Loss training: 25.176939\n",
            "Loss training: 41.288273\n",
            "Loss training: 34.33926\n",
            "\n",
            "***************************\n",
            "Trained on 6350 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.644913\n",
            "Loss training: 28.069584\n",
            "Loss training: 35.859566\n",
            "Loss training: 31.665352\n",
            "Loss training: 27.209745\n",
            "Loss training: 12.605268\n",
            "Loss training: 36.371143\n",
            "Loss training: 26.319891\n",
            "Loss training: 35.304375\n",
            "Loss training: 30.692932\n",
            "\n",
            "***************************\n",
            "Trained on 6360 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.81863\n",
            "Loss training: 27.603134\n",
            "Loss training: 30.47987\n",
            "Loss training: 30.062468\n",
            "Loss training: 28.088057\n",
            "Loss training: 34.40175\n",
            "Loss training: 26.52335\n",
            "Loss training: 26.520844\n",
            "Loss training: 30.839067\n",
            "Loss training: 35.428734\n",
            "\n",
            "***************************\n",
            "Trained on 6370 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.113724\n",
            "Loss training: 15.36343\n",
            "Loss training: 14.708621\n",
            "Loss training: 34.752174\n",
            "Loss training: 14.707775\n",
            "Loss training: 25.177275\n",
            "Loss training: 27.356165\n",
            "Loss training: 33.659607\n",
            "Loss training: 12.144043\n",
            "Loss training: 27.247467\n",
            "\n",
            "***************************\n",
            "Trained on 6380 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.408466\n",
            "Loss training: 26.989859\n",
            "Loss training: 33.316723\n",
            "Loss training: 24.430637\n",
            "Loss training: 34.75995\n",
            "Loss training: 36.023277\n",
            "Loss training: 20.527077\n",
            "Loss training: 30.111408\n",
            "Loss training: 27.47539\n",
            "Loss training: 29.278559\n",
            "\n",
            "***************************\n",
            "Trained on 6390 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.122004\n",
            "Loss training: 34.786602\n",
            "Loss training: 29.808802\n",
            "Loss training: 34.992256\n",
            "Loss training: 26.530567\n",
            "Loss training: 29.819994\n",
            "Loss training: 32.17947\n",
            "Loss training: 40.688663\n",
            "Loss training: 12.235145\n",
            "Loss training: 34.75276\n",
            "\n",
            "***************************\n",
            "Trained on 6400 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.320621\n",
            "Loss training: 34.41561\n",
            "Loss training: 20.30694\n",
            "Loss training: 32.141087\n",
            "Loss training: 33.089924\n",
            "Loss training: 28.927208\n",
            "Loss training: 22.08876\n",
            "Loss training: 29.823822\n",
            "Loss training: 24.863947\n",
            "Loss training: 30.011797\n",
            "\n",
            "***************************\n",
            "Trained on 6410 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.273994\n",
            "Loss training: 27.940737\n",
            "Loss training: 36.323124\n",
            "Loss training: 26.418627\n",
            "Loss training: 26.13705\n",
            "Loss training: 30.559107\n",
            "Loss training: 29.641977\n",
            "Loss training: 40.526707\n",
            "Loss training: 24.657362\n",
            "Loss training: 31.745085\n",
            "\n",
            "***************************\n",
            "Trained on 6420 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.562769\n",
            "Loss training: 30.309584\n",
            "Loss training: 27.692917\n",
            "Loss training: 27.542519\n",
            "Loss training: 20.215918\n",
            "Loss training: 32.70322\n",
            "Loss training: 11.84194\n",
            "Loss training: 20.34891\n",
            "Loss training: 24.65428\n",
            "Loss training: 24.59638\n",
            "\n",
            "***************************\n",
            "Trained on 6430 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.649382\n",
            "Loss training: 28.578512\n",
            "Loss training: 14.822423\n",
            "Loss training: 26.425245\n",
            "Loss training: 26.108004\n",
            "Loss training: 40.322624\n",
            "Loss training: 29.524866\n",
            "Loss training: 14.672051\n",
            "Loss training: 33.7125\n",
            "Loss training: 11.996673\n",
            "\n",
            "***************************\n",
            "Trained on 6440 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.765316\n",
            "Loss training: 33.03645\n",
            "Loss training: 36.77346\n",
            "Loss training: 32.349445\n",
            "Loss training: 24.934258\n",
            "Loss training: 35.254353\n",
            "Loss training: 28.617144\n",
            "Loss training: 14.44392\n",
            "Loss training: 29.255587\n",
            "Loss training: 36.72993\n",
            "\n",
            "***************************\n",
            "Trained on 6450 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.405758\n",
            "Loss training: 28.455692\n",
            "Loss training: 14.166417\n",
            "Loss training: 12.09366\n",
            "Loss training: 29.810257\n",
            "Loss training: 11.917958\n",
            "Loss training: 20.368547\n",
            "Loss training: 26.662271\n",
            "Loss training: 29.458004\n",
            "Loss training: 27.18779\n",
            "\n",
            "***************************\n",
            "Trained on 6460 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.95068\n",
            "Loss training: 29.222725\n",
            "Loss training: 30.69697\n",
            "Loss training: 24.482344\n",
            "Loss training: 24.441727\n",
            "Loss training: 25.911627\n",
            "Loss training: 20.803526\n",
            "Loss training: 25.363504\n",
            "Loss training: 32.348206\n",
            "Loss training: 29.23171\n",
            "\n",
            "***************************\n",
            "Trained on 6470 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.397505\n",
            "Loss training: 36.247486\n",
            "Loss training: 24.73723\n",
            "Loss training: 32.573284\n",
            "Loss training: 30.18538\n",
            "Loss training: 34.725216\n",
            "Loss training: 27.544798\n",
            "Loss training: 26.304611\n",
            "Loss training: 33.536293\n",
            "Loss training: 29.544107\n",
            "\n",
            "***************************\n",
            "Trained on 6480 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.502136\n",
            "Loss training: 29.978077\n",
            "Loss training: 30.905973\n",
            "Loss training: 29.457413\n",
            "Loss training: 26.30959\n",
            "Loss training: 40.457035\n",
            "Loss training: 31.489782\n",
            "Loss training: 35.980873\n",
            "Loss training: 26.819984\n",
            "Loss training: 30.476692\n",
            "\n",
            "***************************\n",
            "Trained on 6490 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 12.11834\n",
            "Loss training: 25.93124\n",
            "Loss training: 34.587776\n",
            "Loss training: 29.939022\n",
            "Loss training: 15.475255\n",
            "Loss training: 26.486122\n",
            "Loss training: 29.405384\n",
            "Loss training: 25.023115\n",
            "Loss training: 27.831656\n",
            "Loss training: 31.573572\n",
            "\n",
            "***************************\n",
            "Trained on 6500 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.832407\n",
            "Loss training: 30.618185\n",
            "Loss training: 33.514748\n",
            "Loss training: 28.289772\n",
            "Loss training: 25.111391\n",
            "Loss training: 34.87206\n",
            "Loss training: 36.573544\n",
            "Loss training: 33.602745\n",
            "Loss training: 35.953747\n",
            "Loss training: 33.792797\n",
            "\n",
            "***************************\n",
            "Trained on 6510 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.970592\n",
            "Loss training: 29.073166\n",
            "Loss training: 36.039566\n",
            "Loss training: 26.778496\n",
            "Loss training: 26.450083\n",
            "Loss training: 30.350695\n",
            "Loss training: 28.789495\n",
            "Loss training: 30.830849\n",
            "Loss training: 37.121174\n",
            "Loss training: 30.123756\n",
            "\n",
            "***************************\n",
            "Trained on 6520 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.792664\n",
            "Loss training: 27.40239\n",
            "Loss training: 30.130022\n",
            "Loss training: 30.875925\n",
            "Loss training: 30.581957\n",
            "Loss training: 33.13705\n",
            "Loss training: 25.6064\n",
            "Loss training: 29.60994\n",
            "Loss training: 31.303267\n",
            "Loss training: 29.650333\n",
            "\n",
            "***************************\n",
            "Trained on 6530 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.216341\n",
            "Loss training: 30.993616\n",
            "Loss training: 29.154308\n",
            "Loss training: 27.579283\n",
            "Loss training: 30.902542\n",
            "Loss training: 27.922735\n",
            "Loss training: 12.1374655\n",
            "Loss training: 32.843987\n",
            "Loss training: 35.001984\n",
            "Loss training: 29.075626\n",
            "\n",
            "***************************\n",
            "Trained on 6540 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.529894\n",
            "Loss training: 29.649181\n",
            "Loss training: 33.800755\n",
            "Loss training: 15.002203\n",
            "Loss training: 36.588932\n",
            "Loss training: 34.415024\n",
            "Loss training: 27.39377\n",
            "Loss training: 33.294704\n",
            "Loss training: 31.46004\n",
            "Loss training: 30.078032\n",
            "\n",
            "***************************\n",
            "Trained on 6550 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.012842\n",
            "Loss training: 32.081852\n",
            "Loss training: 27.239876\n",
            "Loss training: 34.16962\n",
            "Loss training: 14.761413\n",
            "Loss training: 30.680407\n",
            "Loss training: 29.8777\n",
            "Loss training: 36.569397\n",
            "Loss training: 29.07148\n",
            "Loss training: 29.593657\n",
            "\n",
            "***************************\n",
            "Trained on 6560 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 20.835371\n",
            "Loss training: 26.744738\n",
            "Loss training: 26.784304\n",
            "Loss training: 33.06017\n",
            "Loss training: 24.987917\n",
            "Loss training: 24.999815\n",
            "Loss training: 14.774639\n",
            "Loss training: 25.968029\n",
            "Loss training: 36.868565\n",
            "Loss training: 30.67795\n",
            "\n",
            "***************************\n",
            "Trained on 6570 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.71033\n",
            "Loss training: 24.677376\n",
            "Loss training: 30.321198\n",
            "Loss training: 27.434717\n",
            "Loss training: 35.40008\n",
            "Loss training: 36.42724\n",
            "Loss training: 35.41573\n",
            "Loss training: 22.229767\n",
            "Loss training: 31.032892\n",
            "Loss training: 28.058767\n",
            "\n",
            "***************************\n",
            "Trained on 6580 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.783304\n",
            "Loss training: 29.104374\n",
            "Loss training: 34.289223\n",
            "Loss training: 32.48131\n",
            "Loss training: 27.282333\n",
            "Loss training: 11.951616\n",
            "Loss training: 20.096897\n",
            "Loss training: 30.66036\n",
            "Loss training: 33.980953\n",
            "Loss training: 29.778206\n",
            "\n",
            "***************************\n",
            "Trained on 6590 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.754221\n",
            "Loss training: 26.967371\n",
            "Loss training: 29.191305\n",
            "Loss training: 31.935663\n",
            "Loss training: 33.71826\n",
            "Loss training: 20.342468\n",
            "Loss training: 30.322832\n",
            "Loss training: 26.553131\n",
            "Loss training: 35.99044\n",
            "Loss training: 29.187014\n",
            "\n",
            "***************************\n",
            "Trained on 6600 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.872025\n",
            "Loss training: 30.738073\n",
            "Loss training: 35.803444\n",
            "Loss training: 26.347237\n",
            "Loss training: 34.3772\n",
            "Loss training: 27.071726\n",
            "Loss training: 31.514149\n",
            "Loss training: 25.128546\n",
            "Loss training: 25.487848\n",
            "Loss training: 20.26521\n",
            "\n",
            "***************************\n",
            "Trained on 6610 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.672413\n",
            "Loss training: 11.856232\n",
            "Loss training: 36.199226\n",
            "Loss training: 25.865248\n",
            "Loss training: 26.320726\n",
            "Loss training: 30.776018\n",
            "Loss training: 24.93113\n",
            "Loss training: 27.06396\n",
            "Loss training: 34.249638\n",
            "Loss training: 27.730597\n",
            "\n",
            "***************************\n",
            "Trained on 6620 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 41.370186\n",
            "Loss training: 40.83601\n",
            "Loss training: 30.413307\n",
            "Loss training: 24.898293\n",
            "Loss training: 28.484888\n",
            "Loss training: 24.99526\n",
            "Loss training: 29.904465\n",
            "Loss training: 31.738832\n",
            "Loss training: 24.405087\n",
            "Loss training: 37.40822\n",
            "\n",
            "***************************\n",
            "Trained on 6630 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.389355\n",
            "Loss training: 24.646267\n",
            "Loss training: 30.517223\n",
            "Loss training: 20.47025\n",
            "Loss training: 28.474318\n",
            "Loss training: 31.994135\n",
            "Loss training: 20.210426\n",
            "Loss training: 25.020576\n",
            "Loss training: 29.932945\n",
            "Loss training: 20.158485\n",
            "\n",
            "***************************\n",
            "Trained on 6640 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.231926\n",
            "Loss training: 20.431335\n",
            "Loss training: 34.47735\n",
            "Loss training: 29.299292\n",
            "Loss training: 26.388767\n",
            "Loss training: 27.762268\n",
            "Loss training: 33.390556\n",
            "Loss training: 32.089073\n",
            "Loss training: 26.530352\n",
            "Loss training: 29.460817\n",
            "\n",
            "***************************\n",
            "Trained on 6650 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.64191\n",
            "Loss training: 33.532978\n",
            "Loss training: 31.726118\n",
            "Loss training: 12.157669\n",
            "Loss training: 31.041826\n",
            "Loss training: 24.377832\n",
            "Loss training: 29.07193\n",
            "Loss training: 30.599646\n",
            "Loss training: 27.074741\n",
            "Loss training: 33.3228\n",
            "\n",
            "***************************\n",
            "Trained on 6660 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.769024\n",
            "Loss training: 24.663645\n",
            "Loss training: 26.903017\n",
            "Loss training: 29.472311\n",
            "Loss training: 30.37723\n",
            "Loss training: 33.929886\n",
            "Loss training: 27.34521\n",
            "Loss training: 26.574505\n",
            "Loss training: 28.821377\n",
            "Loss training: 28.944572\n",
            "\n",
            "***************************\n",
            "Trained on 6670 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.556173\n",
            "Loss training: 26.266392\n",
            "Loss training: 20.274889\n",
            "Loss training: 26.398851\n",
            "Loss training: 30.273329\n",
            "Loss training: 41.330097\n",
            "Loss training: 30.01385\n",
            "Loss training: 36.079403\n",
            "Loss training: 29.476006\n",
            "Loss training: 33.324043\n",
            "\n",
            "***************************\n",
            "Trained on 6680 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.737345\n",
            "Loss training: 30.87458\n",
            "Loss training: 35.111454\n",
            "Loss training: 20.580673\n",
            "Loss training: 24.449146\n",
            "Loss training: 25.590002\n",
            "Loss training: 14.574715\n",
            "Loss training: 14.64502\n",
            "Loss training: 29.865917\n",
            "Loss training: 26.070047\n",
            "\n",
            "***************************\n",
            "Trained on 6690 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.639809\n",
            "Loss training: 33.44476\n",
            "Loss training: 26.933037\n",
            "Loss training: 27.692947\n",
            "Loss training: 30.005955\n",
            "Loss training: 37.1181\n",
            "Loss training: 24.499393\n",
            "Loss training: 15.114063\n",
            "Loss training: 33.421467\n",
            "Loss training: 14.578014\n",
            "\n",
            "***************************\n",
            "Trained on 6700 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.081278\n",
            "Loss training: 14.299986\n",
            "Loss training: 32.12495\n",
            "Loss training: 21.828098\n",
            "Loss training: 28.537146\n",
            "Loss training: 41.0064\n",
            "Loss training: 31.61453\n",
            "Loss training: 31.629\n",
            "Loss training: 36.027584\n",
            "Loss training: 35.72641\n",
            "\n",
            "***************************\n",
            "Trained on 6710 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.553806\n",
            "Loss training: 29.212692\n",
            "Loss training: 29.872013\n",
            "Loss training: 15.796699\n",
            "Loss training: 27.68069\n",
            "Loss training: 26.52455\n",
            "Loss training: 28.8593\n",
            "Loss training: 28.8683\n",
            "Loss training: 32.186302\n",
            "Loss training: 22.11793\n",
            "\n",
            "***************************\n",
            "Trained on 6720 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.485065\n",
            "Loss training: 20.80333\n",
            "Loss training: 31.11853\n",
            "Loss training: 29.368534\n",
            "Loss training: 30.648851\n",
            "Loss training: 30.118958\n",
            "Loss training: 32.30564\n",
            "Loss training: 41.073185\n",
            "Loss training: 33.526993\n",
            "Loss training: 26.3693\n",
            "\n",
            "***************************\n",
            "Trained on 6730 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.120995\n",
            "Loss training: 26.025509\n",
            "Loss training: 30.691021\n",
            "Loss training: 33.725727\n",
            "Loss training: 30.490509\n",
            "Loss training: 30.135515\n",
            "Loss training: 25.353699\n",
            "Loss training: 26.272652\n",
            "Loss training: 30.523136\n",
            "Loss training: 14.619847\n",
            "\n",
            "***************************\n",
            "Trained on 6740 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.223173\n",
            "Loss training: 36.514275\n",
            "Loss training: 24.82795\n",
            "Loss training: 29.720266\n",
            "Loss training: 34.416817\n",
            "Loss training: 22.192005\n",
            "Loss training: 33.142616\n",
            "Loss training: 25.76365\n",
            "Loss training: 21.43657\n",
            "Loss training: 25.328506\n",
            "\n",
            "***************************\n",
            "Trained on 6750 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.186356\n",
            "Loss training: 29.734947\n",
            "Loss training: 27.741465\n",
            "Loss training: 34.430893\n",
            "Loss training: 33.99104\n",
            "Loss training: 20.120314\n",
            "Loss training: 33.52853\n",
            "Loss training: 29.013348\n",
            "Loss training: 15.147998\n",
            "Loss training: 28.611689\n",
            "\n",
            "***************************\n",
            "Trained on 6760 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.38602\n",
            "Loss training: 14.156875\n",
            "Loss training: 33.95427\n",
            "Loss training: 35.53959\n",
            "Loss training: 26.439007\n",
            "Loss training: 26.355873\n",
            "Loss training: 27.029612\n",
            "Loss training: 32.481895\n",
            "Loss training: 29.179293\n",
            "Loss training: 24.76756\n",
            "\n",
            "***************************\n",
            "Trained on 6770 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.61031\n",
            "Loss training: 26.880163\n",
            "Loss training: 34.323887\n",
            "Loss training: 34.052353\n",
            "Loss training: 30.86337\n",
            "Loss training: 33.722843\n",
            "Loss training: 26.619783\n",
            "Loss training: 35.948383\n",
            "Loss training: 32.608467\n",
            "Loss training: 35.646015\n",
            "\n",
            "***************************\n",
            "Trained on 6780 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.530558\n",
            "Loss training: 31.64443\n",
            "Loss training: 34.627853\n",
            "Loss training: 34.68339\n",
            "Loss training: 30.328508\n",
            "Loss training: 57.983757\n",
            "Loss training: 34.013287\n",
            "Loss training: 31.33593\n",
            "Loss training: 20.59879\n",
            "Loss training: 25.021864\n",
            "\n",
            "***************************\n",
            "Trained on 6790 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.905615\n",
            "Loss training: 23.640919\n",
            "Loss training: 33.067955\n",
            "Loss training: 33.794\n",
            "Loss training: 46.632877\n",
            "Loss training: 38.093452\n",
            "Loss training: 15.803682\n",
            "Loss training: 33.28418\n",
            "Loss training: 35.055634\n",
            "Loss training: 37.834457\n",
            "\n",
            "***************************\n",
            "Trained on 6800 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.73756\n",
            "Loss training: 21.039259\n",
            "Loss training: 37.01799\n",
            "Loss training: 27.313925\n",
            "Loss training: 33.759983\n",
            "Loss training: 41.6087\n",
            "Loss training: 28.758324\n",
            "Loss training: 27.182589\n",
            "Loss training: 25.855019\n",
            "Loss training: 40.813313\n",
            "\n",
            "***************************\n",
            "Trained on 6810 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 43.547993\n",
            "Loss training: 16.803816\n",
            "Loss training: 28.787172\n",
            "Loss training: 34.896774\n",
            "Loss training: 39.027664\n",
            "Loss training: 35.885983\n",
            "Loss training: 21.472532\n",
            "Loss training: 25.488499\n",
            "Loss training: 27.65293\n",
            "Loss training: 13.500873\n",
            "\n",
            "***************************\n",
            "Trained on 6820 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.250116\n",
            "Loss training: 27.729134\n",
            "Loss training: 12.651859\n",
            "Loss training: 40.883987\n",
            "Loss training: 39.12318\n",
            "Loss training: 25.003414\n",
            "Loss training: 31.46223\n",
            "Loss training: 25.516766\n",
            "Loss training: 37.72775\n",
            "Loss training: 41.635036\n",
            "\n",
            "***************************\n",
            "Trained on 6830 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.787376\n",
            "Loss training: 30.432583\n",
            "Loss training: 34.185894\n",
            "Loss training: 41.648968\n",
            "Loss training: 41.10883\n",
            "Loss training: 26.858362\n",
            "Loss training: 33.149246\n",
            "Loss training: 21.11365\n",
            "Loss training: 37.887882\n",
            "Loss training: 37.256035\n",
            "\n",
            "***************************\n",
            "Trained on 6840 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.253004\n",
            "Loss training: 32.966953\n",
            "Loss training: 12.248619\n",
            "Loss training: 31.926622\n",
            "Loss training: 27.919836\n",
            "Loss training: 26.578728\n",
            "Loss training: 21.487259\n",
            "Loss training: 12.688066\n",
            "Loss training: 33.376015\n",
            "Loss training: 39.840057\n",
            "\n",
            "***************************\n",
            "Trained on 6850 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 17.047226\n",
            "Loss training: 26.757708\n",
            "Loss training: 28.035875\n",
            "Loss training: 28.3451\n",
            "Loss training: 26.046347\n",
            "Loss training: 38.157295\n",
            "Loss training: 34.489944\n",
            "Loss training: 36.600437\n",
            "Loss training: 25.27054\n",
            "Loss training: 23.448362\n",
            "\n",
            "***************************\n",
            "Trained on 6860 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.880169\n",
            "Loss training: 26.80297\n",
            "Loss training: 26.695604\n",
            "Loss training: 36.99918\n",
            "Loss training: 38.99633\n",
            "Loss training: 36.14674\n",
            "Loss training: 31.102907\n",
            "Loss training: 31.507011\n",
            "Loss training: 38.329838\n",
            "Loss training: 22.476372\n",
            "\n",
            "***************************\n",
            "Trained on 6870 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.038965\n",
            "Loss training: 25.180315\n",
            "Loss training: 32.58962\n",
            "Loss training: 24.920162\n",
            "Loss training: 30.79232\n",
            "Loss training: 41.87356\n",
            "Loss training: 28.02121\n",
            "Loss training: 27.012226\n",
            "Loss training: 16.770437\n",
            "Loss training: 36.60033\n",
            "\n",
            "***************************\n",
            "Trained on 6880 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.304388\n",
            "Loss training: 34.532047\n",
            "Loss training: 36.16492\n",
            "Loss training: 28.33005\n",
            "Loss training: 28.003355\n",
            "Loss training: 32.270294\n",
            "Loss training: 35.617855\n",
            "Loss training: 34.89082\n",
            "Loss training: 12.074062\n",
            "Loss training: 28.29037\n",
            "\n",
            "***************************\n",
            "Trained on 6890 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.888084\n",
            "Loss training: 30.623457\n",
            "Loss training: 34.708927\n",
            "Loss training: 35.91666\n",
            "Loss training: 30.85164\n",
            "Loss training: 26.832062\n",
            "Loss training: 27.854275\n",
            "Loss training: 35.22876\n",
            "Loss training: 27.571167\n",
            "Loss training: 12.225693\n",
            "\n",
            "***************************\n",
            "Trained on 6900 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.64112\n",
            "Loss training: 26.872168\n",
            "Loss training: 27.830002\n",
            "Loss training: 26.649954\n",
            "Loss training: 30.768642\n",
            "Loss training: 34.39329\n",
            "Loss training: 34.00526\n",
            "Loss training: 33.730263\n",
            "Loss training: 33.423656\n",
            "Loss training: 34.637794\n",
            "\n",
            "***************************\n",
            "Trained on 6910 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.496744\n",
            "Loss training: 22.578543\n",
            "Loss training: 31.212395\n",
            "Loss training: 35.30528\n",
            "Loss training: 28.705496\n",
            "Loss training: 32.970848\n",
            "Loss training: 34.99857\n",
            "Loss training: 12.024904\n",
            "Loss training: 32.555393\n",
            "Loss training: 34.28935\n",
            "\n",
            "***************************\n",
            "Trained on 6920 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.06958\n",
            "Loss training: 28.630306\n",
            "Loss training: 29.604727\n",
            "Loss training: 24.983574\n",
            "Loss training: 31.5494\n",
            "Loss training: 24.91009\n",
            "Loss training: 34.67318\n",
            "Loss training: 20.766403\n",
            "Loss training: 28.591288\n",
            "Loss training: 34.344418\n",
            "\n",
            "***************************\n",
            "Trained on 6930 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 20.552044\n",
            "Loss training: 32.177296\n",
            "Loss training: 27.871237\n",
            "Loss training: 33.98956\n",
            "Loss training: 22.094212\n",
            "Loss training: 34.746178\n",
            "Loss training: 34.017315\n",
            "Loss training: 33.965168\n",
            "Loss training: 36.38257\n",
            "Loss training: 29.517342\n",
            "\n",
            "***************************\n",
            "Trained on 6940 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.957275\n",
            "Loss training: 27.516207\n",
            "Loss training: 20.18117\n",
            "Loss training: 28.867748\n",
            "Loss training: 34.731754\n",
            "Loss training: 11.919355\n",
            "Loss training: 30.696224\n",
            "Loss training: 31.667606\n",
            "Loss training: 27.671469\n",
            "Loss training: 32.128002\n",
            "\n",
            "***************************\n",
            "Trained on 6950 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.442917\n",
            "Loss training: 20.232422\n",
            "Loss training: 39.997875\n",
            "Loss training: 26.582867\n",
            "Loss training: 28.316687\n",
            "Loss training: 20.266115\n",
            "Loss training: 30.218441\n",
            "Loss training: 27.593414\n",
            "Loss training: 34.47959\n",
            "Loss training: 19.835247\n",
            "\n",
            "***************************\n",
            "Trained on 6960 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.95176\n",
            "Loss training: 14.927434\n",
            "Loss training: 34.280434\n",
            "Loss training: 24.995325\n",
            "Loss training: 24.825497\n",
            "Loss training: 29.775665\n",
            "Loss training: 20.396645\n",
            "Loss training: 27.730946\n",
            "Loss training: 31.097439\n",
            "Loss training: 33.32627\n",
            "\n",
            "***************************\n",
            "Trained on 6970 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 12.213924\n",
            "Loss training: 20.389807\n",
            "Loss training: 37.403107\n",
            "Loss training: 31.097195\n",
            "Loss training: 30.12953\n",
            "Loss training: 31.925188\n",
            "Loss training: 30.794535\n",
            "Loss training: 20.274607\n",
            "Loss training: 34.4419\n",
            "Loss training: 30.867805\n",
            "\n",
            "***************************\n",
            "Trained on 6980 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.71441\n",
            "Loss training: 26.580776\n",
            "Loss training: 24.988712\n",
            "Loss training: 26.464376\n",
            "Loss training: 30.17636\n",
            "Loss training: 24.61059\n",
            "Loss training: 35.64501\n",
            "Loss training: 34.34113\n",
            "Loss training: 35.11517\n",
            "Loss training: 29.651127\n",
            "\n",
            "***************************\n",
            "Trained on 6990 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.187073\n",
            "Loss training: 24.436485\n",
            "Loss training: 29.80067\n",
            "Loss training: 20.559612\n",
            "Loss training: 27.677855\n",
            "Loss training: 11.815571\n",
            "Loss training: 29.922411\n",
            "Loss training: 33.433666\n",
            "Loss training: 24.337473\n",
            "Loss training: 26.619242\n",
            "\n",
            "***************************\n",
            "Trained on 7000 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.575548\n",
            "Loss training: 33.65088\n",
            "Loss training: 29.768848\n",
            "Loss training: 29.575191\n",
            "Loss training: 30.425228\n",
            "Loss training: 32.198112\n",
            "Loss training: 33.179314\n",
            "Loss training: 14.761996\n",
            "Loss training: 14.5777235\n",
            "Loss training: 29.744923\n",
            "\n",
            "***************************\n",
            "Trained on 7010 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.03604\n",
            "Loss training: 24.65957\n",
            "Loss training: 27.223066\n",
            "Loss training: 29.103144\n",
            "Loss training: 29.70836\n",
            "Loss training: 25.087729\n",
            "Loss training: 26.868914\n",
            "Loss training: 11.8302765\n",
            "Loss training: 14.850374\n",
            "Loss training: 24.3881\n",
            "\n",
            "***************************\n",
            "Trained on 7020 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.039776\n",
            "Loss training: 28.284996\n",
            "Loss training: 26.779722\n",
            "Loss training: 26.07124\n",
            "Loss training: 24.462553\n",
            "Loss training: 26.37927\n",
            "Loss training: 29.776928\n",
            "Loss training: 26.430157\n",
            "Loss training: 28.90987\n",
            "Loss training: 24.271988\n",
            "\n",
            "***************************\n",
            "Trained on 7030 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.27802\n",
            "Loss training: 14.444035\n",
            "Loss training: 36.161198\n",
            "Loss training: 28.54553\n",
            "Loss training: 34.027317\n",
            "Loss training: 29.013844\n",
            "Loss training: 41.09944\n",
            "Loss training: 35.80283\n",
            "Loss training: 24.168356\n",
            "Loss training: 28.761429\n",
            "\n",
            "***************************\n",
            "Trained on 7040 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.823406\n",
            "Loss training: 26.798214\n",
            "Loss training: 21.78689\n",
            "Loss training: 29.123068\n",
            "Loss training: 32.444134\n",
            "Loss training: 26.633337\n",
            "Loss training: 29.412653\n",
            "Loss training: 29.359335\n",
            "Loss training: 28.511808\n",
            "Loss training: 19.753353\n",
            "\n",
            "***************************\n",
            "Trained on 7050 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.811806\n",
            "Loss training: 28.853159\n",
            "Loss training: 28.93672\n",
            "Loss training: 28.99208\n",
            "Loss training: 32.95055\n",
            "Loss training: 26.475836\n",
            "Loss training: 32.816242\n",
            "Loss training: 25.278624\n",
            "Loss training: 28.806515\n",
            "Loss training: 11.737299\n",
            "\n",
            "***************************\n",
            "Trained on 7060 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.33957\n",
            "Loss training: 28.320755\n",
            "Loss training: 19.562243\n",
            "Loss training: 40.465725\n",
            "Loss training: 35.746304\n",
            "Loss training: 28.146294\n",
            "Loss training: 29.265764\n",
            "Loss training: 28.385685\n",
            "Loss training: 24.263353\n",
            "Loss training: 30.234638\n",
            "\n",
            "***************************\n",
            "Trained on 7070 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.634346\n",
            "Loss training: 24.134151\n",
            "Loss training: 28.004078\n",
            "Loss training: 25.297737\n",
            "Loss training: 27.182007\n",
            "Loss training: 35.738293\n",
            "Loss training: 32.019802\n",
            "Loss training: 35.566536\n",
            "Loss training: 26.674158\n",
            "Loss training: 11.65136\n",
            "\n",
            "***************************\n",
            "Trained on 7080 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 19.819288\n",
            "Loss training: 29.576889\n",
            "Loss training: 40.02945\n",
            "Loss training: 24.87376\n",
            "Loss training: 28.228224\n",
            "Loss training: 28.043423\n",
            "Loss training: 28.60691\n",
            "Loss training: 39.767006\n",
            "Loss training: 11.727251\n",
            "Loss training: 32.75591\n",
            "\n",
            "***************************\n",
            "Trained on 7090 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.089392\n",
            "Loss training: 24.692808\n",
            "Loss training: 28.879538\n",
            "Loss training: 32.12097\n",
            "Loss training: 27.895699\n",
            "Loss training: 24.495718\n",
            "Loss training: 28.365683\n",
            "Loss training: 32.625267\n",
            "Loss training: 28.126438\n",
            "Loss training: 21.63316\n",
            "\n",
            "***************************\n",
            "Trained on 7100 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.307896\n",
            "Loss training: 28.086294\n",
            "Loss training: 27.248327\n",
            "Loss training: 26.558613\n",
            "Loss training: 32.44425\n",
            "Loss training: 27.553179\n",
            "Loss training: 28.636545\n",
            "Loss training: 28.20668\n",
            "Loss training: 31.245064\n",
            "Loss training: 26.15891\n",
            "\n",
            "***************************\n",
            "Trained on 7110 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.747547\n",
            "Loss training: 30.359879\n",
            "Loss training: 26.036474\n",
            "Loss training: 30.10998\n",
            "Loss training: 24.775148\n",
            "Loss training: 29.978468\n",
            "Loss training: 33.025845\n",
            "Loss training: 24.90463\n",
            "Loss training: 24.334677\n",
            "Loss training: 14.63232\n",
            "\n",
            "***************************\n",
            "Trained on 7120 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 14.616954\n",
            "Loss training: 27.935555\n",
            "Loss training: 28.037203\n",
            "Loss training: 20.264545\n",
            "Loss training: 26.260405\n",
            "Loss training: 29.473452\n",
            "Loss training: 33.879246\n",
            "Loss training: 28.606459\n",
            "Loss training: 28.07704\n",
            "Loss training: 29.3837\n",
            "\n",
            "***************************\n",
            "Trained on 7130 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.50418\n",
            "Loss training: 24.005379\n",
            "Loss training: 39.767914\n",
            "Loss training: 26.30507\n",
            "Loss training: 28.718262\n",
            "Loss training: 11.676837\n",
            "Loss training: 28.270046\n",
            "Loss training: 15.312635\n",
            "Loss training: 31.912968\n",
            "Loss training: 29.147972\n",
            "\n",
            "***************************\n",
            "Trained on 7140 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 19.735186\n",
            "Loss training: 27.934315\n",
            "Loss training: 27.167799\n",
            "Loss training: 14.29039\n",
            "Loss training: 31.968004\n",
            "Loss training: 26.32991\n",
            "Loss training: 32.17449\n",
            "Loss training: 14.103027\n",
            "Loss training: 28.350845\n",
            "Loss training: 23.934402\n",
            "\n",
            "***************************\n",
            "Trained on 7150 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.354656\n",
            "Loss training: 28.444727\n",
            "Loss training: 33.11937\n",
            "Loss training: 13.987556\n",
            "Loss training: 19.493935\n",
            "Loss training: 33.3979\n",
            "Loss training: 24.372232\n",
            "Loss training: 33.62643\n",
            "Loss training: 26.464993\n",
            "Loss training: 19.913582\n",
            "\n",
            "***************************\n",
            "Trained on 7160 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.306913\n",
            "Loss training: 39.91296\n",
            "Loss training: 32.262817\n",
            "Loss training: 28.736288\n",
            "Loss training: 28.511694\n",
            "Loss training: 24.866247\n",
            "Loss training: 27.262321\n",
            "Loss training: 27.768845\n",
            "Loss training: 19.602835\n",
            "Loss training: 12.263353\n",
            "\n",
            "***************************\n",
            "Trained on 7170 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.329803\n",
            "Loss training: 31.8158\n",
            "Loss training: 34.09485\n",
            "Loss training: 28.048515\n",
            "Loss training: 32.24543\n",
            "Loss training: 36.40247\n",
            "Loss training: 24.338703\n",
            "Loss training: 35.524483\n",
            "Loss training: 19.777641\n",
            "Loss training: 26.638138\n",
            "\n",
            "***************************\n",
            "Trained on 7180 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.967558\n",
            "Loss training: 27.798754\n",
            "Loss training: 20.073812\n",
            "Loss training: 16.816217\n",
            "Loss training: 27.69871\n",
            "Loss training: 32.331314\n",
            "Loss training: 31.154041\n",
            "Loss training: 31.57318\n",
            "Loss training: 27.636658\n",
            "Loss training: 26.184353\n",
            "\n",
            "***************************\n",
            "Trained on 7190 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.651276\n",
            "Loss training: 28.617245\n",
            "Loss training: 26.482067\n",
            "Loss training: 29.574087\n",
            "Loss training: 28.212648\n",
            "Loss training: 33.4132\n",
            "Loss training: 27.002472\n",
            "Loss training: 27.053621\n",
            "Loss training: 34.36219\n",
            "Loss training: 27.06666\n",
            "\n",
            "***************************\n",
            "Trained on 7200 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.19192\n",
            "Loss training: 36.471226\n",
            "Loss training: 29.713226\n",
            "Loss training: 25.99205\n",
            "Loss training: 11.787397\n",
            "Loss training: 25.472864\n",
            "Loss training: 28.359842\n",
            "Loss training: 24.414333\n",
            "Loss training: 30.387499\n",
            "Loss training: 32.699276\n",
            "\n",
            "***************************\n",
            "Trained on 7210 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.120613\n",
            "Loss training: 33.50255\n",
            "Loss training: 33.88657\n",
            "Loss training: 31.496778\n",
            "Loss training: 28.790981\n",
            "Loss training: 31.357653\n",
            "Loss training: 27.415646\n",
            "Loss training: 26.422428\n",
            "Loss training: 26.18809\n",
            "Loss training: 24.088282\n",
            "\n",
            "***************************\n",
            "Trained on 7220 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.854233\n",
            "Loss training: 30.346863\n",
            "Loss training: 25.086637\n",
            "Loss training: 11.621886\n",
            "Loss training: 25.87955\n",
            "Loss training: 30.07707\n",
            "Loss training: 33.695312\n",
            "Loss training: 19.51768\n",
            "Loss training: 14.1546545\n",
            "Loss training: 28.408062\n",
            "\n",
            "***************************\n",
            "Trained on 7230 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.515913\n",
            "Loss training: 24.852633\n",
            "Loss training: 39.761593\n",
            "Loss training: 23.853525\n",
            "Loss training: 32.937496\n",
            "Loss training: 31.946753\n",
            "Loss training: 29.134113\n",
            "Loss training: 27.369986\n",
            "Loss training: 25.713272\n",
            "Loss training: 31.32588\n",
            "\n",
            "***************************\n",
            "Trained on 7240 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.550413\n",
            "Loss training: 31.209042\n",
            "Loss training: 23.937489\n",
            "Loss training: 26.608162\n",
            "Loss training: 29.766287\n",
            "Loss training: 28.39018\n",
            "Loss training: 27.081545\n",
            "Loss training: 26.452496\n",
            "Loss training: 35.608818\n",
            "Loss training: 24.677658\n",
            "\n",
            "***************************\n",
            "Trained on 7250 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.306591\n",
            "Loss training: 29.021368\n",
            "Loss training: 36.464718\n",
            "Loss training: 14.32191\n",
            "Loss training: 33.112175\n",
            "Loss training: 35.899956\n",
            "Loss training: 32.147247\n",
            "Loss training: 28.629564\n",
            "Loss training: 25.276548\n",
            "Loss training: 24.001373\n",
            "\n",
            "***************************\n",
            "Trained on 7260 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.325214\n",
            "Loss training: 32.253265\n",
            "Loss training: 26.436016\n",
            "Loss training: 27.421242\n",
            "Loss training: 14.074587\n",
            "Loss training: 24.731981\n",
            "Loss training: 33.131355\n",
            "Loss training: 29.420547\n",
            "Loss training: 28.84957\n",
            "Loss training: 21.886871\n",
            "\n",
            "***************************\n",
            "Trained on 7270 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.163553\n",
            "Loss training: 33.211414\n",
            "Loss training: 32.907616\n",
            "Loss training: 31.072212\n",
            "Loss training: 32.774494\n",
            "Loss training: 34.036724\n",
            "Loss training: 29.583208\n",
            "Loss training: 35.948772\n",
            "Loss training: 20.072939\n",
            "Loss training: 33.95261\n",
            "\n",
            "***************************\n",
            "Trained on 7280 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 11.726702\n",
            "Loss training: 27.68858\n",
            "Loss training: 19.909607\n",
            "Loss training: 28.315973\n",
            "Loss training: 32.15247\n",
            "Loss training: 35.302723\n",
            "Loss training: 33.131996\n",
            "Loss training: 25.453253\n",
            "Loss training: 26.53462\n",
            "Loss training: 20.241734\n",
            "\n",
            "***************************\n",
            "Trained on 7290 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.721966\n",
            "Loss training: 29.370354\n",
            "Loss training: 28.806574\n",
            "Loss training: 13.868765\n",
            "Loss training: 29.86846\n",
            "Loss training: 35.301136\n",
            "Loss training: 32.30263\n",
            "Loss training: 33.699852\n",
            "Loss training: 22.741415\n",
            "Loss training: 33.343147\n",
            "\n",
            "***************************\n",
            "Trained on 7300 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.965136\n",
            "Loss training: 28.935608\n",
            "Loss training: 14.755585\n",
            "Loss training: 28.14341\n",
            "Loss training: 35.10182\n",
            "Loss training: 21.682365\n",
            "Loss training: 31.911076\n",
            "Loss training: 30.663506\n",
            "Loss training: 32.72659\n",
            "Loss training: 26.893269\n",
            "\n",
            "***************************\n",
            "Trained on 7310 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.400465\n",
            "Loss training: 25.247824\n",
            "Loss training: 25.03119\n",
            "Loss training: 12.873566\n",
            "Loss training: 30.902397\n",
            "Loss training: 32.276257\n",
            "Loss training: 41.79419\n",
            "Loss training: 29.625374\n",
            "Loss training: 30.135496\n",
            "Loss training: 35.742718\n",
            "\n",
            "***************************\n",
            "Trained on 7320 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 20.672356\n",
            "Loss training: 27.920162\n",
            "Loss training: 29.12023\n",
            "Loss training: 31.844713\n",
            "Loss training: 39.180176\n",
            "Loss training: 30.198494\n",
            "Loss training: 26.29989\n",
            "Loss training: 26.891026\n",
            "Loss training: 27.89087\n",
            "Loss training: 28.343506\n",
            "\n",
            "***************************\n",
            "Trained on 7330 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 21.790575\n",
            "Loss training: 27.322607\n",
            "Loss training: 25.604078\n",
            "Loss training: 31.882303\n",
            "Loss training: 36.939377\n",
            "Loss training: 14.662457\n",
            "Loss training: 29.819387\n",
            "Loss training: 29.018454\n",
            "Loss training: 35.09841\n",
            "Loss training: 29.473341\n",
            "\n",
            "***************************\n",
            "Trained on 7340 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.918907\n",
            "Loss training: 29.749977\n",
            "Loss training: 25.452349\n",
            "Loss training: 25.016813\n",
            "Loss training: 24.512735\n",
            "Loss training: 24.28577\n",
            "Loss training: 35.926167\n",
            "Loss training: 26.33642\n",
            "Loss training: 19.937584\n",
            "Loss training: 33.42552\n",
            "\n",
            "***************************\n",
            "Trained on 7350 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.447369\n",
            "Loss training: 19.577204\n",
            "Loss training: 28.240183\n",
            "Loss training: 28.833738\n",
            "Loss training: 35.554253\n",
            "Loss training: 27.792362\n",
            "Loss training: 26.924488\n",
            "Loss training: 25.102049\n",
            "Loss training: 26.6813\n",
            "Loss training: 28.6216\n",
            "\n",
            "***************************\n",
            "Trained on 7360 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.803156\n",
            "Loss training: 35.94263\n",
            "Loss training: 29.337954\n",
            "Loss training: 40.137108\n",
            "Loss training: 35.006626\n",
            "Loss training: 27.7104\n",
            "Loss training: 34.71551\n",
            "Loss training: 40.042465\n",
            "Loss training: 28.080408\n",
            "Loss training: 26.587809\n",
            "\n",
            "***************************\n",
            "Trained on 7370 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.879383\n",
            "Loss training: 30.584427\n",
            "Loss training: 19.194315\n",
            "Loss training: 32.716305\n",
            "Loss training: 34.073364\n",
            "Loss training: 13.973766\n",
            "Loss training: 31.509546\n",
            "Loss training: 33.485638\n",
            "Loss training: 36.15781\n",
            "Loss training: 29.507277\n",
            "\n",
            "***************************\n",
            "Trained on 7380 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 40.724476\n",
            "Loss training: 27.79112\n",
            "Loss training: 21.920094\n",
            "Loss training: 26.014185\n",
            "Loss training: 25.659338\n",
            "Loss training: 31.475706\n",
            "Loss training: 29.961168\n",
            "Loss training: 30.388615\n",
            "Loss training: 12.126444\n",
            "Loss training: 35.915432\n",
            "\n",
            "***************************\n",
            "Trained on 7390 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.701283\n",
            "Loss training: 21.859411\n",
            "Loss training: 35.284306\n",
            "Loss training: 11.897573\n",
            "Loss training: 29.315784\n",
            "Loss training: 11.852259\n",
            "Loss training: 14.052256\n",
            "Loss training: 13.855938\n",
            "Loss training: 37.37127\n",
            "Loss training: 34.039745\n",
            "\n",
            "***************************\n",
            "Trained on 7400 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.865305\n",
            "Loss training: 33.556377\n",
            "Loss training: 20.145586\n",
            "Loss training: 25.961437\n",
            "Loss training: 35.029278\n",
            "Loss training: 26.14139\n",
            "Loss training: 29.206556\n",
            "Loss training: 34.115562\n",
            "Loss training: 28.927755\n",
            "Loss training: 24.914154\n",
            "\n",
            "***************************\n",
            "Trained on 7410 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.578377\n",
            "Loss training: 26.862894\n",
            "Loss training: 27.981968\n",
            "Loss training: 24.611645\n",
            "Loss training: 28.552353\n",
            "Loss training: 24.503963\n",
            "Loss training: 31.732307\n",
            "Loss training: 24.712488\n",
            "Loss training: 32.337116\n",
            "Loss training: 34.409935\n",
            "\n",
            "***************************\n",
            "Trained on 7420 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.014706\n",
            "Loss training: 24.50195\n",
            "Loss training: 30.845732\n",
            "Loss training: 24.329092\n",
            "Loss training: 21.313332\n",
            "Loss training: 30.09493\n",
            "Loss training: 28.866262\n",
            "Loss training: 26.44558\n",
            "Loss training: 28.139002\n",
            "Loss training: 28.189524\n",
            "\n",
            "***************************\n",
            "Trained on 7430 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.626034\n",
            "Loss training: 28.089588\n",
            "Loss training: 12.039419\n",
            "Loss training: 24.527607\n",
            "Loss training: 30.432768\n",
            "Loss training: 27.85822\n",
            "Loss training: 28.940128\n",
            "Loss training: 35.324127\n",
            "Loss training: 34.02352\n",
            "Loss training: 27.039032\n",
            "\n",
            "***************************\n",
            "Trained on 7440 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.379543\n",
            "Loss training: 33.368973\n",
            "Loss training: 26.509521\n",
            "Loss training: 29.062414\n",
            "Loss training: 25.934263\n",
            "Loss training: 20.663612\n",
            "Loss training: 32.961952\n",
            "Loss training: 33.935658\n",
            "Loss training: 33.349506\n",
            "Loss training: 32.46381\n",
            "\n",
            "***************************\n",
            "Trained on 7450 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.002684\n",
            "Loss training: 26.242855\n",
            "Loss training: 24.658627\n",
            "Loss training: 28.9466\n",
            "Loss training: 26.627218\n",
            "Loss training: 30.03419\n",
            "Loss training: 27.013786\n",
            "Loss training: 40.502693\n",
            "Loss training: 32.935932\n",
            "Loss training: 30.711563\n",
            "\n",
            "***************************\n",
            "Trained on 7460 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.245838\n",
            "Loss training: 35.87236\n",
            "Loss training: 30.243183\n",
            "Loss training: 28.563948\n",
            "Loss training: 32.6305\n",
            "Loss training: 32.540855\n",
            "Loss training: 29.380535\n",
            "Loss training: 35.899124\n",
            "Loss training: 28.93736\n",
            "Loss training: 28.784138\n",
            "\n",
            "***************************\n",
            "Trained on 7470 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.777042\n",
            "Loss training: 28.221262\n",
            "Loss training: 34.695805\n",
            "Loss training: 30.660686\n",
            "Loss training: 14.692676\n",
            "Loss training: 34.181232\n",
            "Loss training: 26.378662\n",
            "Loss training: 29.64192\n",
            "Loss training: 27.275873\n",
            "Loss training: 26.995111\n",
            "\n",
            "***************************\n",
            "Trained on 7480 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.426308\n",
            "Loss training: 33.97322\n",
            "Loss training: 34.24784\n",
            "Loss training: 33.046383\n",
            "Loss training: 32.889637\n",
            "Loss training: 27.612337\n",
            "Loss training: 25.048555\n",
            "Loss training: 27.79744\n",
            "Loss training: 30.371683\n",
            "Loss training: 14.785088\n",
            "\n",
            "***************************\n",
            "Trained on 7490 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.603382\n",
            "Loss training: 13.752783\n",
            "Loss training: 24.095278\n",
            "Loss training: 26.300787\n",
            "Loss training: 29.378113\n",
            "Loss training: 28.823025\n",
            "Loss training: 27.635023\n",
            "Loss training: 40.16404\n",
            "Loss training: 28.284655\n",
            "Loss training: 28.612839\n",
            "\n",
            "***************************\n",
            "Trained on 7500 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.259377\n",
            "Loss training: 15.338827\n",
            "Loss training: 29.543148\n",
            "Loss training: 29.654472\n",
            "Loss training: 23.911053\n",
            "Loss training: 26.221268\n",
            "Loss training: 30.982542\n",
            "Loss training: 35.69864\n",
            "Loss training: 33.067646\n",
            "Loss training: 32.71331\n",
            "\n",
            "***************************\n",
            "Trained on 7510 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.71341\n",
            "Loss training: 35.20437\n",
            "Loss training: 14.75762\n",
            "Loss training: 26.109495\n",
            "Loss training: 33.23742\n",
            "Loss training: 23.717945\n",
            "Loss training: 32.729202\n",
            "Loss training: 35.18256\n",
            "Loss training: 20.184626\n",
            "Loss training: 14.311207\n",
            "\n",
            "***************************\n",
            "Trained on 7520 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.76132\n",
            "Loss training: 27.4261\n",
            "Loss training: 31.252354\n",
            "Loss training: 32.44703\n",
            "Loss training: 29.000124\n",
            "Loss training: 28.13479\n",
            "Loss training: 28.797716\n",
            "Loss training: 27.813396\n",
            "Loss training: 31.160128\n",
            "Loss training: 28.753027\n",
            "\n",
            "***************************\n",
            "Trained on 7530 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 13.746693\n",
            "Loss training: 34.20008\n",
            "Loss training: 32.96938\n",
            "Loss training: 27.850893\n",
            "Loss training: 23.483519\n",
            "Loss training: 28.65925\n",
            "Loss training: 29.73384\n",
            "Loss training: 29.17766\n",
            "Loss training: 29.811272\n",
            "Loss training: 29.717316\n",
            "\n",
            "***************************\n",
            "Trained on 7540 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.6398\n",
            "Loss training: 32.530876\n",
            "Loss training: 28.37403\n",
            "Loss training: 13.639349\n",
            "Loss training: 34.46547\n",
            "Loss training: 33.044167\n",
            "Loss training: 27.043856\n",
            "Loss training: 34.79943\n",
            "Loss training: 34.62954\n",
            "Loss training: 30.629732\n",
            "\n",
            "***************************\n",
            "Trained on 7550 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 13.493859\n",
            "Loss training: 22.042524\n",
            "Loss training: 26.421762\n",
            "Loss training: 29.696318\n",
            "Loss training: 30.742434\n",
            "Loss training: 33.70402\n",
            "Loss training: 40.406353\n",
            "Loss training: 27.683329\n",
            "Loss training: 28.72374\n",
            "Loss training: 28.00041\n",
            "\n",
            "***************************\n",
            "Trained on 7560 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.075548\n",
            "Loss training: 34.178623\n",
            "Loss training: 26.319613\n",
            "Loss training: 33.761665\n",
            "Loss training: 27.923788\n",
            "Loss training: 24.40063\n",
            "Loss training: 31.167458\n",
            "Loss training: 33.99609\n",
            "Loss training: 14.352463\n",
            "Loss training: 26.016312\n",
            "\n",
            "***************************\n",
            "Trained on 7570 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.464537\n",
            "Loss training: 19.99387\n",
            "Loss training: 27.828867\n",
            "Loss training: 25.412664\n",
            "Loss training: 33.34448\n",
            "Loss training: 30.712044\n",
            "Loss training: 28.299309\n",
            "Loss training: 26.337038\n",
            "Loss training: 26.840355\n",
            "Loss training: 33.271362\n",
            "\n",
            "***************************\n",
            "Trained on 7580 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.746494\n",
            "Loss training: 27.506168\n",
            "Loss training: 29.758808\n",
            "Loss training: 32.022476\n",
            "Loss training: 33.138298\n",
            "Loss training: 33.33482\n",
            "Loss training: 32.98588\n",
            "Loss training: 35.190804\n",
            "Loss training: 40.36033\n",
            "Loss training: 28.360865\n",
            "\n",
            "***************************\n",
            "Trained on 7590 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.742176\n",
            "Loss training: 33.094833\n",
            "Loss training: 30.435875\n",
            "Loss training: 23.78275\n",
            "Loss training: 32.8718\n",
            "Loss training: 11.903284\n",
            "Loss training: 27.077736\n",
            "Loss training: 11.73484\n",
            "Loss training: 31.178934\n",
            "Loss training: 27.065681\n",
            "\n",
            "***************************\n",
            "Trained on 7600 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.658659\n",
            "Loss training: 23.573572\n",
            "Loss training: 19.91523\n",
            "Loss training: 31.165415\n",
            "Loss training: 27.738138\n",
            "Loss training: 29.398472\n",
            "Loss training: 28.060032\n",
            "Loss training: 27.714958\n",
            "Loss training: 33.163837\n",
            "Loss training: 32.62583\n",
            "\n",
            "***************************\n",
            "Trained on 7610 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.54219\n",
            "Loss training: 27.943132\n",
            "Loss training: 22.370916\n",
            "Loss training: 29.27851\n",
            "Loss training: 26.654625\n",
            "Loss training: 29.227495\n",
            "Loss training: 27.495703\n",
            "Loss training: 13.456096\n",
            "Loss training: 29.169054\n",
            "Loss training: 26.803682\n",
            "\n",
            "***************************\n",
            "Trained on 7620 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 13.107573\n",
            "Loss training: 31.043787\n",
            "Loss training: 33.209415\n",
            "Loss training: 24.966587\n",
            "Loss training: 19.434443\n",
            "Loss training: 28.902292\n",
            "Loss training: 21.62767\n",
            "Loss training: 20.038027\n",
            "Loss training: 19.477165\n",
            "Loss training: 23.748453\n",
            "\n",
            "***************************\n",
            "Trained on 7630 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.747065\n",
            "Loss training: 39.075737\n",
            "Loss training: 14.228591\n",
            "Loss training: 11.705063\n",
            "Loss training: 33.62667\n",
            "Loss training: 13.595023\n",
            "Loss training: 35.600613\n",
            "Loss training: 27.234459\n",
            "Loss training: 13.022681\n",
            "Loss training: 31.545225\n",
            "\n",
            "***************************\n",
            "Trained on 7640 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.026552\n",
            "Loss training: 19.386507\n",
            "Loss training: 24.91185\n",
            "Loss training: 27.858347\n",
            "Loss training: 19.1962\n",
            "Loss training: 34.203625\n",
            "Loss training: 26.606028\n",
            "Loss training: 28.700874\n",
            "Loss training: 27.126904\n",
            "Loss training: 26.89573\n",
            "\n",
            "***************************\n",
            "Trained on 7650 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.048176\n",
            "Loss training: 33.35022\n",
            "Loss training: 28.579596\n",
            "Loss training: 36.280525\n",
            "Loss training: 26.5982\n",
            "Loss training: 31.415321\n",
            "Loss training: 27.9712\n",
            "Loss training: 24.950325\n",
            "Loss training: 21.387638\n",
            "Loss training: 26.798834\n",
            "\n",
            "***************************\n",
            "Trained on 7660 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.848595\n",
            "Loss training: 14.524103\n",
            "Loss training: 14.197781\n",
            "Loss training: 28.800686\n",
            "Loss training: 31.903751\n",
            "Loss training: 26.623894\n",
            "Loss training: 33.99323\n",
            "Loss training: 31.249193\n",
            "Loss training: 14.649217\n",
            "Loss training: 30.853268\n",
            "\n",
            "***************************\n",
            "Trained on 7670 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.505009\n",
            "Loss training: 30.738499\n",
            "Loss training: 28.367228\n",
            "Loss training: 20.65041\n",
            "Loss training: 27.403475\n",
            "Loss training: 28.814518\n",
            "Loss training: 14.399629\n",
            "Loss training: 14.2380295\n",
            "Loss training: 25.720345\n",
            "Loss training: 26.301413\n",
            "\n",
            "***************************\n",
            "Trained on 7680 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 19.413631\n",
            "Loss training: 28.12928\n",
            "Loss training: 32.898262\n",
            "Loss training: 33.317497\n",
            "Loss training: 25.163927\n",
            "Loss training: 12.486362\n",
            "Loss training: 31.578835\n",
            "Loss training: 26.293999\n",
            "Loss training: 24.404598\n",
            "Loss training: 26.22707\n",
            "\n",
            "***************************\n",
            "Trained on 7690 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.11834\n",
            "Loss training: 31.57557\n",
            "Loss training: 19.411566\n",
            "Loss training: 29.560543\n",
            "Loss training: 24.061485\n",
            "Loss training: 27.66514\n",
            "Loss training: 35.758648\n",
            "Loss training: 32.59691\n",
            "Loss training: 28.323187\n",
            "Loss training: 33.019703\n",
            "\n",
            "***************************\n",
            "Trained on 7700 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.12371\n",
            "Loss training: 26.619722\n",
            "Loss training: 27.637348\n",
            "Loss training: 25.022284\n",
            "Loss training: 26.1284\n",
            "Loss training: 31.298079\n",
            "Loss training: 27.323332\n",
            "Loss training: 24.802914\n",
            "Loss training: 32.48664\n",
            "Loss training: 30.902945\n",
            "\n",
            "***************************\n",
            "Trained on 7710 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.353916\n",
            "Loss training: 19.233517\n",
            "Loss training: 35.079906\n",
            "Loss training: 32.252815\n",
            "Loss training: 31.489162\n",
            "Loss training: 24.383507\n",
            "Loss training: 26.969315\n",
            "Loss training: 31.320074\n",
            "Loss training: 26.786388\n",
            "Loss training: 39.926754\n",
            "\n",
            "***************************\n",
            "Trained on 7720 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.573154\n",
            "Loss training: 13.16976\n",
            "Loss training: 23.988815\n",
            "Loss training: 39.423077\n",
            "Loss training: 29.249369\n",
            "Loss training: 24.91762\n",
            "Loss training: 28.835123\n",
            "Loss training: 26.195024\n",
            "Loss training: 12.032589\n",
            "Loss training: 28.527847\n",
            "\n",
            "***************************\n",
            "Trained on 7730 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.520042\n",
            "Loss training: 25.133545\n",
            "Loss training: 28.619387\n",
            "Loss training: 28.72311\n",
            "Loss training: 32.409267\n",
            "Loss training: 28.916975\n",
            "Loss training: 21.573298\n",
            "Loss training: 28.134445\n",
            "Loss training: 24.362108\n",
            "Loss training: 25.588848\n",
            "\n",
            "***************************\n",
            "Trained on 7740 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.876724\n",
            "Loss training: 28.46701\n",
            "Loss training: 30.04395\n",
            "Loss training: 13.210056\n",
            "Loss training: 26.467003\n",
            "Loss training: 31.47861\n",
            "Loss training: 21.243998\n",
            "Loss training: 33.98406\n",
            "Loss training: 30.728518\n",
            "Loss training: 33.67844\n",
            "\n",
            "***************************\n",
            "Trained on 7750 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 20.570332\n",
            "Loss training: 33.37761\n",
            "Loss training: 25.052786\n",
            "Loss training: 32.80439\n",
            "Loss training: 27.995667\n",
            "Loss training: 27.829533\n",
            "Loss training: 33.1906\n",
            "Loss training: 32.883934\n",
            "Loss training: 24.448492\n",
            "Loss training: 31.04123\n",
            "\n",
            "***************************\n",
            "Trained on 7760 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.117691\n",
            "Loss training: 35.26936\n",
            "Loss training: 23.988758\n",
            "Loss training: 32.759148\n",
            "Loss training: 33.366528\n",
            "Loss training: 14.345695\n",
            "Loss training: 28.600077\n",
            "Loss training: 26.516062\n",
            "Loss training: 28.714508\n",
            "Loss training: 13.899757\n",
            "\n",
            "***************************\n",
            "Trained on 7770 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.662735\n",
            "Loss training: 26.81573\n",
            "Loss training: 19.438896\n",
            "Loss training: 26.258615\n",
            "Loss training: 26.239588\n",
            "Loss training: 24.431406\n",
            "Loss training: 20.230356\n",
            "Loss training: 22.086796\n",
            "Loss training: 31.885326\n",
            "Loss training: 24.497795\n",
            "\n",
            "***************************\n",
            "Trained on 7780 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.31938\n",
            "Loss training: 33.164967\n",
            "Loss training: 24.602648\n",
            "Loss training: 29.813461\n",
            "Loss training: 31.286053\n",
            "Loss training: 27.05611\n",
            "Loss training: 26.636171\n",
            "Loss training: 28.331005\n",
            "Loss training: 15.064882\n",
            "Loss training: 29.836906\n",
            "\n",
            "***************************\n",
            "Trained on 7790 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.83972\n",
            "Loss training: 33.604977\n",
            "Loss training: 34.628292\n",
            "Loss training: 34.616516\n",
            "Loss training: 24.467892\n",
            "Loss training: 33.56926\n",
            "Loss training: 25.674055\n",
            "Loss training: 13.989305\n",
            "Loss training: 26.43127\n",
            "Loss training: 19.713573\n",
            "\n",
            "***************************\n",
            "Trained on 7800 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.21284\n",
            "Loss training: 29.286125\n",
            "Loss training: 14.881526\n",
            "Loss training: 31.671171\n",
            "Loss training: 32.975822\n",
            "Loss training: 27.258625\n",
            "Loss training: 26.749996\n",
            "Loss training: 35.412567\n",
            "Loss training: 21.081856\n",
            "Loss training: 28.452175\n",
            "\n",
            "***************************\n",
            "Trained on 7810 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.801834\n",
            "Loss training: 26.71934\n",
            "Loss training: 27.416496\n",
            "Loss training: 21.289728\n",
            "Loss training: 26.734184\n",
            "Loss training: 23.941065\n",
            "Loss training: 31.500036\n",
            "Loss training: 14.128827\n",
            "Loss training: 12.941962\n",
            "Loss training: 31.362082\n",
            "\n",
            "***************************\n",
            "Trained on 7820 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.675776\n",
            "Loss training: 26.419415\n",
            "Loss training: 24.63952\n",
            "Loss training: 32.365395\n",
            "Loss training: 26.727058\n",
            "Loss training: 24.67832\n",
            "Loss training: 25.254536\n",
            "Loss training: 39.853466\n",
            "Loss training: 32.969887\n",
            "Loss training: 24.173267\n",
            "\n",
            "***************************\n",
            "Trained on 7830 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.31862\n",
            "Loss training: 28.584156\n",
            "Loss training: 26.746399\n",
            "Loss training: 26.30987\n",
            "Loss training: 31.333092\n",
            "Loss training: 32.460796\n",
            "Loss training: 27.214035\n",
            "Loss training: 14.537194\n",
            "Loss training: 19.86277\n",
            "Loss training: 33.784348\n",
            "\n",
            "***************************\n",
            "Trained on 7840 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.477577\n",
            "Loss training: 24.626682\n",
            "Loss training: 34.225372\n",
            "Loss training: 26.118979\n",
            "Loss training: 13.897064\n",
            "Loss training: 24.408169\n",
            "Loss training: 28.10369\n",
            "Loss training: 35.4018\n",
            "Loss training: 26.518375\n",
            "Loss training: 33.09721\n",
            "\n",
            "***************************\n",
            "Trained on 7850 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 13.935511\n",
            "Loss training: 30.392878\n",
            "Loss training: 28.603613\n",
            "Loss training: 37.677807\n",
            "Loss training: 29.174534\n",
            "Loss training: 29.195507\n",
            "Loss training: 31.150103\n",
            "Loss training: 40.88755\n",
            "Loss training: 28.314066\n",
            "Loss training: 20.023499\n",
            "\n",
            "***************************\n",
            "Trained on 7860 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.876503\n",
            "Loss training: 35.19825\n",
            "Loss training: 12.299046\n",
            "Loss training: 35.849804\n",
            "Loss training: 32.743477\n",
            "Loss training: 14.167413\n",
            "Loss training: 33.132366\n",
            "Loss training: 38.45585\n",
            "Loss training: 31.923468\n",
            "Loss training: 32.578945\n",
            "\n",
            "***************************\n",
            "Trained on 7870 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.988266\n",
            "Loss training: 21.130632\n",
            "Loss training: 45.43112\n",
            "Loss training: 29.898148\n",
            "Loss training: 27.787834\n",
            "Loss training: 30.0599\n",
            "Loss training: 35.946297\n",
            "Loss training: 24.587345\n",
            "Loss training: 22.559147\n",
            "Loss training: 30.009167\n",
            "\n",
            "***************************\n",
            "Trained on 7880 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.754333\n",
            "Loss training: 34.340767\n",
            "Loss training: 20.8271\n",
            "Loss training: 40.42743\n",
            "Loss training: 15.6140995\n",
            "Loss training: 33.47874\n",
            "Loss training: 31.90272\n",
            "Loss training: 37.79131\n",
            "Loss training: 20.178518\n",
            "Loss training: 33.65478\n",
            "\n",
            "***************************\n",
            "Trained on 7890 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.47834\n",
            "Loss training: 39.494896\n",
            "Loss training: 26.706287\n",
            "Loss training: 23.465391\n",
            "Loss training: 31.390842\n",
            "Loss training: 32.919422\n",
            "Loss training: 27.366392\n",
            "Loss training: 37.30274\n",
            "Loss training: 36.915367\n",
            "Loss training: 28.754265\n",
            "\n",
            "***************************\n",
            "Trained on 7900 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 37.39974\n",
            "Loss training: 31.976671\n",
            "Loss training: 34.03133\n",
            "Loss training: 20.289946\n",
            "Loss training: 23.040873\n",
            "Loss training: 28.525133\n",
            "Loss training: 38.99764\n",
            "Loss training: 14.44176\n",
            "Loss training: 25.762667\n",
            "Loss training: 42.12856\n",
            "\n",
            "***************************\n",
            "Trained on 7910 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.523525\n",
            "Loss training: 31.282444\n",
            "Loss training: 33.994526\n",
            "Loss training: 29.657858\n",
            "Loss training: 27.00681\n",
            "Loss training: 29.487799\n",
            "Loss training: 26.7438\n",
            "Loss training: 12.35185\n",
            "Loss training: 30.957394\n",
            "Loss training: 30.885208\n",
            "\n",
            "***************************\n",
            "Trained on 7920 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 20.453918\n",
            "Loss training: 30.272263\n",
            "Loss training: 29.614836\n",
            "Loss training: 29.313488\n",
            "Loss training: 27.600718\n",
            "Loss training: 28.587368\n",
            "Loss training: 36.70217\n",
            "Loss training: 25.419144\n",
            "Loss training: 28.47458\n",
            "Loss training: 28.494274\n",
            "\n",
            "***************************\n",
            "Trained on 7930 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.287169\n",
            "Loss training: 29.025791\n",
            "Loss training: 29.18437\n",
            "Loss training: 33.83873\n",
            "Loss training: 20.48031\n",
            "Loss training: 20.385883\n",
            "Loss training: 25.14751\n",
            "Loss training: 28.494204\n",
            "Loss training: 29.899702\n",
            "Loss training: 28.141212\n",
            "\n",
            "***************************\n",
            "Trained on 7940 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 19.884151\n",
            "Loss training: 19.66288\n",
            "Loss training: 27.795176\n",
            "Loss training: 26.3249\n",
            "Loss training: 32.848873\n",
            "Loss training: 27.838871\n",
            "Loss training: 27.560331\n",
            "Loss training: 27.696375\n",
            "Loss training: 34.023163\n",
            "Loss training: 32.96316\n",
            "\n",
            "***************************\n",
            "Trained on 7950 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.915234\n",
            "Loss training: 24.269295\n",
            "Loss training: 31.772161\n",
            "Loss training: 29.485634\n",
            "Loss training: 24.490234\n",
            "Loss training: 32.344902\n",
            "Loss training: 30.471125\n",
            "Loss training: 19.612738\n",
            "Loss training: 28.795197\n",
            "Loss training: 32.512276\n",
            "\n",
            "***************************\n",
            "Trained on 7960 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.23567\n",
            "Loss training: 13.724339\n",
            "Loss training: 31.783056\n",
            "Loss training: 27.249062\n",
            "Loss training: 24.33661\n",
            "Loss training: 24.244144\n",
            "Loss training: 13.8922\n",
            "Loss training: 25.494915\n",
            "Loss training: 21.32825\n",
            "Loss training: 28.393103\n",
            "\n",
            "***************************\n",
            "Trained on 7970 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.432194\n",
            "Loss training: 24.159834\n",
            "Loss training: 26.721127\n",
            "Loss training: 27.97665\n",
            "Loss training: 27.28074\n",
            "Loss training: 27.809061\n",
            "Loss training: 26.301296\n",
            "Loss training: 35.48345\n",
            "Loss training: 31.177317\n",
            "Loss training: 25.090427\n",
            "\n",
            "***************************\n",
            "Trained on 7980 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.71183\n",
            "Loss training: 27.048986\n",
            "Loss training: 24.459171\n",
            "Loss training: 31.701372\n",
            "Loss training: 13.608767\n",
            "Loss training: 13.580115\n",
            "Loss training: 24.470367\n",
            "Loss training: 25.118185\n",
            "Loss training: 11.791691\n",
            "Loss training: 26.542677\n",
            "\n",
            "***************************\n",
            "Trained on 7990 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.21673\n",
            "Loss training: 26.921621\n",
            "Loss training: 28.594912\n",
            "Loss training: 31.617228\n",
            "Loss training: 13.353993\n",
            "Loss training: 28.19478\n",
            "Loss training: 24.515902\n",
            "Loss training: 32.358234\n",
            "Loss training: 32.39279\n",
            "Loss training: 29.142632\n",
            "\n",
            "***************************\n",
            "Trained on 8000 graphs\n",
            "***************************\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4f229874-55c4-4ada-9ee6-7be07c2abfa2\", \"params_epochs_8000.pickle\", 22204504)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b7c9323e-cf59-4dfb-935e-714618623e9f\", \"opt_state_epochs_8000.pickle\", 44409100)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss training: 26.229452\n",
            "Loss training: 30.999815\n",
            "Loss training: 24.495356\n",
            "Loss training: 35.772564\n",
            "Loss training: 19.816284\n",
            "Loss training: 32.604286\n",
            "Loss training: 26.436354\n",
            "Loss training: 32.651726\n",
            "Loss training: 27.894955\n",
            "Loss training: 23.814297\n",
            "\n",
            "***************************\n",
            "Trained on 8010 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.824644\n",
            "Loss training: 35.167294\n",
            "Loss training: 32.42112\n",
            "Loss training: 19.60101\n",
            "Loss training: 19.521427\n",
            "Loss training: 26.204403\n",
            "Loss training: 29.035566\n",
            "Loss training: 25.30192\n",
            "Loss training: 28.78936\n",
            "Loss training: 33.875134\n",
            "\n",
            "***************************\n",
            "Trained on 8020 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 39.70866\n",
            "Loss training: 13.534309\n",
            "Loss training: 27.093792\n",
            "Loss training: 28.826439\n",
            "Loss training: 24.59459\n",
            "Loss training: 32.3\n",
            "Loss training: 26.95456\n",
            "Loss training: 31.170254\n",
            "Loss training: 30.878843\n",
            "Loss training: 28.170162\n",
            "\n",
            "***************************\n",
            "Trained on 8030 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.462025\n",
            "Loss training: 26.58766\n",
            "Loss training: 13.156139\n",
            "Loss training: 28.90755\n",
            "Loss training: 27.845482\n",
            "Loss training: 24.672478\n",
            "Loss training: 13.660911\n",
            "Loss training: 28.167011\n",
            "Loss training: 26.144478\n",
            "Loss training: 21.123861\n",
            "\n",
            "***************************\n",
            "Trained on 8040 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.216633\n",
            "Loss training: 24.65704\n",
            "Loss training: 27.68738\n",
            "Loss training: 19.578888\n",
            "Loss training: 28.28249\n",
            "Loss training: 27.05093\n",
            "Loss training: 27.818344\n",
            "Loss training: 24.490961\n",
            "Loss training: 19.474813\n",
            "Loss training: 30.92042\n",
            "\n",
            "***************************\n",
            "Trained on 8050 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.367413\n",
            "Loss training: 24.498669\n",
            "Loss training: 19.253942\n",
            "Loss training: 27.77206\n",
            "Loss training: 26.27448\n",
            "Loss training: 26.414211\n",
            "Loss training: 30.720606\n",
            "Loss training: 27.769732\n",
            "Loss training: 28.704262\n",
            "Loss training: 28.014668\n",
            "\n",
            "***************************\n",
            "Trained on 8060 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.316727\n",
            "Loss training: 31.05817\n",
            "Loss training: 24.391678\n",
            "Loss training: 24.625858\n",
            "Loss training: 26.685589\n",
            "Loss training: 12.808543\n",
            "Loss training: 26.057314\n",
            "Loss training: 26.688679\n",
            "Loss training: 19.976301\n",
            "Loss training: 32.359188\n",
            "\n",
            "***************************\n",
            "Trained on 8070 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 12.525539\n",
            "Loss training: 25.979853\n",
            "Loss training: 32.155117\n",
            "Loss training: 30.337286\n",
            "Loss training: 27.98756\n",
            "Loss training: 26.259916\n",
            "Loss training: 32.14053\n",
            "Loss training: 11.70001\n",
            "Loss training: 26.190626\n",
            "Loss training: 25.606798\n",
            "\n",
            "***************************\n",
            "Trained on 8080 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.647125\n",
            "Loss training: 28.592487\n",
            "Loss training: 19.910238\n",
            "Loss training: 27.7489\n",
            "Loss training: 26.100504\n",
            "Loss training: 32.63766\n",
            "Loss training: 26.238556\n",
            "Loss training: 19.197948\n",
            "Loss training: 28.778833\n",
            "Loss training: 28.448643\n",
            "\n",
            "***************************\n",
            "Trained on 8090 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.334333\n",
            "Loss training: 27.476667\n",
            "Loss training: 27.330437\n",
            "Loss training: 19.546806\n",
            "Loss training: 30.498219\n",
            "Loss training: 32.60927\n",
            "Loss training: 19.448042\n",
            "Loss training: 24.411715\n",
            "Loss training: 13.903628\n",
            "Loss training: 28.102789\n",
            "\n",
            "***************************\n",
            "Trained on 8100 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.33007\n",
            "Loss training: 28.59133\n",
            "Loss training: 13.586144\n",
            "Loss training: 24.113129\n",
            "Loss training: 28.14553\n",
            "Loss training: 19.550472\n",
            "Loss training: 19.432117\n",
            "Loss training: 32.157024\n",
            "Loss training: 27.371302\n",
            "Loss training: 31.070173\n",
            "\n",
            "***************************\n",
            "Trained on 8110 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.316252\n",
            "Loss training: 24.22506\n",
            "Loss training: 32.44408\n",
            "Loss training: 12.019955\n",
            "Loss training: 26.428595\n",
            "Loss training: 24.388062\n",
            "Loss training: 32.14998\n",
            "Loss training: 26.139992\n",
            "Loss training: 29.495775\n",
            "Loss training: 19.514513\n",
            "\n",
            "***************************\n",
            "Trained on 8120 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.51418\n",
            "Loss training: 40.017647\n",
            "Loss training: 24.659788\n",
            "Loss training: 27.746134\n",
            "Loss training: 27.669292\n",
            "Loss training: 26.014687\n",
            "Loss training: 32.472366\n",
            "Loss training: 12.890679\n",
            "Loss training: 25.76557\n",
            "Loss training: 33.444176\n",
            "\n",
            "***************************\n",
            "Trained on 8130 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.081095\n",
            "Loss training: 19.947893\n",
            "Loss training: 26.962654\n",
            "Loss training: 32.656467\n",
            "Loss training: 32.328575\n",
            "Loss training: 30.298008\n",
            "Loss training: 27.953815\n",
            "Loss training: 26.77816\n",
            "Loss training: 27.989723\n",
            "Loss training: 13.772341\n",
            "\n",
            "***************************\n",
            "Trained on 8140 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.03788\n",
            "Loss training: 26.126347\n",
            "Loss training: 26.083273\n",
            "Loss training: 31.792374\n",
            "Loss training: 33.103573\n",
            "Loss training: 28.993095\n",
            "Loss training: 27.82609\n",
            "Loss training: 30.548523\n",
            "Loss training: 27.499142\n",
            "Loss training: 28.22388\n",
            "\n",
            "***************************\n",
            "Trained on 8150 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.145737\n",
            "Loss training: 13.759504\n",
            "Loss training: 26.618668\n",
            "Loss training: 32.777313\n",
            "Loss training: 28.966585\n",
            "Loss training: 28.429169\n",
            "Loss training: 27.565666\n",
            "Loss training: 25.257893\n",
            "Loss training: 21.584097\n",
            "Loss training: 27.29763\n",
            "\n",
            "***************************\n",
            "Trained on 8160 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.135637\n",
            "Loss training: 24.746931\n",
            "Loss training: 30.963377\n",
            "Loss training: 27.985535\n",
            "Loss training: 41.058197\n",
            "Loss training: 25.91804\n",
            "Loss training: 28.92127\n",
            "Loss training: 24.393354\n",
            "Loss training: 30.213829\n",
            "Loss training: 24.642176\n",
            "\n",
            "***************************\n",
            "Trained on 8170 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.54409\n",
            "Loss training: 27.221205\n",
            "Loss training: 28.722446\n",
            "Loss training: 28.595995\n",
            "Loss training: 32.465965\n",
            "Loss training: 27.912962\n",
            "Loss training: 27.730686\n",
            "Loss training: 32.12604\n",
            "Loss training: 27.843285\n",
            "Loss training: 31.982626\n",
            "\n",
            "***************************\n",
            "Trained on 8180 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.727306\n",
            "Loss training: 27.116026\n",
            "Loss training: 24.83459\n",
            "Loss training: 31.741062\n",
            "Loss training: 19.547415\n",
            "Loss training: 27.050009\n",
            "Loss training: 34.143536\n",
            "Loss training: 28.082226\n",
            "Loss training: 33.64355\n",
            "Loss training: 31.640125\n",
            "\n",
            "***************************\n",
            "Trained on 8190 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 11.984351\n",
            "Loss training: 13.751824\n",
            "Loss training: 26.63794\n",
            "Loss training: 24.473576\n",
            "Loss training: 21.379192\n",
            "Loss training: 24.731699\n",
            "Loss training: 28.702011\n",
            "Loss training: 34.61522\n",
            "Loss training: 31.79655\n",
            "Loss training: 21.016764\n",
            "\n",
            "***************************\n",
            "Trained on 8200 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.80341\n",
            "Loss training: 24.645515\n",
            "Loss training: 27.095955\n",
            "Loss training: 31.46758\n",
            "Loss training: 24.297121\n",
            "Loss training: 32.69074\n",
            "Loss training: 41.04879\n",
            "Loss training: 30.26717\n",
            "Loss training: 19.89399\n",
            "Loss training: 24.807419\n",
            "\n",
            "***************************\n",
            "Trained on 8210 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.229525\n",
            "Loss training: 26.278885\n",
            "Loss training: 25.581524\n",
            "Loss training: 29.088163\n",
            "Loss training: 23.952976\n",
            "Loss training: 34.67039\n",
            "Loss training: 28.420197\n",
            "Loss training: 27.183247\n",
            "Loss training: 13.969593\n",
            "Loss training: 36.309944\n",
            "\n",
            "***************************\n",
            "Trained on 8220 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.721811\n",
            "Loss training: 26.37537\n",
            "Loss training: 28.405153\n",
            "Loss training: 28.025229\n",
            "Loss training: 20.744942\n",
            "Loss training: 32.228752\n",
            "Loss training: 20.65068\n",
            "Loss training: 19.133303\n",
            "Loss training: 28.251204\n",
            "Loss training: 13.317155\n",
            "\n",
            "***************************\n",
            "Trained on 8230 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 12.924559\n",
            "Loss training: 24.330921\n",
            "Loss training: 26.499493\n",
            "Loss training: 26.346327\n",
            "Loss training: 24.592133\n",
            "Loss training: 33.038113\n",
            "Loss training: 34.737045\n",
            "Loss training: 19.014816\n",
            "Loss training: 20.867325\n",
            "Loss training: 28.178478\n",
            "\n",
            "***************************\n",
            "Trained on 8240 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.456453\n",
            "Loss training: 18.7784\n",
            "Loss training: 40.123188\n",
            "Loss training: 26.57065\n",
            "Loss training: 20.61018\n",
            "Loss training: 39.60962\n",
            "Loss training: 27.836376\n",
            "Loss training: 26.18279\n",
            "Loss training: 27.122456\n",
            "Loss training: 31.9\n",
            "\n",
            "***************************\n",
            "Trained on 8250 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.43293\n",
            "Loss training: 27.562893\n",
            "Loss training: 13.913525\n",
            "Loss training: 25.616962\n",
            "Loss training: 31.836489\n",
            "Loss training: 30.859776\n",
            "Loss training: 13.581766\n",
            "Loss training: 32.50452\n",
            "Loss training: 31.064589\n",
            "Loss training: 26.20975\n",
            "\n",
            "***************************\n",
            "Trained on 8260 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.057934\n",
            "Loss training: 11.962011\n",
            "Loss training: 28.69734\n",
            "Loss training: 25.221418\n",
            "Loss training: 32.50715\n",
            "Loss training: 27.636482\n",
            "Loss training: 35.59009\n",
            "Loss training: 24.526052\n",
            "Loss training: 24.314068\n",
            "Loss training: 24.111946\n",
            "\n",
            "***************************\n",
            "Trained on 8270 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.089594\n",
            "Loss training: 31.491827\n",
            "Loss training: 26.278381\n",
            "Loss training: 30.926521\n",
            "Loss training: 28.061668\n",
            "Loss training: 31.952715\n",
            "Loss training: 24.213451\n",
            "Loss training: 11.891524\n",
            "Loss training: 26.818745\n",
            "Loss training: 20.216656\n",
            "\n",
            "***************************\n",
            "Trained on 8280 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.924145\n",
            "Loss training: 23.905252\n",
            "Loss training: 26.240784\n",
            "Loss training: 29.284864\n",
            "Loss training: 24.761974\n",
            "Loss training: 32.489304\n",
            "Loss training: 33.93992\n",
            "Loss training: 13.876273\n",
            "Loss training: 20.580143\n",
            "Loss training: 31.7499\n",
            "\n",
            "***************************\n",
            "Trained on 8290 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.380888\n",
            "Loss training: 27.739555\n",
            "Loss training: 24.477545\n",
            "Loss training: 33.120296\n",
            "Loss training: 19.356255\n",
            "Loss training: 29.450409\n",
            "Loss training: 19.734444\n",
            "Loss training: 24.409033\n",
            "Loss training: 13.658226\n",
            "Loss training: 28.203976\n",
            "\n",
            "***************************\n",
            "Trained on 8300 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.215372\n",
            "Loss training: 18.873928\n",
            "Loss training: 26.08207\n",
            "Loss training: 23.698536\n",
            "Loss training: 39.493237\n",
            "Loss training: 27.603273\n",
            "Loss training: 19.932625\n",
            "Loss training: 19.705023\n",
            "Loss training: 13.851724\n",
            "Loss training: 31.927837\n",
            "\n",
            "***************************\n",
            "Trained on 8310 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 19.296513\n",
            "Loss training: 26.355333\n",
            "Loss training: 27.792349\n",
            "Loss training: 24.93715\n",
            "Loss training: 32.903107\n",
            "Loss training: 20.880955\n",
            "Loss training: 20.734882\n",
            "Loss training: 30.741226\n",
            "Loss training: 27.45797\n",
            "Loss training: 34.11961\n",
            "\n",
            "***************************\n",
            "Trained on 8320 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 18.932846\n",
            "Loss training: 27.386002\n",
            "Loss training: 13.612342\n",
            "Loss training: 31.878925\n",
            "Loss training: 26.576773\n",
            "Loss training: 27.495378\n",
            "Loss training: 13.564802\n",
            "Loss training: 26.054207\n",
            "Loss training: 31.798782\n",
            "Loss training: 24.9818\n",
            "\n",
            "***************************\n",
            "Trained on 8330 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.149374\n",
            "Loss training: 26.140734\n",
            "Loss training: 24.150324\n",
            "Loss training: 28.3327\n",
            "Loss training: 13.475491\n",
            "Loss training: 39.981457\n",
            "Loss training: 39.453533\n",
            "Loss training: 12.450327\n",
            "Loss training: 30.584606\n",
            "Loss training: 31.14049\n",
            "\n",
            "***************************\n",
            "Trained on 8340 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.196012\n",
            "Loss training: 31.971603\n",
            "Loss training: 26.53987\n",
            "Loss training: 28.177168\n",
            "Loss training: 27.144703\n",
            "Loss training: 27.001562\n",
            "Loss training: 23.83806\n",
            "Loss training: 39.53695\n",
            "Loss training: 30.678524\n",
            "Loss training: 28.519855\n",
            "\n",
            "***************************\n",
            "Trained on 8350 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.686787\n",
            "Loss training: 26.231092\n",
            "Loss training: 31.327768\n",
            "Loss training: 30.80885\n",
            "Loss training: 28.06609\n",
            "Loss training: 38.83789\n",
            "Loss training: 35.989124\n",
            "Loss training: 28.36199\n",
            "Loss training: 19.65495\n",
            "Loss training: 28.115768\n",
            "\n",
            "***************************\n",
            "Trained on 8360 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.457422\n",
            "Loss training: 26.23289\n",
            "Loss training: 26.9105\n",
            "Loss training: 24.291306\n",
            "Loss training: 26.919384\n",
            "Loss training: 30.893417\n",
            "Loss training: 24.169662\n",
            "Loss training: 30.953436\n",
            "Loss training: 28.092636\n",
            "Loss training: 27.045431\n",
            "\n",
            "***************************\n",
            "Trained on 8370 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.292131\n",
            "Loss training: 31.424683\n",
            "Loss training: 18.779163\n",
            "Loss training: 26.704313\n",
            "Loss training: 32.171204\n",
            "Loss training: 11.736641\n",
            "Loss training: 36.0774\n",
            "Loss training: 30.657614\n",
            "Loss training: 28.13769\n",
            "Loss training: 27.96112\n",
            "\n",
            "***************************\n",
            "Trained on 8380 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.81233\n",
            "Loss training: 23.881584\n",
            "Loss training: 27.76853\n",
            "Loss training: 27.37183\n",
            "Loss training: 24.613253\n",
            "Loss training: 32.64541\n",
            "Loss training: 26.638594\n",
            "Loss training: 26.66536\n",
            "Loss training: 28.128025\n",
            "Loss training: 33.42039\n",
            "\n",
            "***************************\n",
            "Trained on 8390 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.212212\n",
            "Loss training: 19.535109\n",
            "Loss training: 32.085598\n",
            "Loss training: 31.14346\n",
            "Loss training: 27.435436\n",
            "Loss training: 26.752235\n",
            "Loss training: 20.525646\n",
            "Loss training: 30.897701\n",
            "Loss training: 24.207047\n",
            "Loss training: 26.026821\n",
            "\n",
            "***************************\n",
            "Trained on 8400 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.372412\n",
            "Loss training: 13.607873\n",
            "Loss training: 30.460506\n",
            "Loss training: 27.59431\n",
            "Loss training: 23.946432\n",
            "Loss training: 26.787415\n",
            "Loss training: 19.386953\n",
            "Loss training: 35.448925\n",
            "Loss training: 25.989887\n",
            "Loss training: 31.658302\n",
            "\n",
            "***************************\n",
            "Trained on 8410 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.533707\n",
            "Loss training: 35.11746\n",
            "Loss training: 26.390467\n",
            "Loss training: 32.692772\n",
            "Loss training: 33.442093\n",
            "Loss training: 27.706219\n",
            "Loss training: 26.284971\n",
            "Loss training: 13.542028\n",
            "Loss training: 32.965855\n",
            "Loss training: 24.037025\n",
            "\n",
            "***************************\n",
            "Trained on 8420 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.564924\n",
            "Loss training: 19.516613\n",
            "Loss training: 26.385906\n",
            "Loss training: 28.188694\n",
            "Loss training: 26.688738\n",
            "Loss training: 26.141146\n",
            "Loss training: 23.314812\n",
            "Loss training: 23.725561\n",
            "Loss training: 25.34921\n",
            "Loss training: 29.12138\n",
            "\n",
            "***************************\n",
            "Trained on 8430 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.393724\n",
            "Loss training: 23.977036\n",
            "Loss training: 30.669376\n",
            "Loss training: 25.01453\n",
            "Loss training: 20.661814\n",
            "Loss training: 26.1335\n",
            "Loss training: 27.522972\n",
            "Loss training: 20.342405\n",
            "Loss training: 24.055113\n",
            "Loss training: 27.50558\n",
            "\n",
            "***************************\n",
            "Trained on 8440 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.645233\n",
            "Loss training: 11.81408\n",
            "Loss training: 25.947903\n",
            "Loss training: 27.15117\n",
            "Loss training: 26.82829\n",
            "Loss training: 28.364927\n",
            "Loss training: 26.38695\n",
            "Loss training: 39.503254\n",
            "Loss training: 26.755024\n",
            "Loss training: 24.171556\n",
            "\n",
            "***************************\n",
            "Trained on 8450 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.096163\n",
            "Loss training: 27.274195\n",
            "Loss training: 20.26258\n",
            "Loss training: 23.654701\n",
            "Loss training: 29.87616\n",
            "Loss training: 38.88627\n",
            "Loss training: 31.777645\n",
            "Loss training: 26.43329\n",
            "Loss training: 24.849634\n",
            "Loss training: 27.679188\n",
            "\n",
            "***************************\n",
            "Trained on 8460 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.573746\n",
            "Loss training: 25.16261\n",
            "Loss training: 30.389784\n",
            "Loss training: 30.179522\n",
            "Loss training: 38.51875\n",
            "Loss training: 23.84688\n",
            "Loss training: 26.809328\n",
            "Loss training: 24.20443\n",
            "Loss training: 24.73727\n",
            "Loss training: 26.270452\n",
            "\n",
            "***************************\n",
            "Trained on 8470 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.173294\n",
            "Loss training: 26.250235\n",
            "Loss training: 33.4868\n",
            "Loss training: 13.438923\n",
            "Loss training: 31.061054\n",
            "Loss training: 35.534348\n",
            "Loss training: 27.161661\n",
            "Loss training: 26.46367\n",
            "Loss training: 32.040062\n",
            "Loss training: 28.44693\n",
            "\n",
            "***************************\n",
            "Trained on 8480 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.20745\n",
            "Loss training: 12.671489\n",
            "Loss training: 31.441498\n",
            "Loss training: 24.505344\n",
            "Loss training: 35.531475\n",
            "Loss training: 28.079256\n",
            "Loss training: 29.218594\n",
            "Loss training: 19.578514\n",
            "Loss training: 36.163597\n",
            "Loss training: 26.349108\n",
            "\n",
            "***************************\n",
            "Trained on 8490 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 19.36124\n",
            "Loss training: 25.14075\n",
            "Loss training: 27.919647\n",
            "Loss training: 24.544067\n",
            "Loss training: 33.80579\n",
            "Loss training: 32.86835\n",
            "Loss training: 27.87481\n",
            "Loss training: 31.345943\n",
            "Loss training: 26.42905\n",
            "Loss training: 13.78908\n",
            "\n",
            "***************************\n",
            "Trained on 8500 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.686045\n",
            "Loss training: 35.499893\n",
            "Loss training: 28.576311\n",
            "Loss training: 11.848279\n",
            "Loss training: 31.484222\n",
            "Loss training: 41.975254\n",
            "Loss training: 29.181362\n",
            "Loss training: 32.65635\n",
            "Loss training: 13.089247\n",
            "Loss training: 28.086435\n",
            "\n",
            "***************************\n",
            "Trained on 8510 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.632753\n",
            "Loss training: 29.128754\n",
            "Loss training: 37.06375\n",
            "Loss training: 28.92854\n",
            "Loss training: 27.295929\n",
            "Loss training: 32.967064\n",
            "Loss training: 28.007141\n",
            "Loss training: 27.311216\n",
            "Loss training: 25.514545\n",
            "Loss training: 26.933615\n",
            "\n",
            "***************************\n",
            "Trained on 8520 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 13.199411\n",
            "Loss training: 31.538275\n",
            "Loss training: 36.608223\n",
            "Loss training: 11.902436\n",
            "Loss training: 28.47521\n",
            "Loss training: 27.65789\n",
            "Loss training: 30.665586\n",
            "Loss training: 27.317057\n",
            "Loss training: 28.19362\n",
            "Loss training: 28.625105\n",
            "\n",
            "***************************\n",
            "Trained on 8530 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.165627\n",
            "Loss training: 32.208916\n",
            "Loss training: 24.916222\n",
            "Loss training: 33.664616\n",
            "Loss training: 26.395155\n",
            "Loss training: 38.875664\n",
            "Loss training: 27.151737\n",
            "Loss training: 19.427973\n",
            "Loss training: 28.41332\n",
            "Loss training: 13.775336\n",
            "\n",
            "***************************\n",
            "Trained on 8540 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.995667\n",
            "Loss training: 32.51377\n",
            "Loss training: 24.497894\n",
            "Loss training: 25.13702\n",
            "Loss training: 26.193954\n",
            "Loss training: 26.18058\n",
            "Loss training: 27.279552\n",
            "Loss training: 26.094772\n",
            "Loss training: 34.31603\n",
            "Loss training: 35.133682\n",
            "\n",
            "***************************\n",
            "Trained on 8550 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.159973\n",
            "Loss training: 12.860242\n",
            "Loss training: 24.585716\n",
            "Loss training: 33.215836\n",
            "Loss training: 23.92006\n",
            "Loss training: 12.793293\n",
            "Loss training: 20.562824\n",
            "Loss training: 39.113712\n",
            "Loss training: 32.6636\n",
            "Loss training: 13.463899\n",
            "\n",
            "***************************\n",
            "Trained on 8560 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.521275\n",
            "Loss training: 30.910875\n",
            "Loss training: 25.995564\n",
            "Loss training: 19.171677\n",
            "Loss training: 31.300777\n",
            "Loss training: 32.77307\n",
            "Loss training: 29.626587\n",
            "Loss training: 19.48483\n",
            "Loss training: 13.224056\n",
            "Loss training: 38.31977\n",
            "\n",
            "***************************\n",
            "Trained on 8570 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.761982\n",
            "Loss training: 34.62736\n",
            "Loss training: 28.591803\n",
            "Loss training: 13.383915\n",
            "Loss training: 25.935146\n",
            "Loss training: 12.4777155\n",
            "Loss training: 20.47959\n",
            "Loss training: 11.800214\n",
            "Loss training: 18.849806\n",
            "Loss training: 19.506763\n",
            "\n",
            "***************************\n",
            "Trained on 8580 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.485851\n",
            "Loss training: 13.183412\n",
            "Loss training: 32.728333\n",
            "Loss training: 38.374966\n",
            "Loss training: 30.426449\n",
            "Loss training: 27.825647\n",
            "Loss training: 23.88015\n",
            "Loss training: 31.015524\n",
            "Loss training: 31.316395\n",
            "Loss training: 25.580711\n",
            "\n",
            "***************************\n",
            "Trained on 8590 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 11.859795\n",
            "Loss training: 20.415863\n",
            "Loss training: 13.132306\n",
            "Loss training: 35.196358\n",
            "Loss training: 34.870403\n",
            "Loss training: 11.596919\n",
            "Loss training: 31.975609\n",
            "Loss training: 19.784494\n",
            "Loss training: 30.381266\n",
            "Loss training: 38.51442\n",
            "\n",
            "***************************\n",
            "Trained on 8600 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 12.444206\n",
            "Loss training: 27.467762\n",
            "Loss training: 19.436316\n",
            "Loss training: 33.965534\n",
            "Loss training: 27.279661\n",
            "Loss training: 31.595484\n",
            "Loss training: 11.480836\n",
            "Loss training: 24.78646\n",
            "Loss training: 27.112123\n",
            "Loss training: 23.497166\n",
            "\n",
            "***************************\n",
            "Trained on 8610 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.23212\n",
            "Loss training: 12.332232\n",
            "Loss training: 26.915838\n",
            "Loss training: 24.033453\n",
            "Loss training: 20.268497\n",
            "Loss training: 20.101362\n",
            "Loss training: 11.726397\n",
            "Loss training: 31.30923\n",
            "Loss training: 26.587801\n",
            "Loss training: 24.112036\n",
            "\n",
            "***************************\n",
            "Trained on 8620 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.680267\n",
            "Loss training: 23.933943\n",
            "Loss training: 30.753496\n",
            "Loss training: 26.806437\n",
            "Loss training: 25.957947\n",
            "Loss training: 29.996607\n",
            "Loss training: 28.9124\n",
            "Loss training: 27.32186\n",
            "Loss training: 27.77063\n",
            "Loss training: 24.05412\n",
            "\n",
            "***************************\n",
            "Trained on 8630 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.79849\n",
            "Loss training: 33.400497\n",
            "Loss training: 26.560356\n",
            "Loss training: 24.681423\n",
            "Loss training: 13.275328\n",
            "Loss training: 31.931732\n",
            "Loss training: 30.315454\n",
            "Loss training: 24.630827\n",
            "Loss training: 28.063414\n",
            "Loss training: 26.340311\n",
            "\n",
            "***************************\n",
            "Trained on 8640 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.666046\n",
            "Loss training: 12.468502\n",
            "Loss training: 24.39654\n",
            "Loss training: 39.407368\n",
            "Loss training: 38.702564\n",
            "Loss training: 26.591522\n",
            "Loss training: 25.01229\n",
            "Loss training: 31.509823\n",
            "Loss training: 35.69589\n",
            "Loss training: 12.215789\n",
            "\n",
            "***************************\n",
            "Trained on 8650 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.65955\n",
            "Loss training: 28.776232\n",
            "Loss training: 31.729198\n",
            "Loss training: 13.320786\n",
            "Loss training: 26.575594\n",
            "Loss training: 27.058681\n",
            "Loss training: 32.36208\n",
            "Loss training: 38.66295\n",
            "Loss training: 26.675402\n",
            "Loss training: 31.843393\n",
            "\n",
            "***************************\n",
            "Trained on 8660 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.110783\n",
            "Loss training: 25.958044\n",
            "Loss training: 30.393623\n",
            "Loss training: 29.092121\n",
            "Loss training: 26.230455\n",
            "Loss training: 29.113667\n",
            "Loss training: 28.054705\n",
            "Loss training: 24.362991\n",
            "Loss training: 19.645233\n",
            "Loss training: 13.281761\n",
            "\n",
            "***************************\n",
            "Trained on 8670 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.679817\n",
            "Loss training: 29.647854\n",
            "Loss training: 29.357307\n",
            "Loss training: 27.806707\n",
            "Loss training: 28.768631\n",
            "Loss training: 26.491407\n",
            "Loss training: 27.001112\n",
            "Loss training: 26.38239\n",
            "Loss training: 27.153952\n",
            "Loss training: 25.673058\n",
            "\n",
            "***************************\n",
            "Trained on 8680 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.82696\n",
            "Loss training: 25.49337\n",
            "Loss training: 32.445312\n",
            "Loss training: 38.334118\n",
            "Loss training: 25.6193\n",
            "Loss training: 31.741032\n",
            "Loss training: 35.148903\n",
            "Loss training: 28.911608\n",
            "Loss training: 25.520664\n",
            "Loss training: 24.392414\n",
            "\n",
            "***************************\n",
            "Trained on 8690 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.190763\n",
            "Loss training: 29.196548\n",
            "Loss training: 29.276205\n",
            "Loss training: 31.557737\n",
            "Loss training: 31.470121\n",
            "Loss training: 12.997252\n",
            "Loss training: 27.428314\n",
            "Loss training: 37.275055\n",
            "Loss training: 28.023561\n",
            "Loss training: 12.119373\n",
            "\n",
            "***************************\n",
            "Trained on 8700 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.00842\n",
            "Loss training: 28.280464\n",
            "Loss training: 27.898893\n",
            "Loss training: 24.607533\n",
            "Loss training: 27.230965\n",
            "Loss training: 31.007154\n",
            "Loss training: 29.405893\n",
            "Loss training: 23.992167\n",
            "Loss training: 27.21305\n",
            "Loss training: 13.159327\n",
            "\n",
            "***************************\n",
            "Trained on 8710 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.13275\n",
            "Loss training: 31.064302\n",
            "Loss training: 32.211052\n",
            "Loss training: 31.805714\n",
            "Loss training: 39.244877\n",
            "Loss training: 27.190773\n",
            "Loss training: 31.257961\n",
            "Loss training: 23.448217\n",
            "Loss training: 14.0817795\n",
            "Loss training: 29.341541\n",
            "\n",
            "***************************\n",
            "Trained on 8720 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 13.274615\n",
            "Loss training: 27.748896\n",
            "Loss training: 27.063227\n",
            "Loss training: 27.51526\n",
            "Loss training: 23.637077\n",
            "Loss training: 31.259245\n",
            "Loss training: 31.478323\n",
            "Loss training: 24.087603\n",
            "Loss training: 23.86379\n",
            "Loss training: 26.748055\n",
            "\n",
            "***************************\n",
            "Trained on 8730 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.390467\n",
            "Loss training: 26.299023\n",
            "Loss training: 30.426964\n",
            "Loss training: 23.85123\n",
            "Loss training: 32.26568\n",
            "Loss training: 30.835924\n",
            "Loss training: 31.092918\n",
            "Loss training: 27.812614\n",
            "Loss training: 31.586718\n",
            "Loss training: 27.10781\n",
            "\n",
            "***************************\n",
            "Trained on 8740 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.671028\n",
            "Loss training: 28.895823\n",
            "Loss training: 28.118336\n",
            "Loss training: 27.123936\n",
            "Loss training: 25.173918\n",
            "Loss training: 36.967133\n",
            "Loss training: 30.372818\n",
            "Loss training: 30.885271\n",
            "Loss training: 26.974743\n",
            "Loss training: 29.076838\n",
            "\n",
            "***************************\n",
            "Trained on 8750 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.985647\n",
            "Loss training: 34.868134\n",
            "Loss training: 24.669025\n",
            "Loss training: 25.96239\n",
            "Loss training: 28.675402\n",
            "Loss training: 28.193022\n",
            "Loss training: 28.027752\n",
            "Loss training: 26.57918\n",
            "Loss training: 31.137676\n",
            "Loss training: 29.728046\n",
            "\n",
            "***************************\n",
            "Trained on 8760 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.76537\n",
            "Loss training: 36.98061\n",
            "Loss training: 39.18694\n",
            "Loss training: 14.667487\n",
            "Loss training: 30.307945\n",
            "Loss training: 33.800644\n",
            "Loss training: 27.905138\n",
            "Loss training: 20.324186\n",
            "Loss training: 31.947899\n",
            "Loss training: 31.626495\n",
            "\n",
            "***************************\n",
            "Trained on 8770 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.6106\n",
            "Loss training: 29.552734\n",
            "Loss training: 30.217077\n",
            "Loss training: 28.71949\n",
            "Loss training: 34.50575\n",
            "Loss training: 26.773613\n",
            "Loss training: 26.341576\n",
            "Loss training: 12.763871\n",
            "Loss training: 28.97249\n",
            "Loss training: 35.46829\n",
            "\n",
            "***************************\n",
            "Trained on 8780 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 36.97203\n",
            "Loss training: 31.789873\n",
            "Loss training: 26.225908\n",
            "Loss training: 30.620193\n",
            "Loss training: 31.350874\n",
            "Loss training: 26.78191\n",
            "Loss training: 26.64785\n",
            "Loss training: 27.801374\n",
            "Loss training: 39.825314\n",
            "Loss training: 19.890533\n",
            "\n",
            "***************************\n",
            "Trained on 8790 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 20.562346\n",
            "Loss training: 29.0536\n",
            "Loss training: 29.562841\n",
            "Loss training: 27.791624\n",
            "Loss training: 21.982712\n",
            "Loss training: 26.789606\n",
            "Loss training: 29.811024\n",
            "Loss training: 25.400223\n",
            "Loss training: 33.939617\n",
            "Loss training: 33.54088\n",
            "\n",
            "***************************\n",
            "Trained on 8800 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 13.264702\n",
            "Loss training: 28.728333\n",
            "Loss training: 13.976111\n",
            "Loss training: 28.12676\n",
            "Loss training: 25.086966\n",
            "Loss training: 25.229887\n",
            "Loss training: 29.50635\n",
            "Loss training: 26.243689\n",
            "Loss training: 28.051695\n",
            "Loss training: 23.956772\n",
            "\n",
            "***************************\n",
            "Trained on 8810 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 19.722559\n",
            "Loss training: 13.769195\n",
            "Loss training: 31.89185\n",
            "Loss training: 32.212505\n",
            "Loss training: 26.991253\n",
            "Loss training: 39.392742\n",
            "Loss training: 23.720615\n",
            "Loss training: 23.681711\n",
            "Loss training: 28.82872\n",
            "Loss training: 26.320105\n",
            "\n",
            "***************************\n",
            "Trained on 8820 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.921207\n",
            "Loss training: 28.40009\n",
            "Loss training: 26.485735\n",
            "Loss training: 19.621265\n",
            "Loss training: 29.518139\n",
            "Loss training: 31.70381\n",
            "Loss training: 19.333372\n",
            "Loss training: 24.45917\n",
            "Loss training: 24.593468\n",
            "Loss training: 26.979101\n",
            "\n",
            "***************************\n",
            "Trained on 8830 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.133926\n",
            "Loss training: 14.006281\n",
            "Loss training: 26.32981\n",
            "Loss training: 27.250498\n",
            "Loss training: 20.962389\n",
            "Loss training: 24.04818\n",
            "Loss training: 19.608446\n",
            "Loss training: 31.656834\n",
            "Loss training: 38.492844\n",
            "Loss training: 28.279951\n",
            "\n",
            "***************************\n",
            "Trained on 8840 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.094643\n",
            "Loss training: 33.029774\n",
            "Loss training: 25.475792\n",
            "Loss training: 28.903534\n",
            "Loss training: 24.711544\n",
            "Loss training: 27.024445\n",
            "Loss training: 26.161673\n",
            "Loss training: 27.31426\n",
            "Loss training: 30.737038\n",
            "Loss training: 24.072538\n",
            "\n",
            "***************************\n",
            "Trained on 8850 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.866314\n",
            "Loss training: 26.499159\n",
            "Loss training: 26.422735\n",
            "Loss training: 28.727299\n",
            "Loss training: 26.954554\n",
            "Loss training: 31.737291\n",
            "Loss training: 26.157671\n",
            "Loss training: 20.226763\n",
            "Loss training: 30.999996\n",
            "Loss training: 19.084244\n",
            "\n",
            "***************************\n",
            "Trained on 8860 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.416788\n",
            "Loss training: 27.887188\n",
            "Loss training: 26.336704\n",
            "Loss training: 26.348137\n",
            "Loss training: 30.849606\n",
            "Loss training: 24.005255\n",
            "Loss training: 26.103157\n",
            "Loss training: 33.50147\n",
            "Loss training: 26.080185\n",
            "Loss training: 23.846474\n",
            "\n",
            "***************************\n",
            "Trained on 8870 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.035515\n",
            "Loss training: 24.165955\n",
            "Loss training: 26.14449\n",
            "Loss training: 32.17954\n",
            "Loss training: 29.047445\n",
            "Loss training: 26.008188\n",
            "Loss training: 23.342888\n",
            "Loss training: 30.449978\n",
            "Loss training: 27.528103\n",
            "Loss training: 31.951859\n",
            "\n",
            "***************************\n",
            "Trained on 8880 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.089025\n",
            "Loss training: 31.656153\n",
            "Loss training: 26.01828\n",
            "Loss training: 32.860287\n",
            "Loss training: 27.10379\n",
            "Loss training: 24.025818\n",
            "Loss training: 30.286837\n",
            "Loss training: 26.216099\n",
            "Loss training: 24.459646\n",
            "Loss training: 25.521643\n",
            "\n",
            "***************************\n",
            "Trained on 8890 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.962944\n",
            "Loss training: 12.487413\n",
            "Loss training: 28.713137\n",
            "Loss training: 33.017372\n",
            "Loss training: 25.28892\n",
            "Loss training: 25.21928\n",
            "Loss training: 31.583021\n",
            "Loss training: 13.22865\n",
            "Loss training: 27.887344\n",
            "Loss training: 30.55656\n",
            "\n",
            "***************************\n",
            "Trained on 8900 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 19.29195\n",
            "Loss training: 35.430134\n",
            "Loss training: 19.241926\n",
            "Loss training: 24.655153\n",
            "Loss training: 27.492884\n",
            "Loss training: 29.837357\n",
            "Loss training: 31.365788\n",
            "Loss training: 31.812243\n",
            "Loss training: 29.544235\n",
            "Loss training: 25.09995\n",
            "\n",
            "***************************\n",
            "Trained on 8910 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 19.561142\n",
            "Loss training: 27.369867\n",
            "Loss training: 30.566967\n",
            "Loss training: 25.130196\n",
            "Loss training: 25.758816\n",
            "Loss training: 18.885202\n",
            "Loss training: 23.658033\n",
            "Loss training: 20.802464\n",
            "Loss training: 12.289453\n",
            "Loss training: 12.054692\n",
            "\n",
            "***************************\n",
            "Trained on 8920 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.965048\n",
            "Loss training: 25.35087\n",
            "Loss training: 29.984018\n",
            "Loss training: 27.009724\n",
            "Loss training: 26.038548\n",
            "Loss training: 28.108566\n",
            "Loss training: 27.368078\n",
            "Loss training: 26.269226\n",
            "Loss training: 19.601913\n",
            "Loss training: 24.148115\n",
            "\n",
            "***************************\n",
            "Trained on 8930 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.578342\n",
            "Loss training: 26.74218\n",
            "Loss training: 31.738997\n",
            "Loss training: 24.750208\n",
            "Loss training: 13.771019\n",
            "Loss training: 11.935524\n",
            "Loss training: 29.384222\n",
            "Loss training: 24.066727\n",
            "Loss training: 26.89235\n",
            "Loss training: 13.11497\n",
            "\n",
            "***************************\n",
            "Trained on 8940 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.346733\n",
            "Loss training: 27.18427\n",
            "Loss training: 23.98859\n",
            "Loss training: 26.937716\n",
            "Loss training: 29.828438\n",
            "Loss training: 26.447817\n",
            "Loss training: 24.74674\n",
            "Loss training: 23.618715\n",
            "Loss training: 23.465918\n",
            "Loss training: 31.114576\n",
            "\n",
            "***************************\n",
            "Trained on 8950 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.42486\n",
            "Loss training: 38.181026\n",
            "Loss training: 26.080303\n",
            "Loss training: 32.462265\n",
            "Loss training: 23.57555\n",
            "Loss training: 18.832283\n",
            "Loss training: 26.518217\n",
            "Loss training: 23.784925\n",
            "Loss training: 18.687962\n",
            "Loss training: 30.005688\n",
            "\n",
            "***************************\n",
            "Trained on 8960 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.464518\n",
            "Loss training: 30.797682\n",
            "Loss training: 23.395792\n",
            "Loss training: 11.948503\n",
            "Loss training: 33.62769\n",
            "Loss training: 30.016024\n",
            "Loss training: 20.696857\n",
            "Loss training: 30.70043\n",
            "Loss training: 27.271204\n",
            "Loss training: 26.285822\n",
            "\n",
            "***************************\n",
            "Trained on 8970 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.753448\n",
            "Loss training: 26.220024\n",
            "Loss training: 25.836802\n",
            "Loss training: 25.051964\n",
            "Loss training: 38.3143\n",
            "Loss training: 31.425224\n",
            "Loss training: 21.07953\n",
            "Loss training: 20.854395\n",
            "Loss training: 32.575012\n",
            "Loss training: 27.146978\n",
            "\n",
            "***************************\n",
            "Trained on 8980 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.54924\n",
            "Loss training: 19.555027\n",
            "Loss training: 30.974873\n",
            "Loss training: 24.310709\n",
            "Loss training: 23.492664\n",
            "Loss training: 26.510843\n",
            "Loss training: 29.366076\n",
            "Loss training: 24.20476\n",
            "Loss training: 38.59682\n",
            "Loss training: 30.387255\n",
            "\n",
            "***************************\n",
            "Trained on 8990 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.316109\n",
            "Loss training: 26.02386\n",
            "Loss training: 11.770985\n",
            "Loss training: 29.866344\n",
            "Loss training: 25.689795\n",
            "Loss training: 29.617369\n",
            "Loss training: 26.876091\n",
            "Loss training: 23.494238\n",
            "Loss training: 25.07582\n",
            "Loss training: 19.356678\n",
            "\n",
            "***************************\n",
            "Trained on 9000 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 13.642507\n",
            "Loss training: 31.610685\n",
            "Loss training: 28.132961\n",
            "Loss training: 29.75953\n",
            "Loss training: 33.138382\n",
            "Loss training: 25.835266\n",
            "Loss training: 19.370956\n",
            "Loss training: 24.753883\n",
            "Loss training: 25.914923\n",
            "Loss training: 19.127579\n",
            "\n",
            "***************************\n",
            "Trained on 9010 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.443256\n",
            "Loss training: 33.10362\n",
            "Loss training: 19.220188\n",
            "Loss training: 19.16634\n",
            "Loss training: 26.248714\n",
            "Loss training: 27.56418\n",
            "Loss training: 25.558977\n",
            "Loss training: 31.959696\n",
            "Loss training: 30.514967\n",
            "Loss training: 19.188364\n",
            "\n",
            "***************************\n",
            "Trained on 9020 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.932224\n",
            "Loss training: 28.70875\n",
            "Loss training: 20.598337\n",
            "Loss training: 19.389593\n",
            "Loss training: 26.043905\n",
            "Loss training: 18.840267\n",
            "Loss training: 13.407666\n",
            "Loss training: 38.303394\n",
            "Loss training: 26.157686\n",
            "Loss training: 25.38782\n",
            "\n",
            "***************************\n",
            "Trained on 9030 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.654802\n",
            "Loss training: 12.149351\n",
            "Loss training: 12.1484995\n",
            "Loss training: 18.452723\n",
            "Loss training: 27.052017\n",
            "Loss training: 25.952257\n",
            "Loss training: 13.321772\n",
            "Loss training: 30.257051\n",
            "Loss training: 33.65043\n",
            "Loss training: 25.440468\n",
            "\n",
            "***************************\n",
            "Trained on 9040 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.848547\n",
            "Loss training: 25.077946\n",
            "Loss training: 31.866766\n",
            "Loss training: 24.664139\n",
            "Loss training: 24.001387\n",
            "Loss training: 18.780481\n",
            "Loss training: 27.659481\n",
            "Loss training: 27.161863\n",
            "Loss training: 25.304573\n",
            "Loss training: 26.07096\n",
            "\n",
            "***************************\n",
            "Trained on 9050 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 19.930906\n",
            "Loss training: 26.226606\n",
            "Loss training: 13.2748\n",
            "Loss training: 34.242588\n",
            "Loss training: 31.757334\n",
            "Loss training: 19.170635\n",
            "Loss training: 31.156343\n",
            "Loss training: 23.949284\n",
            "Loss training: 31.1679\n",
            "Loss training: 30.696987\n",
            "\n",
            "***************************\n",
            "Trained on 9060 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.625683\n",
            "Loss training: 24.144688\n",
            "Loss training: 27.441935\n",
            "Loss training: 25.650473\n",
            "Loss training: 24.892303\n",
            "Loss training: 26.850868\n",
            "Loss training: 12.148206\n",
            "Loss training: 31.03232\n",
            "Loss training: 11.68745\n",
            "Loss training: 24.803902\n",
            "\n",
            "***************************\n",
            "Trained on 9070 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 18.600292\n",
            "Loss training: 25.142435\n",
            "Loss training: 25.983522\n",
            "Loss training: 27.42331\n",
            "Loss training: 31.057634\n",
            "Loss training: 23.624762\n",
            "Loss training: 25.934065\n",
            "Loss training: 19.559639\n",
            "Loss training: 25.893236\n",
            "Loss training: 24.919931\n",
            "\n",
            "***************************\n",
            "Trained on 9080 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.88682\n",
            "Loss training: 24.24239\n",
            "Loss training: 30.93818\n",
            "Loss training: 23.61264\n",
            "Loss training: 23.841492\n",
            "Loss training: 38.508724\n",
            "Loss training: 31.672337\n",
            "Loss training: 13.117647\n",
            "Loss training: 32.10006\n",
            "Loss training: 26.767155\n",
            "\n",
            "***************************\n",
            "Trained on 9090 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.439695\n",
            "Loss training: 29.32613\n",
            "Loss training: 32.731632\n",
            "Loss training: 26.423239\n",
            "Loss training: 29.23552\n",
            "Loss training: 38.328487\n",
            "Loss training: 30.489126\n",
            "Loss training: 26.556442\n",
            "Loss training: 13.364647\n",
            "Loss training: 26.869162\n",
            "\n",
            "***************************\n",
            "Trained on 9100 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.803429\n",
            "Loss training: 27.670065\n",
            "Loss training: 27.620361\n",
            "Loss training: 34.343426\n",
            "Loss training: 35.223614\n",
            "Loss training: 27.47852\n",
            "Loss training: 27.022448\n",
            "Loss training: 27.7325\n",
            "Loss training: 27.079906\n",
            "Loss training: 27.500044\n",
            "\n",
            "***************************\n",
            "Trained on 9110 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.667807\n",
            "Loss training: 27.302061\n",
            "Loss training: 32.895306\n",
            "Loss training: 27.277554\n",
            "Loss training: 30.680862\n",
            "Loss training: 14.630844\n",
            "Loss training: 13.7755\n",
            "Loss training: 29.277264\n",
            "Loss training: 28.274435\n",
            "Loss training: 27.713684\n",
            "\n",
            "***************************\n",
            "Trained on 9120 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 19.666056\n",
            "Loss training: 28.341608\n",
            "Loss training: 27.987997\n",
            "Loss training: 24.518793\n",
            "Loss training: 24.139217\n",
            "Loss training: 28.647457\n",
            "Loss training: 23.946161\n",
            "Loss training: 13.347864\n",
            "Loss training: 24.64188\n",
            "Loss training: 24.262167\n",
            "\n",
            "***************************\n",
            "Trained on 9130 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.362583\n",
            "Loss training: 27.224594\n",
            "Loss training: 27.202524\n",
            "Loss training: 32.274574\n",
            "Loss training: 24.88989\n",
            "Loss training: 26.410448\n",
            "Loss training: 29.05755\n",
            "Loss training: 28.832712\n",
            "Loss training: 32.554573\n",
            "Loss training: 31.718132\n",
            "\n",
            "***************************\n",
            "Trained on 9140 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.200294\n",
            "Loss training: 13.664524\n",
            "Loss training: 11.930221\n",
            "Loss training: 30.375826\n",
            "Loss training: 24.639181\n",
            "Loss training: 19.800203\n",
            "Loss training: 30.33975\n",
            "Loss training: 34.455025\n",
            "Loss training: 33.787045\n",
            "Loss training: 27.891882\n",
            "\n",
            "***************************\n",
            "Trained on 9150 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.853971\n",
            "Loss training: 12.322081\n",
            "Loss training: 26.608288\n",
            "Loss training: 26.698433\n",
            "Loss training: 28.322893\n",
            "Loss training: 29.848402\n",
            "Loss training: 29.176651\n",
            "Loss training: 26.31689\n",
            "Loss training: 27.407946\n",
            "Loss training: 26.078632\n",
            "\n",
            "***************************\n",
            "Trained on 9160 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.486153\n",
            "Loss training: 26.70116\n",
            "Loss training: 28.596037\n",
            "Loss training: 19.774551\n",
            "Loss training: 33.316273\n",
            "Loss training: 39.07031\n",
            "Loss training: 33.645252\n",
            "Loss training: 35.31783\n",
            "Loss training: 13.460448\n",
            "Loss training: 25.478298\n",
            "\n",
            "***************************\n",
            "Trained on 9170 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.856352\n",
            "Loss training: 31.016964\n",
            "Loss training: 27.642025\n",
            "Loss training: 24.510485\n",
            "Loss training: 23.549301\n",
            "Loss training: 21.212315\n",
            "Loss training: 27.779387\n",
            "Loss training: 31.030313\n",
            "Loss training: 19.49878\n",
            "Loss training: 19.14727\n",
            "\n",
            "***************************\n",
            "Trained on 9180 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.928534\n",
            "Loss training: 26.196999\n",
            "Loss training: 18.877195\n",
            "Loss training: 25.022211\n",
            "Loss training: 26.818544\n",
            "Loss training: 26.837532\n",
            "Loss training: 23.977467\n",
            "Loss training: 23.771662\n",
            "Loss training: 26.176874\n",
            "Loss training: 12.169152\n",
            "\n",
            "***************************\n",
            "Trained on 9190 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.26124\n",
            "Loss training: 11.755015\n",
            "Loss training: 24.960201\n",
            "Loss training: 23.790142\n",
            "Loss training: 33.3181\n",
            "Loss training: 26.823374\n",
            "Loss training: 29.899616\n",
            "Loss training: 26.524052\n",
            "Loss training: 26.546623\n",
            "Loss training: 24.953344\n",
            "\n",
            "***************************\n",
            "Trained on 9200 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.499681\n",
            "Loss training: 24.638046\n",
            "Loss training: 37.87309\n",
            "Loss training: 24.04441\n",
            "Loss training: 25.026453\n",
            "Loss training: 28.532007\n",
            "Loss training: 19.332619\n",
            "Loss training: 30.094595\n",
            "Loss training: 22.69517\n",
            "Loss training: 13.199222\n",
            "\n",
            "***************************\n",
            "Trained on 9210 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.242199\n",
            "Loss training: 34.975937\n",
            "Loss training: 28.347944\n",
            "Loss training: 23.516487\n",
            "Loss training: 27.961283\n",
            "Loss training: 21.394976\n",
            "Loss training: 23.6928\n",
            "Loss training: 19.409237\n",
            "Loss training: 27.52299\n",
            "Loss training: 25.013788\n",
            "\n",
            "***************************\n",
            "Trained on 9220 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.996244\n",
            "Loss training: 26.324041\n",
            "Loss training: 30.121328\n",
            "Loss training: 24.925411\n",
            "Loss training: 24.419664\n",
            "Loss training: 30.617422\n",
            "Loss training: 26.452658\n",
            "Loss training: 19.030497\n",
            "Loss training: 29.213741\n",
            "Loss training: 12.013257\n",
            "\n",
            "***************************\n",
            "Trained on 9230 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.231863\n",
            "Loss training: 27.414675\n",
            "Loss training: 29.914057\n",
            "Loss training: 29.264402\n",
            "Loss training: 24.019648\n",
            "Loss training: 23.464003\n",
            "Loss training: 12.017121\n",
            "Loss training: 27.249094\n",
            "Loss training: 19.35189\n",
            "Loss training: 35.03814\n",
            "\n",
            "***************************\n",
            "Trained on 9240 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.797108\n",
            "Loss training: 11.533623\n",
            "Loss training: 34.036972\n",
            "Loss training: 19.634007\n",
            "Loss training: 20.958567\n",
            "Loss training: 27.694183\n",
            "Loss training: 30.396502\n",
            "Loss training: 18.670895\n",
            "Loss training: 23.963955\n",
            "Loss training: 26.206564\n",
            "\n",
            "***************************\n",
            "Trained on 9250 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 11.7383795\n",
            "Loss training: 24.070776\n",
            "Loss training: 30.939764\n",
            "Loss training: 23.741482\n",
            "Loss training: 24.30708\n",
            "Loss training: 27.868423\n",
            "Loss training: 25.972723\n",
            "Loss training: 23.838083\n",
            "Loss training: 23.496143\n",
            "Loss training: 30.454676\n",
            "\n",
            "***************************\n",
            "Trained on 9260 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.516027\n",
            "Loss training: 27.613539\n",
            "Loss training: 20.498526\n",
            "Loss training: 24.180458\n",
            "Loss training: 29.981016\n",
            "Loss training: 23.550322\n",
            "Loss training: 26.855673\n",
            "Loss training: 27.384138\n",
            "Loss training: 20.035402\n",
            "Loss training: 11.877229\n",
            "\n",
            "***************************\n",
            "Trained on 9270 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.674181\n",
            "Loss training: 19.24095\n",
            "Loss training: 27.12969\n",
            "Loss training: 11.201911\n",
            "Loss training: 26.807625\n",
            "Loss training: 26.436314\n",
            "Loss training: 32.96461\n",
            "Loss training: 23.845404\n",
            "Loss training: 23.752148\n",
            "Loss training: 24.674772\n",
            "\n",
            "***************************\n",
            "Trained on 9280 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.22766\n",
            "Loss training: 24.443052\n",
            "Loss training: 26.448729\n",
            "Loss training: 24.32027\n",
            "Loss training: 20.109104\n",
            "Loss training: 19.643118\n",
            "Loss training: 26.57073\n",
            "Loss training: 31.353252\n",
            "Loss training: 26.906443\n",
            "Loss training: 18.710297\n",
            "\n",
            "***************************\n",
            "Trained on 9290 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.465487\n",
            "Loss training: 24.136858\n",
            "Loss training: 34.92974\n",
            "Loss training: 31.648333\n",
            "Loss training: 33.13095\n",
            "Loss training: 23.171577\n",
            "Loss training: 29.204264\n",
            "Loss training: 26.708553\n",
            "Loss training: 30.851128\n",
            "Loss training: 22.927279\n",
            "\n",
            "***************************\n",
            "Trained on 9300 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.627333\n",
            "Loss training: 30.981176\n",
            "Loss training: 27.712736\n",
            "Loss training: 31.804869\n",
            "Loss training: 35.25966\n",
            "Loss training: 26.501898\n",
            "Loss training: 34.756485\n",
            "Loss training: 31.796\n",
            "Loss training: 19.193707\n",
            "Loss training: 30.370516\n",
            "\n",
            "***************************\n",
            "Trained on 9310 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.147705\n",
            "Loss training: 26.933735\n",
            "Loss training: 13.326275\n",
            "Loss training: 38.919228\n",
            "Loss training: 19.199871\n",
            "Loss training: 26.205507\n",
            "Loss training: 28.0156\n",
            "Loss training: 25.428675\n",
            "Loss training: 32.57704\n",
            "Loss training: 27.754461\n",
            "\n",
            "***************************\n",
            "Trained on 9320 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 19.566818\n",
            "Loss training: 38.387474\n",
            "Loss training: 29.82125\n",
            "Loss training: 30.981697\n",
            "Loss training: 12.022286\n",
            "Loss training: 26.64917\n",
            "Loss training: 30.725533\n",
            "Loss training: 11.764906\n",
            "Loss training: 27.428886\n",
            "Loss training: 27.761673\n",
            "\n",
            "***************************\n",
            "Trained on 9330 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.174812\n",
            "Loss training: 26.236294\n",
            "Loss training: 23.587942\n",
            "Loss training: 30.533173\n",
            "Loss training: 24.657722\n",
            "Loss training: 29.945507\n",
            "Loss training: 27.187428\n",
            "Loss training: 26.235815\n",
            "Loss training: 23.635279\n",
            "Loss training: 26.907696\n",
            "\n",
            "***************************\n",
            "Trained on 9340 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 13.307383\n",
            "Loss training: 29.706564\n",
            "Loss training: 24.36757\n",
            "Loss training: 24.11568\n",
            "Loss training: 31.61828\n",
            "Loss training: 32.433758\n",
            "Loss training: 34.530045\n",
            "Loss training: 27.9334\n",
            "Loss training: 29.792362\n",
            "Loss training: 30.250874\n",
            "\n",
            "***************************\n",
            "Trained on 9350 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.285507\n",
            "Loss training: 11.574616\n",
            "Loss training: 26.080555\n",
            "Loss training: 26.685156\n",
            "Loss training: 26.497347\n",
            "Loss training: 26.866604\n",
            "Loss training: 27.064163\n",
            "Loss training: 30.06075\n",
            "Loss training: 26.236101\n",
            "Loss training: 30.195406\n",
            "\n",
            "***************************\n",
            "Trained on 9360 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.233852\n",
            "Loss training: 23.57092\n",
            "Loss training: 23.726423\n",
            "Loss training: 31.992023\n",
            "Loss training: 34.921967\n",
            "Loss training: 25.703144\n",
            "Loss training: 30.618755\n",
            "Loss training: 25.939793\n",
            "Loss training: 11.951578\n",
            "Loss training: 34.152775\n",
            "\n",
            "***************************\n",
            "Trained on 9370 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 22.888676\n",
            "Loss training: 32.625004\n",
            "Loss training: 24.370218\n",
            "Loss training: 26.748964\n",
            "Loss training: 26.07681\n",
            "Loss training: 23.968002\n",
            "Loss training: 27.534906\n",
            "Loss training: 18.718634\n",
            "Loss training: 25.933832\n",
            "Loss training: 24.136831\n",
            "\n",
            "***************************\n",
            "Trained on 9380 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.642385\n",
            "Loss training: 33.62211\n",
            "Loss training: 29.051565\n",
            "Loss training: 26.877626\n",
            "Loss training: 32.23091\n",
            "Loss training: 27.892326\n",
            "Loss training: 13.367361\n",
            "Loss training: 31.249855\n",
            "Loss training: 29.793484\n",
            "Loss training: 28.811092\n",
            "\n",
            "***************************\n",
            "Trained on 9390 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.041372\n",
            "Loss training: 30.38055\n",
            "Loss training: 24.517216\n",
            "Loss training: 29.497519\n",
            "Loss training: 27.717342\n",
            "Loss training: 27.590193\n",
            "Loss training: 11.826821\n",
            "Loss training: 32.28062\n",
            "Loss training: 33.453243\n",
            "Loss training: 26.11711\n",
            "\n",
            "***************************\n",
            "Trained on 9400 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 18.786966\n",
            "Loss training: 23.725042\n",
            "Loss training: 19.47597\n",
            "Loss training: 24.18507\n",
            "Loss training: 26.17658\n",
            "Loss training: 24.106344\n",
            "Loss training: 39.534542\n",
            "Loss training: 23.737097\n",
            "Loss training: 31.902182\n",
            "Loss training: 11.821246\n",
            "\n",
            "***************************\n",
            "Trained on 9410 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.851786\n",
            "Loss training: 31.000845\n",
            "Loss training: 25.941458\n",
            "Loss training: 24.129353\n",
            "Loss training: 24.131609\n",
            "Loss training: 34.677227\n",
            "Loss training: 23.588476\n",
            "Loss training: 27.32254\n",
            "Loss training: 30.415863\n",
            "Loss training: 24.34783\n",
            "\n",
            "***************************\n",
            "Trained on 9420 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 20.907919\n",
            "Loss training: 23.815985\n",
            "Loss training: 25.596992\n",
            "Loss training: 18.560726\n",
            "Loss training: 30.088305\n",
            "Loss training: 19.454353\n",
            "Loss training: 28.653713\n",
            "Loss training: 30.68532\n",
            "Loss training: 32.492813\n",
            "Loss training: 23.462605\n",
            "\n",
            "***************************\n",
            "Trained on 9430 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 29.856152\n",
            "Loss training: 23.90947\n",
            "Loss training: 23.165655\n",
            "Loss training: 38.53521\n",
            "Loss training: 26.28512\n",
            "Loss training: 32.820038\n",
            "Loss training: 26.610826\n",
            "Loss training: 26.249176\n",
            "Loss training: 34.777206\n",
            "Loss training: 29.928637\n",
            "\n",
            "***************************\n",
            "Trained on 9440 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.921461\n",
            "Loss training: 38.46483\n",
            "Loss training: 25.225935\n",
            "Loss training: 33.698326\n",
            "Loss training: 23.690825\n",
            "Loss training: 29.070833\n",
            "Loss training: 28.070585\n",
            "Loss training: 11.822964\n",
            "Loss training: 11.553624\n",
            "Loss training: 28.344727\n",
            "\n",
            "***************************\n",
            "Trained on 9450 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.273159\n",
            "Loss training: 34.91624\n",
            "Loss training: 31.558153\n",
            "Loss training: 25.821144\n",
            "Loss training: 29.654858\n",
            "Loss training: 34.326077\n",
            "Loss training: 19.605295\n",
            "Loss training: 13.461722\n",
            "Loss training: 32.050713\n",
            "Loss training: 29.310104\n",
            "\n",
            "***************************\n",
            "Trained on 9460 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 35.601852\n",
            "Loss training: 26.021729\n",
            "Loss training: 32.471085\n",
            "Loss training: 31.8189\n",
            "Loss training: 26.594\n",
            "Loss training: 23.722366\n",
            "Loss training: 26.544954\n",
            "Loss training: 23.818224\n",
            "Loss training: 27.88587\n",
            "Loss training: 30.681795\n",
            "\n",
            "***************************\n",
            "Trained on 9470 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.47588\n",
            "Loss training: 29.06358\n",
            "Loss training: 32.12146\n",
            "Loss training: 13.619542\n",
            "Loss training: 30.697186\n",
            "Loss training: 33.289635\n",
            "Loss training: 29.894527\n",
            "Loss training: 31.669811\n",
            "Loss training: 29.910357\n",
            "Loss training: 32.198025\n",
            "\n",
            "***************************\n",
            "Trained on 9480 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.791853\n",
            "Loss training: 38.140938\n",
            "Loss training: 32.762753\n",
            "Loss training: 23.864197\n",
            "Loss training: 18.769981\n",
            "Loss training: 23.884315\n",
            "Loss training: 30.731056\n",
            "Loss training: 26.025713\n",
            "Loss training: 24.207872\n",
            "Loss training: 38.340317\n",
            "\n",
            "***************************\n",
            "Trained on 9490 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.055904\n",
            "Loss training: 26.000357\n",
            "Loss training: 26.853474\n",
            "Loss training: 23.546803\n",
            "Loss training: 31.354038\n",
            "Loss training: 29.862318\n",
            "Loss training: 27.309837\n",
            "Loss training: 27.12498\n",
            "Loss training: 32.782146\n",
            "Loss training: 25.179337\n",
            "\n",
            "***************************\n",
            "Trained on 9500 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.25738\n",
            "Loss training: 23.681242\n",
            "Loss training: 32.38229\n",
            "Loss training: 23.42286\n",
            "Loss training: 18.498047\n",
            "Loss training: 19.383535\n",
            "Loss training: 20.559576\n",
            "Loss training: 31.311726\n",
            "Loss training: 12.998559\n",
            "Loss training: 27.568247\n",
            "\n",
            "***************************\n",
            "Trained on 9510 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.26334\n",
            "Loss training: 34.76672\n",
            "Loss training: 13.400355\n",
            "Loss training: 31.565062\n",
            "Loss training: 26.257305\n",
            "Loss training: 18.755737\n",
            "Loss training: 26.104101\n",
            "Loss training: 22.846079\n",
            "Loss training: 26.047937\n",
            "Loss training: 35.053448\n",
            "\n",
            "***************************\n",
            "Trained on 9520 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.788624\n",
            "Loss training: 26.24939\n",
            "Loss training: 20.317785\n",
            "Loss training: 13.056998\n",
            "Loss training: 26.059492\n",
            "Loss training: 31.047516\n",
            "Loss training: 26.173685\n",
            "Loss training: 25.22459\n",
            "Loss training: 23.092064\n",
            "Loss training: 27.454218\n",
            "\n",
            "***************************\n",
            "Trained on 9530 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.733906\n",
            "Loss training: 35.491993\n",
            "Loss training: 26.340227\n",
            "Loss training: 24.563412\n",
            "Loss training: 31.185324\n",
            "Loss training: 25.932861\n",
            "Loss training: 30.327618\n",
            "Loss training: 13.392219\n",
            "Loss training: 27.080063\n",
            "Loss training: 33.439087\n",
            "\n",
            "***************************\n",
            "Trained on 9540 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.270245\n",
            "Loss training: 39.54594\n",
            "Loss training: 27.43758\n",
            "Loss training: 19.347462\n",
            "Loss training: 23.902117\n",
            "Loss training: 26.44537\n",
            "Loss training: 24.669626\n",
            "Loss training: 34.030025\n",
            "Loss training: 35.847263\n",
            "Loss training: 13.82376\n",
            "\n",
            "***************************\n",
            "Trained on 9550 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.076572\n",
            "Loss training: 26.029274\n",
            "Loss training: 30.72649\n",
            "Loss training: 19.778595\n",
            "Loss training: 29.783932\n",
            "Loss training: 13.58778\n",
            "Loss training: 28.690083\n",
            "Loss training: 24.666842\n",
            "Loss training: 26.39106\n",
            "Loss training: 36.222496\n",
            "\n",
            "***************************\n",
            "Trained on 9560 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.655981\n",
            "Loss training: 29.641209\n",
            "Loss training: 29.202713\n",
            "Loss training: 31.921171\n",
            "Loss training: 26.734642\n",
            "Loss training: 28.75779\n",
            "Loss training: 27.083921\n",
            "Loss training: 25.125399\n",
            "Loss training: 24.41665\n",
            "Loss training: 28.591799\n",
            "\n",
            "***************************\n",
            "Trained on 9570 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.82022\n",
            "Loss training: 31.020943\n",
            "Loss training: 26.860176\n",
            "Loss training: 32.802635\n",
            "Loss training: 24.967373\n",
            "Loss training: 28.179565\n",
            "Loss training: 30.886728\n",
            "Loss training: 30.650597\n",
            "Loss training: 31.676497\n",
            "Loss training: 33.334698\n",
            "\n",
            "***************************\n",
            "Trained on 9580 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.26369\n",
            "Loss training: 31.44624\n",
            "Loss training: 27.113253\n",
            "Loss training: 31.54013\n",
            "Loss training: 28.381958\n",
            "Loss training: 27.010435\n",
            "Loss training: 33.53296\n",
            "Loss training: 13.276371\n",
            "Loss training: 29.78777\n",
            "Loss training: 28.17762\n",
            "\n",
            "***************************\n",
            "Trained on 9590 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.43782\n",
            "Loss training: 19.698818\n",
            "Loss training: 19.463078\n",
            "Loss training: 24.041334\n",
            "Loss training: 27.814108\n",
            "Loss training: 31.672623\n",
            "Loss training: 31.90689\n",
            "Loss training: 19.688862\n",
            "Loss training: 24.703863\n",
            "Loss training: 22.520851\n",
            "\n",
            "***************************\n",
            "Trained on 9600 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.938103\n",
            "Loss training: 30.195013\n",
            "Loss training: 23.504711\n",
            "Loss training: 24.745142\n",
            "Loss training: 23.634325\n",
            "Loss training: 26.231136\n",
            "Loss training: 27.367886\n",
            "Loss training: 32.28597\n",
            "Loss training: 12.171773\n",
            "Loss training: 34.943905\n",
            "\n",
            "***************************\n",
            "Trained on 9610 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.78837\n",
            "Loss training: 12.315548\n",
            "Loss training: 27.299238\n",
            "Loss training: 27.087635\n",
            "Loss training: 31.675741\n",
            "Loss training: 19.630186\n",
            "Loss training: 32.309956\n",
            "Loss training: 32.36521\n",
            "Loss training: 24.14452\n",
            "Loss training: 27.295652\n",
            "\n",
            "***************************\n",
            "Trained on 9620 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.31008\n",
            "Loss training: 24.366003\n",
            "Loss training: 23.442076\n",
            "Loss training: 27.542545\n",
            "Loss training: 27.010366\n",
            "Loss training: 27.812609\n",
            "Loss training: 23.828941\n",
            "Loss training: 28.423483\n",
            "Loss training: 27.553684\n",
            "Loss training: 31.06413\n",
            "\n",
            "***************************\n",
            "Trained on 9630 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 13.106457\n",
            "Loss training: 27.692648\n",
            "Loss training: 30.038939\n",
            "Loss training: 29.848038\n",
            "Loss training: 30.805279\n",
            "Loss training: 27.611568\n",
            "Loss training: 34.505634\n",
            "Loss training: 26.74442\n",
            "Loss training: 12.092993\n",
            "Loss training: 26.374435\n",
            "\n",
            "***************************\n",
            "Trained on 9640 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.034832\n",
            "Loss training: 26.288273\n",
            "Loss training: 11.467542\n",
            "Loss training: 23.647858\n",
            "Loss training: 25.653353\n",
            "Loss training: 25.832314\n",
            "Loss training: 31.051107\n",
            "Loss training: 12.865496\n",
            "Loss training: 31.52533\n",
            "Loss training: 18.701677\n",
            "\n",
            "***************************\n",
            "Trained on 9650 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.376305\n",
            "Loss training: 27.139423\n",
            "Loss training: 23.49778\n",
            "Loss training: 11.846835\n",
            "Loss training: 23.522537\n",
            "Loss training: 22.149445\n",
            "Loss training: 38.043022\n",
            "Loss training: 26.571064\n",
            "Loss training: 19.360077\n",
            "Loss training: 22.927822\n",
            "\n",
            "***************************\n",
            "Trained on 9660 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.367702\n",
            "Loss training: 23.205566\n",
            "Loss training: 26.223982\n",
            "Loss training: 27.014233\n",
            "Loss training: 25.472126\n",
            "Loss training: 27.062267\n",
            "Loss training: 28.692255\n",
            "Loss training: 26.538612\n",
            "Loss training: 25.883184\n",
            "Loss training: 31.533863\n",
            "\n",
            "***************************\n",
            "Trained on 9670 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.325987\n",
            "Loss training: 24.718317\n",
            "Loss training: 30.66524\n",
            "Loss training: 31.044544\n",
            "Loss training: 26.116337\n",
            "Loss training: 25.867393\n",
            "Loss training: 28.155909\n",
            "Loss training: 32.676792\n",
            "Loss training: 26.791624\n",
            "Loss training: 25.781284\n",
            "\n",
            "***************************\n",
            "Trained on 9680 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.480423\n",
            "Loss training: 30.410213\n",
            "Loss training: 12.7888565\n",
            "Loss training: 26.35613\n",
            "Loss training: 28.882013\n",
            "Loss training: 23.572716\n",
            "Loss training: 29.814827\n",
            "Loss training: 26.63618\n",
            "Loss training: 26.210176\n",
            "Loss training: 38.07781\n",
            "\n",
            "***************************\n",
            "Trained on 9690 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.302992\n",
            "Loss training: 26.116383\n",
            "Loss training: 25.630594\n",
            "Loss training: 23.280666\n",
            "Loss training: 23.26178\n",
            "Loss training: 37.53096\n",
            "Loss training: 23.056175\n",
            "Loss training: 29.555485\n",
            "Loss training: 12.69935\n",
            "Loss training: 28.059021\n",
            "\n",
            "***************************\n",
            "Trained on 9700 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.98114\n",
            "Loss training: 27.043634\n",
            "Loss training: 27.053316\n",
            "Loss training: 23.26627\n",
            "Loss training: 30.580547\n",
            "Loss training: 25.873371\n",
            "Loss training: 28.724598\n",
            "Loss training: 25.899473\n",
            "Loss training: 11.728397\n",
            "Loss training: 34.104942\n",
            "\n",
            "***************************\n",
            "Trained on 9710 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.254972\n",
            "Loss training: 27.348036\n",
            "Loss training: 22.714304\n",
            "Loss training: 29.628534\n",
            "Loss training: 23.428755\n",
            "Loss training: 26.728983\n",
            "Loss training: 32.58436\n",
            "Loss training: 32.322903\n",
            "Loss training: 23.427546\n",
            "Loss training: 23.303406\n",
            "\n",
            "***************************\n",
            "Trained on 9720 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.07857\n",
            "Loss training: 26.481377\n",
            "Loss training: 26.743343\n",
            "Loss training: 22.895912\n",
            "Loss training: 26.175198\n",
            "Loss training: 22.663656\n",
            "Loss training: 23.572088\n",
            "Loss training: 29.636026\n",
            "Loss training: 23.15139\n",
            "Loss training: 18.480988\n",
            "\n",
            "***************************\n",
            "Trained on 9730 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.076193\n",
            "Loss training: 23.231665\n",
            "Loss training: 26.524506\n",
            "Loss training: 29.604317\n",
            "Loss training: 24.797083\n",
            "Loss training: 23.853912\n",
            "Loss training: 18.27021\n",
            "Loss training: 24.295027\n",
            "Loss training: 24.219383\n",
            "Loss training: 31.349539\n",
            "\n",
            "***************************\n",
            "Trained on 9740 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.417725\n",
            "Loss training: 25.798351\n",
            "Loss training: 24.015585\n",
            "Loss training: 23.865059\n",
            "Loss training: 26.139267\n",
            "Loss training: 26.099962\n",
            "Loss training: 26.5616\n",
            "Loss training: 18.145987\n",
            "Loss training: 24.04861\n",
            "Loss training: 23.707317\n",
            "\n",
            "***************************\n",
            "Trained on 9750 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.152822\n",
            "Loss training: 29.090664\n",
            "Loss training: 19.250805\n",
            "Loss training: 28.056992\n",
            "Loss training: 32.48799\n",
            "Loss training: 29.085829\n",
            "Loss training: 11.670311\n",
            "Loss training: 30.44554\n",
            "Loss training: 24.515724\n",
            "Loss training: 28.394869\n",
            "\n",
            "***************************\n",
            "Trained on 9760 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.124817\n",
            "Loss training: 24.084084\n",
            "Loss training: 26.719875\n",
            "Loss training: 30.709717\n",
            "Loss training: 23.918074\n",
            "Loss training: 26.361578\n",
            "Loss training: 31.04918\n",
            "Loss training: 23.631222\n",
            "Loss training: 18.23568\n",
            "Loss training: 26.411118\n",
            "\n",
            "***************************\n",
            "Trained on 9770 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.4929\n",
            "Loss training: 11.558032\n",
            "Loss training: 25.795715\n",
            "Loss training: 29.529795\n",
            "Loss training: 25.703716\n",
            "Loss training: 26.963634\n",
            "Loss training: 31.93771\n",
            "Loss training: 26.360691\n",
            "Loss training: 26.705126\n",
            "Loss training: 34.22565\n",
            "\n",
            "***************************\n",
            "Trained on 9780 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 34.129265\n",
            "Loss training: 28.57446\n",
            "Loss training: 25.580772\n",
            "Loss training: 12.833579\n",
            "Loss training: 28.306423\n",
            "Loss training: 11.997232\n",
            "Loss training: 11.561052\n",
            "Loss training: 11.180668\n",
            "Loss training: 31.612305\n",
            "Loss training: 11.907129\n",
            "\n",
            "***************************\n",
            "Trained on 9790 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.184458\n",
            "Loss training: 27.067863\n",
            "Loss training: 26.028563\n",
            "Loss training: 23.280964\n",
            "Loss training: 23.149288\n",
            "Loss training: 32.556786\n",
            "Loss training: 27.41576\n",
            "Loss training: 26.438637\n",
            "Loss training: 30.55896\n",
            "Loss training: 25.636797\n",
            "\n",
            "***************************\n",
            "Trained on 9800 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 32.054276\n",
            "Loss training: 23.422178\n",
            "Loss training: 30.992647\n",
            "Loss training: 34.4947\n",
            "Loss training: 24.38856\n",
            "Loss training: 23.946362\n",
            "Loss training: 23.00751\n",
            "Loss training: 32.0666\n",
            "Loss training: 38.93341\n",
            "Loss training: 23.434698\n",
            "\n",
            "***************************\n",
            "Trained on 9810 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.607449\n",
            "Loss training: 27.130167\n",
            "Loss training: 30.659468\n",
            "Loss training: 16.394213\n",
            "Loss training: 31.677246\n",
            "Loss training: 13.383488\n",
            "Loss training: 31.255423\n",
            "Loss training: 23.715147\n",
            "Loss training: 19.69824\n",
            "Loss training: 23.51983\n",
            "\n",
            "***************************\n",
            "Trained on 9820 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.74204\n",
            "Loss training: 18.553164\n",
            "Loss training: 12.710316\n",
            "Loss training: 23.930077\n",
            "Loss training: 26.92879\n",
            "Loss training: 34.91995\n",
            "Loss training: 28.466253\n",
            "Loss training: 20.620823\n",
            "Loss training: 26.030184\n",
            "Loss training: 26.36324\n",
            "\n",
            "***************************\n",
            "Trained on 9830 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.154825\n",
            "Loss training: 33.473587\n",
            "Loss training: 38.984184\n",
            "Loss training: 28.804245\n",
            "Loss training: 23.689808\n",
            "Loss training: 28.053493\n",
            "Loss training: 27.3798\n",
            "Loss training: 26.989895\n",
            "Loss training: 26.153141\n",
            "Loss training: 28.672686\n",
            "\n",
            "***************************\n",
            "Trained on 9840 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 38.64487\n",
            "Loss training: 26.53065\n",
            "Loss training: 26.711292\n",
            "Loss training: 34.209045\n",
            "Loss training: 26.527222\n",
            "Loss training: 26.754448\n",
            "Loss training: 25.906643\n",
            "Loss training: 26.40381\n",
            "Loss training: 12.934609\n",
            "Loss training: 19.751595\n",
            "\n",
            "***************************\n",
            "Trained on 9850 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.272585\n",
            "Loss training: 23.61617\n",
            "Loss training: 34.43538\n",
            "Loss training: 26.015568\n",
            "Loss training: 27.37592\n",
            "Loss training: 11.925285\n",
            "Loss training: 23.359018\n",
            "Loss training: 23.449163\n",
            "Loss training: 24.087708\n",
            "Loss training: 19.608486\n",
            "\n",
            "***************************\n",
            "Trained on 9860 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 23.207266\n",
            "Loss training: 29.53091\n",
            "Loss training: 31.522867\n",
            "Loss training: 26.521845\n",
            "Loss training: 27.190956\n",
            "Loss training: 27.14193\n",
            "Loss training: 11.7143135\n",
            "Loss training: 31.844337\n",
            "Loss training: 23.70276\n",
            "Loss training: 30.388458\n",
            "\n",
            "***************************\n",
            "Trained on 9870 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 31.205875\n",
            "Loss training: 20.323416\n",
            "Loss training: 12.753268\n",
            "Loss training: 25.785616\n",
            "Loss training: 30.374704\n",
            "Loss training: 26.330267\n",
            "Loss training: 23.801497\n",
            "Loss training: 31.30446\n",
            "Loss training: 25.95451\n",
            "Loss training: 32.209934\n",
            "\n",
            "***************************\n",
            "Trained on 9880 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.200163\n",
            "Loss training: 29.85806\n",
            "Loss training: 18.313244\n",
            "Loss training: 28.170254\n",
            "Loss training: 37.558014\n",
            "Loss training: 29.345179\n",
            "Loss training: 25.979633\n",
            "Loss training: 31.003653\n",
            "Loss training: 28.116198\n",
            "Loss training: 29.200193\n",
            "\n",
            "***************************\n",
            "Trained on 9890 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.559282\n",
            "Loss training: 17.894804\n",
            "Loss training: 17.865294\n",
            "Loss training: 29.546339\n",
            "Loss training: 26.908298\n",
            "Loss training: 29.544987\n",
            "Loss training: 24.005209\n",
            "Loss training: 30.600658\n",
            "Loss training: 26.58621\n",
            "Loss training: 30.044111\n",
            "\n",
            "***************************\n",
            "Trained on 9900 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 11.890737\n",
            "Loss training: 32.870728\n",
            "Loss training: 23.754732\n",
            "Loss training: 26.812332\n",
            "Loss training: 19.126373\n",
            "Loss training: 18.64879\n",
            "Loss training: 28.754786\n",
            "Loss training: 24.058098\n",
            "Loss training: 31.344202\n",
            "Loss training: 25.96858\n",
            "\n",
            "***************************\n",
            "Trained on 9910 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 19.058365\n",
            "Loss training: 29.86521\n",
            "Loss training: 23.183584\n",
            "Loss training: 29.662874\n",
            "Loss training: 25.866863\n",
            "Loss training: 23.674576\n",
            "Loss training: 25.486109\n",
            "Loss training: 25.359543\n",
            "Loss training: 26.60823\n",
            "Loss training: 12.179798\n",
            "\n",
            "***************************\n",
            "Trained on 9920 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 33.020557\n",
            "Loss training: 26.788342\n",
            "Loss training: 30.151867\n",
            "Loss training: 23.18805\n",
            "Loss training: 26.46728\n",
            "Loss training: 25.27426\n",
            "Loss training: 26.842737\n",
            "Loss training: 18.582209\n",
            "Loss training: 28.848795\n",
            "Loss training: 37.739643\n",
            "\n",
            "***************************\n",
            "Trained on 9930 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 24.366993\n",
            "Loss training: 12.673657\n",
            "Loss training: 24.31223\n",
            "Loss training: 30.23351\n",
            "Loss training: 37.680183\n",
            "Loss training: 25.06911\n",
            "Loss training: 26.027517\n",
            "Loss training: 20.544453\n",
            "Loss training: 25.274813\n",
            "Loss training: 25.86121\n",
            "\n",
            "***************************\n",
            "Trained on 9940 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 26.55495\n",
            "Loss training: 23.859327\n",
            "Loss training: 19.326778\n",
            "Loss training: 23.240524\n",
            "Loss training: 11.629963\n",
            "Loss training: 28.291729\n",
            "Loss training: 23.338205\n",
            "Loss training: 22.985413\n",
            "Loss training: 11.466487\n",
            "Loss training: 26.13864\n",
            "\n",
            "***************************\n",
            "Trained on 9950 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 25.786053\n",
            "Loss training: 26.407383\n",
            "Loss training: 32.25008\n",
            "Loss training: 19.078278\n",
            "Loss training: 12.974852\n",
            "Loss training: 23.42137\n",
            "Loss training: 24.729227\n",
            "Loss training: 23.326742\n",
            "Loss training: 37.203255\n",
            "Loss training: 26.617464\n",
            "\n",
            "***************************\n",
            "Trained on 9960 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.82518\n",
            "Loss training: 23.624659\n",
            "Loss training: 25.693699\n",
            "Loss training: 26.793615\n",
            "Loss training: 11.636945\n",
            "Loss training: 13.198955\n",
            "Loss training: 22.987202\n",
            "Loss training: 20.121645\n",
            "Loss training: 11.572004\n",
            "Loss training: 27.223389\n",
            "\n",
            "***************************\n",
            "Trained on 9970 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 27.038202\n",
            "Loss training: 17.974754\n",
            "Loss training: 29.391878\n",
            "Loss training: 11.165575\n",
            "Loss training: 31.013353\n",
            "Loss training: 23.221094\n",
            "Loss training: 25.638071\n",
            "Loss training: 23.57715\n",
            "Loss training: 28.179045\n",
            "Loss training: 27.463173\n",
            "\n",
            "***************************\n",
            "Trained on 9980 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 30.30196\n",
            "Loss training: 23.884274\n",
            "Loss training: 22.947376\n",
            "Loss training: 26.363464\n",
            "Loss training: 17.861925\n",
            "Loss training: 27.967001\n",
            "Loss training: 37.31363\n",
            "Loss training: 30.385872\n",
            "Loss training: 23.178831\n",
            "Loss training: 23.90306\n",
            "\n",
            "***************************\n",
            "Trained on 9990 graphs\n",
            "***************************\n",
            "\n",
            "Loss training: 28.635866\n",
            "Loss training: 28.871292\n",
            "Loss training: 28.227974\n",
            "Loss training: 23.658352\n",
            "Loss training: 26.098734\n",
            "Loss training: 23.508913\n",
            "Loss training: 23.010984\n",
            "Loss training: 26.572977\n",
            "Loss training: 31.314281\n",
            "Loss training: 27.215902\n",
            "\n",
            "***************************\n",
            "Trained on 10000 graphs\n",
            "***************************\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9c2e1521-528b-4da7-ab72-96ab7db62753\", \"params_epochs_10000.pickle\", 22204504)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_96a8f380-eb58-48fe-b019-dd57e3ac197a\", \"opt_state_epochs_10000.pickle\", 44409100)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluated 10 / 35 subgraphs...\n",
            "Evaluated 20 / 35 subgraphs...\n",
            "Evaluated 30 / 35 subgraphs...\n",
            "\n",
            "Final ROC on the train set 0.8873957875287084\n",
            "Final ROC on the validation set 0.604810879077169\n",
            "Final ROC on the test set 0.57961973018596\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  Previous runs (padding to power of 2)\\n  (1) Configuration\\n        learning_rate = 0.001\\n        num_partitions = 50\\n        hidden_dimension = 128\\n        num_message_passing_steps = 5\\n        num_training_steps = 1000\\n    ROC on the train set 0.7348797273386144\\n    ROC on the validation set 0.6025038939324504\\n    ROC on the test set 0.5896861508337246\\n\\n  (2) Configuration\\n        learning_rate = 0.001\\n        num_partitions = 50\\n        hidden_dimension = 128\\n        num_message_passing_steps = 5\\n        num_training_steps = 3000\\n    ROC on the train set 0.8050085464161815\\n    ROC on the validation set 0.6327603823722211\\n    ROC on the test set 0.5078022533003436\\n\\n  (3) Configuration\\n        learning_rate = 0.1 (Question: I think this might be too high -- based on the results in (5) with lower number of epochs)\\n        num_partitions = 100\\n        hidden_dimension = 128\\n        num_message_passing_steps = 5\\n        num_training_steps = 1000\\n    ROC on the train set 0.5\\n    ROC on the validation set 0.5 \\n    ROC on the test set 0.5\\n\\n  (4) Configuration\\n        learning_rate = 0.01\\n        num_partitions = 100\\n        hidden_dimension = 128\\n        num_message_passing_steps = 5\\n        num_training_steps = 100\\n    ROC on the train set 0.6501172261188106\\n    ROC on the validation set 0.5281974299591566\\n    ROC on the test set 0.47652056321124514\\n\\n  (5) Configuration\\n        learning_rate = 0.01\\n        num_partitions = 100\\n        hidden_dimension = 128\\n        num_message_passing_steps = 5\\n        num_training_steps = 500\\n    ROC on the train set 0.6939371049645034\\n    ROC on the validation set 0.559224577731843\\n    ROC on the test set 0.5488968392833208\\n\\n  (6) Configuration\\n        opt: LAMB\\n        learning_rate: 1e-4\\n        num-partitions = 100\\n        hidden_dimension = 128\\n        num_message_passing_steps = 5\\n        num_training_steps = 500\\n  ROC on the train set 0.6299712777663571\\n  ROC on the validation set 0.5054189612195771\\n  ROC on the test set 0.5083185060310427\\n\\n  ********************************************\\n\\n  Previous runs (padding to multiple of 8)\\n  (1) Configuration\\n        opt: LAMB\\n        learning_rate: 1e-4\\n        num-partitions = 35\\n        hidden_dimension = 128\\n        num_message_passing_steps = 5\\n        num_training_steps = 500\\n  ROC on the train set 0.6438794374674618\\n  ROC on the validation set 0.5162833891590899\\n  ROC on the test set 0.5175085147535061\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###################################### FUNCTIONS FOR TESTING ######################################\n",
        "run_overfit_on_single_partition = True\n",
        "run_overfit_on_demo_graph = False"
      ],
      "metadata": {
        "id": "yK0Qy8CdU6tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def overfit_on_single_graph(\n",
        "    num_training_steps, \n",
        "    learning_rate,\n",
        "    graph,\n",
        "    labels,\n",
        "    mask\n",
        "    ):\n",
        "  replicated_params = jax.pmap(network_definition.init, axis_name = 'i')(bcast_local_devices(jax.random.PRNGKey(42)), graph)\n",
        "\n",
        "  opt_init, opt_update = optax.adam(learning_rate = learning_rate)  \n",
        "  replicated_opt_state = jax.pmap(opt_init, axis_name = 'i')(replicated_params)\n",
        "\n",
        "  @functools.partial(jax.pmap, axis_name='i')\n",
        "  def update(params, opt_state, graph, targets, mask):\n",
        "    # Compute the gradients on the given minibatch (individually on each device).\n",
        "    loss, grads = jax.value_and_grad(compute_loss)(params, graph, targets, mask)\n",
        "\n",
        "    # Combine the gradient across all devices (by taking their mean).\n",
        "    grads = jax.lax.pmean(grads, axis_name='i')\n",
        "\n",
        "    # Also combine the loss. Unnecessary for the update, but useful for logging.\n",
        "    loss = jax.lax.pmean(loss, axis_name='i')\n",
        "\n",
        "    updates, opt_state = opt_update(grads, opt_state)\n",
        "\n",
        "    return optax.apply_updates(params, updates), opt_state, loss\n",
        "\n",
        "  # Train on a single partition\n",
        "  for idx in range(num_training_steps):\n",
        "    replicated_params, replicated_opt_state, loss = update(\n",
        "      replicated_params, \n",
        "      replicated_opt_state, \n",
        "      graph, \n",
        "      labels,\n",
        "      mask\n",
        "      ) \n",
        "\n",
        "    print('Loss training:', reshape_broadcasted_data(loss))\n",
        "\n",
        "    if (idx + 1) % 10 == 0:\n",
        "      print()\n",
        "      print(f'***************************')\n",
        "      print(f'Trained on {idx + 1} graphs')\n",
        "      print(f'***************************')\n",
        "      print()"
      ],
      "metadata": {
        "id": "DNchIrnxPk1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_demo_training_graph(num_nodes, num_edges):\n",
        "  rand_dgl_graph = dgl.rand_graph(num_nodes = num_nodes, num_edges = num_edges)\n",
        "\n",
        "  node_features = jnp.array([[randint(0, 7)] for i in range(num_nodes)])\n",
        "  edge_features = jnp.array([[0.1 * randint(0, 10) for _ in range(8)] for i in range(num_edges)])\n",
        "\n",
        "  senders = jnp.array(rand_dgl_graph.edges()[0])\n",
        "  receivers = jnp.array(rand_dgl_graph.edges()[1])\n",
        "\n",
        "  in_tuple = jraph.GraphsTuple(\n",
        "            nodes = node_features.astype(np.float32),\n",
        "            edges = edge_features.astype(np.float32),  \n",
        "            senders = senders.astype(np.int32), \n",
        "            receivers = receivers.astype(np.int32),\n",
        "            n_node = jnp.array([num_nodes]), \n",
        "            n_edge = jnp.array([num_edges]),\n",
        "            globals = None  # No global features\n",
        "          )\n",
        "  \n",
        "  labels = jnp.array([[randint(0, 1) for j in range(112)] for i in range(num_nodes)])\n",
        "  \n",
        "  in_tuple = in_tuple._replace(\n",
        "      nodes = {\n",
        "          'inputs': in_tuple.nodes, \n",
        "          'targets': labels, \n",
        "          'train_mask': jnp.ones((num_nodes, 1)), # No nodes are masked \n",
        "          }\n",
        "  )\n",
        "\n",
        "  # in_tuple = pad_graph_to_nearest_multiple_of_8(in_tuple)\n",
        "  \n",
        "  return sharded_graphnet.graphs_tuple_to_broadcasted_sharded_graphs_tuple(\n",
        "      in_tuple,\n",
        "      num_shards = num_devices\n",
        "      )\n",
        "\n",
        "def overfit_on_demo_graph(num_training_steps, learning_rate):\n",
        "  demo_graph = get_demo_training_graph(num_nodes = 16, num_edges = 8)\n",
        "  demo_labels = demo_graph.nodes['targets']\n",
        "  demo_mask = demo_graph.nodes['train_mask']\n",
        "  demo_graph = demo_graph._replace(nodes = demo_graph.nodes['inputs'])\n",
        "\n",
        "  overfit_on_single_graph(\n",
        "      num_training_steps = num_training_steps,\n",
        "      learning_rate = learning_rate,\n",
        "      graph = demo_graph,\n",
        "      labels = demo_labels,\n",
        "      mask = demo_mask\n",
        "  )"
      ],
      "metadata": {
        "id": "zQdvmYQETQNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Overfit on an existing partition\n",
        "if run_overfit_on_single_partition:\n",
        "  print('*** Trying to overfit on a single partition ***')\n",
        "  overfit_on_single_graph(\n",
        "      num_training_steps = 5000,\n",
        "      learning_rate = 1e-4,\n",
        "      graph = processed_graphs['partition_0']['graph'],\n",
        "      labels = processed_graphs['partition_0']['labels'],\n",
        "      mask = processed_graphs['partition_0']['train_mask']\n",
        "      )"
      ],
      "metadata": {
        "id": "xfmlIPlpUfNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if run_overfit_on_demo_graph:\n",
        "  print('*** Trying to overfit on a random demo graph ***')\n",
        "  overfit_on_demo_graph(\n",
        "      num_training_steps = 1000,\n",
        "      learning_rate = 0.001\n",
        "  )"
      ],
      "metadata": {
        "id": "ILwic3w-d5mI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}