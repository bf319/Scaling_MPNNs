{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "version-1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMdgTaKVmpadTufm+6iEU2t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bf319/Scaling_MPNNs/blob/main/version_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q git+https://github.com/deepmind/dm-haiku\n",
        "%pip install -q jraph\n",
        "%pip install -q git+https://github.com/deepmind/jaxline\n",
        "%pip install -q ogb\n",
        "# %pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+$cu111.html\n",
        "# %pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+$cu111.html\n",
        "%pip install -q dgl\n",
        "%pip install -q optax\n",
        "\n",
        "# Remove GitHub repo if it has already been cloned in order to reflect the most recent changes\n",
        "%rm -r Scaling_MPNNs\n",
        "!git clone https://github.com/bf319/Scaling_MPNNs.git\n",
        "\n",
        "%load Scaling_MPNNs/models.py\n",
        "%cd Scaling_MPNNs/\n",
        "%mkdir -p '/content/Scaling_MPNNs/something'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4rKEoR7AfGU",
        "outputId": "828a9ad0-fdf8-4dd4-a6a6-95002dbbf0df"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Cloning into 'Scaling_MPNNs'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 15 (delta 3), reused 8 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (15/15), done.\n",
            "/content/Scaling_MPNNs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Initialize the TPU\n",
        "# import jax.tools.colab_tpu\n",
        "# jax.tools.colab_tpu.setup_tpu()"
      ],
      "metadata": {
        "id": "mjOleIc9A5h6",
        "outputId": "fc97a293-186f-442a-c713-2c21e02e0df9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-264a27fda176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize the TPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab_tpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab_tpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_tpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/tools/colab_tpu.py\u001b[0m in \u001b[0;36msetup_tpu\u001b[0;34m(tpu_driver_version)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTPU_DRIVER_MODE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mcolab_tpu_addr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'COLAB_TPU_ADDR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'http://{colab_tpu_addr}:8475/requestversion/{tpu_driver_version}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'COLAB_TPU_ADDR'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ogb.nodeproppred import Evaluator\n",
        "from ogb.nodeproppred import NodePropPredDataset\n",
        "\n",
        "## Problem 1\n",
        "## Loading \"ogbn-arxiv\" works (small dataset), but \"ogbn-proteins\" crashes because all RAM is used.\n",
        "## Most likely the dataset is too large (medium dataset), which is why I would need to use Mantis\n",
        "## for splitting the graphs.\n",
        "## TODO: Figure out a way to use ogbn-proteins\n",
        "dataset = NodePropPredDataset(name = \"ogbn-arxiv\")\n",
        "print(dataset.meta_info)"
      ],
      "metadata": {
        "id": "eESdtWapqZ5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_idx = dataset.get_idx_split()\n",
        "train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
        "# graph, label = dataset[0] # graph: library-agnostic graph object"
      ],
      "metadata": {
        "id": "KyKy073Mugyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# From https://github.com/deepmind/jraph/blob/master/jraph/ogb_examples/train.py\n",
        "import functools\n",
        "import logging\n",
        "import pathlib\n",
        "import pickle\n",
        "from absl import app\n",
        "from absl import flags\n",
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jraph\n",
        "from jraph.ogb_examples import data_utils\n",
        "import optax"
      ],
      "metadata": {
        "id": "31rdLauLWljM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/deepmind/jraph.git"
      ],
      "metadata": {
        "id": "zlqGAxj8XFCq",
        "outputId": "55aeaa12-b529-47dc-a2ad-5dcc316dc598",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'jraph'...\n",
            "remote: Enumerating objects: 385, done.\u001b[K\n",
            "remote: Counting objects: 100% (385/385), done.\u001b[K\n",
            "remote: Compressing objects: 100% (193/193), done.\u001b[K\n",
            "remote: Total 385 (delta 219), reused 347 (delta 181), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (385/385), 204.82 KiB | 5.69 MiB/s, done.\n",
            "Resolving deltas: 100% (219/219), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@jraph.concatenated_args\n",
        "def edge_update_fn(feats: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Edge update function for graph net.\"\"\"\n",
        "  net = hk.Sequential(\n",
        "      [hk.Linear(128), jax.nn.relu,\n",
        "       hk.Linear(128)])\n",
        "  return net(feats)\n",
        "\n",
        "\n",
        "@jraph.concatenated_args\n",
        "def node_update_fn(feats: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Node update function for graph net.\"\"\"\n",
        "  net = hk.Sequential(\n",
        "      [hk.Linear(128), jax.nn.relu,\n",
        "       hk.Linear(128)])\n",
        "  return net(feats)\n",
        "\n",
        "\n",
        "@jraph.concatenated_args\n",
        "def update_global_fn(feats: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Global update function for graph net.\"\"\"\n",
        "  # Molhiv is a binary classification task, so output pos neg logits.\n",
        "  net = hk.Sequential(\n",
        "      [hk.Linear(128), jax.nn.relu,\n",
        "       hk.Linear(2)])\n",
        "  return net(feats)\n",
        "\n",
        "\n",
        "def net_fn(graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
        "  \"\"\"Graph net function.\"\"\"\n",
        "  # Add a global paramater for graph classification.\n",
        "  graph = graph._replace(globals=jnp.zeros([graph.n_node.shape[0], 1]))\n",
        "  embedder = jraph.GraphMapFeatures(\n",
        "      hk.Linear(128), hk.Linear(128), hk.Linear(128))\n",
        "  net = jraph.GraphNetwork(\n",
        "      update_node_fn=node_update_fn,\n",
        "      update_edge_fn=edge_update_fn,\n",
        "      update_global_fn=update_global_fn)\n",
        "  return net(embedder(graph))\n",
        "\n",
        "\n",
        "def _nearest_bigger_power_of_two(x: int) -> int:\n",
        "  \"\"\"Computes the nearest power of two greater than x for padding.\"\"\"\n",
        "  y = 2\n",
        "  while y < x:\n",
        "    y *= 2\n",
        "  return y\n",
        "\n",
        "\n",
        "def pad_graph_to_nearest_power_of_two(\n",
        "    graphs_tuple: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
        "  \"\"\"Pads a batched `GraphsTuple` to the nearest power of two.\n",
        "  For example, if a `GraphsTuple` has 7 nodes, 5 edges and 3 graphs, this method\n",
        "  would pad the `GraphsTuple` nodes and edges:\n",
        "    7 nodes --> 8 nodes (2^3)\n",
        "    5 edges --> 8 edges (2^3)\n",
        "  And since padding is accomplished using `jraph.pad_with_graphs`, an extra\n",
        "  graph and node is added:\n",
        "    8 nodes --> 9 nodes\n",
        "    3 graphs --> 4 graphs\n",
        "  Args:\n",
        "    graphs_tuple: a batched `GraphsTuple` (can be batch size 1).\n",
        "  Returns:\n",
        "    A graphs_tuple batched to the nearest power of two.\n",
        "  \"\"\"\n",
        "  # Add 1 since we need at least one padding node for pad_with_graphs.\n",
        "  pad_nodes_to = _nearest_bigger_power_of_two(jnp.sum(graphs_tuple.n_node)) + 1\n",
        "  pad_edges_to = _nearest_bigger_power_of_two(jnp.sum(graphs_tuple.n_edge))\n",
        "  # Add 1 since we need at least one padding graph for pad_with_graphs.\n",
        "  # We do not pad to nearest power of two because the batch size is fixed.\n",
        "  pad_graphs_to = graphs_tuple.n_node.shape[0] + 1\n",
        "  return jraph.pad_with_graphs(graphs_tuple, pad_nodes_to, pad_edges_to,\n",
        "                               pad_graphs_to)\n",
        "\n",
        "\n",
        "def compute_loss(params, graph, label, net):\n",
        "  \"\"\"Computes loss.\"\"\"\n",
        "  pred_graph = net.apply(params, graph)\n",
        "  preds = jax.nn.log_softmax(pred_graph.globals)\n",
        "  targets = jax.nn.one_hot(label, 2)\n",
        "\n",
        "  # Since we have an extra 'dummy' graph in our batch due to padding, we want\n",
        "  # to mask out any loss associated with the dummy graph.\n",
        "  # Since we padded with `pad_with_graphs` we can recover the mask by using\n",
        "  # get_graph_padding_mask.\n",
        "  mask = jraph.get_graph_padding_mask(pred_graph)\n",
        "\n",
        "  # Cross entropy loss.\n",
        "  loss = -jnp.mean(preds * targets * mask[:, None])\n",
        "\n",
        "  # Accuracy taking into account the mask.\n",
        "  accuracy = jnp.sum(\n",
        "      (jnp.argmax(pred_graph.globals, axis=1) == label) * mask)/jnp.sum(mask)\n",
        "  return loss, accuracy\n",
        "\n",
        "\n",
        "def train(data_path, master_csv_path, split_path, batch_size,\n",
        "          num_training_steps, save_dir):\n",
        "  \"\"\"OGB Training Script.\"\"\"\n",
        "  # Initialize the dataset reader.\n",
        "  reader = data_utils.DataReader(\n",
        "      data_path=data_path,\n",
        "      master_csv_path=master_csv_path,\n",
        "      split_path=split_path,\n",
        "      batch_size=batch_size)\n",
        "  # Repeat the dataset forever for training.\n",
        "  reader.repeat()\n",
        "\n",
        "  # Transform impure `net_fn` to pure functions with hk.transform.\n",
        "  net = hk.without_apply_rng(hk.transform(net_fn))\n",
        "  # Get a candidate graph and label to initialize the network.\n",
        "  graph, _ = reader.get_graph_by_idx(0)\n",
        "\n",
        "  # Initialize the network.\n",
        "  logging.info('Initializing network.')\n",
        "  params = net.init(jax.random.PRNGKey(42), graph)\n",
        "  # Initialize the optimizer.\n",
        "  opt_init, opt_update = optax.adam(1e-4)\n",
        "  opt_state = opt_init(params)\n",
        "\n",
        "  compute_loss_fn = functools.partial(compute_loss, net=net)\n",
        "  # We jit the computation of our loss, since this is the main computation.\n",
        "  # Using jax.jit means that we will use a single accelerator. If you want\n",
        "  # to use more than 1 accelerator, use jax.pmap. More information can be\n",
        "  # found in the jax documentation.\n",
        "  compute_loss_fn = jax.jit(jax.value_and_grad(\n",
        "      compute_loss_fn, has_aux=True))\n",
        "\n",
        "  for idx in range(num_training_steps):\n",
        "    graph, label = next(reader)\n",
        "    # Jax will re-jit your graphnet every time a new graph shape is encountered.\n",
        "    # In the limit, this means a new compilation every training step, which\n",
        "    # will result in *extremely* slow training. To prevent this, pad each\n",
        "    # batch of graphs to the nearest power of two. Since jax maintains a cache\n",
        "    # of compiled programs, the compilation cost is amortized.\n",
        "    graph = pad_graph_to_nearest_power_of_two(graph)\n",
        "\n",
        "    # Since padding is implemented with pad_with_graphs, an extra graph has\n",
        "    # been added to the batch, which means there should be an extra label.\n",
        "    label = jnp.concatenate([label, jnp.array([0])])\n",
        "\n",
        "    (loss, acc), grad = compute_loss_fn(params, graph, label)\n",
        "    updates, opt_state = opt_update(grad, opt_state, params)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    if idx % 100 == 0:\n",
        "      logging.info('step: %s, loss: %s, acc: %s', idx, loss, acc)\n",
        "  if save_dir is not None:\n",
        "    with pathlib.Path(save_dir, 'molhiv.pkl').open('wb') as fp:\n",
        "      logging.info('Saving model to %s', save_dir)\n",
        "      pickle.dump(params, fp)\n",
        "  logging.info('Training finished')\n",
        "\n",
        "\n",
        "def evaluate(data_path, master_csv_path, split_path, save_dir):\n",
        "  \"\"\"Evaluation Script.\"\"\"\n",
        "  logging.info('Evaluating OGB molviv')\n",
        "  logging.info('Dataset split: %s', split_path)\n",
        "  # Initialize the dataset reader.\n",
        "  reader = data_utils.DataReader(\n",
        "      data_path=data_path,\n",
        "      master_csv_path=master_csv_path,\n",
        "      split_path=split_path,\n",
        "      batch_size=1)\n",
        "  # Transform impure `net_fn` to pure functions with hk.transform.\n",
        "  net = hk.without_apply_rng(hk.transform(net_fn))\n",
        "  # Get a candidate graph and label to initialize the network.\n",
        "  graph, _ = reader.get_graph_by_idx(0)\n",
        "  with pathlib.Path(save_dir, 'molhiv.pkl').open('rb') as fp:\n",
        "    params = pickle.load(fp)\n",
        "  accumulated_loss = 0\n",
        "  accumulated_accuracy = 0\n",
        "  idx = 0\n",
        "\n",
        "  # We jit the computation of our loss, since this is the main computation.\n",
        "  # Using jax.jit means that we will use a single accelerator. If you want\n",
        "  # to use more than 1 accelerator, use jax.pmap. More information can be\n",
        "  # found in the jax documentation.\n",
        "  compute_loss_fn = jax.jit(functools.partial(compute_loss, net=net))\n",
        "  for graph, label in reader:\n",
        "\n",
        "    # Jax will re-jit your graphnet every time a new graph shape is encountered.\n",
        "    # In the limit, this means a new compilation every training step, which\n",
        "    # will result in *extremely* slow training. To prevent this, pad each\n",
        "    # batch of graphs to the nearest power of two. Since jax maintains a cache\n",
        "    # of compiled programs, the compilation cost is amortized.\n",
        "    graph = pad_graph_to_nearest_power_of_two(graph)\n",
        "\n",
        "    # Since padding is implemented with pad_with_graphs, an extra graph has\n",
        "    # been added to the batch, which means there should be an extra label.\n",
        "    label = jnp.concatenate([label, jnp.array([0])])\n",
        "    loss, acc = compute_loss_fn(params, graph, label)\n",
        "    accumulated_accuracy += acc\n",
        "    accumulated_loss += loss\n",
        "    idx += 1\n",
        "    if idx % 100 == 0:\n",
        "      logging.info('Evaluated %s graphs', idx)\n",
        "  logging.info('Completed evaluation.')\n",
        "  loss = accumulated_loss / idx\n",
        "  accuracy = accumulated_accuracy / idx\n",
        "  logging.info('Eval loss: %s, accuracy %s', loss, accuracy)\n",
        "  return loss, accuracy\n",
        "\n",
        "def main():\n",
        "  train(\n",
        "      '/content/Scaling_MPNNs/jraph/jraph/ogb_examples/test_data', \n",
        "      '/content/Scaling_MPNNs/jraph/jraph/ogb_examples/test_data/master.csv', \n",
        "      '/content/Scaling_MPNNs/jraph/jraph/ogb_examples/test_data/train.csv.gz', \n",
        "      1, \n",
        "      101, \n",
        "      '/content/Scaling_MPNNs/something'\n",
        "      )\n",
        "\n",
        "  evaluate(\n",
        "      '/content/Scaling_MPNNs/jraph/jraph/ogb_examples/test_data', \n",
        "      '/content/Scaling_MPNNs/jraph/jraph/ogb_examples/test_data/master.csv', \n",
        "      '/content/Scaling_MPNNs/jraph/jraph/ogb_examples/test_data/train.csv.gz',\n",
        "      '/content/Scaling_MPNNs/something'\n",
        "      )\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "HnepMptKutka"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}